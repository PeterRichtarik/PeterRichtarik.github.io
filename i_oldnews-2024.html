<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="style.css">
    <title>Peter Richtarik</title>


    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-168147887-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-168147887-2');
    </script>





  </head>
  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <script src="table_header.js"></script> </div>
      </div>
      <ul class="menu">
        <li><a href="index.html">News</a></li>
        <li><a class="active" href="i_oldnews-2024.html">Old News</a></li>
        <li><a href="i_papers.html">Papers</a></li>
        <li><a href="i_talks.html">Talks</a></li>
        <li><a href="i_videotalks.html">Video Talks</a></li>
        <li><a href="i_events.html">Events</a></li>
      <!--  <li><a href="i_seminar.html">Seminar</a></li> !-->
        <li><a href="i_software.html">Code</a></li>
        <li><a href="i_team.html">Team</a></li>
        <li><a href="i_apply.html">Apply</a></li>
        <li><a href="i_bio.html">Bio</a></li>
        <li><a href="i_teaching.html">Teaching</a></li>
        <li><a href="i_consulting.html">Consulting</a></li>
      </ul>
      <div id="wrapper" class="main">
        <div id="content">
        


<h3>December 27, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2412.19916">
"On the Convergence of DP-SGD with Adaptive Clipping"</a> -
joint work with Egor Shulgin.
<br>
<br>
  
Abstract: <i>Stochastic Gradient Descent (SGD) with gradient clipping is a powerful technique for enabling differentially private optimization. Although prior works extensively investigated clipping with a constant threshold, private training remains highly sensitive to threshold selection, which can be expensive or even infeasible to tune. This sensitivity motivates the development of adaptive approaches, such as quantile clipping, which have demonstrated empirical success but lack a solid theoretical understanding. This paper provides the first comprehensive convergence analysis of SGD with quantile clipping (QC-SGD). We demonstrate that QC-SGD suffers from a bias problem similar to constant-threshold clipped SGD but show how this can be mitigated through a carefully designed quantile and step size schedule. Our analysis reveals crucial relationships between quantile selection, step size, and convergence behavior, providing practical guidelines for parameter selection. We extend these results to differentially private optimization, establishing the first theoretical guarantees for DP-QC-SGD. Our findings provide theoretical foundations for widely used adaptive clipping heuristic and highlight open avenues for future research.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 

        
        

<h3>December 22, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2412.17082">
"MARINA-P: Superior Performance in Non-smooth Federated Optimization with Adaptive Stepsizes"</a> -
joint work with Igor Sokolov.
<br>
<br>
  
Abstract: <i>Non-smooth communication-efficient federated optimization is crucial for many machine learning applications, yet remains largely unexplored theoretically. Recent advancements have primarily focused on smooth convex and non-convex regimes, leaving a significant gap in understanding the non-smooth convex setting. Additionally, existing literature often overlooks efficient server-to-worker communication (downlink), focusing primarily on worker-to-server communication (uplink). We consider a setup where uplink costs are negligible and focus on optimizing downlink communication by improving state-of-the-art schemes like EF21-P (arXiv:2209.15218) and MARINA-P (arXiv:2402.06412) in the non-smooth convex setting. We extend the non-smooth convex theory of EF21-P [Anonymous, 2024], originally developed for single-node scenarios, to the distributed setting, and extend MARINA-P to the non-smooth convex setting. For both algorithms, we prove an optimal O(1/T^{1/2}) convergence rate and establish communication complexity bounds matching classical subgradient methods. We provide theoretical guarantees under constant, decreasing, and adaptive (Polyak-type) stepsizes. Our experiments demonstrate that MARINA-P with correlated compressors outperforms other methods in both smooth non-convex and non-smooth convex settings. This work presents the first theoretical results for distributed non-smooth optimization with server-to-worker compression, along with comprehensive analysis for various stepsize schemes.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>December 22, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2412.17054">
"Differentially Private Random Block Coordinate Descent"</a> -
joint work with Artavazd Maranjyan and Abdurakhmon Sadiev.
<br>
<br>
  
Abstract: <i>Coordinate Descent (CD) methods have gained significant attention in machine learning due to their effectiveness in solving high-dimensional problems and their ability to decompose complex optimization tasks. However, classical CD methods were neither designed nor analyzed with data privacy in mind, a critical concern when handling sensitive information. This has led to the development of differentially private CD methods, such as DP-CD (Differentially Private Coordinate Descent) proposed by Mangold et al (ICML 2022), yet a disparity remains between non-private CD and DP-CD methods. In our work, we propose a differentially private random block coordinate descent method that selects multiple coordinates with varying probabilities in each iteration using sketch matrices. Our algorithm generalizes both DP-CD and the classical DP-SGD (Differentially Private Stochastic Gradient Descent), while preserving the same utility guarantees. Furthermore, we demonstrate that better utility can be achieved through importance sampling, as our method takes advantage of the heterogeneity in coordinate-wise smoothness constants, leading to improved convergence rates.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 




<h3>December 19, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2412.13619">
"Speeding up Stochastic Proximal Optimization in the High Hessian Dissimilarity Setting"</a> -
joint work with Elnur Gasanov.
<br>
<br>
  
Abstract: <i>Stochastic proximal point methods have recently garnered renewed attention within the optimization community, primarily due to their desirable theoretical properties. Notably, these methods exhibit a convergence rate that is independent of the Lipschitz smoothness constants of the loss function, a feature often missing in the loss functions of modern ML applications. In this paper, we revisit the analysis of the Loopless Stochastic Variance Reduced Proximal Point Method (L-SVRP). Building on existing work, we establish a theoretical improvement in the convergence rate in scenarios characterized by high Hessian dissimilarity among the functions. Our concise analysis, which does not require smoothness assumptions, demonstrates a significant improvement in communication complexity compared to standard stochastic gradient descent.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>December 13, 2024</h3>

<h1>Attending KAUST Commencement</h1>

I am attending the Commencement at KAUST. My MS student <a href="https://ivan-ilin.netlify.app">Ivan Ilin</a> got his MS degree! <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


        
<h3>December 3, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2412.02781">
"Methods with Local Steps and Random Reshuffling for Generally Smooth Non-Convex Federated Optimization"</a> -
joint work with Yury Demidovich, Petr Ostroukhov, Grigory Malinovsky, Samuel Horváth, Martin Takáč,  and Eduard Gorbunov.
<br>
<br>
  
Abstract: <i>Non-convex Machine Learning problems typically do not adhere to the standard smoothness assumption. Based on empirical findings, Zhang et al. (2020b) proposed a more realistic generalized $(L_0, L_1)$-smoothness assumption, though it remains largely unexplored. Many existing algorithms designed for standard smooth problems need to be revised. However, in the context of Federated Learning, only a few works address this problem but rely on additional limiting assumptions. In this paper, we address this gap in the literature: we propose and analyze new methods with local steps, partial participation of clients, and Random Reshuffling without extra restrictive assumptions beyond generalized smoothness. The proposed methods are based on the proper interplay between clients' and server's stepsizes and gradient clipping. Furthermore, we perform the first analysis of these methods under the Polyak-Łojasiewicz condition. Our theory is consistent with the known results for standard smooth problems, and our experimental results support the theoretical insights.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


        
<h3>November 26, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2411.17525">
"Pushing the Limits of Large Language Model Quantization via the Linearity Theorem"</a> -
joint work with Vladimir Malinovskii, Andrei Panferov, Ivan Ilin, Han Guo, and Dan Alistarh.
<br>
<br>
  
Abstract: <i>Quantizing large language models has become a standard way to reduce their memory and computational costs. Typically, existing methods focus on breaking down the problem into individual layer-wise sub-problems, and minimizing per-layer error, measured via various metrics. Yet, this approach currently lacks theoretical justification and the metrics employed may be sub-optimal. In this paper, we present a "linearity theorem" establishing a direct relationship between the layer-wise L2 reconstruction error and the model perplexity increase due to quantization. This insight enables two novel applications: (1) a simple data-free LLM quantization method using Hadamard rotations and MSE-optimal grids, dubbed HIGGS, which outperforms all prior data-free approaches such as the extremely popular NF4 quantized format, and (2) an optimal solution to the problem of finding non-uniform per-layer quantization levels which match a given compression constraint in the medium bitwidth regime, obtained by reduction to dynamic programming. On the practical side, we demonstrate improved accuracy-compression trade-offs on Llama-3.1 and 3.2-family models, as well as on Qwen family models. Further, we show that our method can be efficiently supported in terms of GPU kernels at various batch sizes, advancing both data-free and non-uniform quantization for LLMs.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>November 15, 2024</h3>

<h1>Visiting China</h1>

On my way to China -- will be there for two weeks, until November 29, 2024. I am giving a series of five lectures on Optimization for Machine Learning at the Beijing Institute for Mathematical Sciences and Applications (BIMSA), and three seminar talks, at Tsinghua Univeristy, Shanghai Institute for Mathematics and Interdisciplinary Sciences (SIMIS), and Peking University. Once I arrived, I was invited to visit various other places, including Microsoft Research Asia, Nanjing University, Fudan University, Tongji University and Zhejiang University. However, my program was already too full for me to be able to accommodate these invites. Hopefully, next time!
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>October 22, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2410.16871">
"Error Feedback under $(L_0,L_1)$-Smoothness: Normalization and Momentum "</a> -
joint work with Sarit Khirirat, Abdurakhmon Sadiev, Artem Riabinin, and Eduard Gorbunov.
<br>
<br>
  
Abstract: <i>We provide the first proof of convergence for normalized error feedback algorithms across a wide range of machine learning problems. Despite their popularity and efficiency in training deep neural networks, traditional analyses of error feedback algorithms rely on the smoothness assumption that does not capture the properties of objective functions in these problems. Rather, these problems have recently been shown to satisfy generalized smoothness assumptions, and the theoretical understanding of error feedback algorithms under these assumptions remains largely unexplored. Moreover, to the best of our knowledge, all existing analyses under generalized smoothness either i) focus on single-node settings or ii) make unrealistically strong assumptions for distributed settings, such as requiring data heterogeneity, and almost surely bounded stochastic gradient noise variance. In this paper, we propose distributed error feedback algorithms that utilize normalization to achieve the O(1/√K) convergence rate for nonconvex problems under generalized smoothness. Our analyses apply for distributed settings without data heterogeneity conditions, and enable stepsize tuning that is independent of problem parameters. Additionally, we provide strong convergence guarantees of normalized error feedback algorithms for stochastic settings. Finally, we show that due to their larger allowable stepsizes, our new normalized error feedback algorithms outperform their non-normalized counterparts on various tasks, including the minimization of polynomial functions, logistic regression, and ResNet-20 training.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>October 20, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2410.15368">
"Tighter Performance Theory of FedExProx"</a> -
joint work with Wojciech Anyszka, Kaja Gruntkowska, and Alexander Tyurin.
<br>
<br>
  
Abstract: <i>We revisit FedExProx - a recently proposed distributed optimization method designed to enhance convergence properties of parallel proximal algorithms via extrapolation. In the process, we uncover a surprising flaw: its known theoretical guarantees on quadratic optimization tasks are no better than those offered by the vanilla Gradient Descent (GD) method. Motivated by this observation, we develop a novel analysis framework, establishing a tighter linear convergence rate for non-strongly convex quadratic problems. By incorporating both computation and communication costs, we demonstrate that FedExProx can indeed provably outperform GD, in stark contrast to the original analysis. Furthermore, we consider partial participation scenarios and analyze two adaptive extrapolation strategies - based on gradient diversity and Polyak stepsizes - again significantly outperforming previous results. Moving beyond quadratics, we extend the applicability of our analysis to general functions satisfying the Polyak-Lojasiewicz condition, outperforming the previous strongly convex analysis while operating under weaker assumptions. Backed by empirical results, our findings point to a new and stronger potential of FedExProx, paving the way for further exploration of the benefits of extrapolation in federated learning.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 

<h3>October 11, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2410.08760">
"Unlocking FedNL: Self-Contained Compute-Optimized Implementation"</a> -
joint work with Konstantin Burlachenko.
<br>
<br>
  
Abstract: <i>Federated Learning (FL) is an emerging paradigm that enables intelligent agents to collaboratively train Machine Learning (ML) models in a distributed manner, eliminating the need for sharing their local data. The recent work (arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL) algorithms, marking a significant step towards applying second-order methods to FL and large-scale optimization. However, the reference FedNL prototype exhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch a single experiment in a sever-grade workstation; (ii) The prototype only simulates multi-node setting; (iii) Prototype integration into resource-constrained applications is challenging. To bridge the gap between theory and practice, we present a self-contained implementation of FedNL, FedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves the aforementioned issues and reduces the wall clock time by x1000. With this FedNL outperforms alternatives for training logistic regression in a single-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark (arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose two practical-orientated compressors for FedNL - adaptive TopLEK and cache-aware RandSeqK, which fulfill the theory of FedNL.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>October 10, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2410.08305">
"Randomized Asymmetric Chain of LoRA: The First Meaningful Theoretical Framework for Low-Rank Adaptation"</a> -
joint work with Grigory Malinovsky, Umberto Michieli, Hasan Abed Al Kader Hammoud, Taha Ceritli, Hayder Elesedy, and Mete Ozay.
<br>
<br>
  
Abstract: <i>Fine-tuning has become a popular approach to adapting large foundational models to specific tasks. As the size of models and datasets grows, parameter-efficient fine-tuning techniques are increasingly important. One of the most widely used methods is Low-Rank Adaptation (LoRA), with adaptation update expressed as the product of two low-rank matrices. While LoRA was shown to possess strong performance in fine-tuning, it often under-performs when compared to full-parameter fine-tuning (FPFT). Although many variants of LoRA have been extensively studied empirically, their theoretical optimization analysis is heavily under-explored. The starting point of our work is a demonstration that LoRA and its two extensions, Asymmetric LoRA and Chain of LoRA, indeed encounter convergence issues. To address these issues, we propose Randomized Asymmetric Chain of LoRA (RAC-LoRA) -- a general optimization framework that rigorously analyzes the convergence rates of LoRA-based methods. Our approach inherits the empirical benefits of LoRA-style heuristics, but introduces several small but important algorithmic modifications which turn it into a provably convergent method. Our framework serves as a bridge between FPFT and low-rank adaptation. We provide provable guarantees of convergence to the same solution as FPFT, along with the rate of convergence. Additionally, we present a convergence analysis for smooth, non-convex loss functions, covering gradient descent, stochastic gradient descent, and federated learning settings. Our theoretical findings are supported by experimental results.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>October 9, 2024</h3>
<h1>Papers Accepted to NeurIPS 2024</h1>
<br>
We've had several papers accepted to the <a href="https://nips.cc/Conferences/2024">38th Annual Conference on Neural Information Processing Systems (NeurIPS 2024)</a>, which will run during December 10--15, 2024 in Vancouver, Canada.

<br><br>

Conference papers:

<br><br>

<b> 1) "PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression" </b><br>
<a href="https://arxiv.org/abs/2405.14852">[arXiv]</a> <br>
by Vladimir Malinovskii, Denis Mazur, Ivan Ilin, Denis Kuznedelev, Konstantin Burlachenko, Kai Yi, Dan Alistarh, and Peter Richtárik<br>
<span class="important">Oral (0.4% acceptance rate)</span><br>  
<br>

<b> 2) "Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity"</b><br>
<a href="https://arxiv.org/abs/2402.06412">[arXiv]</a> <br>
by Kaja Gruntkowska, Alexander Tyurin, and Peter Richtárik
<br>
<span class="important">Spotlight</span><br>  
<br>

<b> 3) "On the Optimal Time Complexities in Decentralized Stochastic Asynchronous Optimization"</b> <br>
<a href="https://arxiv.org/abs/2405.16218">[arXiv]</a>  <br>
by Alexander Tyurin and Peter Richtárik
<br>
<span class="important">Poster</span><br>  
<br>

<b> 4) "Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity"</b><br>
<a href="https://arxiv.org/abs/2402.04785">[arXiv]</a>  <br>
by Alexander Tyurin, Marta Pozzi, Ivan Ilin, and Peter Richtárik
<br>
<span class="important">Poster</span><br>  
<br>

<b> 5) "MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and Provable Convergence"</b><br>
<a href="https://arxiv.org/abs/2405.15593">[arXiv]</a>  <br>
by Ionut-Vlad Modoranu, Mher Safaryan, Grigory Malinovsky, Eldar Kurtic, Thomas Robert, Peter Richtárik, and Dan Alistarh
<br>
<span class="important">Poster</span><br>  
<br>

<b> 6) "Freya PAGE: First Optimal Time Complexity for Large-Scale Nonconvex Finite-Sum Optimization with Heterogeneous Asynchronous Computations"</b><br>
<a href="https://arxiv.org/abs/2405.15545">[arXiv]</a>  <br>
by Alexander Tyurin, Kaja Gruntkowska, and Peter Richtárik
<br>
<span class="important">Poster</span><br>  
<br>

<b> 7) "Don't Compress Gradients in Random Reshuffling: Compress Gradient Differences"</b><br>
<a href="https://arxiv.org/abs/2206.07021">[arXiv]</a>  <br>
by Abdurakhmon Sadiev, Grigory Malinovsky, Eduard Gorbunov, Igor Sokolov, Ahmed Khaled, Konstantin Burlachenko, and Peter Richtárik
<br>
<span class="important">Poster</span><br>  
<br>

<b> 8) "The Power of Extrapolation in Federated Learning"</b><br>
<a href="https://arxiv.org/abs/2405.13766">[arXiv]</a>  <br>
by Hanmin Li, Kirill Acharya, and Peter Richtárik
<br>
<span class="important">Poster</span><br>  
<br>

<b> 9) "Byzantine Robustness and Partial Participation Can Be Achieved at Once: Just Clip Gradient Differences"</b><br>
<a href="https://arxiv.org/abs/2311.14127">[arXiv]</a>  <br>
by Grigory Malinovsky, Peter Richtárik, Samuel Horváth, and Eduard Gorbunov
<br>
<span class="important">Poster</span><br>  
<br>

<br>

Workshop papers:

<br><br>


<b> 10) "Cohort Squeeze: Beyond a Single Communication Round per Cohort in Cross-Device Federated Learning"</b><br>
<a href="https://arxiv.org/abs/2406.01115">[arXiv]</a>  <br>
by Kai Yi, Timur Kharisov, Igor Sokolov, and Peter Richtárik
<br>
<span class="important">FL Workshop Oral</span><br>  
<br>

<b> 11) "SPAM: Stochastic Proximal Point Method with Momentum Variance Reduction for Nonconvex Cross-Device Federated Learning"</b><br>
<a href="https://arxiv.org/abs/2405.20127">[arXiv]</a>  <br>
by Avetik Karagulyan, Egor Shulgin, Abdurakhmon Sadiev, and Peter Richtárik
<br>
<span class="important">NeurIPS 2024 Workshop: Optimization for Machine Learning (OPT 2024)</span><br>  
<br>

<b> 12) "Communication-Efficient Algorithms Under Generalized Smoothness Assumptions"</b><br>
[arXiv]  <br>
by Sarit Khirirat, Abdurakhmon Sadiev, Artem Riabinin, Eduard Gorbunov, and Peter Richtárik
<br>
<span class="important">NeurIPS 2024 Workshop: Optimization for Machine Learning (OPT 2024)</span><br>  
<br>

<b> 13) "LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression"</b><br>
<a href="https://arxiv.org/abs/2403.04348">[arXiv]</a>  <br>
by Laurent Condat, Artavazd Maranjyan, and Peter Richtárik
<br>
<span class="important">NeurIPS 2024 Workshop: Optimization for Machine Learning (OPT 2024)</span><br>  
<br>

<b> 14) "MindFlayer: Efficient Asynchronous Parallel SGD in the Presence of Heterogeneous and Random Worker Compute Times"</b><br>
<a href="https://arxiv.org/abs/2410.04285">[arXiv]</a>  <br>
by Artavazd Maranjyan, Omar Shaikh Omar, and Peter Richtárik
<br>
<span class="important">NeurIPS 2024 Workshop: Optimization for Machine Learning (OPT 2024)</span><br>  
<br>

<b> 15) "Differentially Private Random Block Coordinate Descent"</b><br>
[arXiv] <br>
by Arto Maranjyan, Abdurakhmon Sadiev, and Peter Richtárik
<br>
<span class="important">NeurIPS 2024 Workshop: Optimization for Machine Learning (OPT 2024)</span><br>  
<br>

<b> 16) "Local Curvature Descent: Squeezing More Curvature out of Standard and Polyak Gradient Descent"</b><br>
<a href="https://arxiv.org/abs/2405.16574">[arXiv]</a>  <br>
by Peter Richtárik, Simone Maria Giancola, Dymitr Lubczyk, and Robin Yadav
<br>
<span class="important">NeurIPS 2024 Workshop: Optimization for Machine Learning (OPT 2024)</span><br>  
<br>

<b> 17) "On the Convergence of FedProx with Extrapolation and Inexact Prox"</b><br>
<a href="https://arxiv.org/abs/2410.01410">[arXiv]</a>  <br>
by Hanmin Li and Peter Richtárik
<br>
<span class="important">NeurIPS 2024 Workshop: Optimization for Machine Learning (OPT 2024)</span><br>  
<br>

<b> 18) "Stochastic Proximal Point Methods for Monotone Inclusions under Expected Similarity"</b><br>
<a href="https://arxiv.org/abs/2405.14255">[arXiv]</a>  <br>
by Abdurakhmon Sadiev, Laurent Condat, and Peter Richtárik
<br>
<span class="important">NeurIPS 2024 Workshop: Optimization for Machine Learning (OPT 2024)</span><br>  
<br>

<b> 19) "On Convergence of SGD with Adaptive Clipping"</b><br>
[arXiv]  <br>
by Egor Shulgin and Peter Richtárik
<br>
<span class="important">NeurIPS 2024 Workshop: Optimization for Machine Learning (OPT 2024)</span><br>  
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>October 2, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2409.14989">
"On the Convergence of FedProx with Extrapolation and Inexact Prox"</a> -
joint work with Hanmin Li.
 <br>
 <br>
  
Abstract: <i>Enhancing the FedProx federated learning algorithm (Li et al., 2020) with server-side extrapolation, Li et al. (2024a) recently introduced the FedExProx method. Their theoretical analysis, however, relies on the assumption that each client computes a certain proximal operator exactly, which is impractical since this is virtually never possible to do in real settings. In this paper, we investigate the behavior of FedExProx without this exactness assumption in the smooth and globally strongly convex setting. We establish a general convergence result, showing that inexactness leads to convergence to a neighborhood of the solution. Additionally, we demonstrate that, with careful control, the adverse effects of this inexactness can be mitigated. By linking inexactness to biased compression (Beznosikov et al., 2023), we refine our analysis, highlighting robustness of extrapolation to inexact proximal updates. We also examine the local iteration complexity required by each client to achieved the required level of inexactness using various local optimizers. Our theoretical insights are validated through comprehensive numerical experiments.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>September 25, 2024</h3>

<h1>Charles Broyden Prize</h1>

 <br>
 

Just learned that together with Albert S. Berahas, Majid Jahani and Martin Takáč  we've won the  Charles Broyden Prize  for the paper "Quasi-Newton methods for deep learning: forget the past, just sample".  Unexpected; thanks!

<br>  
<br>

<a href="https://www.tandfonline.com/doi/full/10.1080/10556788.2021.1977806">
<img alt="" src="imgs/Broyden-paper.jpeg" width="500">
</a>

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>September 24, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2409.14989">
"Methods for Convex $(L_0,L_1)$-Smooth Optimization: Clipping, Acceleration, and Adaptivity"</a> -
joint work with Eduard Gorbunov, Nazarii Tupitsa, Sayantan Choudhury, Alen Aliev, Samuel Horváth, and Martin Takáč.

<img alt="" src="imgs/fancy-line.png" width="196" height="36">

<br>

 <br>
 <br>
  
Abstract: <i>Due to the non-smoothness of optimization problems in Machine Learning, generalized smoothness assumptions have been gaining a lot of attention in recent years. One of the most popular assumptions of this type is $(L_0,L_1)$-smoothness (Zhang et al., 2020). In this paper, we focus on the class of (strongly) convex $(L_0,L_1)$-smooth functions and derive new convergence guarantees for several existing methods. In particular, we derive improved convergence rates for Gradient Descent with (Smoothed) Gradient Clipping and for Gradient Descent with Polyak Stepsizes. In contrast to the existing results, our rates do not rely on the standard smoothness assumption and do not suffer from the exponential dependency from the initial distance to the solution. We also extend these results to the stochastic case under the over-parameterization assumption, propose a new accelerated method for convex $(L_0,L_1)$-smooth optimization, and derive new convergence rates for Adaptive Gradient Descent (Malitsky and Mishchenko, 2020).</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>September 18, 2024</h3>

<h1>FTLA 2024 @ Valencia</h1>

Today I am giving a keynote talk at the <a href="https://flta-conference.org">FTLA conference</a> held during September 17-19, 2024, in Valencia, Spain [FTLA 2024 = The 2nd IEEE International Conference on Federated Learning Technologies and Applications].

<br> 

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>September 4, 2024</h3>

<h1>AMCS/STAT Graduate Seminar Talk</h1>

Tomorrow (Sept 5) at noon I am giving a talk at the AMCS/STAT Graduate Seminar here at KAUST: <a href="https://cemse.kaust.edu.sa/amcs/events/event/first-optimal-parallel-stochastic-gradient-descent">Location: Building 9, Level 2, Room 2325.</a> The talk title is: "The First Optimal Parallel SGD (in the Presence of Data, Compute and Communication Heterogeneity)". This is the same talk as the one I gave two days ago here at KAUST to CS audience, but this time around the audience is composed of mathematics and statistics people. 

<br> 

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>September 2, 2024</h3>

<h1>CS Graduate Seminar Talk</h1>

Today at noon I am giving a talk at the Computer Science Graduate Seminar here at KAUST. Location: Building 9, Level 2, Room 2325. The talk title is: "The First Optimal Parallel SGD (in the Presence of Data, Compute and Communication Heterogeneity)".


<br> 

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>August 26, 2024</h3>

<h1>Celebrating Nesterov's 50 Years of Research</h1>

I am on my way to Belgium to attend
the  <a href="https://sites.uclouvain.be/algopt2024/">ALGOPT2024 workshop on
Algorithmic Optimization: Tools for AI and Data Science</a>, held at UCLouvain
during August 27-30, 2024. The conference is a celebration of Yurii Nesterov's 50 years 
anniversary of doing research in optimization. I am giving my talk on the first day of the event. It will be super nice to reconnect
with many friends and colleagues. 


<br> 

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>July 21, 2024</h3>

<h1>ICML 2024 in Vienna</h1>

During July 21-27, I am attending the <a href="https://icml.cc/Conferences/2024">ICML 2024</a> conference held in Vienna.  <br>

<br> 

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


        
<h3>June 17, 2024</h3>

<h1>A keynote talk: FedVision @ CVPR 2024</h1>

I am giving a keynote talk at the <a href="https://fedvision.github.io/fedvision2024/">Federated Learning for Computer Vision (FedVision) Workshop</a> at <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a> today. The conference is held in Seattle, during June 17--21.  <br>


<br>
<br>
<a href="talks/TALK-2024-06-17-FedVision@CVPR2024-ASYNC.pdf"><img alt="" src="imgs/TALK-2024-06-17-FedVision@CVPR2024-ASYNC-thumb.png" width="500" ></a>
<br>
<br> 

<br>
Here is the presentation <a href="talks/TALK-2024-06-17-FedVision@CVPR2024-ASYNC.pdf">(62 slides)</a> supporting my talk. This is the third time I am giving this talk, each time expanded a bit (40 -> 52 -> 62 slides).<br> 

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>June 10, 2024</h3>

<h1>Talk in Paris</h1>

I am attending the <a href="https://aaforml.com">Applied Algorithms for Machine Learning (a workshop on the future of computation)</a>, held during June 10-12 in Paris. The event is organized by some amazing folks from Rice University: Maryam Aliakbarpour, Vladimir Braverman, Ben (Xia) Hu, Nai-Hui Chia,  Anastasios Kyrillidis, Sebastian Perez-Salazar, Anshumali Shrivastava, Arlei Silva and Cesar Uribe. It turn out that Rice happens to have a physical presence in the heart of Paris (Club de la Chasse). <br>


<br>
<br>
<a href="talks/TALK-2024-06-11-ASYNC-Paris.pdf"><img alt="" src="imgs/TALK-2024-06-11-ASYNC-Paris-thumb.png" width="500" ></a>
<br>
<br> 

<br>
Here are the <a href="talks/TALK-2024-06-11-ASYNC-Paris.pdf">52 slides</a>  from my talk. <br> 

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


        
<h3>June 6, 2024</h3>

<h1>Talk in Vienna (update)</h1>

I have given my talk at the <a href="https://www.esi.ac.at/events/e539/">One World Optimization Seminar (Workshop)</a>, held at the Erwin Schrödinger International Institute for Mathematics and Physics in Vienna. The event is organized by <a href="https://www.mat.univie.ac.at/~rabot/">Radu Ioan Boţ</a> and <a href="https://ymalitsky.com">Yura Malitsky</a>. <br>

<br>
<br>
<a href="talks/TALK-2024-06-06-ASYNC.pdf"><img alt="" src="imgs/TALK-2024-06-06-ASYNC-thumb.png" width="500" ></a>
<br>
<br> 



<br>
Here are the <a href="talks/TALK-2024-06-06-ASYNC.pdf">40 slides</a> forming my talk. A recording of the talk is also on <a href="https://www.youtube.com/watch?v=OerKXFxijQs">YouTube</a> now.<br> 

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 

        


<h3>June 3, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2406.01115">
"Cohort Squeeze: Beyond a Single Communication Round per Cohort in Cross-Device Federated Learning"</a> -
joint work with Kai Yi, Timur Kharisov, Igor Sokolov, and Peter Richtárik.
 <br>
 <br>
  
Abstract: <i>Virtually all federated learning (FL) methods, including FedAvg, operate in the following manner: i) an orchestrating server sends the current model parameters to a cohort of clients selected via certain rule, ii) these clients then independently perform a local training procedure (e.g., via SGD or Adam) using their own training data, and iii) the resulting models are shipped to the server for aggregation. This process is repeated until a model of suitable quality is found. A notable feature of these methods is that each cohort is involved in a single communication round with the server only. In this work we challenge this algorithmic design primitive and investigate whether it is possible to ``squeeze more juice" out of each cohort than what is possible in a single communication round. Surprisingly, we find that this is indeed the case, and our approach leads to up to 74 percent reduction in the total communication cost needed to train a FL model in the cross-device setting. Our method is based on a novel variant of the stochastic proximal point method (SPPM-AS) which supports a large collection of client sampling procedures some of which lead to further gains when compared to classical client selection approaches.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 




<h3>June 2, 2024</h3>
<h1>Papers Accepted to ICML 2024</h1>


<img alt="ICML" src="imgs/ICML_logo.png" height="200">
<br>

We've had two papers accepted to <a href="https://icml.cc/Conferences/2024">The 41st International Conference on Machine Learning (ICML 2024)</a>:

<br><br>
Eduard Gorbunov, Abdurakhmon Sadiev, Marina Danilova, Samuel Horváth, Gauthier Gidel, Pavel Dvurechensky, Alexander Gasnikov, and Peter Richtárik<br>
<b>High-probability convergence for composite and distributed stochastic minimization and variational inequalities with heavy-tailed noise</b>  <br>
<span class="important">ORAL PAPER</span> <br>
<a href="https://arxiv.org/abs/2310.01860">[arXiv]</a> 

<br><br>
Egor Shulgin and Peter Richtárik <br>
<b>Towards a better theoretical understanding of independent subnetwork training</b> <br>
<a href="https://arxiv.org/abs/2306.16484">[arXiv]</a> <br>
<br>

<br>

The conference will take place during July 21-27 in Vienna, Austria.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 1, 2024</h3>

<h1>Talk in Vienna</h1>

I am on my way to Vienna, to attend the <a href="https://www.esi.ac.at/events/e539/">One World Optimization Seminar</a>, held at the Erwin Schrödinger International Institute for Mathematics and Physics. The event is organized by <a href="https://www.mat.univie.ac.at/~rabot/">Radu Ioan Boţ</a> and <a href="https://ymalitsky.com">Yura Malitsky</a>. The event has an excellent lineup of speakers; see the <a href="https://www.esi.ac.at/events/e539/">schedule</a> here. <br>
<br>
<a href="https://www.esi.ac.at/events/t1895/">My talk</a> is scheduled for Thursday (June 6).
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>May 31, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2405.20623">
"Prune at the Clients, Not the Server: Accelerated Sparse Training in Federated Learning"</a> -
joint work with Georg Meinhardt, Kai Yi, and Laurent Condat.
 <br>
 <br>
  
Abstract: <i>In the recent paradigm of Federated Learning (FL), multiple clients train a shared model while keeping their local data private. Resource constraints of clients and communication costs pose major problems for training large models in FL. On the one hand, addressing the resource limitations of the clients, sparse training has proven to be a powerful tool in the centralized setting. On the other hand, communication costs in FL can be addressed by local training, where each client takes multiple gradient steps on its local data. Recent work has shown that local training can provably achieve the optimal accelerated communication complexity [Mishchenko et al., 2022]. Hence, one would like an accelerated sparse training algorithm. In this work we show that naive integration of sparse training and acceleration at the server fails, and how to fix it by letting the clients perform these tasks appropriately. We introduce Sparse-ProxSkip, our method developed for the nonconvex setting, inspired by RandProx [Condat and Richtárik, 2022], which provably combines sparse training and acceleration in the convex setting. We demonstrate the good performance of Sparse-ProxSkip in extensive experiments.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>May 30, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2405.20127">
"SPAM: Stochastic Proximal Point Method with Momentum Variance Reduction for Non-convex Cross-Device Federated Learning"</a> -
joint work with Avetik Karagulyan, Egor Shulgin, and Abdurakhmon Sadiev.
 <br>
 <br>
  
Abstract: <i>Cross-device training is a crucial subfield of federated learning, where the number of clients can reach into the billions. Standard approaches and local methods are prone to issues such as client drift and insensitivity to data similarities. We propose a novel algorithm (SPAM) for cross-device federated learning with non-convex losses, which solves both issues. We provide sharp analysis under second-order (Hessian) similarity, a condition satisfied by a variety of machine learning problems in practice. Additionally, we extend our results to the partial participation setting, where a cohort of selected clients communicate with the server at each communication round. Our method is the first in its kind, that does not require the smoothness of the objective and provably benefits from clients having similar data.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>May 30, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2405.19951">
"A Simple Linear Convergence Analysis of the Point-SAGA Algorithm"</a> -
joint work with Laurent Condat.
 <br>
 <br>
  
Abstract: <i>Point-SAGA is a randomized algorithm for minimizing a sum of convex functions using their proximity operators (proxs), proposed by Defazio (2016). At every iteration, the prox of only one randomly chosen function is called. We generalize the algorithm to any number of prox calls per iteration, not only one, and propose a simple proof of linear convergence when the functions are smooth and strongly convex.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>May 30, 2024</h3>

<h1>Lukang Sun Defended his PhD Thesis Today!</h1>

My stellar PhD student  <a href="https://lukangsun.github.io">Lukang Sun</a> defended his PhD thesis <a href="http://hdl.handle.net/10754/698695">Stein Variational Gradient Descent and Consensus-Based Optimization: Towards a Convergence Analysis and Generalization</a> today! 

<br> <br>
Committee: <a href="https://www.maths.ox.ac.uk/people/jose.carrillodelaplata">Jose Carrillo</a> (math @ Oxford), <a href="https://www.kaust.edu.sa/en/study/faculty/diogo-gomes">Diogo Gomes</a> (math @ KAUST), <a href="https://shao3wangdi.github.io">Di Wang</a> (CS @ KAUST), plus yours truly. 

<br><br> Lukang's next move: postdoc at TUM with <a href="https://www.professoren.tum.de/en/fornasier-massimo">Massimo Fornasier</a>. 

<br> <br>
Congrats, Lukang!!!

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 
        
        
<h3>May 26, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2405.16574">
"Local Curvature Descent: Squeezing More Curvature out of Standard and Polyak Gradient Descent"</a> -
joint work with Simone Maria Giancola, Dymitr Lubczyk, and Robin Yadav.
 <br>
 <br>
  
Abstract: <i>We contribute to the growing body of knowledge on more powerful and adaptive stepsizes for convex optimization, empowered by local curvature information. We do not go the route of fully-fledged second-order methods which require the expensive computation of the Hessian. Instead, our key observation is that, for some problems (e.g., when minimizing the sum of squares of absolutely convex functions), certain local curvature information is readily available, and can be used to obtain surprisingly powerful matrix-valued stepsizes, and meaningful theory. In particular, we develop three new methods---LCD1, LCD2 and LCD3---where the abbreviation stands for local curvature descent. While LCD1 generalizes gradient descent with fixed stepsize, LCD2 generalizes gradient descent with Polyak stepsize. Our methods enhance these classical gradient descent baselines with local curvature information, and our theory recovers the known rates in the special case when no curvature information is used. Our last method, LCD3, is a variable metric version of LCD2; this feature leads to a closed-form expression for the iterates. Our empirical results are encouraging, and show that the local curvature descent improves upon gradient descent.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 

        


<h3>May 25, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2405.16218">
"On the Optimal Time Complexities in Decentralized Stochastic Asynchronous Optimization"</a> -
joint work with Alexander Tyurin.
 <br>
 <br>
  
Abstract: <i> We consider the decentralized stochastic asynchronous optimization setup, where many workers asynchronously calculate stochastic gradients and asynchronously communicate with each other using edges in a multigraph. For both homogeneous and heterogeneous setups, we prove new time complexity lower bounds under the assumption that computation and communication speeds are bounded. We develop a new nearly optimal method, Fragile SGD, and a new optimal method, Amelie SGD, that converge under arbitrary heterogeneous computation and communication speeds and match our lower bounds (up to a logarithmic factor in the homogeneous setting). Our time complexities are new, nearly optimal, and provably improve all previous asynchronous/synchronous stochastic methods in the decentralized setup.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 




<h3>May 24, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2405.15941">
"A Unified Theory of Stochastic Proximal Point Methods without Smoothness"</a> -
joint work with Abdurakhmon Sadiev and Yury Demidovich.
<br>
<br>
  
Abstract: <i> This paper presents a comprehensive analysis of a broad range of variations of the stochastic proximal point method (SPPM). Proximal point methods have attracted considerable interest owing to their numerical stability and robustness against imperfect tuning, a trait not shared by the dominant stochastic gradient descent (SGD) algorithm. A framework of assumptions that we introduce encompasses methods employing techniques such as variance reduction and arbitrary sampling. A cornerstone of our general theoretical approach is a parametric assumption on the iterates, correction and control vectors. We establish a single theorem that ensures linear convergence under this assumption and the $\mu$-strong convexity of the loss function, and without the need to invoke smoothness. This integral theorem reinstates best known complexity and convergence guarantees for several existing methods which demonstrates the robustness of our approach. We expand our study by developing three new variants of SPPM, and through numerical experiments we elucidate various properties inherent to them. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>May 24, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2405.15593">
"MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and Provable Convergence"</a> -
joint work with Ionut-Vlad Modoranu, Mher Safaryan, Grigory Malinovsky, Eldar Kurtic, Thomas Robert, and Dan Alistarh.
 <br>
 <br>
  
Abstract: <i> We propose a new variant of the Adam optimizer [Kingma and Ba, 2014] called MICROADAM that specifically minimizes memory overheads, while maintaining theoretical convergence guarantees. We achieve this by compressing the gradient information before it is fed into the optimizer state, thereby reducing its memory footprint significantly. We control the resulting compression error via a novel instance of the classical error feedback mechanism from distributed optimization [Seide et al., 2014, Alistarh et al., 2018, Karimireddy et al., 2019] in which the error correction information is itself compressed to allow for practical memory gains. We prove that the resulting approach maintains theoretical convergence guarantees competitive to those of AMSGrad, while providing good practical performance. Specifically, we show that MICROADAM can be implemented efficiently on GPUs: on both million-scale (BERT) and billion-scale (LLaMA) models, MicroAdam provides practical convergence competitive to that of the uncompressed Adam baseline, with lower memory usage and similar running time. Our code is available at <a href="https://github.com/IST-DASLab/MicroAdam">this https URL</a>. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>May 24, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2405.15545">
"Freya PAGE: First Optimal Time Complexity for Large-Scale Nonconvex Finite-Sum Optimization with Heterogeneous Asynchronous Computations"</a> -
joint work with Alexander Tyurin and Kaja Gruntkowska.
 <br>
 <br>
  
Abstract: <i> In practical distributed systems, workers are typically not homogeneous, and due to differences in hardware configurations and network conditions, can have highly varying processing times. We consider smooth nonconvex finite-sum (empirical risk minimization) problems in this setup and introduce a new parallel method, Freya PAGE, designed to handle arbitrarily heterogeneous and asynchronous computations. By being robust to "stragglers" and adaptively ignoring slow computations, Freya PAGE offers significantly improved time complexity guarantees compared to all previous methods, including Asynchronous SGD, Rennala SGD, SPIDER, and PAGE, while requiring weaker assumptions. The algorithm relies on novel generic stochastic gradient collection strategies with theoretical guarantees that can be of interest on their own, and may be used in the design of future optimization methods. Furthermore, we establish a lower bound for smooth nonconvex finite-sum problems in the asynchronous setup, providing a fundamental time complexity limit. This lower bound is tight and demonstrates the optimality of Freya PAGE in the large-scale regime, i.e., when $\sqrt{m} \geq n$, where $n$ is # of workers, and $m$ is # of data samples. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>May 23, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2405.14852">
"PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"</a> -
joint work with Vladimir Malinovskii, Denis Mazur, Ivan Ilin, Denis Kuznedelev, Konstantin Burlachenko, Kai Yi, and Dan Alistarh.
 <br>
 <br>
  
Abstract: <i> There has been significant interest in "extreme" compression of large language models (LLMs), i.e., to 1-2 bits per parameter, which allows such models to be executed efficiently on resource-constrained devices. Existing work focused on improved one-shot quantization techniques and weight representations; yet, purely post-training approaches are reaching diminishing returns in terms of the accuracy-vs-bit-width trade-off. State-of-the-art quantization methods such as QuIP# and AQLM include fine-tuning (part of) the compressed parameters over a limited amount of calibration data; however, such fine-tuning techniques over compressed weights often make exclusive use of straight-through estimators (STE), whose performance is not well-understood in this setting. In this work, we question the use of STE for extreme LLM compression, showing that it can be sub-optimal, and perform a systematic study of quantization-aware fine-tuning strategies for LLMs. We propose PV-Tuning - a representation-agnostic framework that generalizes and improves upon existing fine-tuning strategies, and provides convergence guarantees in restricted cases. On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms prior techniques for highly-performant models such as Llama and Mistral. Using PV-Tuning, we achieve the first Pareto-optimal quantization for Llama 2 family models at 2 bits per parameter. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>May 23, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2405.14255">
"Stochastic Proximal Point Methods for Monotone Inclusions under Expected Similarity"</a> -
joint work with Abdurakhmon Sadiev and Laurent Condat.
 <br>
 <br>
  
Abstract: <i> Monotone inclusions have a wide range of applications, including minimization, saddle-point, and equilibria problems. We introduce new stochastic algorithms, with or without variance reduction, to estimate a root of the expectation of possibly set-valued monotone operators, using at every iteration one call to the resolvent of a randomly sampled operator. We also introduce a notion of similarity between the operators, which holds even for discontinuous operators. We leverage it to derive linear convergence results in the strongly monotone setting. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 




<h3>May 22, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2405.13766">
"The Power of Extrapolation in Federated Learning"</a> -
joint work with Hanmin Li and Kirill Acharya.
 <br>
 <br>
  
Abstract: <i> We propose and study several server-extrapolation strategies for enhancing the theoretical and empirical convergence properties of the popular federated learning optimizer FedProx [Li et al., 2020]. While it has long been known that some form of extrapolation can help in the practice of FL, only a handful of works provide any theoretical guarantees. The phenomenon seems elusive, and our current theoretical understanding remains severely incomplete. In our work, we focus on smooth convex or strongly convex problems in the interpolation regime. In particular, we propose Extrapolated FedProx (FedExProx), and study three extrapolation strategies: a constant strategy (depending on various smoothness parameters and the number of participating devices), and two smoothness-adaptive strategies; one based on the notion of gradient diversity (FedExProx-GraDS), and the other one based on the stochastic Polyak stepsize (FedExProx-StoPS). Our theory is corroborated with carefully constructed numerical experiments. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>May 23, 2024</h3>

<h1>Back at KAUST (again)</h1>

After a stream of events in Europe, including AISTATS and ICLR, I am back at KAUST again. 

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>May 22, 2024</h3>

<h1>NeurIPS 2024 Deadline</h1>

It's the NeurIPS 2024 deadline today. This does not mean it's necessarily a hard day. It's the months leading to this which were hard, where the real work was done. In any case, still some stuff to do today, as usual!

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


        
<h3>May 12, 2024</h3>

<h1>Final exam for CS 331</h1>

The final exam for my course CS 331: Stochastic Gradient Descent Methods was held today. Best of luck to all the students!

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 

        

<h3>May 7, 2024</h3>

<h1>ICLR 2024 @ Vienna, Austria</h1>

And now I am on my way to Vienna, Austria, to attend the <a href="https://iclr.cc/Conferences/2024">ICLR 2024</a> conference. We are presenting three papers:
<ul>
<li>Kai Yi, Nidham Gazagnadou, Peter Richtárik, and Lingjuan Lyu. <b>FedP3: Personalized and privacy-friendly federated network pruning under model heterogeneity.</b> 12th International Conference on Learning Representations (ICLR 2024). [<a href="https://arxiv.org/abs/2404.09816">arXiv</a>] [<a href="https://openreview.net/forum?id=hbHwZYqk9T">OpenReview</a>]  </li>
<li>Peter Richtárik, Elnur Gasanov, Konstantin Burlachenko. <b>Error feedback reloaded: From quadratic to arithmetic mean of smoothness constants.</b> 12th International Conference on Learning Representations (ICLR 2024). [<a href="https://arxiv.org/abs/2402.10774">arXiv</a>] [<a href="https://openreview.net/forum?id=Ch7WqGcGmb">OpenReview</a>]</li>
<li>Hanmin Li, Avetik Karagulyan and Peter Richtárik. <b>Det-CGD: Compressed gradient descent with matrix stepsizes for non-convex optimization.</b> 12th International Conference on Learning Representations (ICLR 2024).  [<a href="https://arxiv.org/abs/2305.12568">arXiv</a>] [<a href="https://openreview.net/forum?id=ZEZ0CPmoSI">OpenReview</a>]</li>
</ul>  

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>May 1, 2024</h3>

<h1>AISTATS 2024 @ Valencia, Spain</h1>

I am on my way to Valencia, Spain, to attend the <a href="http://aistats.org/aistats2024/">AISTATS 2024</a> conference. We are presenting two papers:

      <ul>
        <li>
Ahmad Rammal, Kaja Gruntkowska, Nikita Fedin, Eduard Gorbunov, and Peter Richtárik. <b>Communication compression for Byzantine robust learning: New efficient algorithms and improved rates. </b>
26th International Conference on Artificial Intelligence and Statistics (AISTATS 2024). [<a href="https://arxiv.org/abs/2310.09804">arXiv</a>]
        </li>
          <li>Rafał Szlendak, Elnur Gasanov, and Peter Richtárik. <b>Understanding progressive training through the framework of randomized coordinate descent.</b>
26th International Conference on Artificial Intelligence and Statistics (AISTATS 2024). [<a href="https://arxiv.org/abs/2306.03626">arXiv</a>]
           </li>
   </ul>        

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>April 13, 2024</h3>

<h1>Back at KAUST</h1>

I am back at KAUST! 

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>April 8, 2024</h3>

<h1>NOPTA 2024</h1>

Hi from Antwerp, Belgium. I am giving a plenary talk at NOPTA 2024 (Workshop on Nonsmooth Optimization and Applications), held during April 8-12, 2024 in honor of the 75th Birthday of Boris Mordukhovich. So far, loving the talks! 

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>March 14, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2403.06677">
"FedComLoc: Communication-efficient distributed training of sparse and quantized models"</a> -
joint work with Kai Yi, Georg Meinhardt, and Laurent Condat.
 <br>
 <br>
  
Abstract: <i> Federated Learning (FL) has garnered increasing attention due to its unique characteristic of allowing heterogeneous clients to process their private data locally and interact with a central server, while being respectful of privacy. A critical bottleneck in FL is the communication cost. A pivotal strategy to mitigate this burden is Local Training, which involves running multiple local stochastic gradient descent iterations between communication phases. Our work is inspired by the innovative Scaffnew algorithm, which has considerably advanced the reduction of communication complexity in FL. We introduce FedComLoc (Federated Compressed and Local Training), integrating practical and effective compression into Scaffnew to further enhance communication efficiency. Extensive experiments, using the popular TopK compressor and quantization, demonstrate its prowess in substantially reducing communication overheads in heterogeneous settings. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>March 11, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2403.06677">
"Streamlining in the Riemannian Realm: Efficient Riemannian Optimization with Loopless Variance Reduction"</a> -
joint work with Yury Demidovich and Grigory Malinovsky.
 <br>
 <br>
  
Abstract: <i> In this study, we investigate stochastic optimization on Riemannian manifolds, focusing on the crucial variance reduction mechanism used in both Euclidean and Riemannian settings. Riemannian variance-reduced methods usually involve a double-loop structure, computing a full gradient at the start of each loop. Determining the optimal inner loop length is challenging in practice, as it depends on strong convexity or smoothness constants, which are often unknown or hard to estimate. Motivated by Euclidean methods, we introduce the Riemannian Loopless SVRG (R-LSVRG) and PAGE (R-PAGE) methods. These methods replace the outer loop with probabilistic gradient computation triggered by a coin flip in each iteration, ensuring simpler proofs, efficient hyperparameter selection, and sharp convergence guarantees. Using R-PAGE as a framework for non-convex Riemannian optimization, we demonstrate its applicability to various important settings. For example, we derive Riemannian MARINA (R-MARINA) for distributed settings with communication compression, providing the best theoretical communication complexity guarantees for non-convex distributed optimization over Riemannian manifolds. Experimental results support our theoretical findings. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>March 7, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2403.04348">
"LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression"</a> -
joint work with Laurent Condat and Artavazd Maranjyan.
 <br>
 <br>
  
Abstract: <i> In Distributed optimization and Learning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of Local training, which reduces the communication frequency, and Compression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogenous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>February 12, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2402.06412">
"Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity"</a> -
joint work with Kaja Gruntkowska and Alexander Tyurin.
 <br>
 <br>
  
Abstract: <i> Effective communication between the server and workers plays a key role in distributed optimization. In this paper, we focus on optimizing the server-to-worker communication, uncovering inefficiencies in prevalent downlink compression approaches. Considering first the pure setup where the uplink communication costs are negligible, we introduce MARINA-P, a novel method for downlink compression, employing a collection of correlated compressors. Theoretical analyses demonstrates that MARINA-P with permutation compressors can achieve a server-to-worker communication complexity improving with the number of workers, thus being provably superior to existing algorithms. We further show that MARINA-P can serve as a starting point for extensions such as methods supporting bidirectional compression. We introduce M3, a method combining MARINA-P with uplink compression and a momentum step, achieving bidirectional compression with provable improvements in total communication complexity as the number of workers increases. Theoretical findings align closely with empirical experiments, underscoring the efficiency of the proposed algorithms. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>February 7, 2024</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2402.04785">
"Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity"</a> -
joint work with Alexander Tyurin, Marta Pozzi, and Ivan Ilin.
 <br>
 <br>
  
Abstract: <i> We consider nonconvex stochastic optimization problems in the asynchronous centralized distributed setup where the communication times from workers to a server can not be ignored, and the computation and communication times are potentially different for all workers. Using an unbiassed compression technique, we develop a new method---Shadowheart SGD---that provably improves the time complexities of all previous centralized methods. Moreover, we show that the time complexity of Shadowheart SGD is optimal in the family of centralized methods with compressed communication. We also consider the bidirectional setup, where broadcasting from the server to the workers is non-negligible, and develop a corresponding method. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>February 4, 2024</h3>

<h1>Four New Research Interns</h1>

Several research interns joined my team in January/February: 
<ul>
<li>
Kirill Acharya (Moscow Institute of Physics and Technology),
</li>
<li>
Robin Yadav (University of British Columbia),
</li>
<li>
Dymitr Lubczyk (Amsterdam),
</li>
<li>
Simone Maria Giancola (Bocconi University).
</li>
</ul>

Welcome!

<br>
  

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>February 3, 2024</h3>

<h1>ICML 2024 deadline</h1>

ICML 2024 paper submission deadline is over; all who submitted a paper deserve some rest!

<br>
  

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 

<h3>February 3, 2024</h3>

<h1>ICML 2024 deadline</h1>

ICML 2024 paper submission deadline is over; all who submitted a paper deserve some rest!

<br>
  

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>January 30, 2024</h3>

<h1>Kaja Gruntkowska: New PhD Student!</h1>

A new PhD student (in Statistics) arrived to join my team today: Kaja Gruntkowska (MS from Oxford). 

<br>
<br>
Kaja interned in my lab in Summer 2022 as an undergraduate research student coming from Warwick. The internship was fruitful; Kaja, Alexander Tyurin and I coauthored a super nice paper: <a href="https://arxiv.org/abs/2209.15218">"EF21-P and friends: Improved theoretical communication complexity for distributed optimization with bidirectional compression"</a>, published in ICML 2023. You can listen to Kaja's University of Birmingham seminar talk on this topic on <a href="https://www.youtube.com/watch?v=CzBrLqV6Fxc">YouTube.</a> 

<br>
<br>
Kaja then joined Oxford as an MS student in Fall 2022, and I supervised her thesis a part of which appeared in another super nice paper, <a href="https://arxiv.org/abs/2310.09804">"Communication compression for Byzantine robust learning: New efficient algorithms and improved rates"</a>, coauthored with Ahmad Rammal, Nikita Fedin, and Eduard Gorbunov, and recently accepted to AISTATS 2024. 

<br>
<br>
Kaja and I thought it was a good idea for her to conduct a second (this time remote) internship, and this effort eventually turned into another beautiful paper: <a href="https://arxiv.org/abs/2402.06412">"Improving the worst-case bidirectional communication complexity for nonconvex distributed optimization under function similarity"</a>, coauthored with Alexander Tyurin. 

<br>
<br>
  
Kaja, welcome! A nice way to start your PhD with three papers in the pocket!

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>January 21, 2023</h3>
<h1>Papers Accepted to AISTATS 2024</h1>

<br>
<img alt="AISTATS" src="imgs/AISTATS_logo.png" height="100">
<br>
<br>

We've had two papers accepted to <a href="http://aistats.org/aistats2024/">27th International Conference on Artificial Intelligence and Statistics (AISTATS 2024)</a>:

<br><br>

Ahmad Rammal, Kaja Gruntkowska, Nikita Fedin, Eduard Gorbunov, and Peter Richtárik <br>
<b>Communication Compression for Byzantine Robust Learning: New Efficient Algorithms and Improved Rates</b>  <br>
<a href="https://arxiv.org/abs/2310.09804">[arXiv]</a> <br>
<br>

Rafał Szlendak, Elnur Gasanov, and Peter Richtárik<br>
<b>Understanding Progressive Training Through the Framework of Randomized Coordinate Descent </b> <br>
<a href="https://arxiv.org/abs/2306.03626">[arXiv]</a> <br>
<br>

<br>
(*) Members of my <a href="https://richtarik.org/i_team.html">Optimization and Machine Learning Lab</a> at KAUST.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>January 16, 2024</h3>
<h1>Papers Accepted to ICLR 2024</h1>


<br>
<img alt="ICLR" src="imgs/ICLR_logo.png" height="100">
<br>

<br>

We've had a few papers accepted to  <a href="https://iclr.cc/Conferences/2024">12th International Conference on Learning Representations (ICLR 2024)</a>:


<br><br>

Kai Yi, Nidham Gazagnadou, Peter Richtárik, and Lingjuan Lyu<br>
<b>FedP3: Personalized and Privacy-friendly Federated Network Pruning under Model Heterogeneity</b> <br>
<a href="https://openreview.net/forum?id=hbHwZYqk9T">[OpenReview]</a> <br>
<br>

Peter Richtárik, Elnur Gasanov, and Konstantin Burlachenko<br>
<b>Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants</b> <br>
<a href="https://openreview.net/forum?id=Ch7WqGcGmb">[OpenReview]</a> <br>
<br>

Hanmin Li, Avetik Karagulyan, and Peter Richtárik<br>
<b>Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization</b> <br>
<a href="https://arxiv.org/abs/2305.12568">[arXiv]</a> <br>
<br>


<br>
(*) Members of my <a href="https://richtarik.org/i_team.html">Optimization and Machine Learning Lab</a> at KAUST.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>December 13, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2312.08053">
"Kimad: Adaptive Gradient Compression with Bandwidth Awareness"</a> -
joint work with Jihao Xin, Ivan Ilin, Shunkang Zhang, and Marco Canini. <br>
  <br>
  
Abstract: <i> In distributed training, communication often emerges as a bottleneck. In response, we introduce Kimad, a solution that offers adaptive gradient compression. By consistently monitoring bandwidth, Kimad refines compression ratios to match specific neural network layer requirements. Our exhaustive tests and proofs confirm Kimad's outstanding performance, establishing it as a benchmark in adaptive compression for distributed deep learning. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>December 9, 2023</h3>

<h1>NeurIPS @ New Orleans</h1>

I am traveling to NOLA to attend the <a href="https://neurips.cc">NeurIPS</a> conference. We will be presenting 10 papers (5 at the conference and 5 at the workshops); plus I will be giving an invited talk at the Federated Learning workshop.

<br>
  

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 

<h3>December 4, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2312.02074">
"Federated Learning is Better with Non-homomorphic Encryption"</a> -
joint work with Konstantin Burlachenko, Abdulmajeed Alrowithi, and Fahad Ali Albalawi.
 <br>
  <br>
  
Abstract: <i> Traditional AI methodologies necessitate centralized data collection, which becomes impractical when facing problems with network communication, data privacy, or storage capacity. Federated Learning (FL) offers a paradigm that empowers distributed AI model training without collecting raw data. There are different choices for providing privacy during FL training. One of the popular methodologies is employing Homomorphic Encryption (HE) - a breakthrough in privacy-preserving computation from Cryptography. However, these methods have a price in the form of extra computation and memory footprint. To resolve these issues, we propose an innovative framework that synergizes permutation-based compressors with Classical Cryptography, even though employing Classical Cryptography was assumed to be impossible in the past in the context of FL. Our framework offers a way to replace HE with cheaper Classical Cryptography primitives which provides security for the training process. It fosters asynchronous communication and provides flexible deployment options in various communication topologies. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>December 1, 2023</h3>  
  
<h1> New VS Intern: Anh Duc Nguyen </h1>

<a href="https://www.linkedin.com/in/anh-duc-nguyen-8a3857159/?originalSubdomain=sg">Anh Duc Nguyen</a> just joined my team as a  VS intern who will be with us in a remote capacity for three months, starting today. Anh Duc is studying towards a BSc (honors) in Mathematics & Data Science at the National University of Singapore.
<br><br>

He attends a special program in mathematics, designed for students with strong passion and high aptitude in mathematics. The courses he took at NUS include:
<ul>
<li>
Bayesian Statistics and Machine Learning,
</li>
<li>
Theory and Algorithms for Online Learning,
</li>
<li>
Advanced Mathematical Programming,
</li>
<li>
Stochastic Processes and Algorithms, and
</li>
<li>
Trustworthy Machine Learning.
</li>
</ul>

<br>

Anh Duc conducted undergraduate research on
<ul>
<li>
robust satisficing with Melvyn Sim,
</li>
<li>
distributed methods for variational inequalities with Volkan Cevher and Kimon Antonakopoulos at EPFL during Summer 2023 (a first-author paper is being prepared), and
</li>
<li>
accelerated stochastic gradient and dual extrapolation methods for partial optimal transport with Kim-Chuan Toh (the paper is under review. for this he was awarded the Outstanding Undergraduate Research Prize at NUS).
</li>
</ul>

<br>

Anh Duc was on Dean's List recently, which means he was among the top 5% students at the NUS Faculty of Science.
In July 2021, he attended (remotely due to Covid 19) "Mathematical Summer in Paris" at Uni Paris-Saclay, listening to lectures on topics ranging from algebraic number theory to data analysis. He is the project director of NUS Mathematics Society (coordinating events such as Singapore Cube Championship and Problemathic - NUS Math Olympiad by and for students), and is the Vice President of NUS CAC Fingerstyle (a fingerstyle guitar enthusiasts club at NUS).
<br>
        
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 




<h3>November 30, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2311.16086">
"MAST: Model-Agnostic Sparsified Training"</a> -
joint work with Yury Demidovich, Grigory Malinovsky, and Egor Shulgin.
 <br>
  <br>
  
Abstract: <i> We introduce a novel optimization problem formulation that departs from the conventional way of minimizing machine learning model loss as a black-box function. Unlike traditional formulations, the proposed approach explicitly incorporates an initially pre-trained model and random sketch operators, allowing for sparsification of both the model and gradient during training. We establish insightful properties of the proposed objective function and highlight its connections to the standard formulation. Furthermore, we present several variants of the Stochastic Gradient Descent (SGD) method adapted to the new problem formulation, including SGD with general sampling, a distributed version, and SGD with variance reduction techniques. We achieve tighter convergence rates and relax assumptions, bridging the gap between theoretical principles and practical applications, covering several important techniques such as Dropout and Sparse training. This work presents promising opportunities to enhance the theoretical understanding of model training through a sparsification-aware optimization approach. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>November 23, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2311.14127">
"Byzantine Robustness and Partial Participation Can Be Achieved Simultaneously: Just Clip Gradient Differences"</a> -
joint work with Grigory Malinovsky, Samuel Horváth, and Eduard Gorbunov.
 <br>
  <br>
  
Abstract: <i> Distributed learning has emerged as a leading paradigm for training large machine learning models. However, in real-world scenarios, participants may be unreliable or malicious, posing a significant challenge to the integrity and accuracy of the trained models. Byzantine fault tolerance mechanisms have been proposed to address these issues, but they often assume full participation from all clients, which is not always practical due to the unavailability of some clients or communication constraints. In our work, we propose the first distributed method with client sampling and provable tolerance to Byzantine workers. The key idea behind the developed method is the use of gradient clipping to control stochastic gradient differences in recursive variance reduction. This allows us to bound the potential harm caused by Byzantine workers, even during iterations when all sampled clients are Byzantine. Furthermore, we incorporate communication compression into the method to enhance communication efficiency. Under quite general assumptions, we prove convergence rates for the proposed method that match the existing state-of-the-art (SOTA) theoretical results. </i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>November 7, 2023</h3>

<h1>New Doctor in Town!</h1>

<a href="https://slavomir-hanzely.github.io/">Slavomír Hanzely</a> defended his PhD thesis entitled "Adaptive Optimization Algorithms for Machine Learning". Congratulations!!!

<br>
<br>

Examiners: Eric Moulines, Martin Jaggi, Di Wang, Ajay Jasra and myself.
  <br>
  

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>October 25, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2310.16610">
"Consensus-Based Optimization with Truncated Noise"</a> -
joint work with Massimo Fornasier, Konstantin Riedl, and Lukang Sun.
 <br>
  <br>
  
Abstract: <i>Consensus-based optimization~(CBO) is a versatile multi-particle metaheuristic optimization method suitable for performing nonconvex and nonsmooth global optimizations in high dimensions. It has proven effective in various applications while at the same time being amenable to a theoretical convergence analysis. In this paper, we explore a variant of CBO, which incorporates truncated noise in order to enhance the well-behavedness of the statistics of the law of the dynamics. By introducing this additional truncation in the noise term of the CBO dynamics, we achieve that, in contrast to the original version, higher moments of the law of the particle system can be effectively bounded. As a result, our proposed variant exhibits enhanced convergence performance, allowing in particular for wider flexibility in choosing the parameters of the method as we confirm experimentally. By analyzing the time-evolution of the Wasserstein-2 distance between the empirical measure of the interacting particle system and the global minimizer of the objective function, we rigorously prove convergence in expectation of the proposed CBO variant requiring only minimal assumptions on the objective function and on the initialization. Numerical evidences clearly demonstrate the benefit of truncating the noise in CBO.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>October 15, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2310.09804">
"Communication Compression for Byzantine Robust Learning: New Efficient Algorithms and Improved Rates"</a> -
joint work with Ahmad Rammal, Kaja Gruntkowska, Nikita Fedin, and Eduard Gorbunov.
 <br>
  <br>
  
Abstract: <i>Byzantine robustness is an essential feature of algorithms for certain distributed optimization problems, typically encountered in collaborative/federated learning. These problems are usually huge-scale, implying that communication compression is also imperative for their resolution. These factors have spurred recent algorithmic and theoretical developments in the literature of Byzantine-robust learning with compression. In this paper, we contribute to this research area in two main directions. First, we propose a new Byzantine-robust method with compression -- Byz-DASHA-PAGE -- and prove that the new method has better convergence rate (for non-convex and Polyak-Lojasiewicz smooth optimization problems), smaller neighborhood size in the heterogeneous case, and tolerates more Byzantine workers under over-parametrization than the previous method with SOTA theoretical convergence guarantees (Byz-VR-MARINA). Secondly, we develop the first Byzantine-robust method with communication compression and error feedback -- Byz-EF21 -- along with its bidirectional compression version -- Byz-EF21-BC -- and derive the convergence rates for these methods for non-convex and Polyak-Lojasiewicz smooth case. We test the proposed methods and illustrate our theoretical findings in the numerical experiments.</i>
  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>November 11, 2023</h3>

<h1>External PhD Examiner for Paul Magold</h1>

Today I attended the PhD defense of <a href="http://pmangold.fr">Paul Mangold</a> as an external examiner. He successfully defended his PhD thesis at Inria Lille under the supervision of  Aurélien Bellet, Marc Tommasi, and Joseph Salmon.
<br>

  

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>October 10, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2310.04614">
"MARINA Meets Matrix Stepsizes: Variance Reduced Distributed Non-convex Optimization"</a> -
joint work with Hanmin Li and Avetik Karagulyan.
 <br>
  <br>
  
Abstract: <i>Matrix-stepsized gradient descent algorithms have been demonstrated to exhibit superior efficiency in non-convex optimization compared to their scalar counterparts. The det-CGD algorithm, as introduced by Li et al. (2023), leverages matrix stepsizes to perform compressed gradient descent for non-convex objectives and matrix-smooth problems in a federated manner. The authors establish the algorithm's convergence to a neighborhood of the weighted stationarity point under a convex condition for the symmetric and positive-definite stepsize matrix. In this paper, we propose a variance-reduced version of the det-CGD algorithm, incorporating the MARINA method. Notably, we establish theoretically and empirically, that det-MARINA outperforms both MARINA and the distributed det-CGD algorithms in terms of iteration and communication complexities.</i>
  
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2310.04614">
<img alt="MARINA Meets Matrix Stepsizes: Variance Reduced Distributed Non-convex Optimization" src="imgs/det-MARINA.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>October 5, 2023</h3>  
  
<h1> A paper accepted to TMLR </h1>
 
The paper 

<ul>
<li> Rustem Islamov, Xun Qian, Slavomír Hanzely, Mher Safaryan, and Peter Richtárik. Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation, <a href="https://arxiv.org/abs/2206.03588">arXiv:2206.03588</a>, 2022,</li>
</ul>
was just accepted to <a href="https://openreview.net/forum?id=NekBTCKJ1H">Transactions on Machine Learning Research</a>.

<br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>October 4, 2023</h3>  
  
<h1> New VSRP Intern: Timur Kharisov </h1>

Today we are welcoming <a href="https://www.linkedin.com/in/tkharisov7/">Timur Kharisov</a> to our team as  a new VSRP intern. Timur is studying towards his B.S. degree in Computer Science and Mathematics at the Moscow Institute of Physics and Technology (MIPT), and was recommended by Aleksandr Beznosikov with whom he worked on a research project before. Timur's GPA at the moment is 9.14/10. 
<br><br>

Timur is simultaneously studying in the Yandex School of Data Science. During an internship at Yandex in 2022, 
<ul>
<li>
he worked in the Quality and Scenarios team at Alice virtual assistant (50+ million monthly users), 
</li>
<li>
debugged the LSTM model and added restoring from the last checkpoint feature,
</li>
<li>
Implemented and released grammars for voice assistant calling scenarios,
</li>
<li>
improved End-To-End overall Alice search app quality by more than 0.4%.
</li>
</ul>

In 2020, Timur was a Prize Winner in the Final of the All-Russian National Olympiad in Mathematics.
As one of the top 5% students at MIPT, he is the recipient of the Abramov Scholarship for Academic Excellence since 2021. Timur's team obtained #1 place (out of 60 teams) in the data engineering "Hack & Change" hackathon in 2021.

<br><br>

In his free time, Timur
<ul>
<li>
acts a member of the Jury at Moscow region stage of All-Russian National Olympiad in Mathematics, 
</li>
<li>
likes teaching/helping school students achieve high results in programming competitions (in the EdTech startup RocketClass),
</li>
<li>
writes NLP code for Automatic Essay Scoring for IELTS writing,
</li>
<li>
writes playable console quest games, and
</li>
<li>
implements template-based STL-like unordered map classes.
</li>
</ul>

<br>

Timur, welcome!!!
<br>
        
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 
 
 


<h3>October 3, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2310.01860">
"High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise"</a> -
joint work with Eduard Gorbunov, Abdurakhmon Sadiev, Marina Danilova, Samuel Horváth, Gauthier Gidel, Pavel Dvurechensky, and Alexander Gasnikov.
 <br>
  <br>
  
Abstract: <i>High-probability analysis of stochastic first-order optimization methods under mild assumptions on the noise has been gaining a lot of attention in recent years. Typically, gradient clipping is one of the key algorithmic ingredients to derive good high-probability guarantees when the noise is heavy-tailed. However, if implemented naïvely, clipping can spoil the convergence of the popular methods for composite and distributed optimization (Prox-SGD / Parallel SGD) even in the absence of any noise. Due to this reason, many works on high-probability analysis consider only unconstrained non-distributed problems, and the existing results for composite/distributed problems do not include some important special cases (like strongly convex problems) and are not optimal. To address this issue, we propose new stochastic methods for composite and distributed optimization based on the clipping of stochastic gradient differences and prove tight high-probability convergence results (including nearly optimal ones) for the new methods. Using similar ideas, we also develop new methods for composite and distributed variational inequalities and analyze the high-probability convergence of these methods.</i>
  
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2310.01860">
<img alt="High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise" src="imgs/HPCCDSMVIHTN.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>October 3, 2023</h3>  
  
<h1> New VSRP Intern: Georg Meinhardt </h1>
 
Georg is studying towards an MSc in Mathematical Sciences at the University of Oxford, currently ranked #2 in the program. In his studies, he focuses on probability, deep learning and combinatorics.

<br>
<br>

Prior to this, Georg obtained two separate BSc degrees from the University of Bonn: one in mathematics, and another one in computer science. He focused on combinatorial optimization, algorithm design and probability theory. In mathematics, he was ranked in the top 3% in his cohort.
Georg is the recipient of the "Studienstiftung des deutschen Volkes" Scholarship, awarded for outstanding academic potential to top 2% of students.
<br>
<br>

Some random facts about Georg:
<ul>
<li>
In 2019, he coauthored a paper: I. Messaris, A. Ascoli, G.S. Meinhardt, R. Tetzlaff, L.O. Chua "Mem-Computing CNNs with Bistable-Like Memristors”, 2019 IEEE International Symposium on Circuits and Systems (ISCAS), 2019  </li>

<li> At Oxford, he studied "Geometric Deep Learning" (final project: Transfer Learning in Neural Algorithmic Reasoning) and "Theories of Deep Learning" (final project: Modifying Attention Heads of BERT)
</li>

<li> The title of his MSc thesis is "Analytical Lower Bounds on the Probability of at Least m out of N Events“
</li>

<li> In his BSc thesis, he worked on Branch-and-Price Algorithm for Vehicle Routing. Thesis title: "Lower Bounds for the Vehicle Routing Problem in a Branch-and-Price Framework“
</li>

<li> He speaks Chinese (approx. CEFR B1) and Polish (CEFR A1)
</li>

<li> He likes rowing (he is a member of the Lady Margaret Hall Boat Club)
</li>

<li> He is a member of the "Young European Federalists" society
</li>

<li> Georg was a "Micro Intern" at EcoSync, Oxford, where he worked as a Junior Data and AI Developer for time series prediction with deep learning (GluonTS)
</li>

<li> He was also a Summer intern at Oliver Wyman, Data and Analytics, Berlin, where he worked on expanding a stress testing framework of a large Australasian Bank
</li>

<li> During 2016-2019, Georg obtained a "Vordiplom" in Information Systems Engineering from Technical University of Dresden. He studied Computer Science and Electrical Engineering.
</li>

<li> Georg likes traveling, plays the viola, photography and snowboarding (maybe he will expand his skills to sand-dunes-boarding here in Saudi?)
</li>
 
</ul>


Georg, welcome!!!
<br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>September 28, 2023</h3>  
  
<h1> ICLR 2024 </h1>
 
The deadline for submitting papers to ICLR 2024 passed; my team and I submitted a few papers. Let's see how it will pan out.
<br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>September 26, 2023</h3>  
  
<h1> A paper accepted to SIMODS </h1>
 
The paper 
<ul>
<li> Haoyu Zhao, Konstantin Burlachenko, Zhize Li, and Peter Richtárik. Faster Rates for Compressed Federated Learning with Client-Variance Reduction, <a href="https://arxiv.org/abs/2112.13097">arXiv:2112.13097</a>, 2021,</li>
</ul>
was accepted to SIAM Journal on Mathematics of Data Science (SIMODS).

<br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>September 25, 2023</h3>  
  
<h1> RDIA </h1>
 
I've applied for an RDIA grant. The Research, Development and Innovation Authority (RDIA) is a newly established funding agency in Saudi Arabia.

<br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>September 24, 2023</h3>  
  
  <h1> Paper accepted to TMLR </h1>
 
The paper 

<ul>
<li> Alexander Tyurin, Lukang Sun, Konstantin Burlachenko, and Peter Richtárik. Sharper Rates and Flexible Framework for Nonconvex SGD with Client and Data Sampling, <a href="https://arxiv.org/abs/2206.02275">arXiv:2206.02275</a>, 2022,</li>
</ul>
was accepted to Transactions on Machine Learning Research (TMLR).

<br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 




 
   <h3>August 21, 2023</h3>  
  
  <h1> Paper accepted to JOTA </h1>
 
The paper 

<ul>
<li> Ahmed Khaled, Othmane Sebbouh, Nicolas Loizou, Robert M. Gower, and Peter Richtárik. Unified analysis of stochastic gradient methods for composite convex and smooth optimization, <a href="https://arxiv.org/abs/2006.11573">arXiv:2006.11573</a>, 2020,</li>
</ul>
was accepted to Journal of Optimization Theory and Applications (JOTA).

    <br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 

    <h3>August 16, 2023</h3>  
  
  <h1> New MS/PhD student: Artem Riabinin </h1>
 
Artem Riabinin is a new MS/PhD student in my lab about to arrive to KAUST on August 16! Artem studied in the Mathematics Department, Faculty of Physics, Lomonosov Moscow State University, with a GPA of 4.88/5.00. His past research interests and experience lie in applied mathematics, including numerical methods, inverse ill-posed problems and their applications.  For example, he was involved with the processing of images obtained by laser radars.
<br><br>

Artem developed and delivered two courses aimed at training of school students for experimental round of All-Russian Olympiad in Physics.

<br><br>
Artem's successes in various competitions:
<ul>
<li>Phystech Olympiad in Physics, 2019 (winner) </li>
<li>Phystech Olympiad in Mathematics, 2018 (prize winner) </li>
<li>City Physics Olympiad, N. Novgorod, 2018 (2nd place) </li>
<li>Regional stage of the All Russian Olympiad in Physics, 2018 (prize winner)</li>
</ul>

Artem, welcome to the team!
<br>

        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>August 14, 2023</h3>  
  
<h1> New PhD student: Arto Maranjyan </h1>


<a href="https://artomaranjyan.github.io">Artavazd "Arto" Maranjyan</a> has just arrived to KAUST -- he is joining my team as a PhD student. He is the recipient of the prestigious KAUST Dean Award (a 6,000 USD per year top-up on the already generous KAUST Fellowship, given to a handful of the best incoming students to KAUST).
<br><br>

Arto has has MSc degree in Applied Statistics and Data Science from Yerevan State University. His MSc thesis entitled <a href="https://drive.google.com/file/d/1Eu9QypLzcw5T4jmBzisD5V60hGRWyT0P/view">On local training methods</a> was based on a <a href="https://arxiv.org/abs/2210.16402">joint paper</a> he worked on during his internship co-supervised by my postdoc Mher Safaryan and myself at KAUST during the second half of 2022. Prior to this, Arto obtained his  BSc degree in Informatics and Applied Mathematics  from Yerevan State University. His BSc thesis  <a href="https://drive.google.com/file/d/18VkkGXtqfky94HpInyID9mgzPryVdiaz/view">"On the convergence of series in classical systems</a> was supervised by Martin Grigoryan. Arto got an Outstanding Final Project Award for this  thesis (awarded to 6 students among more than 250 students).

<br><br>
Arto coauthored 4 papers before the start of his PhD:
<ul>
<li>Martin Grigoryan, Anna Kamont, Artavazd Maranjyan. Menshov-type theorem for divergence sets of sequences of localized operators, Journal of Contemporary Mathematical Analysis, vol. 58, no. 2, pp. 81–92, 2023. </li>
<li>Artavazd Maranjyan, Mher Safaryan, Peter Richtárik. GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity, arXiv:2210.16402, 2022.</li>
<li>Martin Grigoryan, Artavazd Maranjyan. On the divergence of Fourier series in the general Haar system, Armenian Journal of Mathematics, vol. 13, p. 1–10, Sep. 2021.</li>
<li>Rigran Grigoryan, Artavazd Maranjyan. On the unconditional convergence of Faber-Schauder series in L1, Proceedings of the YSU A: Physical and Mathematical Sciences, vol. 55, no. 1 (254), pp. 12–19, 2021.</li>
</ul>

Arto, welcome to the team!
<br>

        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>August 2-21, 2023</h3>  
  
  <h1> NeurIPS "vacation" </h1>
 
Most of my August is spent on writing NeurIPS rebuttals and on NeurIPS Area Chair work. What a good way to spend the Summer.  Hawaii comes distant second.
    <br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


   
  <h3>August 10, 2023</h3>  
  
  <h1> Numerische Mathematik </h1>
 
 I accepted an invite to join the <a href="https://www.springer.com/journal/211/editors">Editorial Board</a> of <a href="https://www.springer.com/journal/211">Numerische Mathematik</a>.     <br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>   


       
 <h3>July 22, 2023</h3>

<h1> ICML @ O'ahu, Hawaii </h1>

I am leaving San Francisco for Honolulu to attend ICML 2023. KAUST has a booth, so please stop by! We have 10 papers at the event: 2 conference and 8 workshop papers.<br> <br>

Conference: 
<ul>
<li> <a href="https://proceedings.mlr.press/v202/sadiev23a">High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance</a> (Abdurakhmon Sadiev, Marina Danilova, Eduard Gorbunov, Samuel Horváth, Gauthier Gidel, Pavel Dvurechensky, Alexander Gasnikov, Peter Richtárik)</li>
<li> <a href="https://proceedings.mlr.press/v202/gruntkowska23a">EF21-P and Friends: Improved Theoretical Communication Complexity for Distributed Optimization with Bidirectional Compression</a> (Kaja Gruntkowska, Alexander Tyurin, Peter Richtárik) </li>
</ul>

<br>
Workshops:
<ul>
<li>  <a href="https://arxiv.org/abs/2305.13082">Sketch-and-Project Meets Newton Method: Global O(1/k^2) Convergence with Low-Rank Updates  (Slavomír Hanzely) </li>
<li> <a href="https://arxiv.org/abs/2306.16484">Towards a Better Theoretical Understanding of Independent Subnetwork Training</a> (Egor Shulgin, Peter Richtárik) </li>
<li> <a href="https://arxiv.org/abs/2301.06806">Convergence of First-Order Algorithms for Meta-Learning with Moreau Envelopes</a> (Konstantin Mishchenko, Slavomír Hanzely, Peter Richtárik) </li>
<li>  <a href="https://arxiv.org/abs/2306.03240">Improving Accelerated Federated Learning with Compression and Importance Sampling </a>  (Michał Grudzień, Grigory Malinovsky, Peter Richtárik)</li>
<li> <a href="https://arxiv.org/abs/2302.03662">Federated Learning with Regularized Client Participation</a> (Grigory Malinovsky, Samuel Horváth, Konstantin Burlachenko, Peter Richtárik) </li>
<li> <a href="https://arxiv.org/abs/2305.15155">Momentum Provably Improves Error Feedback!</a> (Ilyas Fatkhullin, Alexander Tyurin, Peter Richtárik) </li>
<li> <a href="https://arxiv.org/abs/2206.07021">Federated Optimization Algorithms with Random Reshuffling and Gradient Compression</a> (Abdurakhmon Sadiev, Grigory Malinovsky, Eduard Gorbunov, Igor Sokolov, Ahmed Khaled, Konstantin Burlachenko, Peter Richtárik)  </li>
<li> <a href="https://arxiv.org/abs/2303.04622">ELF: Federated Langevin Algorithms with Primal, Dual and Bidirectional Compression</a> (Avetik Karagulyan, Peter Richtárik)  </li>
</ul>

<br>
Several members of my team attended as well, including Avetik Karagulyan, Yury Demidovich, Samuel Horváth, Slavomír Hanzely, Egor Shulgin, Igor Sokolov, as well as former interns and coauthors of two of the above papers, Kaja Gruntkowska and Ilyas Fatkhullin.
<br> <br>

We all had a great time. I spoiled by breaking my hand on the last day in Hawaii. Will need 2 months for the hand to heal.  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>   


        
<h3>July 19, 2023</h3>

<h1> Federated and Collaborative Learning Workshop @ Berkeley </h1>

I am attending the <a href="https://simons.berkeley.edu/workshops/federated-collaborative-learning/schedule">Federated and Collaborative Learning</a> workshop organized by John Duchi, Nika Haghtalab, Peter Kairouz, Virginia Smith, Nati Srebro, and Kunal Talwar, taking place at the Simons Institute at UC Berkeley during July 19-20, 2023. My talk is on Day 2 of the event. This is a scoping event prior to applying for a semester-long program on Federated Learning at the Simons Institute.<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>   

        
        
<h3>July 9, 2023</h3>

<h1>Eastern European Machine Learning (EEML) Summer School </h1>

I am one of the lecturers at <a href="https://www.eeml.eu">EEML</a>, which is taking place during July 10-15, 2023, in beautiful Košice, Slovakia. <br> <br>

Eastern European Machine Learning (EEML) summer school is a one-week summer school around core topics regarding machine learning and artificial intelligence. The summer school includes both lectures and practical sessions (labs) to improve the theoretical and practical understanding of these topics. The school is organized in English and is aimed in particular at graduate students, although it is open to anyone interested in the topic. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>        


<h3>July 1, 2023</h3>

<h1>Trip to Yerevan, Armenia</h1>

I am on my way to Yerevan Armenia, to attend the conference <a href="http://mathconf.sci.am/">Mathematics in Armenia: Advances and Perspectives</a>, which is a celebration of the 80th anniversary of the foundation of the Armenian National Academy of Sciences. I am giving a plenary talk on July 5.  <br> <br>

Several members of my team are giving talks, too.
<ul>
<li> Abdurakhmon Sadiev, High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance</li>
<li> Egor Shulgin, Towards a Better Theoretical Understanding of Independent Subnetwork
Training</li>
<li> Slavomír Hanzely, A Second-Order Optimization Method with Low-Rank Updates and Global O(1/k^2) Convergence Rate</li>
<li> Avetik Karagulyan, ELF: Federated Langevin Algorithms with Primal, Dual and Bidirectio-
nal Compression</li>
<li> Grigorii Malinovskii, Can 5th Generation Local Training Methods Support Client Sampling? Yes!</li>
<li> Artavazd Maranjyan, GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity</li>
</ul>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>June 28, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2306.16484">
"Towards a Better Theoretical Understanding of Independent Subnetwork Training"</a> -
joint work with
 <a href="https://shulgin-egor.github.io">Egor Shulgin</a>.
 <br>
  <br>
  
Abstract: <i>Modern advancements in large-scale machine learning would be impossible without the paradigm of data-parallel distributed computing. Since distributed computing with large-scale models imparts excessive pressure on communication channels, significant recent research has been directed toward co-designing communication compression strategies and training algorithms with the goal of reducing communication costs. While pure data parallelism allows better data scaling, it suffers from poor model scaling properties. Indeed, compute nodes are severely limited by memory constraints, preventing further increases in model size. For this reason, the latest achievements in training giant neural network models also rely on some form of model parallelism. In this work, we take a closer theoretical look at Independent Subnetwork Training (IST), which is a recently proposed and highly effective technique for solving the aforementioned problems. We identify fundamental differences between IST and alternative approaches, such as distributed methods with compressed communication, and provide a precise analysis of its optimization performance on a quadratic model.</i>
  
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2306.16484">
<img alt="Towards a Better Theoretical Understanding of Independent Subnetwork Training" src="imgs/IST.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>June 15, 2023</h3>

<h1>Aramco course endgame</h1>

Four more days of teaching (June 15-18) my Introduction to Machine Learning course for Aramco MS students. This the second half of an intensive 48 hours long course delivered over 8 days. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>June 5, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2306.03240">
"Improving Accelerated Federated Learning with Compression and Importance Sampling"</a> -
joint work with
 <a href="https://scholar.google.com/citations?user=vN2ALVYAAAAJ&hl=en">Michał Grudzień</a>, and
 <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>.
 <br>
  <br>
 
We found out how to combine local training, client sampling and communication compression correctly in a single method.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2306.03240">
<img alt="Improving Accelerated Federated Learning with Compression and Importance Sampling" src="imgs/5GCS-CC.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 4, 2023</h3>

<h1>Aramco course halftime</h1>

After full 4 days of teaching (June 1-4), I have week plus to prepare for further and final 4 days. This is an intensive 48 hours long course delivered over 8 days. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 31, 2023</h3>

<h1>Teaching @ Aramco</h1>

On my way to <a href="https://www.google.com/maps/place/Dammam/@26.3622887,48.7235797,8z/data=!4m6!3m5!1s0x3e361d32276b3403:0xefd901ec7a5e5676!8m2!3d26.4206828!4d50.0887943!16zL20vMDJmZ3Z6?entry=ttu">Dammam!</a> I am teaching an "Introduction to Machine Learning" course within an MS in Data Science program offered by KAUST to selected employees of Saudi Aramco. My TAs: Alexander Tyurin and Rafal Szlendak. Four days of teaching in a row, starting tomorrow. Some break. And then again, four more days. Will be fun. Will be tiring.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>May 30, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.18929">
"Clip21: Error Feedback for Gradient Clipping"</a> -
joint work with
 <a href="https://richtarik.org/i_team.html">Sarit Khirirat</a>, 
 <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>, 
 <a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth</a>,
  <a href="https://rustem-islamov.github.io">Rustem Islamov</a>,
  and
 <a href="https://uwaterloo.ca/scholar/karray">Fakhri Karray</a>.
 <br>
  <br>
 
 We found out how to correct for the error caused by gradient clipping.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.18929">
<img alt="Clip21: Error Feedback for Gradient Clipping" src="imgs/Clip21.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 29, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.18627">
"Global-QSGD: Practical Floatless Quantization for Distributed Learning with Theoretical Guarantees"</a> -
joint work with
 <a href="https://jihaoxin.github.io">Jihao Xin</a>, <a href="https://mcanini.github.io">Marco Canini</a> and <a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth</a>.
 
 <br>
  <br>
 
 We found out how to make gradient quantization all-reduce friendly.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.18627">
<img alt="Global-QSGD: Practical Floatless Quantization for Distributed Learning with Theoretical Guarantees" src="imgs/Global-QSGD.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 28, 2023</h3>

<h1>Back @ KAUST</h1>

I am back at KAUST now. Biggest news: I have a new glass whiteboard in my office covering an entire wall. Awesome! <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 26, 2023</h3>

<h1>Talk @ Slovak Academy of Sciences</h1>

After a week, I am visiting the Institute of Informatics of the Slovak Academy of Sciences again. They seem to be very interested in Federated Learning.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>May 25, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.16296">
"A Guide Through the Zoo of Biased SGD"</a> -
joint work with
 <a href="https://combgeo.org/en/members/yury-demidovich/">Yury Demidovich</a>, <a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky</a> and <a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a>.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.16296">
<img alt="A Guide Through the Zoo of Biased SGD" src="imgs/BiasedSGD.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 24, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.15264">
"Error Feedback Shines when Features are Rare"</a> -
joint work with
 <a href="https://elnurgasanov.com">Elnur Gasanov</a> and <a href="https://burlachenkok.github.io">Konstantin Burlachenko</a>.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.15264">
<img alt="Error Feedback Shines when Features are Rare" src="imgs/EF21-sparse.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 24, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.15155">
"Momentum Provably Improves Error Feedback!"</a> -
joint work with
 <a href="https://ai.ethz.ch/about-us/people/ilyas-fatkhullin.html">Ilyas Fatkhullin</a> and <a href="https://k3nfalt.github.io/">Alexander Tyurin</a>.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.15155">
<img alt="Momentum Provably Improves Error Feedback!" src="imgs/EF21-SGDM.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 23, 2023</h3>

<h1>Talk @ Apple</h1>

Today I am giving a talk at Apple, targeting their Federated Learning team. My talk title:  "On 5th Generation of Local Training Methods in Federated Learning". 

<br>

<br>
<a href="talks/TALK-2023-05-23-Apple.pdf"><img alt="My talk." src="imgs/2023-05-23-Apple.png" width="720"></a>
<br>

<a href="talks/TALK-2023-05-23-Apple.pdf">Here are the slides.</a>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 22, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.13170">
"Explicit Personalization and Local Training: Double Communication Acceleration in Federated Learning"</a> -
joint work with
 <a href="https://kaiyi.me">Kai Yi</a> and <a href="https://lcondat.github.io">Laurent Condat</a>.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.13170">
<img alt="Explicit Personalization and Local Training: Double Communication Acceleration in Federated Learning" src="imgs/Scafflix.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 21, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.12387">
"Optimal Time Complexities of Parallel Stochastic Optimization Methods Under a Fixed Computation Model"</a> -
joint work with
 <a href="https://k3nfalt.github.io">Alexander Tyurin</a>.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.12387">
<img alt="Optimal Time Complexities of Parallel Stochastic Optimization Methods Under a Fixed Computation Model" src="imgs/RennalaSGD.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 21, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.12379">
"2Direction: Theoretically Faster Distributed Training with Bidirectional Communication Compression"</a> -
joint work with
 <a href="https://k3nfalt.github.io">Alexander Tyurin</a>.

<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.12379">
<img alt="2Direction: Theoretically Faster Distributed Training with Bidirectional Communication Compression" src="imgs/2Direction.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 21, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.12568">
"Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization"</a> -
joint work with
Hanmin Li and <a href="https://avetx.github.io">Avetik Karagulyan</a>.

<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.12568">
<img alt="Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization" src="imgs/Det-CGD.png" width="500">
</a>
</center>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 19, 2023</h3>

<h1>Talk @ Slovak Academy of Sciences</h1>

I am visiting the Institute of Informatics of the Slovak Academy of Sciences to give a research seminar talk.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 17, 2023</h3>

<h1>NeurIPS 2023 Deadline</h1>

Finalizing our submissions! I'll need to sleep for 24 hours after this...
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 15, 2023</h3>

<h1>Research Talk @ KInIT</h1>

Today, I am giving a research talk at the <a href="https://kinit.sk/">Kempelen Institute of Intelligent Technologies (KInIT)</a> in Bratislava, Slovakia. I will talk about some new results on adaptive stepsizes in optimization.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

        
<h3>May 14, 2023</h3>

<h1>Exam Week @ KAUST</h1>

It's the exam week at KAUST. Students in my Federated Learning (CS 331) class are finalizing their project reports.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>April 29, 2023</h3>

<h1>ICLR 2023 @ Kigali, Rwanda</h1>

I am heading to Kigali, Rwanda, to attend <a href="https://iclr.cc/Conferences/2023">ICLR 2023.</a> Several people from  my team are going, too, including Laurent Condat, Alexander Tyurin, Egor Shulgin, Slavomír Hanzely, and former members Samuel Horváth, Eduard Gorbunov, Adil Salim and Ahmed Khaled. 
<br>
<br>
We will present three papers: 1) <a href="https://openreview.net/forum?id=VA1YpcNr7ul">DASHA</a>, 2) <a href="https://openreview.net/forum?id=pfuqQQCB34">Byz-MARINA</a>, and 3) <a href="https://openreview.net/forum?id=cB4N3G5udUS">RandProx</a>. 
<br>
<br>
Update: I have seen mountain gorillas!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

        
<h3>April 23, 2023</h3>

<h1>Talk @ Qualcomm AI Research</h1>

Later today, I am giving a research talk in the DistributedML Seminar @ Qualcomm AI Research.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

        
        
<h3>March 8, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2303.04622">
"ELF: Federated Langevin Algorithms with Primal, Dual and Bidirectional Compression"</a> -
joint work with
<a href="https://avetx.github.io">Avetik Karagulyan</a>.

<br>
<br>
Abstract:
<i>
Federated sampling algorithms have recently gained great popularity in the community of machine learning and statistics. This paper studies variants of such algorithms called Error Feedback Langevin algorithms (ELF). In particular, we analyze the combinations of EF21 and EF21-P with the federated Langevin Monte-Carlo. We propose three algorithms: P-ELF, D-ELF, and B-ELF that use, respectively, primal, dual, and bidirectional compressors. We analyze the proposed methods under Log-Sobolev inequality and provide non-asymptotic convergence guarantees.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


        
<h3>February 20, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2302.09832">
"TAMUNA: Accelerated Federated Learning with Local Training and Partial Participation"</a> -
joint work with
<a href="https://lcondat.github.io/">Laurent Condat</a> and
<a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky</a>.

<br>
<br>
Abstract:
<i>
In federated learning, a large number of users are involved in a global learning task, in a collaborative way. They alternate local computations and communication with a distant server. Communication, which can be slow and costly, is the main bottleneck in this setting. To accelerate distributed gradient descent, the popular strategy of local training is to communicate less frequently; that is, to perform several iterations of local computations between the communication steps. A recent breakthrough in this field was made by Mishchenko et al. (2022): their Scaffnew algorithm is the first to probably benefit from local training, with accelerated communication complexity. However, it was an open and challenging question to know whether the powerful mechanism behind Scaffnew would be compatible with partial participation, the desirable feature that not all clients need to participate to every round of the training process. We answer this question positively and propose a new algorithm, which handles local training and partial participation, with state-of-the-art communication complexity.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 1, 2023</h3>

<h1>New Research Intern: Dinis Seward (Oxford)</h1>

Dinis Douglas Guerreiro Seward just joined our team as a <a href="https://vsrp.kaust.edu.sa/">VSRP intern</a>. Dinis studies towards an MSc degree in Mathematical Modelling and Scientific Computing at the University of Oxford. Prior to this, he obtained a BSc degree in Applied Mathematics from Universidade de Lisboa, Portugal.
<br>
<br>

Besides mathematics and optimization, Dinis is interested in graph theory, mathematical biology and artificial intelligence. Dinis' numerous accomplishments include:
<ul>

<li>St. Peter’s College Foundation Graduate Award, 2021 (Awarded to cover expenses during graduate studies)</li>

<li>Scholarship New Talents in Artificial Intelligence, Calouste Gulbenkian Foundation, 2019-2020 (10-month research scholarship awarded to 8 promising BSc students nationwide with research potential in the field of Artificial Intelligence.)</li>

<li>IGC Summer School Scholarship, 2019</li>

<li>Merit Diploma for outstanding academic performance, 2019 (Awarded for outstanding academic performance during the 2017-18 academic year)</li>

<li>Erasmus+ Scholarship, 2018 (Awarded by the European Commission to students admitted into the Erasmus+ exchange programme)</li>

<li>Scholarship New Talents in Mathematics, Calouste Gulbenkian Foundation, 2017-2018 (10-month research scholarship awarded to 20 promising BSc students nationwide pursuing degrees with strong mathematical background)</li>

<li>FCIENCIAS.ID Award - Honourable Mention (Awarded to the best students in the first year of a BSc degree at Faculdade de Ciencias da Universidade de Lisboa)</li>

<li>Academic Merit Scholarship (Scholarship covering a 1-year tuition fee. Awarded for outstanding academic performance during the 2016-17 academic year)</li>

<li>Merit Diploma, 2018 (Awarded for outstanding academic performance during the 2016-17 academic year)</li>
</ul>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


        
<h3>February 8, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2302.03662">
"Federated Learning with Regularized Client Participation"</a> -
joint work with
<a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky</a>,
<a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth</a>, and
<a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a>.

<br>
<br>
Abstract:
<i>
Federated Learning (FL) is a distributed machine learning approach where multiple clients work together to solve a machine learning task. One of the key challenges in FL is the issue of partial participation, which occurs when a large number of clients are involved in the training process. The traditional method to address this problem is randomly selecting a subset of clients at each communication round. In our research, we propose a new technique and design a novel regularized client participation scheme. Under this scheme, each client joins the learning process every R communication rounds, which we refer to as a meta epoch. We have found that this participation scheme leads to a reduction in the variance caused by client sampling. Combined with the popular FedAvg algorithm (McMahan et al., 2017), it results in superior rates under standard assumptions. For instance, the optimization term in our main convergence bound decreases linearly with the product of the number of communication rounds and the size of the local dataset of each client, and the statistical term scales with step size quadratically instead of linearly (the case for client sampling with replacement), leading to better convergence rate O(1/T^2) compared to O(1/T), where T is the total number of communication rounds. Furthermore, our results permit arbitrary client availability as long as each client is available for training once per each meta epoch..
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


        

<h3>February 2, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2302.00999">
"High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance"</a> -
joint work with
<a href="https://scholar.google.com/citations?user=R-xZRIAAAAAJ">Abdurakhmon Sadiev</a>,
<a href="https://marinadanya.github.io">Marina Danilova</a>,
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>,
<a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth</a>,
<a href="https://gauthiergidel.github.io">Gauthier Gidel</a>,
<a href="http://wias-berlin.de/people/dvureche/">Pavel Dvurechensky</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.

<br>
<br>
Abstract:
<i>
During recent years the interest of optimization and machine learning communities in high-probability convergence of stochastic optimization methods has been growing. One of the main reasons for this is that high-probability complexity bounds are more accurate and less studied than in-expectation ones. However, SOTA high-probability non-asymptotic convergence results are derived under strong assumptions such as the boundedness of the gradient noise variance or of the objective's gradient itself. In this paper, we propose several algorithms with high-probability convergence results under less restrictive assumptions. In particular, we derive new high-probability convergence results under the assumption that the gradient/operator noise has bounded central α-th moment for α∈(1,2] in the following setups: (i) smooth non-convex / Polyak-Lojasiewicz / convex / strongly convex / quasi-strongly convex minimization problems, (ii) Lipschitz / star-cocoercive and monotone / quasi-strongly monotone variational inequalities. These results justify the usage of the considered methods for solving problems that do not fit standard functional classes studied in stochastic optimization.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 26, 2023</h3>

<h1>ICML 2023 Deadline</h1>

The <a href="https://icml.cc/Conferences/2023">ICML 2023</a> deadline is today, we are all working towards winning tickets to Hawaii!


<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 25, 2023</h3>

<h1>New PhD Student: Ivan Ilin</h1>


Ivan Ilin is joining my team as a PhD student, starting in Spring 2023. He arrived to KAUST today! <br> <br>

Ivan studied BS and MS programs in Automation of Physical
and Technical Research at <a href="https://en.wikipedia.org/wiki/Novosibirsk_State_University">Novosibirsk State University</a> since 2017. According to US News, Novosibirsk State University ranks #5 in Russia,
after Lomonosov Moscow State University (1), Moscow Institute of Physics & Technology (2), National Research Nuclear University MEPhI (3),
and National Research University Higher School of Economics (4), and before Tomsk State University (6), Sechenov First Moscow State Medical
University (7), Saint Petersburg State University (8), Peter the Great St. Petersburg Polytechnic University (9), ITMO University (10),
and Skolkovo Institute of Science & Technology (11). <br> <br>

His work experience includes:
<ul>
<li>Undergraduate Research Assistant, Budker Institute of Nuclear Physics, Novosibirsk, Russia, 2020</li>
<li>Deep learning Junior Researcher, ExpaSoft, Novosibirsk, Russia, since 2020</li>
<li>Lavrentyev Institute of Hydrodynamics, Novosibirsk, Russia, 2018-2019</li>
</ul>

Ivan has so far been interested ML and DL in general, and in these topics in particular: image generation and recognition, NLP,
voice generation, application of ML and DL in games, foundation and advanced 3d graphics generation, physics simulations,
game design, advanced programmed animations by manim or other libraries, product and gadget design, advertisement. <br> <br>

In 2016-2017, Ivan was the captain of the Russian team in International Young Physicists Tournament in Singapore/Russia.<br><br>

Links to Ivan's:
<ul>
<li>website: <a href="http://ilinblog.ru">http://ilinblog.ru</a></li>
<li>youtube channel: <a href="https://www.youtube.com/c/vectozavr">https://www.youtube.com/c/vectozavr</a></li>
<li>online math school project: <a href="https://vectozavr.ru">https://vectozavr.ru</a></li>
<li>facebook: <a href="https://www.facebook.com/ivan.ilin.12935">https://www.facebook.com/ivan.ilin.12935</a></li>
</ul>

Ivan wrote several articles, they can be found on his website in the "Мои статьи и исследования" section.<br><br>

Ivan, Welcome to KAUST and to the team!!!


<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 22, 2023</h3>

<h1>New Semester</h1>

The Spring 2023 semester at KAUST started today. I am teaching CS 332: Federated Learning.


<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 21, 2023</h3>
<h1>Papers Accepted to AISTATS 2023 and ICLR 2023</h1>


<br>
We've had several papers accepted to <a href="http://aistats.org/aistats2023/">26th International Conference on Artificial Intelligence and Statistics (AISTATS 2023) </a>
and <a href="https://iclr.cc/Conferences/2023">11th International Conference on Learning Representations (ICLR 2023)</a>.
Here they are:

<br><br>

<br>
<img alt="AISTATS" src="imgs/AISTATS_logo.png" height="100">
<br>

<a href="https://scholar.google.com/citations?hl=en&user=vN2ALVYAAAAJ">Michał Grudzień*</a>, <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky*</a> and Peter Richtárik<br>
<b>Can 5th Generation Local Training Methods Support Partial Client Participation? Yes!</b>  <br>
Accepted to AISTATS 2023  <a href="https://arxiv.org/abs/2212.14370">[arXiv]</a> <br>
<br>

<a href="https://lukangsun.github.io">Lukang Sun*</a>, <a href="https://avetx.github.io">Avetik Karagulyan*</a> and Peter Richtárik<br>
<b>Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition</b> <br>
Accepted to AISTATS 2023 <a href="https://arxiv.org/abs/2206.00508">[arXiv]</a> <br>
<br>

<a href="https://qianxunk.github.io">Xun Qian*</a>, <a href="http://hendrydong.github.io">Hanze Dong</a>, <a href="http://tongzhang-ml.org">Tong Zhang</a> and Peter Richtárik<br>
<b>Catalyst Acceleration of Error Compensated Methods Leads to Better Communication Complexity</b> <br>
Accepted to AISTATS 2023 <a href="https://arxiv.org/abs/2301.09893">[arXiv]</a> <br>
<br>

<br>
<img alt="ICLR" src="imgs/ICLR_logo.png" height="100">
<br>

<a href="https://k3nfalt.github.io">Alexander Tyurin*</a> and Peter Richtárik<br>
<b>DASHA: Distributed Nonconvex Optimization with Communication Compression and Optimal Oracle Complexity</b> <br>
Accepted to ICLR 2023 <a href="https://arxiv.org/abs/2202.01268">[arXiv]</a> <a href="https://openreview.net/forum?id=VA1YpcNr7ul">[OpenReview]</a> <br>
<br>

<a href="https://eduardgorbunov.github.io/">Eduard Gorbunov*</a>, <a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth*</a>, <a href="https://gauthiergidel.github.io/">Gauthier Gidel</a> and Peter Richtárik<br>
<b>Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top</b> <br>
Accepted to ICLR 2023 <a href="https://arxiv.org/abs/2206.00529">[arXiv]</a> <a href="https://openreview.net/forum?id=pfuqQQCB34">[OpenReview]</a> <br>
<br>

<a href="https://lcondat.github.io/">Laurent Condat*</a> and Peter Richtárik<br>
<b>RandProx: Primal-Dual Optimization Algorithms with Randomized Proximal Updates</b> <br>
Accepted to ICLR 2023 <a href="https://arxiv.org/abs/2207.12891">[arXiv]</a>  <a href="https://openreview.net/forum?id=cB4N3G5udUS">[OpenReview]</a> <br>
<br>


<br>
(*) Members of my <a href="https://richtarik.org/i_team.html">Optimization and Machine Learning Lab</a> at KAUST.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>January 20, 2023</h3>

<h1>Visiting Rice University</h1>

I am giving a <a href="https://events.rice.edu/#!view/event/event_id/340417">CMOR Special Lecture</a> at Rice University today, and meeting several people, including
Shiqian Ma, Anastasios (Tasos) Kyrillidis,  Sebastian Perez-Salazar, Matthias Heinkenschloss, Eric Chi, Jingfeng Wu,
César A. Uribe,  Teng Zhang and Illya Hicks. Looking forward to the conversations!


<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 17, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2301.06806">
"Convergence of First-Order Algorithms for Meta-Learning with Moreau Envelopes"</a> -
joint work with
<a href="https://www.konstmish.com">Konstantin Mishchenko</a> and
<a href="https://slavomir-hanzely.github.io">Slavomír Hanzely</a>.


<br>
<br>
Abstract:
<i>
In this work, we consider the problem of minimizing the sum of Moreau envelopes of given functions, which has previously appeared in the context of meta-learning and personalized federated learning. In contrast to the existing theory that requires running subsolvers until a certain precision is reached, we only assume that a finite number of gradient steps is taken at each iteration. As a special case, our theory allows us to show the convergence of First-Order Model-Agnostic Meta-Learning (FO-MAML) to the vicinity of a solution of Moreau objective. We also study a more general family of first-order algorithms that can be viewed as a generalization of FO-MAML. Our main theoretical achievement is a theoretical improvement upon the inexact SGD framework. In particular, our perturbed-iterate analysis allows for tighter guarantees that improve the dependency on the problem's conditioning. In contrast to the related work on meta-learning, ours does not require any assumptions on the Hessian smoothness, and can leverage smoothness and convexity of the reformulation based on Moreau envelopes. Furthermore, to fill the gaps in the comparison of FO-MAML to the Implicit MAML (iMAML), we show that the objective of iMAML is neither smooth nor convex, implying that it has no convergence guarantees based on the existing theory.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 6, 2023</h3>

<h1>Paper Accepted to TMLR</h1>

The paper <a href="https://arxiv.org/abs/2002.03329">Better Theory for SGD in the Nonconvex World</a>, joint work with <a href="https://rka97.github.io">Ahmed Khaled</a>, was accepted to <a href="https://openreview.net/forum?id=AU4qHN2VkS&referrer=%5BTMLR%5D(%2Fgroup%3Fid%3DTMLR)">TMLR</a>. I have taught the key result from this paper in my CS331 (Stochastic Gradient Descent Methods) course at KAUST for a couple years already!

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>December 29, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2212.14370">
"Can 5th Generation Local Training Methods Support Client Sampling? Yes!"</a> -
joint work with
<a href="https://www.linkedin.com/in/michał-grudzień-2141a2198/?originalSubdomain=uk">Michał Grudzień</a> and
<a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky</a>.


<br>
<br>
Abstract:
<i>
The celebrated FedAvg algorithm of McMahan et al. (2017) is based on three components: client sampling (CS), data sampling (DS) and local training (LT). While the first two are reasonably well understood, the third component, whose role is to reduce the number of communication rounds needed to train the model, resisted all attempts at a satisfactory theoretical explanation. Malinovsky et al. (2022) identified four distinct generations of LT methods based on the quality of the provided theoretical communication complexity guarantees. Despite a lot of progress in this area, none of the existing works were able to show that it is theoretically better to employ multiple local gradient-type steps (i.e., to engage in LT) than to rely on a single local gradient-type step only in the important heterogeneous data regime. In a recent breakthrough embodied in their ProxSkip method and its theoretical analysis, Mishchenko et al. (2022) showed that LT indeed leads to provable communication acceleration for arbitrarily heterogeneous data, thus jump-starting the 5th generation of LT methods. However, while these latest generation LT methods are compatible with DS, none of them support CS. We resolve this open problem in the affirmative. In order to do so, we had to base our algorithmic development on new algorithmic and theoretical foundations.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>December 13, 2022</h3>

<h1>Vacation</h1>

I am on vacation until the end of the year. This includes the 3rd place World Cup match!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>December 3, 2022</h3>

<h1>Attending "Optimization in the Big Data Era" workshop in Singapore</h1>

I am flying to Singapore (my first time ever!) to attend the <a href="https://ims.nus.edu.sg/events/optimization-in-the-big-data-era/">Optimization in the Big Data Era</a> workshop at the  Institute for Mathematical Sciences, National University of Singapore, organized
by <a href="http://pages.cs.wisc.edu/~swright/">Stephen J. Wright</a> (University of Wisconsin), <a href="https://www.polyu.edu.hk/ama/profile/dfsun/">Defeng Sun</a> (The Hong Kong Polytechnic University) and <a href="http://ww1.math.nus.edu.sg/staff.aspx?s=mattohkc">Kim Chuan Toh</a>
(National University of Singapore). The event was planned long ago, but got delayed because of the Covid 19 situation. It's finally taking place now!

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>November 26, 2022</h3>

<h1>NeurIPS 2022 @ New Orleans</h1>

Several members of my lab are attending  the <a href="https://nips.cc/Conferences/2022">36th Annual Conference on Neural Information Processing Systems (NeurIPS 2022)</a> in New Orleans.
We have 12 papers accepted. In addition, we presented 3 workshop papers.

<br><br>
Our NeurIPS 2022 conference papers:
<br>
<br>

<br>
<img alt="KAUST Optimization and Machine Learning Lab" src="imgs/OML-richtarik-NeurIPS2022.jpeg" width="720">
<br>


<b> 1) "Accelerated Primal-Dual Gradient Method for Smooth and Convex-Concave Saddle-Point Problems with Bilinear Coupling"</b>
<a href="https://arxiv.org/abs/2112.15199">[arXiv]</a>  -
joint work with
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 2) "The First Optimal Algorithm for Smooth and Strongly-Convex-Strongly-Concave Minimax Optimization"</b>
<a href="https://arxiv.org/abs/2205.05653">[arXiv]</a>  -
the work of
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 3) "A Damped Newton Method Achieves Global $O(1/k^2)$ and Local Quadratic Convergence Rate"</b> -
joint work with
<a href="https://slavomir-hanzely.github.io/">Slavomir Hanzely (*)</a>
<a href="https://www.researchgate.net/profile/Dmitry-Kamzolov">Dmitry Kamzolov</a>
<a href="http://dmivilensky.ru/">Dmitry Pasechnyuk</a>
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>, and
<a href="https://mtakac.com/">Martin Takáč</a>.
<br>
<br>

<b> 4) "Variance Reduced ProxSkip: Algorithm, Theory and Application to Federated Learning"</b>
<a href="https://arxiv.org/abs/2207.04338">[arXiv]</a>  -
joint work with
<a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky (*)</a> and
<a href="https://kaiyi.me/">Kai Yi (*)</a>.
<br>
<br>

<b> 5) "Theoretically Better and Numerically Faster Distributed Optimization with Smoothness-Aware Quantization Techniques"</b>
<a href="https://arxiv.org/abs/2106.03524">[arXiv]</a>  -
joint work with
<a href="https://bokun-wang.github.io/">Bokun Wang (*)</a> and
<a href="https://mher-safaryan.github.io/">Mher Safaryan (*)</a>.
<br>
<br>

<b> 6) "Communication Acceleration of Local Gradient Methods via an Accelerated Primal-Dual Algorithm with an Inexact Prox"</b>
<a href="https://arxiv.org/abs/2207.03957">[arXiv]</a>  -
joint work with
<a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">Abdurakhmon Sadiev (*)</a> and
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a>.
<br>
<br>

<b> 7) "Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees"</b>
<a href="https://arxiv.org/abs/2110.03313">[arXiv]</a>  -
joint work with
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://www.hse.ru/en/staff/yhn112">Michael Diskin</a>,
<a href="https://scholar.google.com/citations?user=930PERsAAAAJ&hl=en">Max Ryabinin</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 8) "BEER: Fast $O(1/T)$ Rate for Decentralized Nonconvex Optimization with Communication Compression"</b>
<a href="https://arxiv.org/abs/2201.13320">[arXiv]</a>  -
joint work with
<a href="https://hyzhao.me/">Haoyu Zhao</a>,
<a href="https://www.lti.cs.cmu.edu/people/222218029/boyue-li">Boyue Li</a>,
<a href="https://zhizeli.github.io/">Zhize Li (*)</a> and
<a href="https://users.ece.cmu.edu/~yuejiec/">Yuejie Chi</a>.
<br>
<br>

<b> 9) "The First Optimal Acceleration of High-Order Methods in Smooth Convex Optimization"</b>
<a href="https://arxiv.org/abs/2205.09647">[arXiv]</a>  -
the work of
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 10) "Optimal Gradient Sliding and its Application to Optimal Distributed Optimization Under Similarity"</b>
<a href="https://arxiv.org/abs/2205.15136">[arXiv]</a>  -
the work of
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a>,
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://scholar.google.be/citations?user=9Dapoy8AAAAJ&hl=fr">Ekaterina Borodich</a>,
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a> and
<a href="https://engineering.purdue.edu/~gscutari/">Gesualdo Scutari</a>.
<br>
<br>

<b> 11) "Optimal Algorithms for Decentralized Stochastic Variational Inequalities"</b>
<a href="https://arxiv.org/abs/2202.02771">[arXiv]</a>  -
joint work with
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a>,
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">Abdurakhmon Sadiev (*)</a>,
<a href="https://engineering.purdue.edu/~gscutari/"> Michael Persiianov</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 12) "EF-BV: A Unified Theory of Error Feedback and Variance Reduction Mechanisms for Biased and Unbiased Compression in
Distributed Optimization"</b>
<a href="https://arxiv.org/abs/2205.04180">[arXiv]</a>  -
joint work with
<a href="https://lcondat.github.io/">Laurent Condat (*)</a> and
<a href="https://kaiyi.me/">Kai Yi (*)</a>.
<br>
<br>

<br><br>
(*) Members of my Optimization and Machine Learning Lab at KAUST.
<br><br>

<br><br>
Our NeurIPS 2022 workshop papers:
<br>
<br>

<b> 13) "Certified Robustness in Federated Learning" @ Federated Learning NeurIPS 2022 Workshop</b>
<a href="https://arxiv.org/abs/2206.02535">[arXiv]</a>  -
joint work with
<a href="https://motasemalfarra.netlify.app">Motasem Alfarra</a>,
<a href="https://www.juancprzs.com">Juan C. Pérez</a>,
<a href="https://shulgin-egor.github.io">Egor Shulgin</a>, and
<a href="https://www.bernardghanem.com">Bernard Ghanem</a>
<br>
<br>

<b> 14) "Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation" @ HOOML NeurIPS 2022 Workshop</b>
<a href="https://arxiv.org/abs/2206.03588">[arXiv]</a>  -
joint work with
<a href="https://rustem-islamov.github.io/">Rustem Islamov</a>,
<a href="https://qianxunk.github.io/">Xun Qian</a>,
<a href="https://slavomir-hanzely.github.io/">Slavomír Hanzely</a>, and
<a href="https://mher-safaryan.github.io/">Mher Safaryan</a>.
<br>
<br>

<b> 15) "RandProx: Primal-Dual Optimization Algorithms with Randomized Proximal Updates" @ OPT NeurIPS 2022 Workshop</b>
<a href="https://arxiv.org/abs/2207.12891">[arXiv]</a>  -
joint work with
<a href="https://lcondat.github.io/">Laurent Condat</a>.
<br>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>November 16, 2022</h3>

<h1>Teaching at Saudi Aramco</h1>

Just arrived to Al Khobar, a Saudi Arabian city on the Arabian Gulf. I will be teaching "Introduction to Optimization" in a
KAUST-Aramco MS program that started this year. My PhD students <a href="https://cemse.kaust.edu.sa/amcs/people/person/igor-sokolov">Igor Sokolov</a> and
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> are here with me as TAs. Four full days of teaching (6 hrs a day), and then four
more at the end of November. Should be fun!
</a>

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>November 9-10, 2022</h3>

<h1>Attending 2022 Workshop on Federated Learning and Analytics</h1>


As in the past years, this year I have again been invited to attend a workshop organized by the Google Federated Learning team entitled
"2022 Workshop on Federated Learning and Analytics". While this is an invite-only event, I can share my slides.

<br>
<a href="talks/TALK-2022-11-10-Google-FL-Workshop.pdf">
<img alt="" src="imgs/FL2022-leading-slide.png" width="700">
</a>
<br>

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>November 2, 2022</h3>

<h1>Teaching at Saudi Aramco</h1>

Just arrived to Al Khobar, a Saudi Arabian city on the Arabian Gulf. I will be teaching "Introduction to Optimization" in a
KAUST-Aramco MS program that started this year. My PhD students <a href="https://cemse.kaust.edu.sa/amcs/people/person/igor-sokolov">Igor Sokolov</a> and
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> are here with me as TAs. Four full days of teaching (6 hrs a day), and then four
more at the end of November. Should be fun!
</a>

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 31, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2211.00188">
"Adaptive Compression for Communication-Efficient Distributed Training"</a> -
joint work with
<a href="https://www.makarenko.co/">Maksim Makarenko</a>,
<a href="https://elnurgasanov.com/">Elnur Gasanov</a>,
<a href="https://rustem-islamov.github.io/">Rustem Islamov</a>, and
<a href="https://scholar.google.com/citations?user=g0CzD50AAAAJ&hl=en">Abdurakhmon Sadiev</a>.


<br>
<br>
Abstract:
<i>
We propose Adaptive Compressed Gradient Descent (AdaCGD) - a novel optimization algorithm for communication-efficient training of supervised machine learning models with adaptive compression level. Our approach is inspired by the recently proposed three point compressor (3PC) framework of Richtarik et al. (2022), which includes error feedback (EF21), lazily aggregated gradient (LAG), and their combination as special cases, and offers the current state-of-the-art rates for these methods under weak assumptions. While the above mechanisms offer a fixed compression level, or adapt between two extremes only, our proposal is to perform a much finer adaptation. In particular, we allow the user to choose any number of arbitrarily chosen contractive compression mechanisms, such as Top-K sparsification with a user-defined selection of sparsification levels K, or quantization with a user-defined selection of quantization levels, or their combination. AdaCGD chooses the appropriate compressor and compression level adaptively during the optimization process. Besides i) proposing a theoretically-grounded multi-adaptive communication compression mechanism, we further ii) extend the 3PC framework to bidirectional compression, i.e., we allow the server to compress as well, and iii) provide sharp convergence bounds in the strongly convex, convex and nonconvex settings. The convex regime results are new even for several key special cases of our general mechanism, including 3PC and EF21. In all regimes, our rates are superior compared to all existing adaptive compression methods.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 31, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2211.00140">
"A Damped Newton Method Achieves Global O(1/k^2) and Local Quadratic Convergence Rate"</a> -
joint work with
<a href="https://slavomir-hanzely.github.io">Slavomír Hanzely</a>,
<a href="https://scholar.google.com/citations?user=7ezSoS-hLVAC&hl=en">Dmitry Kamzolov</a>,
<a href="http://dmivilensky.ru">Dmitry Pasechnyuk</a>,
<a href="https://scholar.google.com/citations?user=AmeE8qkAAAAJ">Alexander Gasnikov</a>, and
<a href="https://mtakac.com">Martin Takáč</a>.


<br>
<br>
Abstract:
<i>
In this paper, we present the first stepsize schedule for Newton method resulting in fast global and local convergence guarantees. In particular, a) we prove an $O(1/k^2)$ global rate, which matches the state-of-the-art global rate of cubically regularized Newton method of Polyak and Nesterov (2006) and of regularized Newton method of Mishchenko (2021) and Doikov and Nesterov (2021), b) we prove a local quadratic rate, which matches the best-known local rate of second-order methods, and c) our stepsize formula is simple, explicit, and does not require solving any subproblem. Our convergence proofs hold under affine-invariance assumptions closely related to the notion of self-concordance. Finally, our method has competitive performance when compared to existing baselines, which share the same fast global convergence guarantees.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 28, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2210.16402">
"GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity"</a> -
joint work with
<a href="https://www.linkedin.com/in/arto-maranjyan/?originalSubdomain=am">Artavazd Maranjyan</a> and <a href="https://mher-safaryan.github.io">Mher Safaryan</a>.


<br>
<br>
Abstract:
<i>
In this work, we study distributed optimization algorithms that reduce the high communication costs of synchronization by allowing clients to perform multiple local gradient steps in each communication round. Recently, Mishchenko et al. (2022) proposed a new type of local method, called ProxSkip, that enjoys an accelerated communication complexity without any data similarity condition. However, their method requires all clients to call local gradient oracles with the same frequency. Because of statistical heterogeneity, we argue that clients with well-conditioned local problems should compute their local gradients less frequently than clients with ill-conditioned local problems. Our first contribution is the extension of the original ProxSkip method to the setup where clients are allowed to perform a different number of local gradient steps in each communication round. We prove that our modified method, GradSkip, still converges linearly, has the same accelerated communication complexity, and the required frequency for local gradient computations is proportional to the local condition number. Next, we generalize our method by extending the randomness of probabilistic alternations to arbitrary unbiased compression operators and considering a generic proximable regularizer. This generalization, GradSkip+, recovers several related methods in the literature. Finally, we present an empirical study to confirm our theoretical claims.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>October 24, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2210.13277">
"Provably Doubly Accelerated Federated Learning: The First Theoretically Successful Combination of Local Training and Compressed Communication"</a> -
joint work with
<a href="https://lcondat.github.io">Laurent Condat</a> and <a href="https://kinit.sk/member/ivan-agarsky/">Ivan Agarský</a>.


<br>
<br>
Abstract:
<i>
In the modern paradigm of federated learning, a large number of users are involved in a global learning task, in a collaborative way. They alternate local computations and two-way communication with a distant orchestrating server. Communication, which can be slow and costly, is the main bottleneck in this setting. To reduce the communication load and therefore accelerate distributed gradient descent, two strategies are popular: 1) communicate less frequently; that is, perform several iterations of local computations between the communication rounds; and 2) communicate compressed information instead of full-dimensional vectors. In this paper, we propose the first algorithm for distributed optimization and federated learning, which harnesses these two strategies jointly and converges linearly to an exact solution, with a doubly accelerated rate: our algorithm benefits from the two acceleration mechanisms provided by local training and compression, namely a better dependency on the condition number of the functions and on the dimension of the model, respectively.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 6, 2022</h3>

<h1>Visiting MBZUAI in Abu Dhabi</h1>

I am on my way to Abu Dhabi to visit <a href="https://mbzuai.ac.ae">MBZUAI</a>, where I am an Adjunct Professor.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 5, 2022</h3>

<h1>Talk at the One World Seminar Series on the Mathematics of Machine Learning</h1>

Today, I am  giving a virtual (Zoom) talk at the <a href="https://www.oneworldml.org/home">One World Seminar Series on the Mathematics of Machine Learning</a>.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>October 2, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2210.00462">
"Improved Stein Variational Gradient Descent with Importance Weights"</a> -
joint work with
<a href="https://lukangsun.github.io">Lukang Sun</a>.


<br>
<br>
Abstract:
<i>
Stein Variational Gradient Descent~(SVGD) is a popular sampling algorithm used in various machine learning tasks. It is well known that SVGD arises from a discretization of the kernelized gradient flow of the Kullback-Leibler divergence $D_{KL}(\cdot\mid\pi)$, where $\pi$ is the target distribution. In this work, we propose to enhance SVGD via the introduction of  importance weights, which leads to a new method for which we coin the name $\beta$-SVGD. In the continuous time and infinite particles regime, the time for this flow to converge to the equilibrium distribution $\pi$, quantified by the Stein Fisher information, depends on $\rho_0$ and $\pi$ very weakly. This is very different from the kernelized gradient flow of Kullback-Leibler divergence, whose time complexity depends on $D_{KL}(\rho_0\mid\pi)$. Under certain assumptions, we provide a descent lemma for the population limit $\beta$-SVGD, which covers the descent lemma for the population limit SVGD when $\beta\to 0$. We also illustrate the advantages of $\beta$-SVGD over SVGD by simple experiments.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>September 30, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2209.15218">
"EF21-P and Friends: Improved Theoretical Communication Complexity for Distributed Optimization with Bidirectional Compression"</a> -
joint work with
Kaja Gruntkowska and <a href="https://k3nfalt.github.io">Alexander Tyurin</a>.


<br>
<br>
Abstract:
<i>
The starting point of this paper is the discovery of a novel and simple error-feedback mechanism, which we call EF21-P, for dealing with the error introduced by a contractive compressor. Unlike all prior works on error feedback, where compression and correction operate in the dual space of gradients, our mechanism operates in the primal space of models. While we believe that EF21-P may be of interest in many situations where it is often advantageous to perform model perturbation prior to the computation of the gradient (e.g., randomized smoothing and generalization), in this work we focus our attention on its use as a key building block in the design of communication-efficient distributed optimization methods supporting bidirectional compression. In particular, we employ EF21-P as the mechanism for compressing and subsequently error-correcting the model broadcast by the server to the workers. By combining EF21-P with suitable methods performing worker-to-server compression, we obtain novel methods supporting bidirectional compression and enjoying new state-of-the-art theoretical communication complexity for convex and nonconvex problems. For example, our bounds are the first that manage to decouple the variance/error coming from the workers-to-server and server-to-workers compression, transforming a multiplicative dependence to an additive one. In the convex regime, we obtain the first bounds that match the theoretical communication complexity of gradient descent. Even in this convex regime, our algorithms work with biased gradient estimators, which is non-standard and requires new proof techniques that may be of independent interest. Finally, our theoretical results are corroborated through suitable experiments.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 29, 2022</h3>

<h1>ICLR 2023</h1>

The ICLR 2023 submission deadline is over. Time to rest for a day before moving on to AISTATS 2023...


<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 16, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2209.07883">
"Minibatch Stochastic Three Points Method for Unconstrained Smooth Minimization"</a> -
joint work with
<a href="https://www.linkedin.com/in/soumia-boucherouite/?originalSubdomain=ma">Soumia Boucherouite</a>,
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>,
and <a href="https://ehbergou.github.io">El Houcine Bergou</a>.


<br>
<br>
Abstract:
<i>
In this paper, we propose a new zero order optimization method called minibatch stochastic three points (MiSTP) method to solve an unconstrained minimization problem in a setting where only an approximation of the objective function evaluation is possible. It is based on the recently proposed stochastic three points (STP) method (Bergou et al., 2020). At each iteration, MiSTP generates a random search direction in a similar manner to STP, but chooses the next iterate based solely on the approximation of the objective function rather than its exact evaluations. We also analyze our method's complexity in the nonconvex and convex cases and evaluate its performance on multiple machine learning tasks.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 15, 2021</h3>
<h1>Papers Accepted to NeurIPS 2022</h1>
<br>
We've had several papers accepted to the <a href="https://nips.cc/Conferences/2022">36th Annual Conference on Neural Information Processing Systems (NeurIPS 2022)</a>, which will run during November 28-December 3, 2022 in New Orleans, USA.

<br><br>
Here they are:

<br>
<br>
<img alt="KAUST Optimization and Machine Learning Lab" src="imgs/OML-richtarik-NeurIPS2022.jpeg" width="720">
<br>
<br>


<b> 1) "Accelerated Primal-Dual Gradient Method for Smooth and Convex-Concave Saddle-Point Problems with Bilinear Coupling"</b>
<a href="https://arxiv.org/abs/2112.15199">[arXiv]</a>  -
joint work with
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 2) "The First Optimal Algorithm for Smooth and Strongly-Convex-Strongly-Concave Minimax Optimization"</b>
<a href="https://arxiv.org/abs/2205.05653">[arXiv]</a>  -
the work of
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 3) "A Damped Newton Method Achieves Global $O(1/k^2)$ and Local Quadratic Convergence Rate"</b> -
joint work with
<a href="https://slavomir-hanzely.github.io/">Slavomir Hanzely (*)</a>
<a href="https://www.researchgate.net/profile/Dmitry-Kamzolov">Dmitry Kamzolov</a>
<a href="http://dmivilensky.ru/">Dmitry Pasechnyuk</a>
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>, and
<a href="https://mtakac.com/">Martin Takáč</a>.
<br>
<br>

<b> 4) "Variance Reduced ProxSkip: Algorithm, Theory and Application to Federated Learning"</b>
<a href="https://arxiv.org/abs/2207.04338">[arXiv]</a>  -
joint work with
<a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky (*)</a> and
<a href="https://kaiyi.me/">Kai Yi (*)</a>.
<br>
<br>

<b> 5) "Theoretically Better and Numerically Faster Distributed Optimization with Smoothness-Aware Quantization Techniques"</b>
<a href="https://arxiv.org/abs/2106.03524">[arXiv]</a>  -
joint work with
<a href="https://bokun-wang.github.io/">Bokun Wang (*)</a> and
<a href="https://mher-safaryan.github.io/">Mher Safaryan (*)</a>.
<br>
<br>

<b> 6) "Communication Acceleration of Local Gradient Methods via an Accelerated Primal-Dual Algorithm with an Inexact Prox"</b>
<a href="https://arxiv.org/abs/2207.03957">[arXiv]</a>  -
joint work with
<a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">Abdurakhmon Sadiev (*)</a> and
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a>.
<br>
<br>

<b> 7) "Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees"</b>
<a href="https://arxiv.org/abs/2110.03313">[arXiv]</a>  -
joint work with
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://www.hse.ru/en/staff/yhn112">Michael Diskin</a>,
<a href="https://scholar.google.com/citations?user=930PERsAAAAJ&hl=en">Max Ryabinin</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 8) "BEER: Fast $O(1/T)$ Rate for Decentralized Nonconvex Optimization with Communication Compression"</b>
<a href="https://arxiv.org/abs/2201.13320">[arXiv]</a>  -
joint work with
<a href="https://hyzhao.me/">Haoyu Zhao</a>,
<a href="https://www.lti.cs.cmu.edu/people/222218029/boyue-li">Boyue Li</a>,
<a href="https://zhizeli.github.io/">Zhize Li (*)</a> and
<a href="https://users.ece.cmu.edu/~yuejiec/">Yuejie Chi</a>.
<br>
<br>

<b> 9) "The First Optimal Acceleration of High-Order Methods in Smooth Convex Optimization"</b>
<a href="https://arxiv.org/abs/2205.09647">[arXiv]</a>  -
the work of
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 10) "Optimal Gradient Sliding and its Application to Optimal Distributed Optimization Under Similarity"</b>
<a href="https://arxiv.org/abs/2205.15136">[arXiv]</a>  -
the work of
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a>,
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://scholar.google.be/citations?user=9Dapoy8AAAAJ&hl=fr">Ekaterina Borodich</a>,
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a> and
<a href="https://engineering.purdue.edu/~gscutari/">Gesualdo Scutari</a>.
<br>
<br>

<b> 11) "Optimal Algorithms for Decentralized Stochastic Variational Inequalities"</b>
<a href="https://arxiv.org/abs/2202.02771">[arXiv]</a>  -
joint work with
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a>,
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">Abdurakhmon Sadiev (*)</a>,
<a href="https://engineering.purdue.edu/~gscutari/"> Michael Persiianov</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 12) "EF-BV: A Unified Theory of Error Feedback and Variance Reduction Mechanisms for Biased and Unbiased Compression in
Distributed Optimization"</b>
<a href="https://arxiv.org/abs/2205.04180">[arXiv]</a>  -
joint work with
<a href="https://lcondat.github.io/">Laurent Condat (*)</a> and
<a href="https://kaiyi.me/">Kai Yi (*)</a>.
<br>
<br>

<br><br>
(*) Members of my Optimization and Machine Learning Lab at KAUST.
<br><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>September 13, 2022</h3>

<h1>Talk at the Mathematics and Applications Colloquium</h1>

I am giving a talk at the <a href="https://cemse.kaust.edu.sa/events/event/resolution-theoretical-question-related-nature-local-training-federated-learning">Mathematics and Applications Colloquium</a> at KAUST.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 12, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2209.05148">
"Personalized Federated Learning with Communication Compression"</a> -
joint work with
<a href="https://ehbergou.github.io">El Houcine Bergou</a>,
<a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a>,
and <a href="http://www.aritradutta.com">Aritra Dutta</a>.


<br>
<br>
Abstract:
<i>
In contrast to training traditional machine learning (ML) models in data centers, federated learning (FL) trains ML models over local datasets contained on resource-constrained heterogeneous edge devices. Existing FL algorithms aim to learn a single global model for all participating devices, which may not be helpful to all devices participating in the training due to the heterogeneity of the data across the devices. Recently, Hanzely and Richtárik (2020) proposed a new formulation for training personalized FL models aimed at balancing the trade-off between the traditional global model and the local models that could be trained by individual devices using their private data only. They derived a new algorithm, called Loopless Gradient Descent (L2GD), to solve it and showed that this algorithms leads to improved communication complexity guarantees in regimes when more personalization is required. In this paper, we equip their L2GD algorithm with a bidirectional compression mechanism to further reduce the communication bottleneck between the local devices and the server. Unlike other compression-based algorithms used in the FL-setting, our compressed L2GD algorithm operates on a probabilistic communication protocol, where communication does not happen on a fixed schedule. Moreover, our compressed L2GD algorithm maintains a similar convergence rate as vanilla SGD without compression. To empirically validate the efficiency of our algorithm, we perform diverse numerical experiments on both convex and non-convex problems and using various compression techniques.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 28, 2022</h3>
<h1>The Fall 2022 Semester Begins!</h1>
<br>


I am back at KAUST - the Fall semester begins! I am teaching CS 331: Stochastic Gradient Descent Methods.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 15, 2022</h3>
<h1>New Research Intern: Wenzi (Tom) Fang</h1>
<br>


I am hereby welcoming <a href="https://wenzhifang.github.io">Wenzhi (Tom) Fang</a> to my team as a (remote) VS research intern! His internship started today.
Tom is an MS student in Communication and Information Systems at University of the Chinese Academy of Sciences / ShanghaiTech University.
He obtained a BS degree in Communication Engineering from Shanghai University. In the past, his research focus was on optimization of the
physical layer of wireless communication. At present, he is more interested in optimization theory and federated learning.
<br>
<br>

Tom co-authored several papers, including:
<ul>
<li>W. Fang, Y. Jiang, Y. Shi, Y. Zhou, W. Chen, and K. Letaief, “Over-the-Air Computation via Reconfigurable Intelligent Surface,” IEEE Transactions on Communications, vol. 69, no. 12, pp. 8612-8626, Dec. 2021</li>
<li>W. Fang, Y. Zou, H. Zhu, Y. Shi, and Y. Zhou, “Optimal Receive Beamforming for Over-the-Air Computation,” in Proc. IEEE SPAWC, Virtual Conferences, Sept. 2021</li>
<li>W. Fang, M. Fu, K. Wang, Y. Shi, and Y. Zhou, “Stochastic Beamforming for Reconfigurable Intelligent Surface Aided Over-the-Air Computation,” in Proc. IEEE Globecom, Virtual Conference, Dec. 2020.</li>
<li>W. Fang, M. Fu, Y. Shi, and Y. Zhou, “Outage Minimization for Intelligent Reflecting Surface Aided MISO Communication Systems via Stochastic Beamforming,” in Proc. IEEE SAM, Virtual Conference, Jun. 2020.</li>
<li>W. Fang, Ziyi Yu, Yuning Jiang, Yuanming Shi, Colin N. Jones, and Yong Zhou, “Communication-Efficient Stochastic Zeroth-Order Optimization for Federated Learning,” submitted to IEEE Transactions on Signal Processing</li>
</ul>


<br>
In 2021, Tom won a China National Scholarship (0.2% acceptance rate nationwide). In 2017 he won a 1st prize in the China National Undergraduate
Electronic Design Competition.

<br>
<br>

Here is Tom's:
<ul>
<li><a href="https://scholar.google.com/citations?user=XdUxykMAAAAJ&hl=en">Google Scholar Profile</a></li>
<li><a href="https://wenzhifang.github.io">Website</a></li>
<li><a href="https://wenzhifang.github.io/photos/CV_fangwenzhi.pdf">CV</a></li>
</ul>



<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 10, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2208.05287">
"Adaptive Learning Rates for Faster Stochastic Gradient Methods"</a> -
joint work with
<a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth</a> and <a href="https://www.konstmish.com">Konstantin Mishchenko</a>.


<br>
<br>
Abstract:
<i>
In this work, we propose new adaptive step size strategies that improve several stochastic gradient methods. Our first method (StoPS) is based on the classical Polyak step size (Polyak, 1987) and is an extension of the recent development of this method for the stochastic optimization-SPS (Loizou et al., 2021), and our second method, denoted GraDS, rescales step size by "diversity of stochastic gradients". We provide a theoretical analysis of these methods for strongly convex smooth functions and show they enjoy deterministic-like rates despite stochastic gradients. Furthermore, we demonstrate the theoretical superiority of our adaptive methods on quadratic objectives. Unfortunately, both StoPS and GraDS depend on unknown quantities, which are only practical for the overparametrized models. To remedy this, we drop this undesired dependence and redefine StoPS and GraDS to StoP and GraD, respectively. We show that these new methods converge linearly to the neighbourhood of the optimal solution under the same assumptions. Finally, we corroborate our theoretical claims by experimental validation, which reveals that GraD is particularly useful for deep learning optimization.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>July 30, 2022</h3>
<h1>On my way back to KAUST</h1>
<br>


I am now on my way back to KAUST. I'll stay for about a week and then take up some vacation.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>July 26, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2207.12891">
"RandProx: Primal-Dual Optimization Algorithms with Randomized Proximal Updates"</a> -
joint work with
<a href="https://lcondat.github.io">Laurent Condat</a>.


<br>
<br>
Abstract:
<i>
Proximal splitting algorithms are well suited to solving large-scale nonsmooth optimization problems, in particular those arising in machine learning. We propose a new primal-dual algorithm, in which the dual update is randomized; equivalently, the proximity operator of one of the function in the problem is replaced by a stochastic oracle. For instance, some randomly chosen dual variables, instead of all, are updated at each iteration. Or, the proximity operator of a function is called with some small probability only. A nonsmooth variance-reduction technique is implemented so that the algorithm finds an exact minimizer of the general problem involving smooth and nonsmooth functions, possibly composed with linear operators. We derive linear convergence results in presence of strong convexity; these results are new even in the deterministic case, when our algorithms reverts to the recently proposed Primal-Dual Davis-Yin algorithm. Some randomized algorithms of the literature are also recovered as particular cases (e.g., Point-SAGA). But our randomization technique is general and encompasses many unbiased mechanisms beyond sampling and probabilistic updates, including compression. Since the convergence speed depends on the slowest among the primal and dual contraction mechanisms, the iteration complexity might remain the same when randomness is used. On the other hand, the computation complexity can be significantly reduced. Overall, randomness helps getting faster algorithms. This has long been known for stochastic-gradient-type algorithms, and our work shows that this fully applies in the more general primal-dual setting as well.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 14, 2022</h3>

<h1>11 Team Members Among Top 10% ICML 2022 Reviewers!</h1>

Thanks to my former and current team members Konstantin Mishchenko, Rafał Szlendak, Samuel Horváth, Igor Sokolov,
Alexander Tyurin, Abdurakhmon Sadiev, Laurent Condat, Ahmed Khaled, Eduard Gorbunov, Elnur Gasanov and Egor Shulgin for their
excellet reviewing efforts for ICML 2022 which resulted in them being recognized as <a href="https://icml.cc/Conferences/2022/Reviewers">Outstanding
(Top 10%) Reviewers at ICML 2022!</a>

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>July 12, 2022</h3>

<h1>New York, Baltimore, Houston and Los Angeles</h1>

Starting today, I'll be  on a tour of the US, giving a few talks and visiting/attending several places and conferences,
including the Flatiron Institute in New York City, ICML in Baltimore, Rice University in Houston, and Los Angeles. <br> <br>

At ICML,
I gave two talks: one on our ProxSkip paper "ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!" [<a href="https://proceedings.mlr.press/v162/mishchenko22b.html">paper</a>] [<a href="slides/ProxSkip-ICML-slides.pdf">slides</a>] [poster]
[<a href="https://www.youtube.com/watch?v=OMVSzgsd7ZY">71 min talk</a>] [<a href="https://www.youtube.com/watch?v=yIF_l2g8rGY">90 min talk</a>], and the other one on our
3PC paper "3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation" [<a href="https://proceedings.mlr.press/v162/richtarik22a.html">paper</a>] [<a href="https://icml.cc/media/PosterPDFs/ICML%202022/b6417f112bd27848533e54885b66c288_r5C1iLn.png">poster</a>] [<a href="slides/3PC-ICML-slides.pdf">slides</a>].
My team had three more accepted papers: "A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1" presented by <a href="https://adil-salim.github.io">Adil Salim</a>
[<a href="https://proceedings.mlr.press/v162/salim22a.html">paper</a>] [<a href="https://icml.cc/media/icml-2022/Slides/17594.pdf">slides</a>],
"FedNL: Making Newton-Type Methods Applicable to Federated Learning" presented by <a href="https://mher-safaryan.github.io">Mher Safaryan</a> [<a href="https://proceedings.mlr.press/v162/safaryan22a.html">paper</a>] [<a href="https://icml.cc/media/PosterPDFs/ICML%202022/7385db9a3f11415bc0e9e2625fae3734.png">poster</a>] and
"Proximal and Federated Random Reshuffling" presented by <a href="https://www.akhaled.org">Ahmed Khaled</a> [<a href="https://proceedings.mlr.press/v162/mishchenko22a.html">paper</a>].
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 9, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2207.04338">
"Variance Reduced ProxSkip: Algorithm, Theory and Application to Federated Learning"</a> -
joint work with
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> and <a href="https://kaiyi.me">Kai Yi</a>.


<br>
<br>
Abstract:
<i>
We study distributed optimization methods based on the local training (LT) paradigm: achieving communication efficiency
by performing richer local gradient-based training on the clients before  parameter averaging.  Looking back at the progress
of the field, we identify 5 generations of LT methods: 1) heuristic, 2) homogeneous, 3) sublinear, 4) linear, and 5) accelerated.
The 5th generation, initiated by the ProxSkip method of Mishchenko, Malinovsky, Stich and Richtárik (2022) and its analysis,
is characterized by the first theoretical confirmation that  LT is a communication  acceleration mechanism. Inspired by this recent
progress, we contribute to the 5th generation of LT methods by showing that it is possible to enhance them further using
variance reduction. While all previous theoretical results for LT methods ignore the cost of local work altogether, and are framed
purely in terms of the number of communication rounds, we show that our methods can be substantially faster in terms of the
total training cost than the state-of-the-art method ProxSkip in theory and practice in the regime when local computation is
sufficiently expensive. We characterize this threshold theoretically, and confirm our theoretical predictions with empirical results.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 7, 2022</h3>
<h1>New Research Intern: Michał Grudzień (Oxford)</h1>
<br>


Let us all welcome <a href="https://www.linkedin.com/in/michał-grudzień-2141a2198/?originalSubdomain=uk">Michał Grudzień</a> to the team as a new VSRP research intern! Michal arrived to KAUST yesterday.
Michał tudies toward an MA degree in Mathematics and Statistics at Oxford University. He is the recipient of the
<a href="https://www.ox.ac.uk/admissions/undergraduate/fees-and-funding/oxford-support/palgrave-brown-scholarship-non-uk">Palgrave Brown Scholarship</a>. Prior to this, Michal studied in Warsaw, Poland, at the famous Stanislaw Staszic Lyceum.
Some of Michał's achievements:
<ul>
<li>Member of a project for Oxford "Engineers without borders" society (classification and segmentation on Kaggle), 2021-now </li>
<li> Research Internship with Yaodong Yang on bilevel optimization, 2021</li>
<li>Finalist and Laureate in National Mathematical Olympiad, Poland, 2018 and 2019</li>
<li>Volunteer for Women in Tech Summit II, 2018-2019</li>
<li>Semi-Finalist in the Physics Olympiad, Poland, 2018</li>
<li>Laureate and Finalist in Junior Mathematical Olympiad, Poland, 2016 and 2017</li>
</ul>

Michał has an "obsessive passion for machine learning and data analysis". Besides being native in Polish and fluent in English,
he speaks a bit of Japanese, Chinese and Spanish. Also interestingly, Michal watched 300 anime shows in middle school. So, open with
this subject at your own peril! Most importantly though, Michał can benchpress 125kg. I need a proof of that though. Who volunteers
to join him in the Gym and video-tape this super-human feat?

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 6, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2207.03957">
"Communication Acceleration of Local Gradient Methods via an Accelerated Primal-Dual Algorithm with Inexact Prox"</a> -
joint work with
<a href="https://scholar.google.com/citations?user=R-xZRIAAAAAJ&hl=ru">Abdurakhmon Sadiev</a> and <a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>.


<br>
<br>
Abstract:
<i>
  Inspired by a recent breakthrough of Mishchenko et al. (2022), who for the first time showed that local gradient steps
  can lead to provable communication acceleration, we propose an alternative algorithm which obtains the same communication
  acceleration as their method (ProxSkip). Our approach is very different, however: it is based on the celebrated method of
  Chambolle and Pock (2011), with several nontrivial modifications: i) we allow for an inexact computation of the prox operator
  of a certain smooth strongly convex function via a suitable gradient-based method (e.g., GD, Fast GD or FSFOM), ii) we perform
  a careful modification of the dual update step in order to retain linear convergence. Our general results offer the new
  state-of-the-art rates for the class of strongly convex-concave saddle-point problems with bilinear coupling characterized
  by the absence of smoothness in the dual function. When applied to federated learning, we obtain a theoretically better
  alternative to ProxSkip: our method requires fewer local steps (O(κ^{1/3}) or O(κ^{1/4}), compared to O(κ^{1/2}) of ProxSkip),
  and performs a deterministic number of local steps instead. Like ProxSkip, our method can be applied to optimization over a
  connected network, and we obtain theoretical improvements here as well.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 27, 2022</h3>

<h1>Samuel Horváth defended his PhD thesis today!</h1>

Today, my PhD student <a href="https://samuelhorvath.github.io">Samuel Horváth</a>
<br>
<br>
<img alt="Dr Samuel Horváth" src="imgs/Samuel.jpg" width="200">
<br>
<br>

defended his PhD thesis entitled <a href="https://arxiv.org/abs/2207.00392">Better Methods and Theory for Federated Learning: Compression, Client Selection, and Heterogeneity.</a> <br><br>


His PhD thesis committee members were:
<a href="https://www.avestimehr.com/">Salman Avestimehr</a> (University of Southern California and FedML), <a href="https://mcanini.github.io/">Marco Canini</a> (KAUST),
<a href="https://www.kaust.edu.sa/en/study/faculty/marc-genton">Marc G. Genton</a> (KAUST),
<a href="https://ai.facebook.com/people/michael-rabbat">Mike Rabbat</a> (Facebook AI Research) and myself as the advisor and committee chair.
<br>

<br>
<img alt="" src="imgs/Samuel-defense-Zoom.png" width="700">
<br>
<br>

Samuel joined my <a href="https://richtarik.org/i_team.html">Optimization and Machine Learning Lab</a> at KAUST in August 2017 as an MS/PhD student, right after completing his BS in Financial Mathematics at <a href="https://uniba.sk/en/">Comenius University, Bratislava,
Slovakia</a>, ranked #1 in the program. He obtained his MS degree in Statistics at KAUST in December 2018.

<br>
<br>
Interestingly, Samuel came to KAUST just 5 months after I joined KAUST in March 2017!
<br>
<br>

Samuel has gone a long way since then; <a href="https://samuelhorvath.github.io/files/cv.pdf">look at his CV</a>!  His post-2017 accomplishments include:
<br>
<br>

<ul>
<li> Accepted the position of an <b>Assistant Prof</b> at <a href="https://mbzuai.ac.ae">MBZUAI, Abu Dhabi, United Arab Emirates</a>, about to start in Fall 2022</li>
<li> Co-authored <a href="https://scholar.google.com/citations?user=k252J7kAAAAJ">20 papers during his PhD</a></li>
<li> h index = 11, with 600+ citations according to <a href="https://scholar.google.com/citations?user=k252J7kAAAAJ">Google Scholar</a> </li>
<li> 3 industrial internships during his PhD (Amazon AWS, Samsung AI and Facebook) </li>
<li> <b>Al‐Kindi Statistics Research Student Award</b> (research award for top Statistics PhD student at KAUST), 2021 </li>
<li> <b>Spotlight Paper at NeurIPS 2021</b> for the first-author paper <a href="https://openreview.net/forum?id=4fLr7H5D_eT">FjORD: Fair and Accurate Federated Learning Under Heterogeneous Targets with Ordered Dropout</a> </li>
<li> Progress Towards PhD rated as “Outstanding”, KAUST, in years 2019, 2020 and 2021 </li>
<li> <b>Best Paper Award (+ 1,888 USD cash prize) at the NeurIPS 2020 Spicy Federated Learning Workshop</b> for the paper <a href="https://openreview.net/pdf?id=vYVI1CHPaQg">A Better Alternative to Error Feedback for Communication‐Efficient Distributed Learning</a>, later published in ICLR 2021</li>
<li> <b>Best Reviewer Award (Top 10%) @ NeurIPS 2020</b> </li>
<li> <b>Spotlight paper at the NeuriPS 2020 Optimization for Machine Learning Workshop</b> for the first-authored paper <a href="https://epubs.siam.org/doi/abs/10.1137/21M1394308?journalCode=sjmdaq">Adaptivity of Stochastic Gradient Methods for Nonconvex Optimization</a>, later published by SIAM J on Mathematics of Data Science </a>
<li> <b>Oral Paper at the NeuriPS 2020 Workshop on Scalability, Privacy, and Security in Federated Learning</b> for the paper <a href="https://arxiv.org/abs/2002.12410">On Biased Compression for Distributed Learning</a> </li>
<li> Accepted to Machine Learning Summer School (MLSS) 2020, Tübingen (acceptance rate 180/1300+); also accepted to MLSS 2020 Indonesia </li>
<li> <b>Top Reviewer Award @ NeurIPS 2019</b> </li>
<li> <b>Best Poster Prize</b> for the paper <a href="https://arxiv.org/abs/1904.05115">Stochastic Distributed Learning with Gradient Quantization and Variance Reduction</a> at the Control, Information and Optimization Summer School, Voronovo, Russia (organized by B. Polyak). The poster was presented by coauthor D. Kovalev.
<li> 157th/4049, IEEEXtreme 24‐Hour Programming Competition 12.0 joint with Dmitry Kovalev, 2018 </li>
<li> <b>Best Poster Prize</b> (+ 500 EUR cash prize) for a poster based on the paper <a href="http://proceedings.mlr.press/v97/horvath19a.html">Nonconvex Variance Reduced Optimization with Arbitrary Sampling</a>, awarded as one of 2 out of total 170 posters presented at the Data Science Summer School (DS3), École Polytechnique, 2018  </li>
<li> 131st /3,350 place, IEEEXtreme 24‐Hour Programming Competition 11.0 joint with Konstantin Mishchenko, 2017 </li>
</ul>

<br>
Samuel is very unique in that he is at the same time highly talented (in science and sports), hard-working, has impeccable work ethics, and is incredibly humble and down-to-earth. An amazing person to work with!

<br> <br>
Dr Samuel Horváth, and soon-to-become Prof Samuel Horváth: all the best in your new adventures in Abu Dhabi!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>June 21, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.10452">
"Shifted Compression Framework: Generalizations and Improvements"</a> - joint work with
<a href="https://shulgin-egor.github.io">Egor Shulgin</a>. An earlier version of this paper appeared
in <a href="https://opt-ml.org/papers/2021/paper20.pdf">OPT2021: 13th Annual Workshop on Optimization for Machine Learning.</a>
The final version was recently accepted to the <a href="https://www.auai.org/uai2022/">38th Conference on Uncertainty in Artificial Intelligence (UAI 2022)</a>.


<br>
<br>
Abstract:
<i>
Communication is one of the key bottlenecks in the distributed training of large-scale machine learning models, and lossy compression of exchanged information, such as stochastic gradients or models, is one of the most effective instruments to alleviate this issue. Among the most studied compression techniques is the class of unbiased compression operators with variance bounded by a multiple of the square norm of the vector we wish to compress. By design, this variance may remain high, and only diminishes if the input vector approaches zero. However, unless the model being trained is overparameterized, there is no a-priori reason for the vectors we wish to compress to approach zero during the iterations of classical methods such as distributed compressed SGD, which has adverse effects on the convergence speed. Due to this issue, several more elaborate and seemingly very different algorithms have been proposed recently, with the goal of circumventing this issue. These methods are based on the idea of compressing the difference between the vector we would normally wish to compress and some auxiliary vector which changes throughout the iterative process. In this work we take a step back, and develop a unified framework for studying such methods, conceptually, and theoretically. Our framework incorporates methods compressing both gradients and models, using unbiased and biased compressors, and sheds light on the construction of the auxiliary vectors. Furthermore, our general framework can lead to the improvement of several existing algorithms, and can produce new algorithms. Finally, we performed several numerical experiments which illustrate and support our theoretical findings.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>June 20, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.09709">
"A Note on the Convergence of Mirrored Stein Variational Gradient Descent under (L0,L1)−Smoothness Condition"</a> - joint work with
<a href="https://lukangsun.github.io/">Lukang Sun</a>.


<br>
<br>
Abstract:
<i>
In this note, we establish a descent lemma for the population limit Mirrored Stein Variational Gradient Method (MSVGD).
This descent lemma does not rely on the path information of MSVGD but rather on a simple assumption for the mirrored distribution.
Our analysis demonstrates that MSVGD can be applied to a broader class of constrained sampling problems with non-smooth V. We also
investigate the complexity of the population limit MSVGD in terms of dimension d.</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 18, 2022</h3>
<h1>New Research Intern: Omar Shaikh Omar (U of Washington)</h1>
<br>

Please join me in welcoming <a href="https://www.linkedin.com/in/omar-shaikh-omar-998970176/">Omar Shaikh Omar</a> to our team as a <a href="https://kgsp.kaust.edu.sa">KGSP (KAUST Gifted Student Program)</a>
research intern! Omar just arrived to KAUST and his KGSP orientation is tomorrow! Omar is a CS undergraduate student at the <a href="https://www.washington.edu">University of Washington</a>.
<br>
<br>
Selected distinctions, experience & interests:
<ul>
<li>Student researcher at the University of Washington STAR Lab (http://www.uwstarlab.org), 2021</li>
<li>Deployed two implementations of the YOLO v4 on Jetson Nano and used models on video streams of traﬀic intersections to detect pedestrians both crossing and waiting to cross </li>
<li>Trained custom YOLO v4 model for pedestrian detection using data from the Caltech Pedestrian Detection Benchmark and the MIOVision Traﬀic Camera Dataset </li>
<li>Student researcher at the Washington eXperimental Mathematics Lab (WXML), 2020-2021 </li>
<li>Constructed guesses for compact regions based on a subset of the roots of complex polynomials to find a local analog of the Gauss-Lucas Theorem and disproved the eligibility of multiple guesses </li>
<li>Built a Python library that simulates the process of finding counterexamples for guesses and creates animations of the process using matplotlib, scipy, and sympy </li>
<li>Presented at the Brown University’s Symposium for Undergraduates in the Mathematical Sciences (SUMS), and at the WXML conference </li>
<li>Data Science Intern at Uliza (https://www.uliza.org), 2020</li>
<li>Developed the idea for a spell checker for the African language Luganda with a team of programmers and linguists </li>
<li>Wrote several specification documents for data collection and a terminal interface </li>
<li>Scrapped the Luganda Bible and an English/Luganda dictionary for a total of 80,000+ unique words and 70,000+ sentences </li>
<li>Built core spell checker in Python by connecting an nltk part of speech tagger, a lemmatizer built specifically for Luganda, and an edit-distance suggestion system </li>
<li>Top 500 among US and Canada Universities, Putnam Mathematical Competition, 2019</li>
<li>Volunteer Teaching Assistance at Math Circle, 2019-2020 </li>
<li>Bronze medal in the Balkan Mathematical Olympiad, 2018</li>
</ul>


<br>
Omar, welcome!
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>June 16, 2022</h3>
<h1>New Research Intern: Kaja Gruntkowska (Warwick)</h1>
<br>

Please join me in welcoming Kaja Gruntkowska to the team as a <a href="https://vsrp.kaust.edu.sa/about-vsrp">VSRP intern</a>!!!
Kaja is a 3rd year undergraduate student of
<a href="https://warwick.ac.uk/study/undergraduate/courses/mathsstatsbsc/">Mathematics and Statistics at the University of Warwick, UK</a>.
Kaja will arrive at KAUST later this month. A few remarks about Kaja's current and past experience and successes:

<ul>
<li>one of the top students in her cohort at Warwick (top 3 in 2020-2021)</li>
<li>received the Academic Excellence Prize for overall performance at Warwick, 2020-2021</li>
<li>was a finalist of Polish Statistics Olympiad – 1st place winner on voivodeship stage, 2019</li>
<li>won 2nd place at "First Step to Fields Medal", 2019</li>
<li>won 3rd place at Jagiellonian Mathematical Tournament, 2018</li>
<li>participated (twice) at the Congress of Young Polish Mathematicians, 2016 and 2018</li>
<li>co-organized team Lodz for ‘Naboj Junior’, 2018</li>
<li>led extracurricular Maths classes at Lodz University of Technology High School, 2018-2019 </li>
</ul>



Kaja likes tutoring high school students in mathematics, acts as a mentor coordinator at Warwick, and is interested in sport and food sustainability.

<br>

<br>
Kaja, welcome!
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 15, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.07021">
"Federated Optimization Algorithms with Random Reshuffling and Gradient Compression"</a> - joint work with
<a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">Abdurakhmon Sadiev</a>,
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>,
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>,
<a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a>,
<a href="https://rka97.github.io">Ahmed Khaled</a>,
and
<a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a>.

<br>
<br>
Abstract:
<i>
Gradient compression is a popular technique for improving communication complexity of stochastic first-order methods in distributed training of machine learning models. However, the existing works consider only with-replacement sampling of stochastic gradients. In contrast, it is well-known in practice and recently confirmed in theory that stochastic methods based on without-replacement sampling, e.g., Random Reshuffling (RR) method, perform better than ones that sample the gradients with-replacement. In this work, we close this gap in the literature and provide the first analysis of methods with gradient compression and without-replacement sampling. We first develop a distributed variant of random reshuffling with gradient compression (Q-RR), and show how to reduce the variance coming from gradient quantization through the use of control iterates. Next, to have a better fit to Federated Learning applications, we incorporate local computation and propose a variant of Q-RR called Q-NASTYA. Q-NASTYA uses local gradient steps and different local and global stepsizes. Next, we show how to reduce compression variance in this setting as well. Finally, we prove the convergence results for the proposed methods and outline several settings in which they improve upon existing algorithms.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 12, 2022</h3>

<h1>Conference on the Mathematics of Complex Data</h1>

I have just arrived to Stockholm, Sweden, to attend the <a href="https://www.mathdatalab.org/">Conference on the Mathematics of Complex Data.</a>
This event was supposed to happen in 2020, but as postponed due to the Covid-19 pandemic. My talk is scheduled for Thursday, June 16.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 7, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.03588">
"Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation"</a> - joint work with
<a href="https://rustem-islamov.github.io">Rustem Islamov</a>, <a href="https://qianxunk.github.io">Xun Qian</a>, <a href="https://slavomir-hanzely.github.io">Slavomír Hanzely</a> and <a href="https://mher-safaryan.github.io">Mher Safaryan</a>.

<br>
<br>
Abstract:
<i>
Despite their high computation and communication costs, Newton-type methods remain an appealing option for distributed training due to their robustness against ill-conditioned convex problems. In this work, we study ommunication compression and aggregation mechanisms for curvature information in order to reduce these costs while preserving theoretically superior local convergence guarantees. We prove that the recently developed class of three point compressors (3PC) of Richtarik et al. [2022] for gradient communication can be generalized to Hessian communication as well. This result opens up a wide variety of communication strategies, such as contractive compression} and lazy aggregation, available to our disposal to compress prohibitively costly curvature information. Moreover, we discovered several new 3PC mechanisms, such as adaptive thresholding and Bernoulli aggregation, which require reduced communication and occasional Hessian computations. Furthermore, we extend and analyze our approach to bidirectional communication compression and partial device participation setups to cater to the practical considerations of applications in federated learning. For all our methods, we derive fast condition-number-independent local linear and/or superlinear convergence rates. Finally, with extensive numerical evaluations on convex optimization problems, we illustrate that our designed schemes achieve state-of-the-art communication complexity compared to several key baselines using second-order information.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 6, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.02535">
"Certified Robustness in Federated Learning"</a> - joint work with
<a href="https://scholar.google.com/citations?user=caAyffEAAAAJ&hl=en">Motasem Alfarra</a>, <a href="https://juancprzs.github.io">Juan C. Pérez</a>, <a href="https://shulgin-egor.github.io">Egor Shulgin</a> and <a href="https://www.bernardghanem.com">Bernard Ghanem</a>.

<br>
<br>
Abstract:
<i>
Federated learning has recently gained significant attention and popularity due to its effectiveness in training machine learning models on distributed data privately. However, as in the single-node supervised learning setup, models trained in federated learning suffer from vulnerability to imperceptible input transformations known as adversarial attacks, questioning their deployment in security-related applications. In this work, we study the interplay between federated training, personalization, and certified robustness. In particular, we deploy randomized smoothing, a widely-used and scalable certification method, to certify deep networks trained on a federated setup against input perturbations and transformations. We find that the simple federated averaging technique is effective in building not only more accurate, but also more certifiably-robust models, compared to training solely on local data. We further analyze personalization, a popular technique in federated training that increases the model's bias towards local data, on robustness. We show several advantages of personalization over both~(that is, only training on local data and federated training) in building more robust models with faster training. Finally, we explore the robustness of mixtures of global and local~(\ie personalized) models, and find that the robustness of local models degrades as they diverge from the global model
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 5, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.02275">
"Sharper Rates and Flexible Framework for Nonconvex SGD with Client and Data Sampling"</a> -
joint work with
<a href="https://k3nfalt.github.io">Alexander Tyurin</a>, <a href="https://lukangsun.github.io">Lukang Sun</a> and <a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a>.

<br>
<br>
Abstract:
<i>
We revisit the classical problem of finding an approximately stationary point of the average of n smooth and possibly nonconvex functions.
The optimal complexity of stochastic first-order methods in terms of the number of gradient evaluations of individual functions is
O(n+√n/ε), attained by the optimal SGD methods 𝖲𝖯𝖨𝖣𝖤𝖱 [Fang et al, NeurIPS 2018] and 𝖯𝖠𝖦𝖤 [Zhize et al, ICML 2021], for example,
where ε is the error tolerance. However, i) the big-O notation hides crucial dependencies on the smoothness constants associated with the functions,
and ii) the rates and theory in these methods assume simplistic sampling mechanisms that do not offer any flexibility. In this work we remedy the
situation. First, we generalize the 𝖯𝖠𝖦𝖤 algorithm so that it can provably work with virtually any (unbiased) sampling mechanism. This is particularly
useful in federated learning, as it allows us to construct and better understand the impact of various combinations of client and data sampling strategies.
Second, our analysis is sharper as we make explicit use of certain novel inequalities that capture the intricate interplay between the smoothness
constants and the sampling procedure. Indeed, our analysis is better even for the simple sampling procedure analyzed in the 𝖯𝖠𝖦𝖤 paper.
However, this already improved bound can be further sharpened by a different sampling scheme which we propose. In summary, we provide the most
general and most accurate analysis of optimal SGD in the smooth nonconvex regime. Finally, our theoretical findings are supposed with carefully
designed experiments.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 2, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.00920">
"Federated Learning with a Sampling Algorithm under Isoperimetry"</a> -
joint work with
<a href="https://lukangsun.github.io">Lukang Sun</a> and <a href="https://adil-salim.github.io">Adil Salim</a>.

<br>
<br>
Abstract:
<i>
Federated learning uses a set of techniques to efficiently distribute the training of a machine learning algorithm across several devices, who own the training data. These techniques critically rely on reducing the communication cost -- the main bottleneck -- between the devices and a central server. Federated learning algorithms usually take an optimization approach: they are algorithms for minimizing the training loss subject to communication (and other) constraints. In this work, we instead take a Bayesian approach for the training task, and propose a communication-efficient variant of the Langevin algorithm to sample a posteriori. The latter approach is more robust and provides more knowledge of the \textit{a posteriori} distribution than its optimization counterpart. We analyze our algorithm without assuming that the target distribution is strongly log-concave. Instead, we assume the weaker log Sobolev inequality, which allows for nonconvexity.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 1, 2022</h3>
<h1>New Research Intern: Arto Maranjyan (Yerevan State University)</h1>
<br>


Artavazd "Arto" Maranjyan [<a href="https://www.researchgate.net/profile/Artavazd-Maranjyan">ResearchGate</a>]
[<a href="https://publons.com/researcher/4267694/artavazd-maranjyan/">publons</a>]
[<a href="https://www.linkedin.com/in/arto-maranjyan/?originalSubdomain=am">LinkedIn</a>] joined my Optimization and Machine Learning Lab as
a <a href="https://vsrp.kaust.edu.sa">VSRP intern</a>. His internship started on June 1, and will last for 6 months. Arto is a first year MS
student in Applied Mathematics and Statistics at <a href="https://en.wikipedia.org/wiki/Yerevan_State_University">Yerevan State University, Armenia</a>.
Parts of his BS thesis entitled
"On the Convergence of Series in Classical Systems", supervised by Prof. Martin Grigoryan, have been published as separate papers:
<ul>
<li> On the divergence of Fourier series in the general Haar system, <a href="http://armjmath.sci.am/index.php/ajm/article/view/533">Armenian Journal of Mathematics 13(6):1-10, 2021</a>,</li>
<li> On the unconditional convergence of Faber-Schauder series in L1, <a href="https://journals.ysu.am/index.php/proceedings-phys-math/article/view/vol55_no1_2021_pp012-019">Proceedings of the
  Yerevan State University 55(254):12-19, 2021</a>,</li>
<li> Problems of recovering from the Fourier-Vilenkin series (unpublished).</li>
</ul>

Arto was one of 6 out of 250 YSU students receiving an Outstanding Final Project Award. Arto gained practical and theoretical knowledge in machine
learning, deep learning, natural language processing, and computer vision during a year-long AI training with the
<a href="https://fast.foundation">Foundation for Armenian Science and
Technology (FAST)</a>.
<br>

<br>
Welcome to the team!!!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 1, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.00529">
"Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top"</a> -
joint work with
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>, <a href="https://samuelhorvath.github.io">Samuel Horváth</a> and <a href="https://gauthiergidel.github.io">Gauthier Gidel</a>.

<br>
<br>
Abstract:
<i>
Byzantine-robustness has been gaining a lot of attention due to the growth of the interest in collaborative and federated learning.
However, many fruitful directions, such as the usage of variance reduction for achieving robustness and communication compression
for reducing communication costs, remain weakly explored in the field. This work addresses this gap and proposes Byz-VR-MARINA --
a new Byzantine-tolerant method with variance reduction and compression. A key message of our paper is that variance reduction is
key to fighting Byzantine workers more effectively. At the same time, communication compression is a bonus that makes the process
more communication efficient. We derive theoretical convergence guarantees for Byz-VR-MARINA outperforming previous state of the art
for general non-convex and Polyak-Lojasiewicz loss functions. Unlike the concurrent Byzantine-robust methods with variance reduction
and/or compression, our complexity results are tight and do not rely on restrictive assumptions such as boundedness of the gradients
or limited compression. Moreover, we provide the first analysis of a Byzantine-tolerant method supporting non-uniform sampling of
stochastic gradients. Numerical experiments corroborate our theoretical findings.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 1, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.00508">
"Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition"</a> -
joint work with
<a href="https://lukangsun.github.io">Lukang Sun</a> and <a href="https://avetx.github.io">Avetik Karagulyan</a>.

<br>
<br>
Abstract:
<i>
Stein Variational Gradient Descent (SVGD) is an important alternative to the Langevin-type algorithms for sampling from probability
distributions of the form π(x)∝exp(−V(x)). In the existing theory of Langevin-type algorithms and SVGD, the potential function V is
often assumed to be L-smooth. However, this restrictive condition excludes a large class of potential functions such as polynomials
of degree greater than 2. Our paper studies the convergence of the SVGD algorithm for distributions with (L0,L1)-smooth potentials.
This relaxed smoothness assumption was introduced by Zhang et al. [2019a] for the analysis of gradient clipping algorithms. With the
help of trajectory-independent auxiliary conditions, we provide a descent lemma establishing that the algorithm decreases the KL divergence
at each iteration and prove a complexity bound for SVGD in the population limit in terms of the Stein Fisher information.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 31, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2205.15580">
"A Computation and Communication Efficient Method for Distributed Nonconvex Problems in the Partial Participation Setting"</a> -
joint work with
<a href="https://k3nfalt.github.io">Alexander Tyurin</a>.

<br>
<br>
Abstract:
<i>
We present a new method that includes three key components of distributed optimization and federated learning: variance reduction of stochastic gradients, compressed communication, and partial participation. We prove that the new method has optimal oracle complexity and state-of-the-art communication complexity in the partial participation setting. Moreover, we observe that "1 + 1 + 1 is not 3": by mixing variance reduction of stochastic gradients with compressed communication and partial participation, we do not obtain a fully synergetic effect. We explain the nature of this phenomenon, argue that this is to be expected, and propose possible workarounds.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>May 30, 2022</h3>

<h1>Teaching in Vienna</h1>

During May 30-June 3, I am teaching a course on <a href="https://ufind.univie.ac.at/en/course.html?lv=390015&semester=2022S">Stochastic
Gradient Descent Methods</a> at the <a href="https://vgsco.univie.ac.at">Vienna Graduate School of Computational Optimization (VGSCO).</a>
About 20 PhD students, postdocs and even some professors from 4 Austrian universities (U Wien, IST Austria, TU Wien, WU Wien) are attending.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 15, 2022</h3>

<h1>Five Papers Accepted to ICML 2022</h1>
<br>

We've had several several papers accepted to the <a href="https://icml.cc/Conferences/2022/">International Conference on Machine Learning (ICML 2022).</a>
Here they are:<br><br>


<b> 1) "Proximal and Federated Random Reshuffling"</b> <a href="https://arxiv.org/abs/2102.06704">[arXiv]</a> <a href="https://www.youtube.com/watch?v=hsGty6dPAKU">[video]</a>  -
joint work with
<a href="https://konstmish.github.io">Konstantin Mishchenko</a> and
<a href="https://rka97.github.io">Ahmed Khaled</a>.

<br>
<br>
<br>

<b> 2) "FedNL: Making Newton-Type Methods Applicable to Federated Learning"</b>
<a href="https://arxiv.org/abs/2106.02969">[arXiv]</a> <a href="https://www.youtube.com/watch?v=_VYCEWT17R0">[video]</a>  -
joint work with
<a href="https://mher-safaryan.github.io/">Mher Safaryan</a>,
<a href="https://rustem-islamov.github.io/">Rustem Islamov</a>, and
<a href="https://qianxunk.github.io/">Xun Qian</a>.

<br>
<br>
<br>

<b> 3) "A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1"</b>
<a href="https://arxiv.org/abs/2106.03076">[arXiv]</a>  -
joint work with
<a href="https://lukangsun.github.io">Lukang Sun</a> and
<a href="https://adil-salim.github.io">Adil Salim</a>.

<br>
<br>
<br>

<b> 4) "ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!"</b>
<a href="https://arxiv.org/abs/2202.09357">[arXiv]</a> <a href="https://www.youtube.com/watch?v=OMVSzgsd7ZY">[video]</a> -
joint work with
<a href="https://konstmish.github.io">Konstantin Mishchenko</a>,
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>, and
<a href="https://www.sstich.ch">Sebastian Stich</a>.

<br>
<br>
<br>

<b> 5) "3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation"</b>
<a href="https://arxiv.org/abs/2202.00998">[arXiv]</a>  -
joint work with
<a href="https://cemse.kaust.edu.sa/amcs/people/person/igor-sokolov">Igor Sokolov</a>,
<a href="https://ai.ethz.ch/people/ilyas-fatkhullin.html">Ilyas Fatkhullin</a>,
<a href="https://elnurgasanov.com">Elnur Gasanov</a>,
<a href="https://zhizeli.github.io">Zhize Li</a>, and
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 15, 2022</h3>

<h1>Stochastic Numerics and Statistical Learning Workshop</h1>

Today I gave a talk at <a href="https://cemse.kaust.edu.sa/events/event/snsl-workshop">"Stochastic Numerics and Statistical
  Learning: Theory and Applications Workshop"</a>.
  I spoke about <a href="https://arxiv.org/abs/2202.09357">ProxSkip paper</a> [<a href="https://richtarik.org/talks/2022-ProxSkip-Lagrange_Workshop.pdf">slides</a>]
  [<a href="https://www.youtube.com/watch?v=OMVSzgsd7ZY">video</a>]. The good news from today is that the paper got accepted
  to ICML.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 10, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2205.04180">
"EF-BV: A Unified Theory of Error Feedback and Variance Reduction Mechanisms for Biased and Unbiased Compression in Distributed Optimization"</a> -
joint work with
<a href="https://lcondat.github.io">Laurent Condat</a> and
<a href="https://kaiyi.me">Kai Yi</a>.

<br>
<br>
Abstract:
<i>
In distributed or federated optimization and learning, communication between the different computing units is often the bottleneck, and gradient compression is a widely used technique for reducing the number of bits sent within each communication round of iterative methods. There are two classes of compression operators and separate algorithms making use of them. In the case of unbiased random compressors with bounded variance (e.g., rand-k), the DIANA algorithm of Mishchenko et al. [2019], which implements a variance reduction technique for handling the variance introduced by compression, is the current state of the art. In the case of biased and contractive compressors (e.g., top-k), the EF21 algorithm of Richtárik et al. [2021], which implements an error-feedback mechanism for handling the error introduced by compression, is the current state of the art. These two classes of compression schemes and algorithms are distinct, with different analyses and proof techniques. In this paper, we unify them into a single framework and propose a new algorithm, recovering DIANA and EF21 as particular cases. We prove linear convergence under certain conditions. Our general approach works with a new, larger class of compressors, which includes unbiased and biased compressors as particular cases, and has two parameters, the bias and the variance. These gives a finer control and allows us to inherit the best of the two worlds: biased compressors, whose good performance in practice is recognized, can be used. And independent randomness at the compressors allows to mitigate the effects of compression, with the convergence rate improving when the number of parallel workers is large. This is the first time that an algorithm with all these features is proposed. Our approach takes a step towards better understanding of two so-far distinct worlds of communication-efficient distributed learning.
</i>
<br>




<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>May 10, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2205.03914">
"Federated Random Reshuffling with Compression and Variance Reduction"</a> -
joint work with <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>.

<br>
<br>
Abstract:
<i>
Random Reshuffling (RR), which is a variant of Stochastic Gradient Descent (SGD) employing sampling without replacement, is an immensely popular method for training supervised machine learning models via empirical risk minimization. Due to its superior practical performance, it is embedded and often set as default in standard machine learning software. Under the name FedRR, this method was recently shown to be applicable to federated learning (Mishchenko et al.,2021), with superior performance when compared to common baselines such as Local SGD. Inspired by this development, we design three new algorithms to improve FedRR further: compressed FedRR and two variance reduced extensions: one for taming the variance coming from shuffling and the other for taming the variance due to compression. The variance reduction mechanism for compression allows us to eliminate dependence on the compression parameter, and applying additional controlled linear perturbations for Random Reshuffling, introduced by Malinovsky et al.(2021) helps to eliminate variance at the optimum. We provide the first analysis of compressed local methods under standard assumptions without bounded gradient assumptions and for heterogeneous data, overcoming the limitations of the compression operator. We corroborate our theoretical results with experiments on synthetic and real data sets.
</i>
<br>




<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>







<h3>April 27, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2204.13169">
"FedShuffle: Recipes for Better Use of Local Work in Federated Learning"</a> -
joint work with
<a href="https://samuelhorvath.github.io">Samuel Horváth</a>,
<a href="https://sites.google.com/view/maziar">Maziar Sanjabi</a>,
<a href="https://linxiaolx.github.io">Lin Xiao</a>, and
<a href="https://ai.facebook.com/people/michael-rabbat">Michael Rabbat</a>.

<br>
<br>
Abstract:
<i>
The practice of applying several local updates before aggregation across clients has been empirically shown to be a successful approach to overcoming the communication bottleneck in Federated Learning (FL). In this work, we propose a general recipe, FedShuffle, that better utilizes the local updates in FL, especially in the heterogeneous regime. Unlike many prior works, FedShuffle does not assume any uniformity in the number of updates per device. Our FedShuffle recipe comprises four simple-yet-powerful ingredients: 1) local shuffling of the data, 2) adjustment of the local learning rates, 3) update weighting, and 4) momentum variance reduction (Cutkosky and Orabona, 2019). We present a comprehensive theoretical analysis of FedShuffle and show that both theoretically and empirically, our approach does not suffer from the objective function mismatch that is present in FL methods which assume homogeneous updates in heterogeneous FL setups, e.g., FedAvg (McMahan et al., 2017). In addition, by combining the ingredients above, FedShuffle improves upon FedNova (Wang et al., 2020), which was previously proposed to solve this mismatch. We also show that FedShuffle with momentum variance reduction can improve upon non-local methods under a Hessian similarity assumption. Finally, through experiments on synthetic and real-world datasets, we illustrate how each of the four ingredients used in FedShuffle helps improve the use of local updates in FL.
</i>
<br>




<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 27, 2022</h3>

<h1>Two Seminar Talks: Baidu and PSU</h1>

I gave two talks on the ProxSkip method [<a href="https://arxiv.org/abs/2202.09357">paper</a>] today: the first talk at the
"Seminar Series in Cognitive Computing at Baidu Research", invited by <a href="https://www.linkedin.com/in/ping-li-a4624389/">Ping Li</a>
[<a href="talks/TALK-2022-04-ProxSkip-Baidu.pdf">my slides</a>], and the second talk at the "Department of
Mathematics Colloqium, Penn State University", invited by
<a href="https://www.personal.psu.edu/jxx1/">Jinchao Xu</a> [<a href="talks/TALK-2022-04-ProxSkip-PSU.pdf">my slides</a>].
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>April 25, 2022</h3>

<h1>Lagrange Workshop on Federated Learning</h1>

Today I am giving a talk entitled <a href="https://arxiv.org/abs/2202.09357">"ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!"</a> at the Lagrange Workshop on Federated Learning, organized by Eric Moulines, Merouane Debbah and Samson Lasaulce as a
"Lagrange Mathematics and Computing Research Center" event. The talk is based on this paper in which we resolve an important open problem in the field of federated learning. In particular, we show that local gradient steps can provably lead to
communication acceleration.

<br>
<br>
<a href="imgs/WS-FL_FLyer.pdf"><img alt="" src="imgs/WS-FL_FLyer.png" width="750"></a>
<br>
<br>

The workshop is a virtual meeting; anyone can join via Zoom. I am looking forward to listening to many interesting talks, including one
by my former student <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>.
<br>

<br>
Here are <a href="talks/2022-ProxSkip-Lagrange_Workshop.pdf">my slides.</a>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>April 7, 2022</h3>

<h1>Apple Workshop on Privacy Preserving Machine Learning</h1>

On Tuesday and Wednesday earlier this week I attended and gave a talk at (an invite-only) workshop on "Privacy Preserving Machine Learning" organized by Apple.
The program started at 7pm my time (morning California time) and lasted until after midnight. Yes, I felt very very tired near the end...
In any case, a nice event.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>April 3, 2022</h3>

<h1>Positions in my Optimization and Machine Learning Lab</h1>

I always have openings for outstanding individuals to join my team as interns, MS/PhD students, PhD students, postdocs and research scientists.
If you are interested in a position, please fill out this <a href="https://apply.interfolio.com/105097">interfolio application form</a>.


<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>March 31, 2022</h3>

<h1>Talk on ProxSkip at the AMCS/STAT Graduate Seminar at KAUST</h1>

Today I have a talk about "ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!"
at the <a href="https://cemse.kaust.edu.sa/events/event/proxskip-yes-local-gradient-steps-provably-lead-communication-acceleration-finally">AMCS/STAT Graduate Seminar</a> here at KAUST. The talk is based on a
joint paper with <a href="https://konstmish.github.io">Konstantin Mishchenko</a>,  <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> and <a href="https://sstich.ch">Sebastian Stich</a>.
I will give the same talk at the <a href="https://sites.google.com/view/one-world-seminar-series-flow/future-talks">Federated Learning One World (FLOW) seminar</a> on May 4, 2022.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>March 24, 2022</h3>

<h1>Transactions on Machine Learning Research Accepting Submissions!</h1>

<a href="https://jmlr.org/tmlr/index.html">Transactions on Machine Learning Research</a> is a new venue for the dissemination of
machine learning research. Yours truly is one of a large number (approx 150) of <a href="https://jmlr.org/tmlr/editorial-board.html">Action
  Editors</a>. You can read <a href="https://jmlr.org/tmlr/ae-guide.html">here</a> in detail what
the responsibilities of an Action Editor are. We are delighted to let you know that TMLR is now <a href="https://jmlr.org/tmlr/news/2022/launch.html">accepting submissions!</a>
<br> <br>


An excerpt from the TMLR website: <i>"TMLR emphasizes technical correctness over subjective significance, to ensure that we facilitate scientific discourse on topics that are deemed less significant by contemporaries but may be important in the future. TMLR caters to the shorter format manuscripts that are usually submitted to conferences, providing fast turnarounds and double blind reviewing. We employ a rolling submission process, shortened review period, flexible timelines, and variable manuscript length, to enable deep and sustained interactions among authors, reviewers, editors and readers. This leads to a high level of quality and rigor for every published article.
TMLR does not accept submissions that have any overlap with previously published work. TMLR maximizes openness and transparency by hosting the review process on OpenReview."</i>
<br> <br>

I have high hopes that TMLR will combine the benefits of a fast conference-like publication process with the high reviewing standards of journals.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 21, 2022</h3>

<h1>Talk on PermK at the CS Graduate Seminar at KAUST</h1>

Today I gave a talk about "Permutation compressors for provably faster distributed nonconvex optimization"
at the <a href="https://cemse.kaust.edu.sa/events/event/permutation-compressors-provably-faster-distributed-nonconvex-optimization">Computer Science Graduate Seminar</a> here at KAUST. The talk is based on
joint paper with <a href="https://www.linkedin.com/in/rafał-szlendak-552936220">Rafał Szlendak</a> and <a href="https://k3nfalt.github.io/">Alexander Tyurin</a> recently accepted to <a href="https://iclr.cc">ICLR 2022</a>. I gave the same talk at the Federated Learning One World (FLOW) seminar in February; this one was recorded and is on <a href="https://www.youtube.com/watch?v=aMEj2pkGrcY">YouTube</a>.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 13, 2022</h3>

<h1>Rising Stars in AI Symposium at KAUST</h1>

A <a href="https://cemse.kaust.edu.sa/ai/aii-symp-2022">Rising Stars in AI Symposium</a> is taking place at KAUST during March 13-15, 2022. This event is organized by
KAUST's <a href="https://cemse.kaust.edu.sa/ai">AI Initiative</a>, led by <a href="https://cemse.kaust.edu.sa/ai/people/person/jurgen-schmidhuber">Jürgen Schmidhuber</a>.
Several people from my team are giving talks: <a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a>, <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>,
<a href="https://k3nfalt.github.io">Alexander Tyurin</a>, <a href="https://elnurgasanov.com">Elnur Gasanov</a>, <a href="https://mher-safaryan.github.io">Mher Safaryan</a>,
<a href="https://shulgin-egor.github.io">Egor Shulgin</a>, <a href="https://samuelhorvath.github.io">Samuel Horváth</a> and <a href="https://zhizeli.github.io">Zhize Li</a>. My former PhD
student <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> is also giving a talk.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>March 12, 2022</h3>

<h1>Back to KAUST</h1>

After spending a few weeks in Europe, I have just arrived back to KAUST.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 20, 2022</h3>

<h1>Dagstuhl</h1>

As of today, and until February 25, I am at Dagstuhl, Germany, attending a Seminar on
 the <a href="https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=22081">Theory of Randomized Optimization Heuristics</a>.

 <br>


 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>


<h3>February 19, 2022</h3>

<h1>New Paper</h1>

<a href="papers/ProxSkip.pdf"><img alt="" src="imgs/ProxSkip-thmb.png" height="1000"></a>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2202.09357">
"ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!"</a> -
joint work with
<a href="https://konstmish.github.io/">Konstantin Mishchenko</a>,
<a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky</a> and
<a href="http://sstich.ch/">Sebastian Stich</a>.

<br>
<br>
Abstract:
<i>
We introduce ProxSkip---a surprisingly simple and provably efficient method for minimizing the sum of a smooth ($f$) and an expensive nonsmooth proximable ($\psi$) function. The canonical approach to solving such problems is via the proximal gradient descent (ProxGD) algorithm, which is based on the evaluation of the gradient of $f$ and the prox operator  of $\psi$ in each iteration. In this work we are specifically interested in the regime in which the evaluation of prox is costly relative to the evaluation of the gradient, which is the case in many applications. ProxSkip allows for the expensive prox operator to be skipped in most iterations: while its iteration complexity is $\cO(\kappa \log \nicefrac{1}{\varepsilon})$, where $\kappa$ is the condition number of $f$, the number of prox evaluations is $\cO(\sqrt{\kappa} \log \nicefrac{1}{\varepsilon})$ only. Our main motivation comes from federated learning, where evaluation of the gradient operator corresponds to taking a local GD step independently on all devices, and evaluation of prox corresponds to (expensive) communication in the form of gradient averaging. In this context, ProxSkip offers an effective <i>acceleration</i> of communication complexity. Unlike other local gradient-type methods, such as FedAvg, Scaffold, S-Local-GD and FedLin, whose theoretical communication complexity is worse than, or at best matching, that of vanilla GD in the heterogeneous data regime, we obtain a provable and large improvement without any heterogeneity-bounding assumptions.
</i>
<br>




<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 19, 2022</h3>

<h1>Dagstuhl Seminar</h1>
<br>
I am on my way to Schloss Dagstuhl, Germany, to attend a week-long seminar on the <a href="https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=22081">Theory
of Randomized Optimization Heuristics</a>.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 9, 2022</h3>

<h1>Talk @ FLOW </h1>
<br>

I just gave a talk at the <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">Federated Learning One World (FLOW)</a>  seminar.
I spoke about <a href="https://arxiv.org/abs/2110.03300">"Permutation compressors for provably faster distributed nonconvex optimization"</a> -
a paper recently accepted to ICLR 2022.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 8, 2022</h3>

<h1>New Paper</h1>
<br>


<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2202.02771">
"Optimal Algorithms for Decentralized Stochastic Variational Inequalities"</a> -
joint work with
<a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>,
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">Abdurakhmon Sadiev</a>,
Michael Persiianov, and
<a href="https://www.researchgate.net/profile/Alexander-Gasnikov">Alexander Gasnikov</a>.

<br>
<br>
Abstract:
<i>
Variational inequalities are a formalism that includes games, minimization, saddle point, and equilibrium problems as special cases. Methods for variational inequalities are therefore universal approaches for many applied tasks, including machine learning problems. This work concentrates on the decentralized setting, which is increasingly important but not well understood. In particular, we consider decentralized stochastic (sum-type) variational inequalities over fixed and time-varying networks. We present lower complexity bounds for both communication and local iterations and construct optimal algorithms that match these lower bounds. Our algorithms are the best among the available literature not only in the decentralized stochastic case, but also in the decentralized deterministic and non-distributed stochastic cases. Experimental results confirm the effectiveness of the presented algorithms.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 7, 2022</h3>

<h1>Talk @ Machine Learning NeEDS Mathematical Optimization Seminar</h1>
<br>

Today I am giving a talk within the <a href="https://congreso.us.es/mlneedsmo/">Machine Learning NeEDS Mathematical Optimization</a>
virtual seminar series. This is the opening talk of the third season of the seminar. I spoke about
<a href="https://arxiv.org/abs/2110.03300">"Permutation compressors for provably faster distributed nonconvex optimization"</a> -
paper recently accepted to ICLR 2022.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 2, 2022</h3>

<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2202.01268">
"Distributed nonconvex optimization with communication compression, optimal oracle complexity, and no client synchronization"</a> -
joint work with
<a href="https://k3nfalt.github.io">Alexander Tyurin</a>.

<br>
<br>
Abstract:
<i>
We develop and analyze DASHA: a new family of methods for nonconvex distributed optimization problems. When the local functions at the nodes have a finite-sum
or an expectation form, our new methods, DASHA-PAGE and DASHA-SYNC-MVR, improve the theoretical oracle and communication complexity of the previous state-of-the-art method MARINA by
Gorbunov et al. (2020). In particular, to achieve an $\varepsilon$-stationary point, and considering the random sparsifier RandK as an
example, our methods compute the optimal number of gradients $O(\sqrt{m}/(\varepsilon\sqrt{n}))$ and $O(\sigma/(\varepsilon^{3/2}n))$ in finite-sum and
expectation form cases, respectively, while maintaining the SOTA communication complexity $O(d/(\varepsilon \sqrt{n}))$. Furthermore, unlike
MARINA, the new methods DASHA, DASHA-PAGE and DASHA-MVR send compressed vectors only and never synchronize the nodes, which makes them more practical for federated learning. We extend our results to the case when the functions satisfy the Polyak-Łojasiewicz condition.
Finally, our theory is corroborated in practice: we see a significant improvement in experiments with nonconvex classification and training of deep learning models.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 2, 2022</h3>
<h1>New Research Intern: Abdurakhmon Sadiev (MIPT)</h1>
<br>


Abdurakhmon Sadiev joined my Optimization and Machine Learning Lab as a research intern. He arrived
today at KAUST, and will be here for 5-6 months.
Abdurakhmon is a final-year MS student in Applied Mathematics at <a href="https://mipt.ru">MIPT</a>; his
advisor there is <a href="http://www.mathnet.ru/eng/person27590">Alexander Gasnikov</a>. He has received
several scholarships and awards during his studies, including:

<ul>
<li> 3rd Degree Prof. Andrei Raigorodskii Personal Scholarship (2021),</li>
<li> Increased State Academic Scholarship for 4 year bachelor and master students at MIPT (2020), and</li>
<li> Abramov Scholarship for 1-3 year bachelor students with the best grades at MIPT (2018).</li>
</ul>

During his MIPT studies, he was a teaching assistant at MIPT for Functional Analysis (Department of Advanced Mathematics)
and Methods of Optimal Control (Department of Mathematical Fundamentals of Control).
<br>
<br>
Abdurakhmon is interested in min-max / saddle-point problems, derivative-free methods and federated learning.
He has coauthored a number of papers, most of which can be
found on his <a href="https://scholar.google.com/citations?hl=en&user=R-xZRIAAAAAJ">Google Scholar</a>
and <a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">ResearchGate</a> profiles:
<ul>
<li>Gradient-Free Methods for Saddle-Point Problem</li>
<li>Zeroth-Order Algorithms for Smooth Saddle-Point Problems</li>
<li>Solving Smooth Min-Min and Min-Max Problems by Mixed Oracle Algorithms</li>
<li>Decentralized Personalized Federated Min-Max Problems</li>
<li>Decentralized Personalized Federated Learning: Lower Bounds and Optimal Algorithm for All Personalization Modes</li>
<li>Decentralized and Personalized Federated Learning</li>
<li>Gradient-Free Methods with Inexact Oracle for Convex-Concave Stochastic Saddle-Point Problem</li>
<li>Optimal Algorithms for Decentralized Stochastic Variational Inequalities</li>
</ul>
<br>
Welcome to the team!!!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 2, 2022</h3>

<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2202.00998">
"3PC: Three point compressors for communication-efficient distributed training and a better theory for lazy aggregation"</a> -
joint work with
<a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a>,
<a href="https://ai.ethz.ch/people/ilyas-fatkhullin.html">Ilyas Fatkhullin</a>,
<a href="https://elnurgasanov.com/">Elnur Gasanov</a>,
<a href="https://zhizeli.github.io/">Zhize Li</a>, and
<a href="https://eduardgorbunov.github.io/">Eduard Gorbunov</a>.

<br>
<br>
Abstract:
<i>
We propose and study a new class of gradient communication mechanisms for communication-efficient training -- three point compressors (3PC) -- as well as efficient distributed nonconvex optimization algorithms that can take advantage of them. Unlike most established approaches, which rely on a static compressor choice (e.g., Top-K), our class allows the compressors to {\em evolve} throughout the training process, with the aim of improving the theoretical communication complexity and practical efficiency of the underlying methods. We show that our general approach can recover the recently proposed state-of-the-art error feedback mechanism EF21 (Richtárik et al., 2021) and its theoretical properties as a special case, but also leads to a number of new efficient methods. Notably, our approach allows us to improve upon the state of the art in the algorithmic and theoretical foundations of the lazy aggregation literature (Chen et al., 2018). As a by-product that may be of independent interest, we provide a new and fundamental link between the lazy aggregation and error feedback literature. A special feature of our work is that we do not require the compressors to be unbiased.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 28, 2022</h3>

<h1>ICML 2022</h1>
<br>

The ICML 2022 submission deadline is over, I'll be sleeping for the rest of the month.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 26, 2022</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2201.11066">
"Server-side stepsizes and sampling without replacement provably help in federated optimization"</a> -
joint work with
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> and
<a href="https://konstmish.github.io">Konstantin Mishchenko</a>.
<br>
<br>
Abstract:
<i>
We present a theoretical study of server-side optimization in federated learning. Our results are the first to show that the widely popular heuristic of scaling the client updates with an extra parameter is very useful in the context of Federated Averaging (FedAvg) with local passes over the client data. Each local pass is performed without replacement using Random Reshuffling, which is a key reason we can show improved complexities. In particular, we prove that whenever the local stepsizes are small, and the update direction is given by FedAvg in conjunction with Random Reshuffling over all clients, one can take a big leap in the obtained direction and improve rates for convex, strongly convex, and non-convex objectives. In particular, in non-convex regime we get an enhancement of the rate of convergence from $O(\epsilon^{-3})$ to $O(\epsilon^{-2})$. This result is new even for Random Reshuffling performed on a single node. In contrast, if the local stepsizes are large, we prove that the noise of client sampling can be controlled by using a small server-side stepsize. To the best of our knowledge, this is the first time that local steps provably help to overcome the communication bottleneck. Together, our results on the advantage of large and small server-side stepsizes give a formal justification for the practice of adaptive server-side optimization in federated learning. Moreover, we consider a variant of our algorithm that supports partial client participation, which makes the method more practical.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 23, 2022</h3>

<h1>The Spring 2022 Semester at KAUST Started</h1>
<br>

The Spring 2022 semester at KAUST started today; I am teaching CS 332: Federated Learning.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 20, 2022</h3>

<h1>Three Papers Accepted to ICLR 2022</h1>
<br>

We've had several three papers accepted to the <a href="https://iclr.cc/">International Conference on Learning Representations 2022.</a>
Here they are:<br><br>


<b> 1) "IntSGD: Floatless Compression of Stochastic Gradients"</b> <a href="https://arxiv.org/abs/2102.08374">[arXiv]</a>  -
joint work with
<a href="https://konstmish.github.io">Konstantin Mishchenko</a>,
<a href="https://dblp.org/pid/207/1922.html">Bokun Wang</a>, and
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>.

<br>
<br>
<br>

<b> 2) "Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information"</b>
<a href="https://arxiv.org/abs/2109.05198">[arXiv]</a>  -
joint work with
<a href="https://coral.ise.lehigh.edu/maj316/">Majid Jahani</a>,
<a href="https://scholar.google.ru/citations?user=tdyw7IAAAAAJ&hl=en">Sergey Rusakov</a>,
<a href="https://coral.ise.lehigh.edu/zhs310/">Zheng Shi</a>,
<a href="https://www.stat.berkeley.edu/~mmahoney/">Michael W. Mahoney</a>, and
<a href="https://mtakac.com/">Martin Takáč</a>.

<br>
<br>
<br>

<b> 3) "Permutation Compressors for Provably Faster Distributed Nonconvex Optimization"</b>
<a href="https://arxiv.org/abs/2110.03300">[arXiv]</a>  -
joint work with
<a href="https://uk.linkedin.com/in/rafa%C5%82-szlendak-552936220">Rafal Szlendak</a> and
<a href="https://k3nfalt.github.io/">Alexander Tyurin.</a>

<br>



<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 18, 2022</h3>
<h1>Three Papers Accepted to AISTATS 2022</h1>
<br>

We've had several three papers accepted to <a href="http://aistats.org/aistats2022/">The 25th International Conference on Artificial Intelligence and Statistics.</a>

Here they are:<br><br>


<b> 1) "An Optimal Algorithm for Strongly Convex Minimization under Affine Constraints"</b> <a href="https://arxiv.org/abs/2102.11079">[arXiv]</a>  -
joint work with
<a href="https://adil-salim.github.io/">Adil Salim</a>,
<a href="https://lcondat.github.io/">Laurent Condat</a>, and
<a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>.

<br>
<br>
<br>

<b> 2) "FLIX: A Simple and Communication-Efficient Alternative to Local Methods in Federated Learning"</b>
<a href="https://arxiv.org/abs/2111.11556">[arXiv]</a>  -
joint work with
<a href="http://elnurgasanov.com/">Elnur Gasanov</a>,
<a href="https://rka97.github.io/">Ahmed Khaled</a>, and
<a href="https://samuelhorvath.github.io/">Samuel Horváth</a>.

<br>
<br>
<br>

<b> 3) "Basis Matters: Better Communication-Efficient Second Order Methods for Federated Learning"</b>
<a href="https://arxiv.org/abs/2111.01847">[arXiv]</a>  -
joint work with
<a href="https://qianxunk.github.io/">Xun Qian</a>,
<a href="https://rustem-islamov.github.io/">Rustem Islamov</a>, and
<a href="https://mher-safaryan.github.io/">Mher Safaryan</a>.

<br>



<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 11, 2022</h3>

<h1>Paper Accepted to SIAM Journal on Mathematics of Data Science</h1>
<br>

The paper <a href="https://arxiv.org/abs/2002.05359">"Adaptivity of stochastic
  gradient methods for nonconvex optimization"</a>, joint work with
  <a href="https://samuelhorvath.github.io">Samuel Horváth</a> and
<a href="https://lihualei71.github.io">Lihua Lei</a>, and
<a href="https://people.eecs.berkeley.edu/~jordan/">Michael I. Jordan</a>
was accepted to
<a href="https://www.frontiersin.org/journals/signal-processing">SIAM Journal on Mathematics of Data Science (SIMODS).
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>December 31, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2112.15199">
  "Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling"</a> -
joint work with
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> and
<a href="https://zhizeli.github.io">Alexander Gasnikov</a>.
<br>
<br>
Abstract: <i>
  In this paper we study a convex-concave saddle-point problem min_x max_y f(x) + y^T A x − g(y), where f(x) and g(y) are smooth and convex functions.
  We propose an Accelerated Primal-Dual Gradient Method for solving this problem which (i) achieves an optimal
  linear convergence rate in the strongly-convex-strongly-concave regime matching the lower complexity bound
  (Zhang et al., 2021) and (ii) achieves an accelerated linear convergence rate in the case when only one of
  the functions f(x) and g(y) is strongly convex or even none of them are. Finally, we obtain a
  linearly-convergent algorithm for the general smooth and convex-concave saddle point problem min_x max_y F(x,y)
  without requirement of strong convexity or strong concavity.
</i>

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>December 24, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2112.13097">
  "Faster rates for compressed federated learning with client-variance reduction"</a> -
joint work with
<a href="http://hyzhao.me">Haoyu Zhao</a>,
<a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a> and
<a href="https://zhizeli.github.io">Zhize Li</a>.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>December 7, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://dl.acm.org/doi/pdf/10.1145/3488659.3493775">"FL_PyTorch: optimization research simulator for federated learning"</a> -
joint work with
<a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a> and
<a href="https://samuelhorvath.github.io">Samuel Horváth.</a>
<br>
<br>
Abstract: <i>
Federated Learning (FL) has emerged as a promising technique for edge devices to collaboratively learn a shared machine learning model while keeping training data locally on the device, thereby removing the need to store and access the full data in the cloud. However, FL is difficult to implement, test and deploy in practice considering heterogeneity in common edge device settings, making it fundamentally hard for researchers to efficiently prototype and test their optimization algorithms. In this work, our aim is to alleviate this problem by introducing FL_PyTorch : a suite of open-source software written in python that builds on top of one the most popular research Deep Learning (DL) framework PyTorch. We built FL_PyTorch as a research simulator for FL to enable fast development, prototyping and experimenting with new and existing FL optimization algorithms. Our system supports abstractions that provide researchers with a sufficient level of flexibility to experiment with existing and novel approaches to advance the state-of-the-art. Furthermore, FL_PyTorch is a simple to use console system, allows to run several clients simultaneously using local CPUs or GPU(s), and even remote compute devices without the need for any distributed implementation provided by the user. FL_PyTorch also offers a Graphical User Interface. For new methods, researchers only provide the centralized implementation of their algorithm. To showcase the possibilities and usefulness of our system, we experiment with several well-known state-of-the-art FL algorithms and a few of the most common FL datasets.
</i>
<br>
<br>
The paper is published in the <a href="https://dl.acm.org/doi/pdf/10.1145/3488659.3493775">Proceedings of the 2nd ACM International Workshop on Distributed Machine Learning.</a>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>December 7, 2021</h3>
<h1>Oral Talk at NeurIPS 2021</h1>
<br>

Today I gave an oral talk at NeurIPS about the <a href="https://arxiv.org/abs/2106.05203">EF21 method.</a>  Come to our poster on Thursday! A longer version of the talk is on <a href="https://www.youtube.com/watch?v=rjWze5rcSUM&feature=emb_logo">YouTube.</a>
<br>

<br>
<img alt="" src="imgs/EF21-neurips.png" width="600">
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>November 24, 2021</h3>
<h1>KAUST-GSAI Workshop</h1>
<br>

Today and tomorrow I am attending (and giving a talk at) the <a href="http://ai.ruc.edu.cn/english/gsainews/20211119100.html">KAUST-GSAI Joint Workshop on Advances in AI</a>.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>November 22, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2111.11556">"FLIX: A Simple and Communication-Efficient Alternative to Local Methods in Federated Learning"</a> -
joint work with
<a href="http://elnurgasanov.com">Elnur Gasanov</a>,
<a href="https://rka97.github.io">Ahmed Khaled</a> and
<a href="https://samuelhorvath.github.io">Samuel Horváth.</a>
<br>
<br>
Abstract: <i>
Federated Learning (FL) is an increasingly popular machine learning paradigm in which multiple nodes try to collaboratively learn under privacy, communication and multiple heterogeneity constraints. A persistent problem in federated learning is that it is not clear what the optimization objective should be: the standard average risk minimization of supervised learning is inadequate in handling several major constraints specific to federated learning, such as communication adaptivity and personalization control. We identify several key desiderata in frameworks for federated learning and introduce a new framework, FLIX, that takes into account the unique challenges brought by federated learning. FLIX has a standard finite-sum form, which enables practitioners to tap into the immense wealth of existing (potentially non-local) methods for distributed optimization. Through a smart initialization that does not require any communication, FLIX does not require the use of local steps but is still provably capable of performing dissimilarity regularization on par with local methods. We give several algorithms for solving the FLIX formulation efficiently under communication constraints. Finally, we corroborate our theoretical results with extensive experimentation.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>November 17, 2021</h3>
<h1>Samuel, Dmitry and Grigory won the 2021 CEMSE Research Excellence Award!</h1>
<br>

Today I am very proud and happy! Three of my students won the CEMSE Research Excellence Award at KAUST: <a href="https://samuelhorvath.github.io">Samuel Horváth</a> (Statistics PhD student),
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> (Computer Science PhD student) and
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> (Applied Math and Computing Sciences MS student). The Statistics award is also known as the "Al-Kindi Research Excellence Award".
<br>
<br>

The award comes with a 1,000 USD cash prize for each. Congratulations to all of you, well deserved!
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>November 10, 2021</h3>
<h1>Talk at the SMAP Colloquium at the University of Portsmouth, United Kingdom</h1>
<br>

Today I gave a 1hr research talk on the EF21 method at the SMAP Colloquium, University of Portsmouth, UK.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




          <h3>November 2, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2111.01847">"Basis Matters: Better Communication-Efficient Second Order Methods for Federated Learning"</a> -
          joint work with
          <a href="https://qianxunk.github.io">Xun Qian</a>,
          <a href="https://rustem-islamov.github.io">Rustem Islamov</a> and
          <a href="https://mher-safaryan.github.io">Mher Safaryan.</a>
          <br>
          <br>
          Abstract: <i>
          Recent advances in distributed optimization have shown that Newton-type methods with proper communication compression mechanisms can guarantee fast local rates and low communication cost compared to first order methods. We discover that the communication cost of these methods can be further reduced, sometimes dramatically so, with a surprisingly simple trick: {\em Basis Learn (BL)}. The idea is to transform the usual representation of the local Hessians via a change of basis in the space of matrices and apply compression tools to the new representation. To demonstrate the potential of using custom bases, we design a new Newton-type method (BL1), which reduces communication cost via both {\em BL} technique and bidirectional compression mechanism. Furthermore, we present two alternative extensions (BL2 and BL3) to partial participation to accommodate federated learning applications. We prove local linear and superlinear rates independent of the condition number. Finally, we support our claims with numerical experiments by comparing several first and second~order~methods.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>November 1, 2021</h3>
          <h1>Talk at the CS Graduate Seminar at KAUST</h1>
          <br>

          Today I am giving <a href="https://cemse.kaust.edu.sa/events/event/ef21-new-simpler-theoretically-better-and-practically-faster-error-feedback">a talk in the CS Graduate Seminar at KAUST.</a>
           <br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>October 25, 2021</h3>
          <h1>Talk at KInIT</h1>
          <br>

          Today at 15:30 I am giving a research talk  at the <a href="https://kinit.sk">Kempelen Institute of Intelligent Technologies (KInIT)</a>, Slovakia.
           <br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>October 22, 2021</h3>
          <h1>Talk at "Matfyz"</h1>
          <br>

      Today at 15:30 I am giving a <a href="https://fmph.uniba.sk/detail-novinky/back_to_page/fakulta-matematiky-fyziky-a-informatiky-uk/article/seminar-zo-strojoveho-ucenia-peter-richtarik-22102021/">talk in the machine learning seminar at "Matfyz"</a>, Comenius University, Slovakia. I will talk about the paper
      <a href="https://arxiv.org/abs/2106.05203">"EF21: A new, simpler, theoretically better, and practically faster error feedback"</a> which was recently accepted to <a href="https://nips.cc/Conferences/2021">NeurIPS 2021</a> as an <span class="important">oral paper</span>
      (less than 1% acceptance rate from more than 9000 paper submissions). The paper is joint work with <a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a> and
      <a href="https://scholar.google.com/citations?user=G2OzFpIAAAAJ&hl=en">Ilyas Fatkhullin.</a>
          <br>

          <br>
          <img alt="" src="imgs/EF21-first-slide.png" width="700" >

          <br>  <br>

          With an extended set of coauthors, we have recently written a follow up paper with many major extensions of the EF21 method; you may wish
          to look at this as well:
          <a href="https://arxiv.org/abs/2110.03294">"EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback".</a> This second paper is joint work with
          <a href="https://scholar.google.com/citations?user=G2OzFpIAAAAJ&hl=en">Ilyas Fatkhullin</a>, <a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a>,
           <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>, and <a href="https://zhizeli.github.io">Zhize Li.</a>
           <br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>October 20, 2021</h3>
          <h1>Paper Accepted to Frontiers in Signal Processing</h1>
          <br>

      The paper <a href="https://www.frontiersin.org/articles/10.3389/frsip.2021.776825/abstract">"Distributed proximal splitting algorithms with rates and acceleration"</a>, joint work with <a href="https://lcondat.github.io">Laurent Condat</a> and
      <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>, was accepted to <a href="https://www.frontiersin.org/journals/signal-processing">Frontiers in Signal Processing, section Signal Processing for Communications.</a> The paper is
      is a part of a special issue ("research topic" in the language of Frontiers) dedicated to <a href="https://www.frontiersin.org/research-topics/21002/distributed-signal-processing-and-machine-learning-for-communication-networks#articles">"Distributed Signal Processing and Machine Learning for Communication Networks".</a>
          <br>



          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>October 15, 2021</h3>
          <h1>2 Students Received the 2021 NeurIPS Outstanding Reviewer Award</h1>
          <br>

        Congratulations to <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> and <a href = "https://konstmish.github.io">Konstantin Mishchenko</a> who received the <a href="https://nips.cc/Conferences/2021/ProgramCommittee">2021 NeurIPS Outstanding Reviewer Award</a> given to the top 8% reviewers, as judged
        by the conference chairs!
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



<h3>October 9, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2110.03313">"Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees"</a> -
joint work with <a href="https://anbeznosikov.github.io">Aleksandr Beznosikov</a>, <a href="https://www.hse.ru/en/staff/yhn112">Michael Diskin</a>, <a href="https://www.hse.ru/en/staff/mryabinin">Max Ryabinin</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.<br>
<br>
Abstract: <i>
Variational inequalities in general and saddle point problems in particular are increasingly relevant in machine learning applications, including adversarial learning, GANs, transport and robust optimization. With increasing data and problem sizes necessary to train high performing models across these and other applications, it is necessary to rely on parallel and distributed computing. However, in distributed training, communication among the compute nodes is a key bottleneck during training, and this problem is exacerbated for high dimensional and over-parameterized models models. Due to these considerations, it is important to equip existing methods with strategies that would allow to reduce the volume of transmitted information during training while obtaining a model of comparable quality. In this paper, we present the first theoretically grounded distributed methods for solving variational inequalities and saddle point problems using compressed communication: MASHA1 and MASHA2. Our theory and methods allow for the use of both unbiased (such as RandK; MASHA1) and contractive (such as TopK; MASHA2) compressors. We empirically validate our conclusions using two experimental setups: a standard bilinear min-max problem, and large-scale distributed adversarial training of transformers.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>October 8, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2110.03300">"Permutation Compressors for Provably Faster Distributed Nonconvex Optimization"</a> -
joint work with <a href="https://www.linkedin.com/in/rafał-szlendak-552936220">Rafał Szlendak</a> and <a href="https://k3nfalt.github.io">Alexander Tyurin</a>.<br>
<br>
Abstract: <i>
We study the MARINA method of Gorbunov et al (ICML, 2021) -- the current state-of-the-art distributed non-convex optimization method in terms of
theoretical communication complexity. Theoretical superiority of this method can be largely attributed to two sources: the use of a carefully
engineered biased stochastic gradient estimator, which leads to a reduction in the number of communication rounds, and  the reliance on
independent stochastic communication compression operators, which leads to a reduction in the number of  transmitted bits within each
 communication round. In this paper we i) extend the theory of MARINA to support a much wider class of potentially correlated
 compressors, extending the reach of the method beyond the classical independent compressors setting, ii) show that a new quantity, for which we
 coin the name Hessian variance, allows us to significantly refine the original analysis of MARINA without any additional assumptions, and
 iii) identify a special class of correlated compressors based on the idea of random  permutations, for which we coin the term PermK, the use of
 which leads to  $O(\sqrt{n})$ (resp. $O(1 + d/\sqrt{n})$) improvement in the theoretical communication complexity of MARINA in the low Hessian
 variance regime when $d\geq n$ (resp. $d \leq n$), where $n$ is the number of workers and $d$ is the number of parameters describing the model
 we are learning. We corroborate our theoretical results with carefully engineered synthetic experiments with minimizing the average of nonconvex
 quadratics, and on autoencoder training with the MNIST dataset.

</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>October 7, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2110.03294">"EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback"</a> -
joint work with
<a href="https://scholar.google.com/citations?user=G2OzFpIAAAAJ&hl=en">Ilyas Fatkhullin</a>,
<a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a>,
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>
and
<a href="https://zhizeli.github.io">Zhize Li</a>.<br>
<br>
Abstract: <i>
First proposed by Seide et al (2014) as a heuristic, error feedback (EF) is a very popular mechanism for enforcing convergence of distributed gradient-based
optimization methods enhanced with communication compression strategies based on the application of contractive compression operators. However, existing
theory of EF relies on very strong assumptions (e.g., bounded gradients), and provides pessimistic convergence rates (e.g., while the best known rate for
EF in the smooth nonconvex regime, and when full gradients are compressed, is O(1/T^{2/3}), the rate of gradient descent in the same regime is O(1/T).
Recently, Richt\'{a}rik et al (2021) proposed a new error feedback mechanism,
EF21, based on the construction of a Markov compressor induced by a contractive compressor. EF21 removes the aforementioned theoretical deficiencies
of EF and at the same time works better in practice. In this work we propose six practical extensions of EF21: partial participation, stochastic
approximation, variance reduction, proximal setting, momentum and bidirectional compression. Our extensions are supported by strong convergence
theory in the smooth nonconvex and also Polyak-Łojasiewicz regimes. Several of these techniques were never analyzed in conjunction with EF before,
and in cases where they were (e.g., bidirectional compression), our rates are vastly superior.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 5, 2021</h3>
<h1>2020 COAP Best Paper Award</h1>
<br>
We have just received this email:
<br><br>
<i>
  Your paper "Momentum and stochastic momentum for
stochastic gradient, Newton, proximal point and subspace descent methods"
published in Computational  Optimization and Applications  was voted by the
editorial board as the best paper appearing in the journal in 2020.
There were 93 papers in  the 2020 competition. Congratulations!
</i>
<br><br>
The <a href="https://link.springer.com/article/10.1007/s10589-020-00220-z">paper</a> is joint work with <a href="https://nicolasloizou.github.io">Nicolas Loizou.</a>

<br>
<br>
<img alt="" src="imgs/COAP-best-paper.png" width="600" >
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>October 4, 2021</h3>
<h1>Konstantin Mishchenko Defended his PhD Thesis</h1>
<br>

<a href="https://konstmish.github.io">Konstantin Mishchenko</a> defended his PhD thesis "On Seven Fundamental Optimization Challenges in Machine Learning" today.
<br> <br>

Having started in Fall 2017 (I joined KAUST in March of the same year), Konstantin is my second PhD student to graduate from KAUST. Konstantin
has done some absolutely remarkable research, described by the committee (<a href="http://optml.mit.edu">Suvrit Sra</a>, <a href="https://www.math.ucla.edu/~wotaoyin/">Wotao Yin</a>,
<a href="https://dblp.org/pid/23/2168.html">Lawrence Carin</a>, <a href="http://www.bernardghanem.com">Bernard Ghanem</a> and myself) in the following way:
"The committee commends Konstantin Mishchenko on his outstanding achievements, including research creativity, depth of technical/mathematical results, volume of published work,
service to the community, and a particularly lucid presentation and defense of his thesis".

<br> <br>
Konstantin wrote more than 20 papers and his works attracted more than 500 citations during his PhD. Konstantin's next destination is a postdoctoral fellowship
position with <a href="https://www.di.ens.fr/~aspremon/">Alexander d'Aspremont</a> and <a href="https://www.di.ens.fr/~fbach/">Francis Bach</a> at INRIA.
Congratulations, Konstantin!

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>September 29, 2021</h3>
<h1>Papers Accepted to NeurIPS 2021</h1>
<br>
We've had several papers accepted to the <a href="https://nips.cc/Conferences/2021">35th Annual Conference on Neural Information Processing Systems (NeurIPS 2021)</a>, which will be run virtually during December 6-14, 2021.
Here they are:<br><br>


<b> 1) "EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback"</b> <a href="https://arxiv.org/abs/2106.05203">[arXiv]</a>  -
joint work with
<a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a> and
<a href="https://scholar.google.com/citations?user=G2OzFpIAAAAJ&hl=en">Ilyas Fatkhullin.</a><br>
<br>
This paper was accepted as an <span class="important">ORAL PAPER (less than 1% of all submissions).</span>
<br><br>

Further links:
<ul>
  <li><a href="https://www.youtube.com/watch?v=rjWze5rcSUM&feature=emb_logo">Long 57 min YouTube talk by me at the FLOW seminar</a> </li>
  <li><a href="https://drive.google.com/file/d/1zNS7wJgd1XSxnK7jTItcGvluTLLs49oL/view?usp=sharing">slides</a></li>
</ul>
<br>

<b> 2) "CANITA: Faster Rates for Distributed Convex Optimization with Communication Compression"</b>
<a href="https://arxiv.org/abs/2107.09461">[arXiv]</a>  -
joint work with
<a href="https://zhizeli.github.io">Zhize Li.</a><br>
<br>
<br>

<b> 3) "Smoothness Matrices Beat Smoothness Constants: Better Communication Compression Techniques for Distributed Optimization"</b>
<a href="https://arxiv.org/abs/2102.07245">[arXiv]</a>  -
joint work with
<a href="https://mher-safaryan.github.io">Mher Safaryan</a> and <a href="https://fhanzely.github.io/index.html">Filip Hanzely.</a><br>
<br>
<br>

<b> 4) "Error Compensated Distributed SGD can be Accelerated"</b>
<a href="https://arxiv.org/abs/2010.00091">[arXiv]</a>  -
joint work with
<a href="https://qianxunk.github.io">Xun Qian</a> and <a href="http://tongzhang-ml.org">Tong Zhang.</a><br>
<br>
<br>

<b> 5) "Lower Bounds and Optimal Algorithms for Smooth and Strongly Convex Decentralized Optimization Over Time-Varying Networks"</b>
<a href="https://arxiv.org/abs/2106.04469">[arXiv]</a>  -
joint work with
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
<a href="http://elnurgasanov.com">Elnur Gasanov</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov.</a><br>
<br>
<br>

<b> 6) "FjORD: Fair and Accurate Federated Learning Under Heterogeneous Targets with Ordered Dropout"</b>
<a href="https://arxiv.org/abs/2102.13451">[arXiv]</a>  -
the work of
<a href="https://samuelhorvath.github.io">Samuel Horváth</a>,
<a href="https://stevelaskaridis.github.io">Stefanos Laskaridis</a>,
Mario Almeida,
<a href="https://leontiadis.net">Ilias Leontiadis</a>,
<a href="https://steliosven10.github.io">Stylianos I. Venieris </a> and
<a href="http://niclane.org">Nicholas D. Lane.</a>
<br>
<br>
<br>

<b> 7) "Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices"</b>
<a href="https://arxiv.org/abs/2103.03239">[arXiv]</a>  -
the work of
<a href="https://www.hse.ru/en/staff/mryabinin">Max Ryabinin</a>,
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>,
<a href="https://research.yandex.com/people/610925">Vsevolod Plokhotnyuk</a> and
<a href="https://www.cs.toronto.edu/~pekhimenko/">Gennady Pekhimenko.</a>





<br>
<br>



<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 23, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2109.10049">"Error Compensated Loopless SVRG, Quartz, and SDCA for Distributed Optimization"</a> -
joint work with <a href="https://qianxunk.github.io">Xun Qian</a>, <a href="https://www.researchgate.net/profile/Hanze-Dong">Hanze Dong</a>, and <a href="http://tongzhang-ml.org">Tong Zhang</a>.<br>
<br>

Abstract: <i>
The communication of gradients is a key bottleneck in distributed training of large scale machine learning models. In order to reduce the communication cost, gradient compression (e.g., sparsification and quantization) and error compensation techniques are often used. In this paper, we propose and study three new efficient methods in this space: error compensated loopless SVRG method (EC-LSVRG), error compensated Quartz (EC-Quartz), and error compensated SDCA (EC-SDCA). Our method is capable of working with any contraction compressor (e.g., TopK compressor), and we perform analysis for convex optimization problems in the composite case and smooth case for EC-LSVRG. We prove linear convergence rates for both cases and show that in the smooth case the rate has a better dependence on the parameter associated with the contraction compressor. Further, we show that in the smooth case, and under some certain conditions, error compensated loopless SVRG has the same convergence rate as the vanilla loopless SVRG method. Then we show that the convergence rates of EC-Quartz and EC-SDCA in the composite case are as good as EC-LSVRG in the smooth case. Finally, numerical experiments are presented to illustrate the efficiency of our methods.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>September 11, 2021</h3>
<h1>New Paper</h1>
 <br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2109.05198">"Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information"</a> -
joint work with <a href="https://coral.ise.lehigh.edu/maj316/">Majid Jahani</a>, <a href="https://scholar.google.com/citations?user=tdyw7IAAAAAJ&hl=lv">Sergey Rusakov</a>, <a href="https://coral.ise.lehigh.edu/zhs310/">Zheng Shi</a>,
<a href="https://www.stat.berkeley.edu/~mmahoney/">Michael W. Mahoney</a>, and
<a href="https://mtakac.com">Martin Takáč</a>.<br>
 <br>
Abstract: <i> We present a novel adaptive optimization algorithm for large-scale machine learning problems. Equipped with a low-cost estimate of local curvature and Lipschitz smoothness, our method dynamically adapts the search direction and step-size. The search direction contains gradient information preconditioned by a well-scaled diagonal preconditioning matrix that captures the local curvature information. Our methodology does not require the tedious task of learning rate tuning, as the learning rate is updated automatically without adding an extra hyperparameter. We provide convergence guarantees on a comprehensive collection of optimization problems, including convex, strongly convex, and nonconvex problems, in both deterministic and stochastic regimes. We also conduct an extensive empirical evaluation on standard machine learning problems, justifying our algorithm's versatility and demonstrating its strong performance compared to other start-of-the-art first-order and second-order methods.
</i>

 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>August 29, 2021</h3>
 <h1>Fall 2021 Semester Started</h1>

 The Fall semester has started at KAUST today; I am teaching CS 331: Stochastic Gradient Descent Methods.
 <br> <br>

 <i>Brief course blurb:</i> Stochastic gradient descent (SGD) in one or another of its many variants is the workhorse method for training modern supervised machine learning models. However, the world of SGD methods is vast and expanding, which makes it hard for practitioners and even experts to understand its landscape and inhabitants. This course is a mathematically rigorous and comprehensive introduction to the field, and is based on the latest results and insights.
 The course develops a convergence and complexity theory for serial, parallel, and distributed variants of SGD, in the strongly convex, convex and nonconvex setup, with randomness coming from sources such as subsampling and compression. Additional topics such as acceleration via Nesterov momentum or curvature information will be covered as well.
 A substantial part of the course offers a unified analysis of a large family of variants of SGD which have so far required different intuitions, convergence analyses, have different applications, and which have been developed separately in various communities. This framework includes methods with and without the following tricks, and their combinations: variance reduction, data sampling, coordinate sampling, arbitrary sampling, importance sampling, mini-batching, quantization, sketching, dithering and sparsification.
 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>August 11, 2021</h3>
 <h1>New Paper</h1>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2108.04755">"FedPAGE: A Fast Local Stochastic Gradient Method for Communication-Efficient Federated Learning"</a> -
 joint work with <a href="http://hyzhao.me">Haoyu Zhao</a> and <a href="https://zhizeli.github.io">Zhize Li</a>.<br>
 <br>

 Abstract: <i> Federated Averaging (FedAvg, also known as Local-SGD) [McMahan et al., 2017] is a classical federated learning algorithm in which
   clients run multiple local SGD steps before communicating their update to an orchestrating server.  We propose a new federated learning algorithm,
   FedPAGE,  able to further reduce the communication complexity by utilizing the recent optimal PAGE method [Li et al., 2021] instead of plain
   SGD in FedAvg. We show that FedPAGE uses much fewer communication rounds than previous local methods for both federated convex and nonconvex
   optimization. Concretely, 1) in the convex setting, the number of communication rounds of FedPAGE is $O(\frac{N^{3/4}}{S\epsilon})$, improving
   the best-known result $O(\frac{N}{S\epsilon})$ of SCAFFOLD [Karimireddy et al., 2020] by a factor of $N^{1/4}$, where $N$ is the total number
   of clients (usually is very large in federated learning), $S$ is the sampled subset of clients in each communication round, and $\epsilon$ is
   the target error; 2) in the nonconvex setting, the number of communication rounds of FedPAGE is $O(\frac{\sqrt{N}+S}{S\epsilon^2})$, improving
   the best-known result  $O(\frac{N^{2/3}}{S^{2/3}\epsilon^2})$ of SCAFFOLD by a factor of $N^{1/6}S^{1/3}$ if the sampled clients $S\leq \sqrt{N}$.
   Note that in both settings, the communication cost for each round is the same for both FedPAGE and SCAFFOLD. As a result, FedPAGE achieves new
   state-of-the-art results in terms of communication complexity for both federated convex and nonconvex optimization.
 </i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>




 <h3>July 20, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2107.09461">"CANITA: Faster Rates for Distributed Convex Optimization with Communication Compression"</a> -
 joint work with <a href="https://zhizeli.github.io">Zhize Li</a>.<br>
 <br>
 <br>
 In this work we develop and analyze the first distributed gradient method  capable in the convex regime of benefiting from
 communication compression and acceleration/momentum at the same time. The strongly convex regime was first handled in the <a href="http://proceedings.mlr.press/v119/li20g.html">ADIANA
 paper (ICML 2020)</a>, and the nonconvex regime in the <a href="http://proceedings.mlr.press/v139/gorbunov21a.html">MARINA paper (ICML 2021)</a>.
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>July 20, 2021</h3>
 <h1>Talk at SIAM Conference on Optimization</h1>
 <br>

 Today I gave a talk in the <a href="https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=71575">Recent Advancements in Optimization Methods for Machine Learning - Part I of III</a> minisymposium
 at the <a href="https://www.siam.org/conferences/cm/conference/op21">SIAM Conference on Optimization.</a> The conference was originally supposed to take place in Hong Kong in 2020, but due to the Covid-19
 situation, this was not to be. Instead, the event is happening this year, and virtually. I was on the organizing committee for the conference, jointly resposible for inviting plenary and tutorial speakers.
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>July 12, 2021</h3>
 <h1>Optimization Without Borders Conference (Nesterov 65)</h1>
 <br>

 Today I am giving a talk at the <a href="https://cs.hse.ru/hdilab/opti/">"Optimization Without Borders"</a> conference, organized in the honor of <a href="https://en.wikipedia.org/wiki/Yurii_Nesterov">Yurii Nesterov's</a> 65th Birthday. This is a hybrid
 event, with online and offline participants. The offline part takes place at the <a href="https://siriusuniversity.ru">Sirius University</a> in Sochi, Russia.
 <br>    <br>
 Other speakers at the event (in order of giving talks at the event): Gasnikov, Nesterov, myself, Spokoiny, Mordukhovich, Bolte, Belomestny, Srebro,
 Zaitseva, Protasov, Shikhman, d'Aspremont, Polyak, Taylor, Stich, Teboulle, Lasserre, Nemirovski,  Vorobiev,
 Yanitsky, Bakhurin, Dudorov, Molokov, Gornov, Rogozin, Hildebrand, Dvurechensky, Moulines, Juditsky,
 Sidford, Tupitsa, Kamzolov, and Anikin.

 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>July 5, 2021</h3>
 <h1>New Postdoc: Alexander Tyurin</h1>
 <br>

 <a href="https://k3nfalt.github.io">Alexander Tyurin</a> has joined my
 Optimization and Machine Learning lab as a postdoc. Welcome!!!

<br> <br>
 Alexander obtained his PhD from
 the Higher School of Economics (HSE) in December 2020, under the supervision of <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ">Alexander
 Gasnikov</a>, with the thesis "Development of a method for solving structural
 optimization problems". His 15 research papers can be found on <a href="https://scholar.google.com/citations?user=Es8-xocAAAAJ">Google Scholar</a>. He has a masters degree in CS from
 HSE (2017), with a GPA of 9.84 / 10, and a BS degree in Computational Mathematics and Cybernetics from
 Lomonosov Moscow State University, with a GPA of 4.97 / 5.

<br> <br>
  During his studies, and for a short period of time after his PhD,
 Alexander worked as a research and development engineer in the Yandex self-driving cars team, where he was developing real-time algorithms for dynamic and static objects detection in a perception team
 for self-driving cars Using lidar (3D point clouds) and cameras (images) sensors.
His primary responsibilities there ranged from the creation of datasets, throught research (Python, SQL, MapReduce) and implementation of the
proposed algorithms (C++). Prior to this, he was a Research Engineer at VisionLabs in Moscow where he developed a face recognition
algorithm that achieved a top 2 result in the FRVT NIST international competition.

 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>July 4, 2021</h3>
 <h1>Two New Interns</h1>
 <br>

 Two new people joined my team as Summer research interns:
 <br><br>

 <b>Rafał Szlendak</b> joined as an undergraduate intern.  Rafal is studying towards a <a href="https://warwick.ac.uk/study/undergraduate/courses/mathsstatsbsc/">BSc degree in Mathematics and Statistics</a> at the <a href="https://warwick.ac.uk">University of Warwick</a>, United Kingdom.
 He was involved in a research project entitled "Properties and characterisations of sequences generated
by weighted context-free grammars with one terminal symbol". Among Rafal’s successes belong
 <br>
 - Ranked #1 in the Mathematics and Statistics Programme at Warwick, 2020
 <br>
 - Finalist, Polish National Mathematical Olympiad, 2017 and 2018
 <br>
 - Member of MATEX: an experimental mathematics programme for gifted students. This high school was ranked the top high school in Poland in the 2019 Perspektywy ranking.



<br>
<br>
 <b>Muhammad Harun Ali Khan</b> joined as an undergraduate intern. Harun is a US citizen of Pakistani ancestry, and studies towards a <a href="https://www.imperial.ac.uk/study/ug/courses/mathematics-department/mathematics-bsc/">BSc degree in Mathematics</a> at <a href="https://www.imperial.ac.uk">Imperial College London</a>. He has interests in
 number theory, artificial intelligence and doing mathematics via the <a href="https://en.wikipedia.org/wiki/Lean_(proof_assistant)">Lean proof assistant</a>. Harun is the Head of Imperial College mathematics competition problem selection committee.
 Harun has been active in various mathematics competitions at high school and university level. Some of his most notable recognitions and awards include
 <br>
 - <a href="https://www.imc-math.org.uk/?act=results&by=sum&year=2020">2nd Prize, International Mathematics Competition for University Students, 2020</a>
 <br>
 - Imperial College UROP Prize (for formalizing Fibonacci Squares in Lean)
 <br>
 - Imperial College Mathematics Competition, First Place in First Round
 <br>
 - <a href="https://www.imo-official.org/participant_r.aspx?id=27280">Bronze Medal, International Mathematical Olympiad, United Kingdom, 2019</a>
 <br>
 - <a href="https://www.apmo-official.org/country_report/PAK/2019">Bronze Medal, Asian Pacific Mathematics Olympiad, 2019</a>
 <br>
 - Honorable Mention, International Mathematical Olympiad, Romania, 2018
 <br>
 - Honorable Mention, International Mathematical Olympiad, Brazil, 2017
<br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>


 <h3>June 9, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2106.05203">"EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback"</a> -
 joint work with <a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a> and Ilyas Fatkhullin.<br>
 <br>

 Abstract: <i>
Error feedback (EF), also known as error compensation, is an immensely popular convergence stabilization mechanism in the context of distributed training of supervised machine learning models enhanced by the use of contractive communication compression mechanisms, such as Top-k. First proposed by Seide et al (2014) as a heuristic, EF resisted any theoretical understanding until recently [Stich et al., 2018, Alistarh et al., 2018]. However, all existing analyses either i) apply to the single node setting only, ii) rely on very strong and often unreasonable assumptions, such global boundedness of the gradients, or iterate-dependent assumptions that cannot be checked a-priori and may not hold in practice, or iii) circumvent these issues via the introduction of additional unbiased compressors, which increase the communication cost. In this work we fix all these deficiencies by proposing and analyzing a new EF mechanism, which we call EF21, which consistently and substantially outperforms EF in practice. Our theoretical analysis relies on standard assumptions only, works in the distributed heterogeneous data setting, and leads to better and more meaningful rates. In particular, we prove that EF21 enjoys a fast O(1/T) convergence rate for smooth nonconvex problems, beating the previous bound of O(1/T^{2/3}), which was shown a bounded gradients assumption. We further improve this to a fast linear rate for PL functions, which is the first linear convergence result for an EF-type method not relying on unbiased compressors. Since EF has a large number of applications where it reigns supreme, we believe that our 2021 variant, EF21, can a large impact on the practice of communication efficient distributed learning.
</i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>June 8, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2106.04469">"Lower Bounds and Optimal Algorithms for Smooth and Strongly Convex Decentralized Optimization Over Time-Varying Networks"</a> -
 joint work with <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="http://elnurgasanov.com">Elnur Gasanov</a> and <a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.<br>
 <br>

 Abstract: <i>
We consider the task of minimizing the sum of smooth and strongly convex functions stored in a decentralized manner across the nodes of a communication network whose links are allowed to change in time. We solve two fundamental problems for this task. First, we establish the first lower bounds on the number of decentralized communication rounds and the number of local computations required to find an ϵ-accurate solution. Second, we design two optimal algorithms that attain these lower bounds: (i) a variant of the recently proposed algorithm ADOM (Kovalev et al., 2021) enhanced via a multi-consensus subroutine, which is optimal in the case when access to the dual gradients is assumed, and (ii) a novel algorithm, called ADOM+, which is optimal in the case when access to the primal gradients is assumed. We corroborate the theoretical efficiency of these algorithms by performing an experimental comparison with existing state-of-the-art methods.
</i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>June 6, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2106.03524">"Smoothness-Aware Quantization Techniques"</a> -
 joint work with Bokun Wang, and <a href="https://mher-safaryan.github.io">Mher Safaryan</a>.<br>
 <br>

 Abstract: <i>
Distributed machine learning has become an indispensable tool for training large supervised machine learning models. To address the high communication costs of distributed training, which is further exacerbated by the fact that modern highly performing models are typically overparameterized, a large body of work has been devoted in recent years to the design of various compression strategies, such as sparsification and quantization, and optimization algorithms capable of using them. Recently, Safaryan et al (2021) pioneered a dramatically different compression design approach: they first use the local training data to form local "smoothness matrices", and then propose to design a compressor capable of exploiting the smoothness information contained therein. While this novel approach leads to substantial savings in communication, it is limited to sparsification as it crucially depends on the linearity of the compression operator. In this work, we resolve this problem by extending their smoothness-aware compression strategy to arbitrary unbiased compression operators, which also includes sparsification. Specializing our results to quantization, we observe significant savings in communication complexity compared to standard quantization. In particular, we show theoretically that block quantization with n blocks outperforms single block quantization, leading to a reduction in communication complexity by an O(n) factor, where n is the number of nodes in the distributed system. Finally, we provide extensive numerical evidence that our smoothness-aware quantization strategies outperform existing quantization schemes as well the aforementioned smoothness-aware sparsification strategies with respect to all relevant success measures: the number of iterations, the total amount of bits communicated, and wall-clock time.
</i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>




 <h3>June 6, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2106.03076">"Complexity Analysis of Stein Variational Gradient Descent under Talagrand's Inequality T1"</a> -
 joint work with
 <a href="https://adil-salim.github.io">Adil Salim</a>, and <a href="https://lukangsun.github.io">Lukang Sun</a>.<br>
 <br>

 Abstract: <i>
We study the complexity of Stein Variational Gradient Descent (SVGD), which is an algorithm to sample from π(x)∝exp(−F(x)) where F smooth and nonconvex.
We provide a clean complexity bound for SVGD in the population limit in terms of the
Stein Fisher Information (or squared Kernelized Stein Discrepancy), as a function of the dimension of the problem d and the desired accuracy ε.
Unlike existing work, we do not make any assumption on the trajectory of the algorithm. Instead, our key assumption is that the target distribution
satisfies Talagrand's inequality T1.
</i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>June 6, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2106.03056">"MURANA: A Generic Framework for Stochastic Variance-Reduced Optimization"</a> -
 joint work with
 <a href="https://lcondat.github.io">Laurent Condat</a>.<br>
 <br>

 Abstract: <i>
We propose a generic variance-reduced algorithm, which we call MUltiple RANdomized Algorithm (MURANA), for minimizing a sum of several smooth functions plus a regularizer, in a sequential or distributed manner. Our method is formulated with general stochastic operators, which allow us to model various strategies for reducing the computational complexity. For example, MURANA supports sparse activation of the gradients, and also reduction of the communication load via compression of the update vectors. This versatility allows MURANA to cover many existing randomization mechanisms within a unified framework. However, MURANA also encodes new methods as special cases. We highlight one of them, which we call ELVIRA, and show that it improves upon Loopless SVRG.
 </i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>June 5, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2106.02969">"FedNL: Making Newton-Type Methods Applicable to Federated Learning"</a> -
 joint work with
 <a href="https://mher-safaryan.github.io">Mher Safaryan</a>,
 <a href="https://rustem-islamov.github.io">Rustem Islamov</a> and <a href="https://qianxunk.github.io">Xun Qian</a>.<br>
 <br>

 Abstract: <i>
  Inspired by recent work of Islamov et al (2021), we propose a family of Federated Newton Learn (FedNL) methods, which we believe is a marked step in the direction of making second-order methods applicable to FL. In contrast to the aforementioned work, FedNL employs a different Hessian learning technique which i) enhances privacy as it does not rely on the training data to be revealed to the coordinating server, ii) makes it applicable beyond generalized linear models, and iii) provably works with general contractive compression operators for compressing the local Hessians, such as Top-K or Rank-R, which are vastly superior in practice. Notably, we do not need to rely on error feedback for our methods to work with contractive compressors. Moreover, we develop FedNL-PP, FedNL-CR and FedNL-LS, which are variants of FedNL that support partial participation, and globalization via cubic regularization and line search, respectively, and FedNL-BC, which is a variant that can further benefit from bidirectional compression of gradients and models, i.e., smart uplink gradient and smart downlink model compression. We prove local convergence rates that are independent of the condition number, the number of training data points, and compression variance. Our communication efficient Hessian learning technique provably learns the Hessian at the optimum. Finally, we perform a variety of numerical experiments that show that our FedNL methods have state-of-the-art communication complexity when compared to key baselines.
 </i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>June 5, 2021</h3>
 <h1>Finally, the NeurIPS Month of Deadlines is Over</h1>

 <br>
 I've been silent here for a while due a stream of NeurIPS deadlines (abstract, paper, supplementary material). Me and my fantastic <a href="https://richtarik.org/i_team.html">team</a> can rest a bit now! <br>


 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>May 10, 2021</h3>
 <h1>Papers Accepted to ICML 2021</h1>

 We've had several papers accepted to the <a href="https://icml.cc/Conferences/2021">International Conference on Machine Learning (ICML 2021)</a>, which will be run virtually during July 18-24, 2021.
 Here they are:<br><br>


 <b> 1) "MARINA: Faster Non-convex Distributed Learning with Compression"</b> <a href="https://arxiv.org/abs/2102.07845">[arXiv]</a> [<a href="http://proceedings.mlr.press/v139/gorbunov21a.html">ICML</a>]  - joint work with
 <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>,
 <a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a> and <a href="https://zhizeli.github.io">Zhize Li</a>.<br>
 <br>

 Abstract: <i>We develop and analyze MARINA: a new communication efficient method for non-convex distributed learning over heterogeneous datasets. MARINA employs a novel communication compression strategy based on the compression of gradient differences which is reminiscent of but different from the strategy employed in the DIANA method of Mishchenko et al (2019). Unlike virtually all competing distributed first-order methods, including DIANA, ours is based on a carefully designed biased gradient estimator, which is the key to its superior theoretical and practical performance. To the best of our knowledge, the communication complexity bounds we prove for MARINA are strictly superior to those of all previous first order methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and PP-MARINA. The first method is designed for the case when the local loss functions owned by clients are either of a finite sum or of an expectation form, and the second method allows for partial participation of clients -- a feature important in federated learning. All our methods are superior to previous state-of-the-art methods in terms of the oracle/communication complexity. Finally, we provide convergence analysis of all methods for problems satisfying the Polyak-Lojasiewicz condition.
 </i>

 <br>
 <br>

 More material:
 <ul>
   <li><a href="https://www.youtube.com/watch?v=o5MwC4DYbGE&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=34">Short 5 min YouTube talk by Konstantin</a></li>
   <li><a href="https://www.youtube.com/watch?v=bj0E94Siq74">Long 70 min YouTube talk by Eduard delivered at the FLOW seminar</a> </li>
   <li><a href="">poster</a></li>
 </ul>
 <br>



 <b> 2) "PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization"</b> <a href="https://arxiv.org/abs/2008.10898">[arXiv]</a> <a href="http://proceedings.mlr.press/v139/li21a.html">[ICML]</a>  - joint work with
 <a href="https://zhizeli.github.io">Zhize Li</a>, <a href="https://cemse.kaust.edu.sa/people/person/hongyan-bao">Hongyan Bao</a>, and <a href="https://cemse.kaust.edu.sa/people/person/xiangliang-zhang">Xiangliang Zhang</a>.


 <br>
 <br>

 Abstract: <i>In this paper, we propose a novel stochastic gradient estimator---ProbAbilistic Gradient Estimator (PAGE)---for nonconvex optimization. PAGE is easy to implement as it is designed via a small adjustment to vanilla SGD:
   in each iteration, PAGE uses the vanilla minibatch SGD update with probability p or reuses the previous gradient with a small adjustment, at a much lower computational cost, with probability 1−p. We give a simple formula for the
   optimal choice of p. We prove tight lower bounds for nonconvex problems, which are of independent interest. Moreover, we prove matching upper bounds both in the finite-sum and online regimes, which establish that PAGE is an optimal
   method. Besides, we show that for nonconvex functions satisfying the Polyak-Łojasiewicz (PL) condition, PAGE can automatically switch to a faster linear convergence rate. Finally, we conduct several deep learning experiments (e.g.,
   LeNet, VGG, ResNet) on real datasets in PyTorch, and the results demonstrate that PAGE not only converges much faster than SGD in training but also achieves the higher test accuracy, validating our theoretical results and confirming
   the practical superiority of PAGE. </i>
   <br>
 <br>

 More material:
 <ul>
   <li><a href="https://www.youtube.com/watch?v=_K3XPxN-vdk&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=29">Short 5 min YouTube talk by Zhize</a></li>
 </ul>


 <br>


 <b> 3) "Distributed Second Order Methods with Fast Rates and Compressed Communication" </b> <a href="https://arxiv.org/abs/2102.07158">[arXiv]</a> <a href="http://proceedings.mlr.press/v139/islamov21a.html">[ICML]</a> - joint work with  <a
 href="https://rustem-islamov.github.io">Rustem Islamov</a> and <a href="https://qianxunk.github.io">Xun Qian</a>.<br>
 <br>

 Abstract: <i>
 We develop several new communication-efficient second-order methods for distributed optimization. Our first method, NEWTON-STAR, is a variant of Newton's method from which it inherits its fast local quadratic rate. However, unlike Newton's method, NEWTON-STAR enjoys the same per iteration communication cost as gradient descent. While this method is impractical as it relies on the use of certain unknown parameters characterizing the Hessian of the objective function at the optimum, it serves as the starting point which enables us design practical variants thereof with strong theoretical guarantees. In particular, we design a stochastic sparsification strategy for learning the unknown parameters in an iterative fashion in a communication efficient manner. Applying this strategy to NEWTON-STAR leads to our next method, NEWTON-LEARN, for which we prove local linear and superlinear rates independent of the condition number. When applicable, this method can have dramatically superior convergence behavior when compared to state-of-the-art methods. Finally, we develop a globalization strategy using cubic regularization which leads to our next method, CUBIC-NEWTON-LEARN, for which we prove global sublinear and linear convergence rates, and a fast superlinear rate. Our results are supported with experimental results on real datasets, and show several orders of magnitude improvement on baseline and state-of-the-art methods in terms of communication complexity.
  </i> <br>
 <br>

 More material:
 <ul>
   <li><a href="https://www.youtube.com/watch?v=iSKBZXlaoWo&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=41">Short 5 min YouTube talk by Rustem</a></li>
   <li><a href="https://www.youtube.com/watch?v=jqgyjN5kLqo">Long 80 min YouTube talk by myself delivered at the FLOW seminar</a></li>
   <li><a href="https://richtarik.org/talks/TALK-2021-Newton-Learn.pdf">my FLOW talk slides</a></li>
   <li><a href="https://richtarik.org/posters/Poster-Newton-Learn.pdf">poster</a></li>
 </ul>
 <br>

 <b> 4) "Stochastic Sign Descent Methods: New Algorithms and Better Theory" </b>  <a href="https://arxiv.org/abs/1905.12938">[arXiv]</a> <a href="http://proceedings.mlr.press/v139/safaryan21a.html">[ICML]</a>  - joint work with <a href="https://mher-safaryan.github.io">Mher Safaryan</a>. <br>
 <br>

 Abstract: <i>
 Various gradient compression schemes have been proposed to mitigate the communication cost in distributed training of large scale machine learning models. Sign-based methods, such as signSGD, have recently been gaining popularity because of their simple compression rule and connection to adaptive gradient methods, like ADAM. In this paper, we analyze sign-based methods for non-convex optimization in three key settings: (i) standard single node, (ii) parallel with shared data and (iii) distributed with partitioned data. For single machine case, we generalize the previous analysis of signSGD relying on intuitive bounds on success probabilities and allowing even biased estimators. Furthermore, we extend the analysis to parallel setting within a parameter server framework, where exponentially fast noise reduction is guaranteed with respect to number of nodes, maintaining 1-bit compression in both directions and using small mini-batch sizes. Next, we identify a fundamental issue with signSGD to converge in distributed environment. To resolve this issue, we propose a new sign-based method, Stochastic Sign Descent with Momentum (SSDM), which converges under standard bounded variance assumption with the optimal asymptotic rate. We validate several aspects of our theoretical findings with numerical experiments.
 </i> <br>
 <br>

 More material:
 <ul>
   <li><a href="https://opt-ml.org/posters/2020/poster_14.png">poster</a></li>
 </ul>
 <br>


 <b> 5) "ADOM: Accelerated Decentralized Optimization Method for Time-Varying Networks"</b> <a href="https://arxiv.org/abs/2102.09234">[arXiv] </a> <a href="http://proceedings.mlr.press/v139/kovalev21a.html">[ICML] </a>
 - joint work with
 <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://shulgin-egor.github.io">Egor Shulgin</a>,
  <a
 href="https://scholar.google.com/citations?hl=ru&user=sEjyzkgAAAAJ">Alexander Rogozin</a> and <a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
 <br>
 <br>

 Abstract: <i>We propose ADOM - an accelerated method for smooth and strongly convex decentralized optimization over time-varying networks. ADOM uses a dual oracle, i.e., we assume access to the gradient of the Fenchel conjugate of the individual loss functions. Up to a constant factor, which depends on the network structure only, its communication complexity is the same as that of accelerated Nesterov gradient method (Nesterov, 2003). To the best of our knowledge, only the algorithm of Rogozin et al. (2019) has a convergence rate with similar properties. However, their algorithm converges under the very restrictive assumption that the number of network changes can not be greater than a tiny percentage of the number of iterations. This assumption is hard to satisfy in practice, as the network topology changes usually can not be controlled. In contrast, ADOM merely requires the network to stay connected throughout time.
 </i><br>

 More material:
 <ul>
   <li>  <a href="https://www.youtube.com/watch?v=jO3t4eZFdkc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=37">Short 5 min YouTube talk by Egor</a> </li>
   <li>  <a href="posters/Poster-ADOM-ICML-2021.pdf">poster</a> </li>
 </ul>



 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



<h3>April 29, 2021</h3>
<h1>Paper Accepted to IEEE Transactions on Information Theory</h1>

<br>
Our paper <a href="https://arxiv.org/abs/1905.08645">Revisiting randomized gossip algorithms: general framework, convergence rates and novel block and accelerated protocols</a>, joint work with
<a href="https://nicolasloizou.github.io">Nicolas Loizou</a>, was accepted to IEEE Transactions on Information Theory.<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 28, 2021</h3>
<h1>KAUST Conference on Artificial Intelligence: 17 Short (up to 5 min) Talks by Members of my Team!</h1>
<br>

Today and tomorrow I am attending the <a href="https://cemse.kaust.edu.sa/ai/aii-conf-2021">KAUST Conference on Artificial Intelligence</a>. Anyone can attend for free by watching the <a href="https://kaust.zoom.us/j/96464686903">LIVE Zoom webinar stream.</a> Today I have given a short 20
  min talk today entitled "Recent Advances in Optimization for Machine Learning". Here are my slides:


  <br>

  <br>
  <a href="slides/Talk-1-KAUST-AI-Conference-2021.pdf"><img alt="" src="imgs/KAUST-AI-Conference-2021-Talk-1-Richtarik.png" width="700"></a>
  <br>


<br>


   I will deliver another 20 min talk tomorrow, entitled "On Solving a Key Challenge in Federated Learning: Local Steps, Compression
  and Personalization". Here are the slides:


    <br>

    <br>
    <a href="slides/Talk-2-KAUST-AI-Conference-2021.pdf"><img alt="" src="imgs/KAUST-AI-Conference-2021-Talk-2-Richtarik.png" width="700"></a>
    <br>


  <br>


<br>
<br>


  More importantly, 17 members (research scientists, postdocs, PhD students,  MS students and interns) of the "Optimization and Machine Learning Lab"  that I lead at KAUST have
  prepared short videos on selected recent papers they co-athored. This includes 9 papers from 2021, 7 papers from 2020 and 1 paper from 2019. Please check out their video talks!
  Here they are:

  <br>
  <br>


A talk by Konstantin Burlachenko (<a href="https://arxiv.org/abs/2102.07845">paper</a>):
<br>
<a href="https://www.youtube.com/watch?v=o5MwC4DYbGE&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=33"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Burlachenko.png" width="350"></a>
<br>

A talk by Laurent Condat (<a href="http://proceedings.mlr.press/v119/malinovskiy20a.html">paper</a>):
<br>
<a href="https://www.youtube.com/watch?v=-QgQp5HnWQY&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=27"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Condat.png" width="350"></a>
<br>

A talk by Eduard Gorbunov (<a href="https://papers.nips.cc/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html">paper</a>):
<br>
<a href="https://www.youtube.com/watch?v=6Hpt6hbzgjU&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=37"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Gorbunov.png" width="350"></a>
<br>

A talk by Filip Hanzely (<a href="http://proceedings.mlr.press/v130/gorbunov21a.html">paper</a>):
<br>
<a href="https://www.youtube.com/watch?v=u_KoimUuc6k&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Hanzely.png" width="350"></a>
<br>

A talk by Slavomir Hanzely:
<br>
<a href="https://www.youtube.com/watch?v=YkZeROHXahc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=34"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Slavomir-Hanzely.png" width="350"></a>
<br>

A talk by Samuel Horvath:
<br>
<a href="https://www.youtube.com/watch?v=M3zHT_qieB4&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=32"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Horvath.png" width="350"></a>
<br>

A talk by Rustem Islamov:
<br>
<a href="https://www.youtube.com/watch?v=iSKBZXlaoWo&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=40"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Islamov.png" width="350"></a>
<br>

A talk by Ahmed Khaled:
<br>
<a href="https://www.youtube.com/watch?v=aHAU6OYNoKA&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=39"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Khaled.png" width="350"></a>
<br>

A talk by Dmitry Kovalev:
<br>
<a href="https://www.youtube.com/watch?v=0bAgav0x-8U&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=31"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Kovalev.png" width="350"></a>
<br>

A talk by Zhize Li:
<br>
<a href="https://www.youtube.com/watch?v=_K3XPxN-vdk&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=28"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Li.png" width="350"></a>
<br>

A talk by Grigory Malinovsky:
<br>
<a href="https://www.youtube.com/watch?v=DK9CJmz6SR8&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=35"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Malinovsky.png" width="350"></a>
<br>

A talk by Konstantin Mishchenko:
<br>
<a href="https://www.youtube.com/watch?v=0ZZY5Y_6fd4&list=PLC28kDljnOrh4XxzWpOFBIu8IHjJzmKQs"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Mishchenko.png" width="350"></a>
<br>

A talk by Xun Qian:
<br>
<a href="https://www.youtube.com/watch?v=QMJDOtm9wxk&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=29"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Qian.png" width="350"></a>
<br>

A talk by Mher Safaryan:
<br>
<a href="https://www.youtube.com/watch?v=vSD-smU0JjE&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=45"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Safaryan.png" width="350"></a>
<br>

A talk by Adil Salim:
<br>
<a href="https://www.youtube.com/watch?v=jz_ylyAhkL8&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=30"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Salim.png" width="350"></a>
<br>

A talk by Egor Shulgin:
<br>
<a href="https://www.youtube.com/watch?v=jO3t4eZFdkc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=36"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Shulgin.png" width="350"></a>
<br>

A talk by Bokun Wang:
<br>
<a href="https://www.youtube.com/watch?v=-YjXDdwkeqc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=38"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Wang.png" width="350"></a>
<br>




<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




          <h3>April 21, 2021</h3>
          <h1>Area Editor for Journal of Optimization Theory and Applications</h1>
          <br>

      I have just become an <a href="https://www.springer.com/journal/10957/editors">Area Editor</a> for <a href="https://www.springer.com/journal/10957">Journal on Optimization Theory and Applications (JOTA)</a>, representing the area
      "Optimization and Machine Learning". Consider sending your best optimizaiton for machine learning papers to JOTA! We aim to provide fast and high quality reviews. <br> <br>

      Established in 1967, JOTA is one of the oldest optimization journals. For example, Mathematical Programming was established in 1972, SIAM J on Control and Optimization in 1976, and SIAM J on Optimization in 1991.

        <br><br>

       According to <a href="https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=phy_mathematicaloptimization">Google Scholar Metrics</a>, JOTA is one of the top optimization journals: <br>

       <br>
       <a href="https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=phy_mathematicaloptimization"><img alt="" src="imgs/JOTA-GoogleScholarMetrics-2021-small.png" width="750"></a>
       <br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>April 22, 2021</h3>
          <h1>Talk at AMCS/STAT Graduate Seminar at KAUST</h1>
          <br>

        Today I gave a talk entitled "Distributed second order methods with fast rates and compressed communication" at
        the AMCS/STAT Graduate Seminar at KAUST. Here is the <a href="https://cemse.kaust.edu.sa/events/event/distributed-second-order-methods-fast-rates-and-compressed-communication">official KAUST blurb.</a> I talked about
        the paper <a href="https://arxiv.org/abs/2102.07158">Distributed Second Order Methods with Fast Rates and Compressed Communication</a>.
        This is joint work with my fantastic intern <a href="https://rustem-islamov.github.io">Rustem Islamov</a> (KAUST and MIPT) and fantastic
        postdoc <a href="https://qianxunk.github.io">Xun Qian</a> (KAUST). <br>



        <br>
        <br>
        <img alt="" src="imgs/fancy-line.png" width="196" height="36">
        <br>
        <br>



        <h3>April 19, 2021</h3>
        <h1>New Paper</h1>
        <br>

        <span class="important">New paper out: </span>
        <a href="https://arxiv.org/abs/2104.09342">"Random Reshuffling with Variance Reduction: New Analysis and Better Rates"</a> -
        joint work with
        <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> and
        <a href="https://cemse.kaust.edu.sa/people/person/alibek-sailanbayev">Alibek Sailanbayev</a>.<br>
        <br>

        Abstract: <i>
         Virtually all state-of-the-art methods for training supervised machine learning models are variants of SGD enhanced with a number of additional tricks, such as minibatching, momentum, and adaptive stepsizes. One of the tricks that works so well in practice that it is used as default in virtually all widely used machine learning software is {\em random reshuffling (RR)}. However, the practical benefits of RR have until very recently been eluding attempts at being satisfactorily explained using theory. Motivated by recent development due to Mishchenko, Khaled and Richt\'{a}rik (2020), in this work we provide the first analysis of SVRG under Random Reshuffling (RR-SVRG) for general finite-sum problems. First, we show that RR-SVRG converges linearly with the rate $O(\kappa^{3/2})$ in the strongly-convex case, and can be improved further to $O(\kappa)$ in the big data regime (when $n > O(\kappa)$), where $\kappa$ is the condition number. This improves upon the previous best rate $O(\kappa^2)$ known for a variance reduced RR method in the strongly-convex case due to Ying, Yuan and Sayed (2020). Second, we obtain the first sublinear rate for general convex problems. Third, we establish similar fast rates for Cyclic-SVRG and Shuffle-Once-SVRG. Finally, we develop and analyze a more general variance reduction scheme for RR, which allows for less frequent updates of the control variate. We corroborate our theoretical results with suitably chosen experiments on synthetic and real datasets.
        </i>
        <br>

        <br>
        <br>
        <img alt="" src="imgs/fancy-line.png" width="196" height="36">
        <br>
        <br>




          <h3>April 14, 2021</h3>
          <h1>Talk at FLOW</h1>
          <br>

        Today I am giving a talk entitled "Beyond Local and Gradient Methods for Federated Learning" at
        the <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">Federated Learning One World Seminar (FLOW).</a>
        After a brief motivation spent on bashing gradient and local methods, I will talk about
        the paper <a href="https://arxiv.org/abs/2102.07158">Distributed Second Order Methods with Fast Rates and Compressed Communication</a>.
        This is joint work with my fantastic intern <a href="https://rustem-islamov.github.io">Rustem Islamov</a> (KAUST and MIPT) and fantastic
        postdoc <a href="https://qianxunk.github.io">Xun Qian</a> (KAUST).



         <br><br>
         The talk was recorded and is now available on      YouTube:


         <br>
         <a href="https://www.youtube.com/watch?v=jqgyjN5kLqo"><img alt="" src="imgs/Newton-Learn-FLOW-small.png" width="750"></a>
         <br>




          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>April 13, 2021</h3>
          <h1>Three Papers Presented to AISTATS 2021</h1>
            <br>
We've had three papers accepted to <a href="https://aistats.org/aistats2021/">The 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)</a>. The conference will be held virtually over the next few days; during April 13-15, 2021. Here are the papers:<br><br>


1. <a href="http://proceedings.mlr.press/v130/kovalev21a.html">A linearly convergent algorithm for decentralized optimization: sending less bits for free!</a>, joint work with  <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://scholar.google.com/citations?user=ldJpvE8AAAAJ&hl=en">Anastasia Koloskova</a>, <a href="https://people.epfl.ch/martin.jaggi/?lang=en">Martin Jaggi</a>, and <a href="https://www.sstich.ch">Sebastian U. Stich</a>.<br><br>

<i>Abstract:</i>  Decentralized optimization methods enable on-device training of machine learning models without a central coordinator. In many scenarios communication between devices is energy demanding and time consuming and forms the bottleneck of the entire system.
We propose a new randomized first-order method which tackles the communication bottleneck by applying randomized compression operators to the communicated messages. By combining our scheme with a new variance reduction technique that progressively throughout the iterations reduces the adverse effect of the injected quantization noise, we obtain the first scheme that converges linearly on strongly convex decentralized problems while using compressed communication only.
We prove that our method can solve the problems without any increase in the number of communications compared to the baseline which does not perform any communication compression while still allowing for a significant compression factor which depends on the conditioning of the problem and the topology of the network. Our key theoretical findings are supported by numerical experiments. <br> <br>

<a href="posters/Poster-Decentralized-DIANA.pdf"> <img alt="" src="posters/Poster-Decentralized-DIANA-small.png" width="750" ></a>
<br> <br>

2. <a href="http://proceedings.mlr.press/v130/gorbunov21a.html">Local SGD: unified theory and new efficient methods</a>, joint work with  <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> and <a href="https://fhanzely.github.io">Filip Hanzely</a>.<br><br>

<i>Abstract:</i> We present a unified framework for analyzing local SGD methods in the convex and strongly convex regimes for distributed/federated training of supervised machine learning models. We recover several known methods as a special case of our general framework, including Local-SGD/FedAvg, SCAFFOLD, and several variants of SGD not originally designed for federated learning. Our framework covers both the identical and heterogeneous data settings, supports both random and deterministic number of local steps, and can work with a wide array of local stochastic gradient estimators, including shifted estimators which are able to adjust the fixed points of local iterations for faster convergence. As an application of our framework, we develop multiple novel FL optimizers which are superior to existing methods. In particular, we develop the first linearly converging local SGD method which does not require any data homogeneity or other strong assumptions. <br> <br>


<a href="posters/Poster-Local-SGD-AISTATS-2021.pdf"> <img alt="" src="posters/Poster-Local-SGD-AISTATS-2021-small.png" width="750" ></a>
<br> <br>





3. <a href="http://proceedings.mlr.press/v130/horvath21a.html">Hyperparameter transfer learning with adaptive complexity</a>, joint work with  <a href="https://samuelhorvath.github.io">Samuel Horváth</a> and <a href="https://aaronkl.github.io">Aaron Klein</a>, and <a href="http://www0.cs.ucl.ac.uk/staff/c.archambeau/">Cedric Archambeau</a>.<br><br>

<i>Abstract:</i>  Bayesian optimization (BO) is a data-efficient approach to automatically tune the hyperparameters of machine learning models. In practice, one frequently has to solve similar hyperparameter tuning problems sequentially. For example, one might have to tune a type of neural network learned across a series of different classification problems. Recent work on multi-task BO exploits knowledge gained from previous hyperparameter tuning tasks to speed up a new tuning task. However, previous approaches do not account for the fact that BO is a sequential decision making procedure. Hence, there is in general a mismatch between the number of evaluations collected in the current tuning task compared to the number of evaluations accumulated in all previously completed tasks. In this work, we enable multi-task BO to compensate for this mismatch, such that the transfer learning procedure is able to handle different data regimes in a principled way. We propose a new multi-task BO method that learns a set of ordered, non-linear basis functions of increasing complexity via nested drop-out and automatic relevance determination. Experiments on a variety of hyperparameter tuning problems show that our method improves the sample efficiency of recently published multi-task BO methods.
<br>

<a href="posters/Poster-ABRAC-AISTATS-2021.pdf"> <img alt="" src="posters/Poster-ABRAC-AISTATS-2021-small.png" width="750" ></a>



          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>





          <h3>April 7, 2021</h3>
          <h1>Talk at All Russian Seminar in Optimization</h1>
          <br>

        Today I am giving a talk at the <a href="http://www.mathnet.ru/php/conference.phtml?&eventID=31&confid=1794&option_lang=eng">All Russian Seminar in Optimization.</a> I am talking about the paper <a href="https://arxiv.org/abs/2102.07158">Distributed Second Order Methods with Fast Rates and Compressed Communication</a>, which is joint work with
        <a href="https://rustem-islamov.github.io">Rustem Islamov (KAUST and MIPT)</a> and <a href="https://qianxunk.github.io">Xun Qian (KAUST)</a>.


         <br><br>
         The talk was recorded and uploaded to  YouTube:

         <br>
         <a href="https://www.youtube.com/watch?v=_1q-vt0nu44"><img alt="" src="imgs/Newton-Learn-All-Russian-seminar-small.png" width="750"></a>
         <br>



        <br>

        Here are the <a href="talks/TALK-2021-Newton-Learn.pdf">slides from my talk</a>, and here is a
          <a href="posters/Poster-Newton-Learn.pdf"> poster </a> that will son be presented by Rustem Islamov at the <a href="https://sites.google.com/ucsd.edu/cedo/">NSF-TRIPODS Workshop on Communication Efficient Distributed Optimization.</a>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>March 24, 2021</h3>
          <h1>Mishchenko and Gorbunov: ICLR 2021 Outstanding Reviewer Award</h1>
          <br>

          Congratulations <a href="https://konstmish.github.io/">Konstantin Mishchenko</a> and
          <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> for receiving an Outstanding
          Reviewer Award from ICLR 2021! I wish the reviews we get for our papers were as good (i.e., insighful,
          expert and thorough) as the reviews Konstantin and Eduard are writing.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>March 19, 2021</h3>
          <h1>Area Chair for NeurIPS 2021</h1>
          <br>



          I will serve as an Area Chair for NeurIPS 2021, to be held during December 6-14, 2021 virtually ( = same location as last year ;-).


          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>





          <h3>March 1, 2021</h3>
          <h1>New PhD Student: Lukang Sun</h1>
          <br>


        Lukang Sun has joined my group as a PhD student. Welcome!!!

        Lukang has an MPhil degree in mathematics form Nanjing University, China (2020), and a BA in mathematics from Jilin University, China (2017). His thesis (written in Chinese) was on the topic of
        "Harmonic functions on metric measure spaces". In this work, Lukang proposed some novel methods using optimal transport theory to generalize some results from Riemannian manifolds to metric measure
        spaces. Lukang has held visiting/exchange/temporary positions at the Hong Kong University of Science and Technology, Georgia Institute of Technology, and the Chinese University of Hong Kong.


          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>February 22, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.11079">"An Optimal Algorithm for Strongly Convex Minimization under Affine Constraints"</a> -
          joint work with
          <a href="https://adil-salim.github.io">Adil Salim</a>,
          <a href="https://lcondat.github.io">Laurent Condat</a> and
          <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>.<br>
          <br>

          Abstract: <i>
           Optimization problems under affine constraints appear in various areas of machine learning. We consider the task of
           minimizing a smooth strongly convex function $F(x)$ under the affine constraint $Kx=b$, with an oracle providing evaluations
           of the gradient of $F$ and matrix-vector multiplications by $K$ and its transpose. We provide lower bounds on the number of
           gradient computations and matrix-vector multiplications to achieve a given accuracy. Then we propose an accelerated
           primal--dual algorithm achieving these lower bounds. Our algorithm is the first optimal algorithm for this class of
           problems.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 19, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.09700">"AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods"</a> -
          joint work with
          <a href="https://coral.ise.lehigh.edu/zhs310/">Zheng Shi</a>,
          <a href="https://nicolasloizou.github.io">Nicolas Loizou</a> and
          <a href="https://engineering.lehigh.edu/faculty/martin-takac">Martin Takáč</a>.<br>
          <br>

          Abstract: <i>
           We present an adaptive stochastic variance reduced method with an implicit approach for adaptivity.
           As a variant of SARAH, our method employs the stochastic recursive gradient yet adjusts step-size
           based on local geometry. We provide convergence guarantees for finite-sum minimization problems
           and show a faster convergence than SARAH can be achieved if local geometry permits. Furthermore,
           we propose a practical, fully adaptive variant, which does not require any knowledge of local geometry
           and any effort of tuning the hyper-parameters. This algorithm implicitly computes step-size and
           efficiently estimates local Lipschitz smoothness of stochastic functions. The numerical experiments
           demonstrate the algorithm's strong performance compared to its classical counterparts and other
           state-of-the-art first-order methods.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>February 18, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.09234">"ADOM: Accelerated Decentralized Optimization Method for Time-Varying Networks"</a> -
          joint work with <a href="http://dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://shulgin-egor.github.io">Egor Shulgin</a>, <a href="https://scholar.google.com/citations?hl=ru&user=sEjyzkgAAAAJ">Alexander Rogozin</a>, and
          <a href="https://mipt.ru/education/chairs/dm/staff/gasnikov-aleksandr-vladimirovich.php">Alexander Gasnikov</a>.<br>
          <br>

          Abstract: <i>
          We propose ADOM - an accelerated method for smooth and strongly convex decentralized optimization over time-varying networks.
          ADOM uses a dual oracle, i.e., we assume access to the gradient of the Fenchel conjugate of the individual loss functions.
          Up to a constant factor, which depends on the network structure only, its communication complexity is the same as that of
          accelerated Nesterov gradient method (Nesterov, 2003). To the best of our knowledge, only the algorithm of Rogozin et al. (2019)
          has a convergence rate with similar properties. However, their algorithm converges under the very restrictive assumption that
          the number of network changes can not be greater than a tiny percentage of the number of iterations. This assumption is hard
          to satisfy in practice, as the network topology changes usually can not be controlled. In contrast, ADOM merely requires the
          network to stay connected throughout time.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 16, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.08374">"IntSGD: Floatless Compression of Stochastic Gradients"</a> -
          joint work with <a href="http://konstmish.github.io">Konstantin Mishchenko</a>, and
          <a href="https://scholar.google.com/citations?user=H9GqvAYAAAAJ&hl=en">Bokun Wang</a> and <a href="http://dmitry-kovalev.com">Dmitry Kovalev</a>.<br>
          <br>

          Abstract: <i>
          We propose a family of lossy integer compressions for Stochastic Gradient Descent (SGD) that do not communicate a single float. This is achieved by multiplying floating-point vectors with a number known to every device and then rounding to an integer number. Our theory shows that the iteration complexity of SGD does not change up to constant factors when the vectors are scaled properly. Moreover, this holds for both convex and non-convex functions, with and without overparameterization. In contrast to other compression-based algorithms, ours preserves the convergence rate of SGD even on non-smooth problems. Finally, we show that when the data is significantly heterogeneous, it may become increasingly hard to keep the integers bounded and propose an alternative algorithm, IntDIANA, to solve this type of problems.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>





          <h3>February 16, 2021</h3>
          <h1>Talk at MBZUAI</h1>
          <br>

          Today I gave a research seminar talk at <a href="https://mbzuai.ac.ae">MBZUAI.</a> I spoke about randomized second order methods.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>





          <h3>February 15, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.07845">"MARINA: Faster Non-Convex Distributed Learning with Compression"</a> -
          joint work with <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>, and
          <a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a> and <a href="https://zhizeli.github.io">Zhize Li</a>.<br>
          <br>

          Abstract: <i>
          We develop and analyze MARINA: a new communication efficient method for non-convex distributed learning over heterogeneous datasets. MARINA employs a novel communication compression strategy based on the compression of gradient differences which is reminiscent of but different from the strategy employed in the DIANA method of Mishchenko et al (2019). Unlike virtually all competing distributed first-order methods, including DIANA, ours is based on a carefully designed biased gradient estimator, which is the key to its superior theoretical and practical performance. To the best of our knowledge, the communication complexity bounds we prove for MARINA are strictly superior to those of all previous first order methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and PP-MARINA. The first method is designed for the case when the local loss functions owned by clients are either of a finite sum or of an expectation form, and the second method allows for partial participation of clients -- a feature important in federated learning. All our methods are superior to previous state-of-the-art methods in terms of the oracle/communication complexity. Finally, we provide convergence analysis of all methods for problems satisfying the Polyak-Lojasiewicz condition.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 14, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.07245">"Smoothness Matrices Beat Smoothness Constants: Better Communication Compression Techniques for Distributed Optimization"</a> -
          joint work with <a href="https://mher-safaryan.github.io">Mher Safaryan</a>, and
          <a href="https://www.ttic.edu/faculty/hanzely/">Filip Hanzely</a>.<br>
          <br>

          Abstract: <i>
           Large scale distributed optimization has become the default tool for the training of supervised machine
           learning models with a large number of parameters and training data. Recent advancements in the field
           provide several mechanisms for speeding up the training, including compressed communication, variance
           reduction and acceleration. However, none of these methods is capable of exploiting the inherently rich data-dependent smoothness
           structure of the local losses beyond standard smoothness constants. In this paper, we argue that when
           training supervised models, smoothness matrices -- information-rich generalizations of the ubiquitous
           smoothness constants -- can and should be exploited for further dramatic gains, both in theory and practice.
           In order to further alleviate the communication burden inherent in distributed optimization, we propose a
           novel communication sparsification strategy that can take full advantage of the smoothness matrices
           associated with local losses. To showcase the power of this tool, we describe how our sparsification
           technique can be adapted to three distributed optimization algorithms -- DCGD, DIANA and ADIANA -- yielding
           significant savings in terms of communication complexity. The new methods always outperform the baselines,
           often dramatically so.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 13, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.07158">"Distributed Second Order Methods with Fast Rates and Compressed Communication"</a> -
          joint work with <a href="https://rustem-islamov.github.io">Rustem Islamov</a>, and
          <a href="https://qianxunk.github.io">Xun Qian</a>.<br>
          <br>

          Abstract: <i>
           We develop several new communication-efficient second-order methods for distributed optimization. Our first method, NEWTON-STAR, is a variant of Newton's method from which it inherits its fast local quadratic rate. However, unlike Newton's method, NEWTON-STAR enjoys the same per iteration communication cost as gradient descent. While this method is impractical as it relies on the use of certain unknown parameters characterizing the Hessian of the objective function at the optimum, it serves as the starting point which enables us design practical variants thereof with strong theoretical guarantees. In particular, we design a stochastic sparsification strategy for learning the unknown parameters in an iterative fashion in a communication efficient manner. Applying this strategy to NEWTON-STAR leads to our next method, NEWTON-LEARN, for which we prove local linear and superlinear rates independent of the condition number. When applicable, this method can have dramatically superior convergence behavior when compared to state-of-the-art methods. Finally, we develop a globalization strategy using cubic regularization which leads to our next method, CUBIC-NEWTON-LEARN, for which we prove global sublinear and linear convergence rates, and a fast superlinear rate. Our results are supported with experimental results on real datasets, and show several orders of magnitude improvement on baseline and state-of-the-art methods in terms of communication complexity.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>February 12, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.06704">"Proximal and Federated Random Reshuffling"</a> -
          joint work with <a href="https://konstmish.github.io">Konstantin Mishchenko</a>, and
          <a href="https://rka97.github.io">Ahmed Khaled</a>.<br>
          <br>

          Abstract: <i>
            Random Reshuffling (RR), also known as Stochastic Gradient Descent (SGD) without replacement, is a popular and
            theoretically grounded method for finite-sum minimization. We propose two new algorithms: Proximal and Federated
            Random Reshuffing (ProxRR and FedRR). The first algorithm, ProxRR, solves composite convex finite-sum minimization
            problems in which the objective is the sum of a (potentially non-smooth) convex regularizer and an average of n smooth
            objectives. We obtain the second algorithm, FedRR, as a special case of ProxRR applied to a reformulation of distributed
            problems with either homogeneous or heterogeneous data. We study the algorithms' convergence properties with constant
            and decreasing stepsizes, and show that they have considerable advantages over Proximal and Local SGD. In particular,
            our methods have superior complexities and ProxRR evaluates the proximal operator once per epoch only. When the proximal
            operator is expensive to compute, this small difference makes ProxRR up to n times faster than algorithms that evaluate
            the proximal operator in every iteration. We give examples of practical optimization tasks where the proximal operator
            is difficult to compute and ProxRR has a clear advantage. Finally, we corroborate our results with experiments on real
            data sets.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 10, 2021</h3>
          <h1>Best Paper Award @ NeurIPS SipcyFL 2020</h1>
  <br>
    Super happy about this surprise prize; and huge congratulations to my outstanding student and collaborator <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.
    The paper was recently accepted to <a href="https://openreview.net/forum?id=vYVI1CHPaQg">ICLR 2021</a>, check it out!

<br>
<br>
<img alt="" src="imgs/NeurIPS-2020-SpicyFL-Prize.jpeg" width="600" >
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>January 24, 2021</h3>
          <h1>Spring 2021 Semester Starts at KAUST</h1>
  <br>
As of today, the Spring semester starts at KAUST. The timing of this every year conflicts with the endgame before the ICML submission deadline, and this year is no different. Except for Covid-19.
I am teaching <a href="https://piazza.com/kaust.edu.sa/spring2021/cs332">CS 332: Federated Learning</a> on Sundays and Tuesdays. The first class is today.
  <br>



          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>January 23, 2021</h3>
          <h1>Three Papers Accepted to AISTATS 2021</h1>
            <br>
We've had some papers accepted to <a href="https://aistats.org/aistats2021/">The 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)</a>. The conference will be held virtually during April 13-15, 2021. Here are the papers:<br><br>


1. <a href="http://proceedings.mlr.press/v130/kovalev21a.html">A linearly convergent algorithm for decentralized optimization: sending less bits for free!</a>, joint work with  <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://scholar.google.com/citations?user=ldJpvE8AAAAJ&hl=en">Anastasia Koloskova</a>, <a href="https://people.epfl.ch/martin.jaggi/?lang=en">Martin Jaggi</a>, and <a href="https://www.sstich.ch">Sebastian U. Stich</a>.<br><br>

<i>Abstract:</i>  Decentralized optimization methods enable on-device training of machine learning models without a central coordinator. In many scenarios communication between devices is energy demanding and time consuming and forms the bottleneck of the entire system.
We propose a new randomized first-order method which tackles the communication bottleneck by applying randomized compression operators to the communicated messages. By combining our scheme with a new variance reduction technique that progressively throughout the iterations reduces the adverse effect of the injected quantization noise, we obtain the first scheme that converges linearly on strongly convex decentralized problems while using compressed communication only.
We prove that our method can solve the problems without any increase in the number of communications compared to the baseline which does not perform any communication compression while still allowing for a significant compression factor which depends on the conditioning of the problem and the topology of the network. Our key theoretical findings are supported by numerical experiments. <br> <br>


2. <a href="http://proceedings.mlr.press/v130/gorbunov21a.html">Local SGD: unified theory and new efficient methods</a>, joint work with  <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> and <a href="https://fhanzely.github.io">Filip Hanzely</a>.<br><br>

<i>Abstract:</i> We present a unified framework for analyzing local SGD methods in the convex and strongly convex regimes for distributed/federated training of supervised machine learning models. We recover several known methods as a special case of our general framework, including Local-SGD/FedAvg, SCAFFOLD, and several variants of SGD not originally designed for federated learning. Our framework covers both the identical and heterogeneous data settings, supports both random and deterministic number of local steps, and can work with a wide array of local stochastic gradient estimators, including shifted estimators which are able to adjust the fixed points of local iterations for faster convergence. As an application of our framework, we develop multiple novel FL optimizers which are superior to existing methods. In particular, we develop the first linearly converging local SGD method which does not require any data homogeneity or other strong assumptions. <br> <br>

3. <a href="http://proceedings.mlr.press/v130/horvath21a.html">Hyperparameter transfer learning with adaptive complexity</a>, joint work with  <a href="https://samuelhorvath.github.io">Samuel Horváth</a> and <a href="https://aaronkl.github.io">Aaron Klein</a>, and <a href="http://www0.cs.ucl.ac.uk/staff/c.archambeau/">Cedric Archambeau</a>.<br><br>

<i>Abstract:</i>  Bayesian optimization (BO) is a data-efficient approach to automatically tune the hyperparameters of machine learning models. In practice, one frequently has to solve similar hyperparameter tuning problems sequentially. For example, one might have to tune a type of neural network learned across a series of different classification problems. Recent work on multi-task BO exploits knowledge gained from previous hyperparameter tuning tasks to speed up a new tuning task. However, previous approaches do not account for the fact that BO is a sequential decision making procedure. Hence, there is in general a mismatch between the number of evaluations collected in the current tuning task compared to the number of evaluations accumulated in all previously completed tasks. In this work, we enable multi-task BO to compensate for this mismatch, such that the transfer learning procedure is able to handle different data regimes in a principled way. We propose a new multi-task BO method that learns a set of ordered, non-linear basis functions of increasing complexity via nested drop-out and automatic relevance determination. Experiments on a variety of hyperparameter tuning problems show that our method improves the sample efficiency of recently published multi-task BO methods.
<br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>January 22, 2021</h3>
          <h1>Paper Accepted to Information and Inference: A Journal of the IMA</h1>
            <br>
          Our paper "Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor”, joint work with  <a href="https://mher-safaryan.github.io">Mher Safaryan</a> and <a href="https://shulgin-egor.github.io">Egor Shulgin</a>,  was accepted to
      <a href="https://academic.oup.com/imaiai">Information and Inference: A Journal of the IMA</a>.<br><br>

    <i>Abstract:</i> In order to mitigate the high communication cost in distributed and federated learning,
    various vector compression schemes, such as quantization, sparsification and dithering, have become
    very popular. In designing a compression method, one aims to communicate as few bits
    as possible, which minimizes the cost per communication round, while at the same time attempting to
    impart as little distortion (variance) to the communicated messages as possible, which minimizes
    the adverse effect of the compression on the overall number of communication rounds. However,
     intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more
     distorted the messages become. We formalize this intuition and prove an  uncertainty principle for
     randomized compression operators, thus quantifying this limitation mathematically, and
     effectively providing asymptotically tight lower bounds on what might be achievable with communication
     compression. Motivated by these developments, we call for the search for the optimal compression
      operator. In an attempt to take a first step in this direction, we consider an unbiased compression
       method inspired by the Kashin representation of vectors, which we call Kashin compression (KC).
       In contrast to all previously proposed compression mechanisms, KC enjoys a dimension independent
       variance bound for which we derive an explicit formula even in the regime when only a few bits
       need to be communicate per each vector entry.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>January 12, 2021</h3>
          <h1>Paper Accepted to ICLR 2021</h1>
  <br>
    Our paper "A Better Alternative to Error Feedback for Communication-efficient Distributed Learning'', joint work with  <a href="https://samuelhorvath.github.io">Samuel Horváth</a>,  was accepted to
      <a href="https://openreview.net/forum?id=vYVI1CHPaQg">The 9th International Conference on Learning Representations (ICLR 2021)</a>.<br><br>

    <i>Abstract:</i>  Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed compute systems. A key bottleneck of such systems is the communication overhead for exchanging information across the workers, such as stochastic gradients. Among the many techniques proposed to remedy this issue, one of the most successful is the framework of compressed communication with error feedback (EF).
    EF remains the only known technique that can deal with the error induced by contractive compressors which are not unbiased, such as Top-K.
    In this paper, we propose a new and theoretically and practically better alternative to EF for dealing with contractive compressors. In particular, we propose a construction which can transform any contractive compressor into an induced unbiased compressor. Following this transformation, existing methods able to work with unbiased compressors can be applied. We show that our approach leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. We further extend our results to federated learning with partial participation following an arbitrary
    distribution over the nodes, and demonstrate the benefits thereof. We perform several numerical experiments which validate our theoretical
    findings.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




<h3>January 11, 2021</h3>
<h1>Call for Al-Khwarizmi Doctoral Fellowships (apply by Jan 22, 2021)</h1>
<br>

If you are from Europe and want to apply for a PhD position in my Optimization and Machine Learning group at KAUST, you may wish to apply for the
<a href="https://cemse.kaust.edu.sa/alkhwarizmi">European Science Foundation Al-Khwarizmi Doctoral Fellowship</a>.

<a href="https://cemse.kaust.edu.sa/alkhwarizmi"> <img alt="" src="imgs/al-khwarizmi.png" width="200"></a>

<br>
 Here is the official blurb:
<br>
<br>
"The Al-Khwarizmi Graduate Fellowship scheme invites applications for doctoral fellowships, with the submission deadline of 22 January 2021, 17:00 CET.
The King Abdullah University of Science and Technology (KAUST) in the Kingdom of Saudi Arabia with support from the European Science Foundation (ESF)
launches a competitive doctoral fellowship scheme to welcome students from the European continent for a research journey to a top international university
in the Middle East. The applications will be evaluated via an independent peer-review process managed by the ESF. The selected applicants will be offered
generous stipends and free tuition for Ph.D. studies within one of KAUST academic programs. Strong applicants who were not awarded a Fellowship but passed
KAUST admission requirements will be offered the possibility to join the University as regular Ph.D. students with the standard benefits that include the
usual stipends and free tuition."
<br>
<br>
- Submission deadline = 22 January 2021 @ 17:00 CET <br>
- Duration of the Fellowship = 3 years (extensions may be considered in duly justified cases)<br>
- Annual living allowance/stipend = USD 38,000  (net)<br>
- Approx USD 50,000 annual benefits = free tuition, free student housing on campus, relocation support, and medical and dental coverage<br>
- Each Fellowship includes a supplementary grant of USD 6,000 at the Fellow’s disposal for research-related expenses such as conference attendance<br>
- The applications must be submitted in two steps, with the formal documents and transcripts to be submitted to KAUST Admissions in Step 1, and
the research proposal to be submitted to the ESF in Step 2. Both steps should be completed in parallel before the call deadline.<br>

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




          <h3>December 15, 2020</h3>
          <h1>Vacation</h1>
          <br>
I am on vacation until early January, 2021.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>December 12, 2020</h3>
          <h1>Paper Accepted to NSDI 2021</h1>
          <br>
Our paper ``Scaling Distributed Machine Learning with In-Network Aggregation'', joint work with  Amedeo Sapio, Marco Canini, Chen-Yu Ho,
Jacob Nelson, Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, and Dan R. K. Ports,  was accepted to
<a href="https://www.usenix.org/conference/nsdi21?ref=infosec-conferences.com">The 18th USENIX Symposium on Networked Systems Design and Implementation
(NSDI '21 Fall)</a>. <br><br>

<i>Abstract:</i>  Training machine learning models in parallel is an increasingly important workload. We accelerate distributed parallel training by designing a communication primitive that uses a programmable switch dataplane to execute a key step of the training process. Our approach, SwitchML, reduces the volume of exchanged data by aggregating the model updates from multiple workers in the network. We co-design the switch processing with the end-host protocols and ML frameworks to provide an efficient solution that speeds up training by up to 5.5× for a number of real-world benchmark models.


          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>December 8, 2020</h3>
          <h1>Fall 2020 Semester at KAUST is Over</h1>
          <br>


          The Fall 2020 semester at KAUST is now over; I've had  alot of fun teaching my CS 331 class (Stochastic Gradient Descent Methods). At the very end I run into some
          LaTeX issues after upgrading to Big Sur on Mac - should not have done that...
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>December 6, 2020</h3>
          <h1>NeurIPS 2020 Started</h1>
          <br>


          Me and the members of my group will be attending <a href="https://neurips.cc/virtual/2020/public/index.html">NeurIPS 2020</a> - the event is starting today.
          Marco Cuturi and me will co-chair the <a href="https://neurips.cc/virtual/2020/public/session_oral_21084.html">Optimization session (Track 21) on Wednesday</a>. I am particularly looking forward to the workshops: <a href="https://opt-ml.org">OPT2020</a>, <a href="https://ppml-workshop.github.io">PPML</a> and <a href="http://128.1.38.43/SpicyFL/2020/">SpicyFL.</a>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>November 24, 2020</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://opt-ml.org/papers/2020/paper_61.pdf">"Error Compensated Loopless SVRG for Distributed Optimization"</a> -
          joint work with <a href="https://qianxunk.github.io/">Xun Qian</a>, <a href="https://www.researchgate.net/profile/Hanze_Dong">Hanze Dong</a>, and
          <a href="http://tongzhang-ml.org/">Tong Zhang</a>.<br>
          <br>

          Abstract: <i>
            A key bottleneck in distributed training of large scale machine learning models is the overhead related
            to communication of gradients. In order to reduce the communicated cost, gradient compression
            (e.g., sparsification and quantization) and error compensation techniques are often used. In this
            paper, we propose and study a new efficient method in this space: error compensated loopless SVRG
            method (L-SVRG). Our method is capable of working with any contraction compressor (e.g., TopK
            compressor), and we perform analysis for strongly convex optimization problems in the composite
            case and smooth case. We prove linear convergence rates for both cases and show that in the smooth
            case the rate has a better dependence on the contraction factor associated with the compressor.
            Further, we show that in the smooth case, and under some certain conditions, error compensated
            L-SVRG has the same convergence rate as the vanilla L-SVRG method. Numerical experiments are
            presented to illustrate the efficiency of our method.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>November 24, 2020</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://opt-ml.org/papers/2020/paper_59.pdf">"Error Compensated Proximal SGD and RDA"</a> -
          joint work with <a href="https://qianxunk.github.io/">Xun Qian</a>, <a href="https://www.researchgate.net/profile/Hanze_Dong">Hanze Dong</a>, and
          <a href="http://tongzhang-ml.org/">Tong Zhang</a>.<br>
          <br>

          Abstract: <i>
            Communication cost is a key bottleneck in distributed training of large machine learning models. In
            order to reduce the amount of communicated data, quantization and error compensation techniques
            have recently been studied. While the error compensated stochastic gradient descent (SGD) with
            contraction compressor (e.g., TopK) was proved to have the same convergence rate as vanilla SGD in
            the smooth case, it is unknown in the regularized case. In this paper, we study the error compensated
            proximal SGD and error compensated regularized dual averaging (RDA) with contraction compressor
            for the composite finite-sum optimization problem. Unlike the smooth case, the leading term in the
            convergence rate of error compensated proximal SGD is dependent on the contraction compressor
            parameter in the composite case, and the dependency can be improved by introducing a reference
            point to reduce the compression noise. For error compensated RDA, we can obtain better dependency
            of compressor parameter in the convergence rate. Extensive numerical experiments are presented to
            validate the theoretical results.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



<h3>November 6, 2020</h3>
<h1>ICML 2021 Area Chair</h1>
<br>


I've accepted an invite to serve the machine learning community as an Area Chair for <a href="https://icml.cc/Conferences/2021">ICML 2021.</a>
I'll be a tough (but friendly) Area Chair:  I expect the best from the reviewers and will do all I can to make sure the reviews and reviewer discussion
are as fair and substantial as possible.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>November 3, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2011.02828">"Local SGD: Unified Theory and New Efficient Methods"</a> -
joint work with <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> and <a href="https://fhanzely.github.io/index.html">Filip Hanzely.</a><br>
<br>

Abstract: <i>
We present a unified framework for analyzing local SGD methods in the convex and strongly convex regimes for distributed/federated training of supervised machine learning models. We recover several known methods as a special case of our general framework, including Local-SGD/FedAvg, SCAFFOLD, and several variants of SGD not originally designed for federated learning. Our framework covers both the identical and heterogeneous data settings, supports both random and deterministic number of local steps, and can work with a wide array of local stochastic gradient estimators, including shifted estimators which are able to adjust the fixed points of local iterations for faster convergence. As an application of our framework, we develop multiple novel FL optimizers which are superior to existing methods. In particular, we develop the first linearly converging local SGD method which does not require any data homogeneity or other strong assumptions.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>November 3, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2011.01697">"A Linearly Convergent Algorithm for Decentralized Optimization: Sending Less Bits for Free!"</a> -
joint work with <a href="https://www.dmitry-kovalev.com ">Dmitry Kovalev</a>, <a href="https://scholar.google.com/citations?user=ldJpvE8AAAAJ&hl=en">Anastasia Koloskova</a>,
<a href="https://www.epfl.ch/labs/mlo/">Martin Jaggi</a> and <a href="https://scholar.google.com/citations?user=8l-mDfQAAAAJ&hl=en">Sebastian U. Stich</a>.<br>
<br>

Abstract: <i> Decentralized optimization methods enable on-device training of machine learning models without a central coordinator. In many scenarios communication between devices is energy demanding and time consuming and forms the bottleneck of the entire system. We propose a new randomized first-order method which tackles the communication bottleneck by applying randomized compression operators to the communicated messages. By combining our scheme with a new variance reduction technique that progressively throughout the iterations reduces the adverse effect of the injected quantization noise, we obtain the first scheme that converges linearly on strongly convex decentralized problems while using compressed communication only. We prove that our method can solve the problems without any increase in the number of communications compared to the baseline which does not perform any communication compression while still allowing for a significant compression factor which depends on the conditioning of the problem and the topology of the network. Our key theoretical findings are supported by numerical experiments.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 26, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.13723">"Optimal Client Sampling for Federated Learning"</a> -
joint work with Wenlin Chen, and <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.<br>
<br>

Abstract: <i>
It is well understood that client-master communication can be a primary bottleneck in Federated Learning. In this work, we address this issue with a novel client subsampling scheme, where we restrict the number of clients allowed to communicate their updates back to the master node. In each communication round, all participated clients compute their updates, but only the ones with "important" updates communicate back to the master. We show that importance can be measured using only the norm of the update and we give a formula for optimal client participation. This formula minimizes the distance between the full update, where all clients participate, and our limited update, where the number of participating clients is restricted. In addition, we provide a simple algorithm that approximates the optimal formula for client participation which only requires secure aggregation and thus does not compromise client privacy. We show both theoretically and empirically that our approach leads to superior performance for Distributed SGD (DSGD) and Federated Averaging (FedAvg) compared to the baseline where participating clients are sampled uniformly. Finally, our approach is orthogonal to and compatible with existing methods for reducing communication overhead, such as local methods and communication compression methods.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 23, 2020</h3>
<h1>New Paper (Spotlight @ NeurIPS 2020)</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.12292">"Linearly Converging Error Compensated SGD"</a> -
joint work with <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>, <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
and Dmitry Makarenko.<br>
<br>

Abstract: <i>
In this paper, we propose a unified analysis of variants of distributed SGD with arbitrary compressions and delayed updates. Our framework is general enough to cover different variants of quantized SGD, Error-Compensated SGD (EC-SGD) and SGD with delayed updates (D-SGD). Via a single theorem, we derive the complexity results for all the methods that fit our framework. For the existing methods, this theorem gives the best-known complexity results. Moreover, using our general scheme, we develop new variants of SGD that combine variance reduction or arbitrary sampling with error feedback and quantization and derive the convergence rates for these methods beating the state-of-the-art results. In order to illustrate the strength of our framework, we develop 16 new methods that fit this. In particular, we propose the first method called EC-SGD-DIANA that is based on error-feedback for biased compression operator and quantization of gradient differences and prove the convergence guarantees showing that EC-SGD-DIANA converges to the exact optimum asymptotically in expectation with constant learning rate for both convex and strongly convex objectives when workers compute full gradients of their loss functions. Moreover, for the case when the loss function of the worker has the form of finite sum, we modified the method and got a new one called EC-LSVRG-DIANA which is the first distributed stochastic method with error feedback and variance reduction that converges to the exact optimum asymptotically in expectation with a constant learning rate.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>October 16, 2020</h3>
<h1>Nicolas Loizou runner-up in OR Society Doctoral Award</h1>
<br>


<a href="https://nicolasloizou.github.io/">Nicolas Loizou</a>, my former PhD student (and last student to have graduated from Edinburgh after I left for KAUST), has been selected as a runner-up in the <a href="https://www.theorsociety.com/membership/awards-medals-and-scholarships/the-doctoral-award/previous-awards/">2019
OR Society Doctoral Award</a> competition. Congratuations! <br><br>

Nicolas' PhD thesis: <a href="https://arxiv.org/abs/1909.12176">Randomized Iterative Methods for Linear Systems: Momentum, Inexactness and Gossip</a>


<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 7, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.03246">"Optimal Gradient Compression for Distributed and Federated Learning"</a> -
joint work with <a href="https://alyazeedbasyoni.wixsite.com/blog">Alyazeed Albasyoni</a>, <a href="https://mher-safaryan.github.io">Mher Safaryan</a>,
and <a href="https://lcondat.github.io">Laurent Condat</a>.<br>
<br>

Abstract: <i>
Communicating information, like gradient vectors, between computing nodes in distributed and federated learning is typically an unavoidable burden, resulting in scalability issues. Indeed, communication might be slow and costly. Recent advances in communication-efficient training algorithms have reduced this bottleneck by using compression techniques, in the form of sparsification, quantization, or low-rank approximation. Since compression is a lossy, or inexact, process, the iteration complexity is typically worsened; but the total communication complexity can improve significantly, possibly leading to large computation time savings. In this paper, we investigate the fundamental trade-off between the number of bits needed to encode compressed vectors and the compression error. We perform both worst-case and average-case analysis, providing tight lower bounds. In the worst-case analysis, we introduce an efficient compression operator, Sparse Dithering, which is very close to the lower bound. In the average-case analysis, we design a simple compression operator, Spherical Compression, which naturally achieves the lower bound. Thus, our new compression schemes significantly outperform the state of the art. We conduct numerical experiments to illustrate this improvement.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 5, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.02372">"Lower Bounds and Optimal Algorithms for Personalized Federated Learning"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>, <a href="https://slavomirhanzely.wordpress.com">Slavomír Hanzely</a>,
and <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.<br>
<br>

Abstract: <i>
In this work, we consider the optimization formulation of personalized federated learning recently introduced by Hanzely and Richtárik (2020)
which was shown to give an alternative explanation to the workings of local SGD methods. Our first contribution is establishing the first lower
bounds for this formulation, for both the communication complexity and the local oracle complexity. Our second contribution is the design of
several optimal methods matching these lower bounds in almost all regimes. These are the first provably optimal methods for personalized federated
learning. Our optimal methods include an accelerated variant of FedProx, and an accelerated variance-reduced version of FedAvg / Local SGD. We
demonstrate the practical superiority of our methods through extensive numerical experiments.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>October 2, 2020</h3>
<h1>New Paper </h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.00952">"Distributed Proximal Splitting Algorithms with Rates and Acceleration"</a> -
joint work with <a href="https://lcondat.github.io">Laurent Condat</a> and Grigory Malinovsky.<br>
<br>

Abstract: <i>
We analyze several generic proximal splitting algorithms well suited for large-scale convex nonsmooth optimization. We derive sublinear and linear convergence results with new rates on the function value suboptimality or distance to the solution, as well as new accelerated versions, using varying stepsizes. In addition, we propose distributed variants of these algorithms, which can be accelerated as well. While most existing results are ergodic, our nonergodic results significantly broaden our understanding of primal-dual optimization algorithms.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 2, 2020</h3>
<h1>New Paper </h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.00892">"Variance-Reduced Methods for Machine Learning"</a> -
joint work with <a href="https://gowerrobert.github.io">Robert Mansel Gower</a>,
<a href="https://scholar.google.com/citations?user=5BtEUJcAAAAJ&hl=en">Mark Schmidt</a> and <a href="https://www.di.ens.fr/~fbach/">Francis Bach</a>.<br>
<br>


Abstract: <i>
Stochastic optimization lies at the heart of machine learning, and its cornerstone is stochastic gradient descent (SGD), a method introduced over 60 years ago. The last 8 years have seen an exciting new development: variance reduction (VR) for stochastic optimization methods. These VR methods excel in settings where more than one pass through the training data is allowed, achieving a faster convergence than SGD in theory as well as practice. These speedups underline the surge of interest in VR methods and the fast-growing body of work on this topic. This review covers the key principles and main developments behind VR methods for optimization with finite data sets and is aimed at non-expert readers. We focus mainly on the convex setting, and leave pointers to readers interested in extensions for minimizing non-convex functions.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 2, 2020</h3>
<h1>New Paper </h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.00091">"Error Compensated Distributed SGD Can Be Accelerated"</a> -
joint work with <a href="https://qianxunk.github.io">Xun Qian</a> and
<a href="https://www.cse.ust.hk/admin/people/faculty/profile/tongzhang">Tong Zhang</a>.<br>
<br>


Abstract: <i>
Gradient compression is a recent and increasingly popular technique for reducing the communication cost in distributed training of large-scale machine learning models. In this work we focus on developing efficient distributed methods that can work for any compressor satisfying a certain contraction property, which includes both unbiased (after appropriate scaling) and biased compressors such as RandK and TopK. Applied naively, gradient compression introduces errors that either slow down convergence or lead to divergence. A popular technique designed to tackle this issue is error compensation/error feedback. Due to the difficulties associated with analyzing biased compressors, it is not known whether gradient compression with error compensation can be combined with Nesterov's acceleration. In this work, we show for the first time that error compensated gradient compression methods can be accelerated. In particular, we propose and study the error compensated loopless Katyusha method, and establish an accelerated linear convergence rate under standard assumptions. We show through numerical experiments that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 30, 2020</h3>
<h1>Eduard Gorbunov Organizes All-Russian Optimization Research Seminar</h1>
<br>

My serial intern and collaborator <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> is the organizer of an <a href="http://www.mathnet.ru/php/conference.phtml?&eventID=31&confid=1794&option_lang=rus">All-Russian
Research Seminar Series on Mathematical Optimization.</a> There have been 14 speakers at this event so far, including Eduard and <a href="https://konstmish.github.io">Konstantin
  Mishchenko.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 28, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/1901.09997v4">"Quasi-Newton Methods for Deep Learning: Forget the Past, Just Sample"</a> -
joint work with <a href="https://ioe.engin.umich.edu/people/albert-s-berahas/">Albert S. Berahas</a>,
<a href="https://coral.ise.lehigh.edu/maj316/">Majid Jahani</a>,  and <a href="http://mtakac.com">Martin Takáč</a>.<br>
<br>


Abstract: <i>
We present two sampled quasi-Newton methods for deep learning: sampled LBFGS (S-LBFGS) and sampled LSR1 (S-LSR1). Contrary to the classical variants of these methods that sequentially build Hessian or inverse Hessian approximations as the optimization progresses, our proposed methods sample points randomly around the current iterate at every iteration to produce these approximations. As a result, the approximations constructed make use of more reliable (recent and local) information, and do not depend on past iterate information that could be significantly stale. Our proposed algorithms are efficient in terms of accessed data points (epochs) and have enough concurrency to take advantage of parallel/distributed computing environments. We provide convergence guarantees for our proposed methods. Numerical tests on a toy classification problem as well as on popular benchmarking neural network training tasks reveal that the methods outperform their classical variants.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>September 22, 2020</h3>
<h1>Adil Salim Giving a Virtual Talk at the Fields Institute</h1>
<br>

<a href="https://adil-salim.github.io">Adil Salim</a> is giving a virtual talk today at the <a href="http://www.fields.utoronto.ca/activities/20-21/dynamical">Second Symposium on Machine Learning and Dynamical Systems</a>, organized at the Fields Institute.
His talk "Primal Dual Interpretation of the Proximal Gradient Langevin Algorithm", based
on <a href="https://arxiv.org/abs/2006.09270">this paper</a>, is <a href="https://www.youtube.com/watch?v=4lGjecpVWzE">available
on YouTube</a>. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 30, 2020</h3>
<h1>Fall 2020 Teaching</h1>
<br>

The Fall 2020 semester started. I am teaching CS 331: Stochastic Gradient Descent Methods. <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>September 15, 2020</h3>
<h1>Four New Group Members</h1>
<br>

Four new people joined my team in August/September 2020:
<br><br>

<b>Konstantin Burlachenko</b> joins as a CS PhD student. Konstantin got a Master’s degree in CS and Control Systems from Bauman Moscow State University in 2009. 
He has since worked at a number of companies, most recently as a Senior Developer at Yandex and NVidia and a Principal Engineer at Huawei. Konstantin is
interested in software development, optimization, federated learning, graphics and vision, and forecasting models. Konstantin attended several courses at
Stanford and obtained two graduate certificates [<a href="https://online.stanford.edu/programs/data-models-and-optimization-graduate-certificate">1</a>]
[<a href="https://online.stanford.edu/programs/artificial-intelligence-graduate-certificate">2</a>].

<br>
<br>
<b>Grigory Malinovsky</b> joins as an MS/PhD student in <a href="https://cemse.kaust.edu.sa/amcs">AMCS</a>  after a successful internship at KAUST in early 2020 which led to
the paper <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/5019-Paper.pdf">“From Local SGD to Local Fixed-Point Methods for Federated Learning"</a>,
joint with <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="http://elnurgasanov.me">Elnur Gasanov</a>, Laurent Condat and myself. The paper appeared in ICML 2020.  Grisha has graduated with a BS degree from Moscow Institute of Physics and Technology (MIPT) with a thesis entitled “Averaged Heavy Ball Method” under the supervision of Boris Polyak.
Among Grigory’s successes belong:
<br>
- Abramov’s scholarship for students with the best grades at MIPT, 2016
<br>
- Participant in the final round of All-Russian Physics Olympiad, 2014
<br>
- Bronze medal at International Zhautykov Olympiad in Physics, 2014
<br>
- Prize winner in the final round of All-Russian Physics Olympiad, 2013
<br>
Grisha enjoys basketball, fitness, football and table tennis. He speaks a bit of Tatar.

<br><br>
<b>Igor Sokolov</b> joins as an MS student in <a href="https://cemse.kaust.edu.sa/amcs">AMCS.</a> Igor has a BS degree from MIPT’s Department of Control and
Applied Mathematics. Igor is the recipient of several prizes, including at the Phystech Olympiad in Physics (2014), and regional stage of the All Russian
Olympiad in Physics (2014). He won 2nd place at the Programming Conference (2012 and 2013) and was a winner of the Programming Olympiad (2011); all at the
Computer Training Center. Igor enjoys snowboarding, cycling and jogging. He coauthored a paper which will soon be posted onto arXiv.

<br><br>
<b>Bokun Wang</b> joins as a remote intern and will work in the lab for 6 months. Bokun coauthored several papers, including <a href="https://arxiv.org/abs/2005.01209">
``Riemannian Stochastic Proximal Gradient Methods for Nonsmooth Optimization over the Stiefel Manifold”.</a> He has recently interned with Tong Zhang (HKUST).
Bokun is a graduate student at UC Davis, and has a BS degree in Computer Science from University of Electronic Science and Technology of China.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 25, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2008.10898">"PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization"</a> -
joint work with <a href="https://zhizeli.github.io">Zhize Li</a>, <a href="https://mine.kaust.edu.sa/Pages/Hongyan.aspx">Hongyan Bao</a>,  and <a href="https://scholar.google.com/citations?user=BhRJe4wAAAAJ&hl=en">Xiangliang Zhang</a>.<br>
<br>


Abstract: <i>
In this paper, we propose a novel stochastic gradient estimator---ProbAbilistic Gradient Estimator (PAGE)---for nonconvex optimization. PAGE is easy to implement as it is designed via a small adjustment to vanilla SGD: in each iteration, PAGE uses the vanilla minibatch SGD update with probability p and reuses the previous gradient with a small adjustment, at a much lower computational cost, with probability 1−p. We give a simple formula for the optimal choice of p. We prove tight lower bounds for nonconvex problems, which are of independent interest. Moreover, we prove matching upper bounds both in the finite-sum and online regimes, which establish that Page is an optimal method. Besides, we show that for nonconvex functions satisfying the Polyak-Łojasiewicz (PL) condition, PAGE can automatically switch to a faster linear convergence rate. Finally, we conduct several deep learning experiments (e.g., LeNet, VGG, ResNet) on real datasets in PyTorch, and the results demonstrate that PAGE converges much faster than SGD in training and also achieves the higher test accuracy, validating our theoretical results and confirming the practical superiority of PAGE.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 25, 2020</h3>
<h1>Apple Virtual Workshop on Privacy Preserving Machine Learning</h1>
<br>

I have been invited to give a talk at the "Virtual Workshop on Privacy Preserving Machine Learning", hosted by <a href="https://www.apple.com">Apple</a>. The workshop is a two-day event starting today.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 19, 2020</h3>
<h1>Paper Accepted to SIAM Journal on Scientific Computing</h1>
<br>

The paper <a href="https://arxiv.org/abs/1903.07971">"Convergence Analysis of Inexact Randomized Iterative Methods"</a>, joint with <a href="https://www.maths.ed.ac.uk/~s1461357/">Nicolas Loizou</a>, was accepted to
<a href="https://www.siam.org/publications/journals/siam-journal-on-scientific-computing-sisc">SIAM Journal on Scientific Computing</a>.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>August 12, 2020</h3>
<h1>Paper Accepted to Computational Optimization and Applications</h1>
<br>

The paper <a href="https://arxiv.org/abs/1712.09677">"Momentum and Stochastic Momentum for Stochastic Gradient, Newton,
Proximal Point and Subspace Descent Methods"</a>, joint with <a href="https://www.maths.ed.ac.uk/~s1461357/">Nicolas Loizou</a>, was accepted to
<a href="https://www.springer.com/journal/10589">Computational Optimization and Applications</a>.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 8, 2020</h3>
<h1>New Research Intern</h1>
<br>



Wenlin Chen has joined my group as a remote intern until about the end of September 2020. During the internship, Wenlin will be working on communication efficient methods for federated learning.
Wenlin has a BS degree in mathematics from University of Manchester (ranked top 1.5% in the cohort), and is about to start an MPhil in Machine Learning at the University of Cambridge in October 2020.
Wenlin is a coauthor of an ECML 2020 paper entitled <a href="http://www.cs.man.ac.uk/%7Egbrown/publications/ecml2020webb.pdf">To Ensemble or Not Ensemble: When does End-To-End Training Fail?</a>
 where he investigated novel information-theoretic methods of training deep neural network ensembles, focusing on the resulting regularization effects and trade-offs between individual model
 capacity and ensemble diversity. He also conducted large-scale ensemble deep learning experiments using the university’s HPC Cluster CSF3.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 7, 2020</h3>
<h1>Senior PC Memeber for IJCAI 2021</h1>
<br>

I've accepted an invite to become  Senior Program Committee member for IJCAI 2021.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>







<h3>August 3, 2020</h3>
<h1>Paper Accepted to SIAM Journal on Optimization</h1>
<br>

The paper <a href="https://arxiv.org/abs/1902.03591">"Stochastic Three Points Method for Unconstrained Smooth Minimization"</a>, joint with <a href="https://ehbergou.github.io">El Houcine Bergou</a>,
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> was accepted to
<a href="https://www.siam.org/publications/journals/siam-journal-on-optimization-siopt">SIAM Journal on Optimization</a>.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 30, 2020</h3>
<h1>Filip Hanzely Defended his PhD Thesis</h1>
<br>

<a href="http://fhanzely.github.io">Filip Hanzely</a> defended his PhD thesis <a href="http://www.optimization-online.org/DB_FILE/2020/08/7967.pdf">"Optimization for
Supervised Machine Learning:  Randomized Algorithms for Data and Parameters"</a> today.
<br> <br>

Having started in Fall 2017
(I joined KAUST in March the same year), Filip is my first PhD student to graduate from KAUST. He managed to complete his
PhD in less than 3 years, and has done some trully amazing research, described by the committee (<a href="http://pages.cs.wisc.edu/~swright/">Stephen J Wright</a>, <a href="http://tongzhang-ml.org">Tong Zhang</a>,
<a href="https://www.mathematik.rwth-aachen.de/ca/cd/btbp/?allou=1&ikz=11&gguid=0x963949FB4292ED4DA761C2EE9DAD832B">Raul F Tempone</a>,
<a href="http://www.bernardghanem.com">Bernard Ghanem</a> and myself) as "Outstanding work, in all aspects. It is comprehensive as it synthesises various strands of current research, and is almost of
 an encyclopedic coverage. The work develops deep theoretical results, some of which answer long-standing open problems. Overall, highly innovative
 research and excellent thesis narrative and structure".

<br> <br>
Filip's next destination is a faculty position at <a href="https://www.ttic.edu">TTIC</a>. Congratulations, Filip!

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 17, 2020</h3>
<h1>ICML Workshop "Beyond First Order Methods in ML Systems"</h1>
<br>

Today, I have given the openinhg plenary talk at the ICML 2020 Workshop "Beyond First Order Methods in ML Systems".
The slides from my talk "Fast Linear Convergence of Randomized BFGS" are <a href="talks/TALK-2020-07-RBFGS-ICML-workshop.pdf">here.</a>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 12, 2020</h3>
<h1>Attending ICML 2020</h1>
<br>

I am attending ICML 2020 - the event is held virtually during July 12-18. My group members are presenting 5 papers, and I
will give the opening  plenary talk at the <a href="https://sites.google.com/view/optml-icml2020/home">Beyond First Order
  Methods in Machine Learning</a> workshops on Friday.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>July 9, 2020</h3>
<h1>Area Chair for ICLR 2021</h1>
<br>

I've accepted an invite to become an Area Chair for ICLR 2021.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 7, 2020</h3>
<h1>Paper Accepted to IEEE Transactions on Signal Processing</h1>
<br>

The paper ``Best Pair Formulation & Accelerated Scheme for Non-convex Principal Component Pursuit'', joint with <a href="https://aritradutta.weebly.com">Aritra Dutta</a>,
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>, and <a href="https://jliang993.github.io">Jingwei Liang</a>, was accepted to
IEEE Transactions on Signal Processing.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>July 3, 2020</h3>
<h1>Dmitry Kovalev Wins the 2020 Ilya Segalovich Scientific Prize</h1>
<br>



It is a great pleasure to announce that my PhD student <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> is one of nine recipients of the <a href="https://yandex.ru/blog/company/novye-laureaty-premii-imeni-segalovicha">2020 Ilya
Segalovich Scientific Prize for Young Researchers</a>, awarded by Yandex (a Russian equivalent of Google or Baidu) for significant advances in Computer Science. The award is focused on research of particular interest to yandex:
machine learning, computer vision, information search and data analysis, NLP and machine translation and speech synthesis/recognition.
The prize carries a cash award of 350,000 RUB ( = approx 5,000 USD).
<br>
<br>

Dmitry started MS studies at KAUST in Fall 2018 and received his MS degree in December 2019. He is a PhD student since January 2020. In this short period of time,
he has co-authored 17 papers, 15 of which are online (<a href="https://scholar.google.com/citations?user=qHFA5z4AAAAJ">Google Scholar</a>).  In my view, he is one of
the most talented young researchers coming in recent years from Russia. Dmitry's research is insighful, creative and deep.
<br>
<br>

Google translate of the announcement in Russian:
<br>
<br><i>For the second time, we selected the laureates of the Ilya Segalovich Scientific Prize. Yandex marks this award for scientists who have made significant
  advances in computer science. The prize is awarded once a year in two categories: “Young Researchers” and “Scientific Advisers”. The first nomination is for students, undergraduates and graduate students, the second - for their mentors.
Mikhail Bilenko (Head of Machine Intelligence and Research at Yandex) said: "The services and technologies of Yandex are based on science. At the same time, we are interested not only in applied developments, but also in theoretical research. They move the entire industry forward and can lead to impressive results in the future. We established the Segalovich Prize to support students and graduate students who are engaged in machine learning and other promising areas of computer science. Often, talented guys go to work in the industry while still studying. We want them to have the opportunity to continue basic research - with our financial support."
The winners are determined by the award council. It includes Yandex executives and scientists who collaborate with the company, including Ilya Muchnik, professor at Rutgers University in New Jersey, Stanislav Smirnov, professor at the University of Geneva and Fields laureate, and Alexei Efros, professor at the University of California at Berkeley. The size of the prize for young researchers is 350 thousand, and for scientific advisers - 700 thousand rubles.
This year, 12 people became laureates: three supervisors and nine young scientists. When choosing laureates among scientific scientists, we first of all took into account the contribution to community development and youth work. For young researchers, the main criterion is scientific achievements.
All laureates in the nomination “Young Researchers” have already managed to present their work at prestigious international conferences. Proceedings for such conferences are selected and reviewed by the world’s best experts in machine learning and artificial intelligence. If the work was accepted for publication at a conference, this is international recognition.</i><br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 23, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.11773">"Optimal and Practical Algorithms for Smooth and Strongly Convex Decentralized Optimization"</a> -
joint work with <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> and <a href="https://adil-salim.github.io">Adil Salim</a>.<br>
<br>

Abstract: <i>
We consider the task of decentralized minimization of the sum of smooth strongly convex functions stored across the nodes a network. For this problem, lower bounds on the number of gradient computations and the number of communication rounds required to achieve ε accuracy have recently been proven. We propose two new algorithms for this decentralized optimization problem and equip them with complexity guarantees. We show that our first method is optimal both in terms of the number of communication rounds and in terms of the number of gradient computations. Unlike existing optimal algorithms, our algorithm does not rely on the expensive evaluation of dual gradients. Our second algorithm is optimal in terms of the number of communication rounds, without a logarithmic factor. Our approach relies on viewing the two proposed algorithms as accelerated variants of the Forward Backward algorithm to solve monotone inclusions associated with the decentralized optimization problem. We also verify the efficacy of our methods against state-of-the-art algorithms through numerical experiments.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 23, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.11573">"Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization"</a> -
joint work with <a href="https://rka97.github.io">Ahmed Khaled</a>, <a href="https://othmanesebbouh.github.io">Othmane Sebbouh</a>,
<a href="https://www.maths.ed.ac.uk/~s1461357/">Nicolas Loizou</a>, and <a href="https://gowerrobert.github.io">Robert M. Gower</a>.<br>
<br>

Abstract: <i>
We present a unified theorem for the convergence analysis of stochastic gradient algorithms for minimizing a smooth and convex loss plus a convex regularizer.
We do this by extending the unified analysis of Gorbunov, Hanzely & Richtárik (2020) and dropping the requirement that the loss function be strongly convex.
Instead, we only rely on convexity of the loss function. Our unified analysis applies to a host of existing algorithms such as proximal SGD, variance reduced
methods, quantization and some coordinate descent type methods. For the variance reduced methods, we recover the best known convergence rates as special cases.
For proximal SGD, the quantization and coordinate type methods, we uncover new state-of-the-art convergence rates. Our analysis also includes any form of sampling
and minibatching. As such, we are able to determine the minibatch size that optimizes the total complexity of variance reduced methods. We showcase this by obtaining
a simple formula for the optimal minibatch size of two variance reduced methods (\textit{L-SVRG} and \textit{SAGA}). This optimal minibatch size not only improves
the theoretical total complexity of the methods but also improves their convergence in practice, as we show in several experiments.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 23, 2020</h3>
<h1>6th FLOW seminar talk tomorrow</h1>
<br>

<a href="https://www.di.ens.fr/hadrien.hendrikx/">Hadrien Hendrikx</a> will give a talk at the  <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">FLOW seminar tomorrow.</a>
Title of his talk: "Statistical Preconditioning for Federated Learning".
  <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 22, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.11077">"A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning"</a> -
joint work with <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.<br>
<br>

Abstract: <i>
Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed compute systems. A key bottleneck of such systems is the communication overhead for exchanging information across the workers, such as stochastic gradients. Among the many techniques proposed to remedy this issue, one of the most successful is the framework of compressed communication with error feedback (EF). EF remains the only known technique that can deal with the error induced by contractive compressors which are not unbiased, such as Top-K. In this paper, we propose a new and theoretically and practically better alternative to EF for dealing with contractive compressors. In particular, we propose a construction which can transform any contractive compressor into an induced unbiased compressor. Following this transformation, existing methods able to work with unbiased compressors can be applied. We show that our approach leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. We further extend our results to federated learning with partial participation following an arbitrary distribution over the nodes, and demonstrate the benefits thereof. We perform several numerical experiments which validate our theoretical findings.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 18, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.09270">"Primal Dual Interpretation of the Proximal Stochastic Gradient Langevin Algorithm"</a> -
joint work with <a href="https://adil-salim.github.io">Adil Salim</a>.<br>
<br>

Abstract: <i>
We consider the task of sampling with respect to a log concave probability distribution. The potential of the target distribution is assumed to be composite, i.e., written as the sum of a smooth convex term, and a nonsmooth convex term possibly taking infinite values. The target distribution can be seen as a minimizer of the Kullback-Leibler divergence defined on the Wasserstein space (i.e., the space of probability measures). In the first part of this paper, we establish a strong duality result for this minimization problem. In the second part of this paper, we use the duality gap arising from the first part to study the complexity of the Proximal Stochastic Gradient Langevin Algorithm (PSGLA), which can be seen as a generalization of the Projected Langevin Algorithm. Our approach relies on viewing PSGLA as a primal dual algorithm and covers many cases where the target distribution is not fully supported. In particular, we show that if the potential is strongly convex, the complexity of PSGLA is $\cO(1/\varepsilon^2)$ in terms of the 2-Wasserstein distance. In contrast, the complexity of the Projected Langevin Algorithm is $\cO(1/\varepsilon^{12})$ in terms of total variation when the potential is convex.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 17, 2020</h3>
<h1>5th FLOW seminar talk today</h1>
<br>

<a href="https://fhanzely.github.io">Filip Hanzely</a> gave a talk at the  <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">FLOW seminar today.</a>
His <a href="https://drive.google.com/file/d/1X57EMpnO8UXLXpbZrxoxuh53heUmZ2h_/view?usp=sharing">slides</a> and
<a href="https://www.youtube.com/watch?v=N10GwFwpt5I&feature=emb_title">video</a> of the talk can be found
<a href="https://sites.google.com/view/one-world-seminar-series-flow/archive#h.64km7mxogc2u">here.</a>  <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 10, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.07013">"A Unified Analysis of Stochastic Gradient Methods for Nonconvex Federated Optimization"</a> -
joint work with <a href="https://zhizeli.github.io">Zhize Li</a>.<br>
<br>

Abstract: <i>
In this paper, we study the performance of a large family of SGD variants in the smooth nonconvex regime. To this end, we propose a generic and flexible assumption capable of accurate modeling of the second moment of the stochastic gradient. Our assumption is satisfied by a large number of specific variants of SGD in the literature, including SGD with arbitrary sampling, SGD with compressed gradients, and a wide variety of variance-reduced SGD methods such as SVRG and SAGA. We provide a single convergence analysis for all methods that satisfy the proposed unified assumption, thereby offering a unified understanding of SGD variants in the nonconvex regime instead of relying on dedicated analyses of each variant. Moreover, our unified analysis is accurate enough to recover or improve upon the best-known convergence results of several classical methods, and also gives new convergence results for many new methods which arise as special cases. In the more general distributed/federated nonconvex optimization setup, we propose two new general algorithmic frameworks differing in whether direct gradient compression (DC) or compression of gradient differences (DIANA) is used. We show that all methods captured by these two frameworks also satisfy our unified assumption. Thus, our unified convergence analysis also captures a large variety of distributed methods utilizing compressed communication. Finally, we also provide a unified analysis for obtaining faster linear convergence rates in this nonconvex regime under the PL condition.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 12, 2020</h3>
<h1>Plenary Talk at Mathematics of Data Science Workshop</h1>
<br>

Today I gave a plenary talk at the <a href="https://maths-of-data.github.io/plenary-talks/">Mathematics of Data
  Science</a> workshop.  I gave the same talk as the one I gave in April at
the <a href="https://owos.univie.ac.at">One World Optimization Seminar:</a> <a href="https://www.youtube.com/watch?v=HGQkct3db-c">“On Second Order Methods and Randomness”,
which is on YouTube.</a> If you ever wondered what a 2nd order version of SGD should and should not look like, you may want to watch
the video talk. Our stochastic Newton (SN) method converges in 4/3 * n/tau * log 1/epsilon iterations when started
close enough from the solution, where n is the number of functions forming the finite sum we want to minimize, and tau is the minibatch size.
We can choose tau to be any value between 1 and n. Note that unlike all 1st order methods, the rate of SN is
independent of the condition number! 4/n The talk is based on joint work with my fantastic students <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> and <a href="https://konstmish.github.io">Konstantin Mishchenko</a>:
<a href="https://arxiv.org/abs/1912.01597">“Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates”</a>,
<a href="https://sites.google.com/site/optneurips19/">NeurIPS 2019 Workshop Beyond First Order Methods in ML</a>, 2019.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 10, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.05988">"Random Reshuffling: Simple Analysis with Vast Improvements"</a> -
joint work with <a href="https://konstmish.github.io">Konstantin Mishchenko</a> and
<a href="https://rka97.github.io">Ahmed Khaled</a>.<br>
<br>

Abstract: <i>
Random Reshuffling (RR) is an algorithm for minimizing finite-sum functions that utilizes iterative gradient
descent steps in conjunction with data reshuffling. Often contrasted with its sibling Stochastic Gradient
Descent (SGD), RR is usually faster in practice and enjoys significant popularity in convex and non-convex
optimization. The convergence rate of RR has attracted substantial attention recently and, for strongly
convex and smooth functions, it was shown to converge faster than SGD if 1) the stepsize is small, 2) the
gradients are bounded, and 3) the number of epochs is large. We remove these 3 assumptions, improve the
ependence on the condition number from $\kappa^2$ to $\kappa$ (resp.\ from $\kappa$ to $\sqrt{kappa}$) and,
in addition, show that RR has a different type of variance. We argue through theory and experiments that the
new variance type gives an additional justification of the superior performance of RR. To go beyond strong
convexity, we present several results for non-strongly convex and non-convex objectives. We show that in all
cases, our theory improves upon existing literature. Finally, we prove fast convergence of the Shuffle-Once
(SO) algorithm, which shuffles the data only once, at the beginning of the optimization process. Our theory
for strongly-convex objectives tightly matches the known lower bounds for both RR and SO and substantiates
the common practical heuristic of shuffling once or only a few times. As a byproduct of our analysis, we also
get new results for the Incremental Gradient algorithm (IG), which does not shuffle the data at all.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 5, 2020</h3>
<h1>NeurIPS Paper Deadline Today</h1>
<br>

The  <a href="https://nips.cc">NeurIPS</a> deadline has passed! Finally, I can relax a bit (= 1 day). Next deadline:
Supplementary Material for NeurIPS, on June 11... <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 1, 2020</h3>
<h1>Five Papers Accepted to ICML 2020</h1>
<br>
We've had five papers accepted to <a href="https://icml.cc/Conferences/2020">ICML 2020</a>, which will be run virtually during July 12-18, 2020.
Here they are:<br><br>


1) <a href="https://arxiv.org/abs/2002.04670">"Variance Reduced Coordinate Descent with Acceleration: New
  Method With a Surprising Application to Finite-Sum Problems"</a> - joint work with
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> and
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev.</a> <br>
<br>
Abstract: <i>We propose an accelerated version of stochastic variance reduced coordinate descent -- ASVRCD. As other variance reduced coordinate descent methods such as SEGA or SVRCD, our method can deal with problems that include a non-separable and non-smooth regularizer, while accessing a random block of partial derivatives in each iteration only. However, ASVRCD incorporates Nesterov's momentum, which offers favorable iteration complexity guarantees over both SEGA and SVRCD. As a by-product of our theory, we show that a variant of Allen-Zhu (2017) is a specific case of ASVRCD, recovering the optimal oracle complexity for the finite sum objective.</i><br>
<br>



2) <a href="https://arxiv.org/abs/2002.09526">"Stochastic Subspace Cubic Newton Method"</a> - joint work with
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>,
<a href="https://scholar.google.ru/citations?user=YNBhhjUAAAAJ&hl=en">Nikita Doikov</a> and <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>.
<br>
<br>
Abstract: <i>In this paper, we propose a new randomized second-order optimization
  algorithm---Stochastic Subspace Cubic Newton (SSCN)---for minimizing a high dimensional
  convex function f. Our method can be seen both as a stochastic extension of the
  cubically-regularized Newton method of Nesterov and Polyak (2006), and a second-order
  enhancement of stochastic subspace descent of Kozak et al. (2019). We prove that as we vary
  the minibatch size, the global convergence rate of SSCN interpolates between the rate of
  stochastic coordinate descent (CD) and the rate of cubic regularized Newton, thus giving new
  insights into the connection between first and second-order methods.
  Remarkably, the local convergence rate of SSCN matches the rate of stochastic subspace
  descent applied to the problem of minimizing the quadratic function 0.5 (x−xopt)^T f''(xopt) (x−xopt),
  where xopt is the minimizer of f, and hence depends on the properties of f at the optimum only.
  Our numerical experiments show that SSCN outperforms non-accelerated first-order CD algorithms
  while being competitive to their accelerated variants. </i><br>
<br>



3) <a href="https://arxiv.org/abs/1910.09529">"Adaptive Gradient Descent Without Descent"</a> - work of  <a
href="https://konstmish.github.io">Konstantin Mishchenko</a> and <a href="https://people.epfl.ch/yurii.malitskyi">Yura Malitsky</a>.<br>
<br>
Abstract: <i>We present a strikingly simple proof that two rules are sufficient to automate gradient descent: 1) don't increase the stepsize too fast and 2) don't overstep the local curvature. No need for functional values, no line search, no information about the function except for the gradients. By following these rules, you get a method adaptive to the local geometry, with convergence guarantees depending only on smoothness in a neighborhood of a solution. Given that the problem is convex, our method will converge even if the global smoothness constant is infinity. As an illustration, it can minimize arbitrary continuously twice-differentiable convex function. We examine its performance on a range of convex and nonconvex problems, including matrix factorization and training of ResNet-18. </i> <br>
<br>


4) <a href="https://arxiv.org/abs/2004.01442">
  "From Local SGD to Local Fixed Point Methods for Federated Learning"</a> - joint work with
Grigory Malinovsky,
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
<a href="http://elnurgasanov.me">Elnur Gasanov</a>, and
<a href="https://lcondat.github.io">Laurent Condat</a>. <br>

<br>
Abstract: <i>Most algorithms for solving optimization problems or finding saddle points of convex-concave functions are fixed point algorithms. In this work we consider the generic problem of finding a fixed point of an average of operators, or an approximation thereof, in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models the computations done locally on a mobile device. We investigate two strategies to achieve such a consensus: one based on a fixed number of local steps, and the other based on randomized computations. In both cases, the goal is to limit communication of the locally-computed variables, which is often the bottleneck in distributed frameworks. We perform convergence analysis of both methods and conduct a number of experiments highlighting the benefits of our approach.</i> <br>
<br>


5) <a href="https://arxiv.org/abs/2002.11364">"Acceleration for Compressed Gradient Descent in Distributed Optimization"</a>
- joint work with
<a href="https://zhizeli.github.io">Zhize Li</a>, <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
and <a
href="https://qianxunk.github.io">Xun Qian</a>.
<br>
<br>
Abstract: <i>The abstract contains a lot of math symbols, so <a href="https://arxiv.org/abs/2002.11364">look here instead.</a> </i><br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>May 27, 2020</h3>
<h1>NeurIPS Abstract Deadline Today</h1>
<br>

Polishing <a href="https://nips.cc">NeurIPS</a> abstracts... <br >


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>May 22, 2020</h3>
<h1>Eduard's ICLR Talk </h1>
<br>

<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> presented our paper <a href="https://openreview.net/forum?id=HylAoJSKvH">"A Stochastic Derivative Free Optimization Method with Momentum"</a> at ICLR. This paper is joint work with
Adel Bibi, Ozan Sener, and El Houcine Bergou. Eduard's 5min talk can be found here: <br >

<br>
<a href="https://iclr.cc/virtual_2020/poster_HylAoJSKvH.html"> <img alt="" src="imgs/ICLR2020-Eduard.png" width="700" ></a>
<br>



<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>May 21, 2020</h3>
<h1>"JacSketch" Paper Appeared in Mathematical Programming</h1>
<br>

The paper "Stochastic quasi-gradient methods: variance reduction via Jacobian sketching", joint work
with Robert M. Gower and Francis Bach, just appeared online on the Mathematical Programming
website: <a href="https://link.springer.com/article/10.1007/s10107-020-01506-0">Mathematical Programming, 2020</a>.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 20, 2020</h3>
<h1>Paper Appeared in SIMAX</h1>
<br>

The paper "Stochastic reformulations of linear systems: algorithms and convergence theory", joint work with
Martin Takáč, just appeared online on the SIMAX website: <a href="https://epubs.siam.org/doi/abs/10.1137/18M1179249">SIAM Journal on Matrix Analysis and Applications 41(2):487–524, 2020</a>.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>May 20, 2020</h3>
<h1>2nd FLOW Talk: Blake Woodworth (TTIC)</h1>
<br>

<a href="https://ttic.uchicago.edu/~blake/">Blake Woodworth (TTIC)</a> has given a great talk at the FLOW seminar today. His talk title was "Is Local SGD Better than Minibatch SGD?".
The slides and YouTube video <a href="https://sites.google.com/view/one-world-seminar-series-flow/archive#h.marh0ook3glo">can be found here.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 15, 2020</h3>
<h1>Paper Accepted to UAI 2020</h1>
<br>

Our paper
<a href="https://arxiv.org/abs/1901.09437">"99% of Distributed Optimization is a Waste of Time: The Issue and How to Fix it"</a> -
joint work with <a href="https://konstmish.github.io">Konstantin Mishchenko</a> and
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>,
was accepted to <a href="http://www.auai.org/uai2020/">Conference on Uncertainty in Artificial Intelligence (UAI 2020)</a>.<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 13, 2020</h3>
<h1>1st FLOW Talk: Ahmed Khaled (Cairo)</h1>
<br>

<a href="https://rka97.github.io">Ahmed Khaled (Cairo)</a> has given his first research talk ever today. Topic: "On the Convergence of Local SGD on Identical and Heterogeneous Data".
It was a great talk - I can't wait to see him give talks in the future. The abstract, link to the relevant papers, slides and YouTube video <a href="https://sites.google.com/view/one-world-seminar-series-flow/archive#h.azhfwca3oax9">are here.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 7, 2020</h3>
<h1>Three Students Attending MLSS 2020</h1>

<br>
<a href="https://samuelhorvath.github.io">Samuel Horváth</a>, <a href = "https://eduardgorbunov.github.io"> Eduard Gorbunov</a> and
<a href="https://shulgin-egor.github.io">Egor Shulgin</a> have been accepted to participate in tis year's <a href="http://mlss.tuebingen.mpg.de/2020/">Machine Learning Summer School (MLSS) in Tübingen, Germany.</a>
As most things this year, the event will be fully virtual. MLSS is highly selective; I am told this year they received more than 1300 applications for 180 spots at the event (less than 14% acceptance rate).
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 5, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2005.01097">"Adaptive Learning of the Optimal Mini-Batch Size of SGD"</a> -
joint work with <a href="https://scholar.google.com/citations?user=caAyffEAAAAJ&hl=en">Motasem Alfarra</a>,
<a href="https://slavomirhanzely.wordpress.com">Slavomír Hanzely</a>,
<a href="https://alyazeedbasyoni.wixsite.com/blog">Alyazeed Albasyoni</a>
and <a href="http://www.bernardghanem.com">Bernard Ghanem</a>.<br>
<br>

Abstract: <i>
Recent advances in the theoretical understandingof SGD (Qian et al., 2019) led to a formula for the optimal mini-batch size minimizing the number of effective data passes, i.e., the number of iterations times the mini-batch size. However, this formula is of no practical value as it depends on the knowledge of the variance of the stochastic gradients evaluated at the optimum. In this paper we design a practical SGD method capable of learning the optimal mini-batch size adaptively throughout its iterations. Our method does this provably, and in our experiments with synthetic and real data robustly exhibits nearly optimal behaviour; that is, it works as if the optimal mini-batch size was known a-priori. Further, we generalize our method to several new mini-batch strategies not considered in the literature before, including a sampling suitable for distributed implementations.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 4, 2020</h3>
<h1>FLOW: Federated Learning One World Seminar</h1>

<br>
Together with <a href = "http://researchers.lille.inria.fr/abellet/">Aurélien Bellet</a> (Inria), <a href = "https://www.cs.cmu.edu/~smithv/">Virginia Smith</a> (Carnegie Mellon) and
<a href= "https://ist.ac.at/en/research/alistarh-group/">Dan Alistarh</a> (IST Austria), we are launching <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">FLOW: Federated Learning One World Seminar.</a>
The seminar will take place on a weekly basis on Wednesdays. All talks will be delivered via Zoom. The first few talks are: <br> <br>
May 13, <a href="https://rka97.github.io">Ahmed Khaled</a> (Cairo): On the Convergence of Local SGD on Identical and Heterogeneous Data <br> <br>
May 20, <a href="http://ttic.uchicago.edu/~blake/">Blake Woodworth</a> (TTIC): Is Local SGD Better than Minibatch SGD? <br> <br>
May 27, <a href="http://papail.io">Dimitris Papailiopoulos</a> (Wisconsin Madison): Robustness in Federated Learning May be Impossible Without an All-knowing Central Authority
 <br> <br>
June 3, No talk due to NeurIPS deadline <br> <br>
June 10, <a href="https://people.epfl.ch/sai.karimireddy">Sai Praneeth Karimireddy</a> (EPFL): Stochastic Controlled Averaging for Federated Learning <br> <br>
June 17, <a href="https://fhanzely.github.io">Filip Hanzely</a> (KAUST): Federated Learning of a Mixture of Global and Local Models: Local SGD and Optimal Algorithms <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 3, 2020</h3>
<h1>Talk at the Montréal Machine Learning and Optimization Seminar</h1>

<br>
On Friday this week (May 8), I will give a talk entitled "On Second Order Methods and Randomness" at the  <a href="https://mtl-mlopt.github.io">Montréal Machine
Learning and Optimization (MTL MLOpt) Seminar</a>. This is an online seminar delivered via Google Meet. Starting time: 9am PDT.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>









<h3>April 25, 2020</h3>
<h1>Talk at the One World Optimization Seminar</h1>

<br>
I will give a talk within the <a href="https://owos.univie.ac.at">One World Optimization Seminar</a>
series on Monday, April 27, at 3pm CEST. This is a new exciting initiative, and my talk is only the
second in the series. I will speak about some new results related to second order methods and randomness.
One of the advantages of this new format is that anyone can attend - indeed, attendance is via
<a href="https://zoom.us">Zoom</a>. However, you need to register online in advance in order to get access.
Hope to "see" many of you there!
<br><br>

Update (April 29): The <a href="https://owos.univie.ac.at/fileadmin/user_upload/k_owos/Peter_Richtarik-Stochastic_Newton.pdf">slides</a> and <a href="https://www.youtube.com/watch?v=HGQkct3db-c">video recording</a> of my talk are now available.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 21, 2020</h3>
<h1>Filip Hanzely Accepted a Position at TTIC</h1>
<br>

<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> accepted a Research Assistant Professorship at <a href="https://www.ttic.edu">Toyota Technological Institute at Chicago</a> (TTIC).
Filip has written his thesis and will submit it soon. He is expected to graduate this Summer, and will start his new position in Chicago in the Fall. Filip has obatined multiple other offers besides this, including a Tenure-Track Assistant Professorship and a Postdoctoral Fellowship in a top machine learning group.

<br><br>
Congratulations!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 20, 2020</h3>
<h1>ICML, UAI and COLT Author Response Deadlines</h1>
<br>

I am busy: <a href="https://icml.cc">ICML</a> and <a href="http://www.auai.org/uai2020/call_for_papers.php">UAI</a> rebuttal deadline is today, and for
<a href="http://learningtheory.org/colt2020/">COLT</a> the deadline is on April 24.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 7, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2004.02635">"Dualize, Split, Randomize: Fast Nonsmooth Optimization Algorithms"</a> -
joint work with <a href="https://adil-salim.github.io">Adil Salim</a>, <a href="https://lcondat.github.io">Laurent Condat</a>, and <a href="https://konstmish.github.io">Konstantin Mishchenko</a>.<br>
<br>

Abstract: <i>
We introduce a new primal-dual algorithm for minimizing the sum of three convex functions, each of which has its own oracle. Namely, the first one is differentiable, smooth and possibly stochastic, the second is proximable, and the last one is a composition of a proximable function with a linear map. Our theory covers several settings that are not tackled by any existing algorithm; we illustrate their importance with real-world applications. By leveraging variance reduction, we obtain convergence with linear rates under strong convexity and fast sublinear convergence under convexity assumptions. The proposed theory is simple and unified by the umbrella of stochastic Davis-Yin splitting, which we design in this work. Finally, we illustrate the efficiency of our method through numerical experiments.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 5, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2004.02163">"On the Convergence Analysis of Asynchronous SGD for Solving Consistent Linear Systems"</a> -
joint work with <a href="https://sands.kaust.edu.sa/authors/atal-narayan-sahu/">Atal Narayan Sahu</a>, <a href="https://www.aritradutta.com">Aritra Dutta</a>, and Aashutosh Tiwari.<br>
<br>

Abstract: <i>
In the realm of big data and machine learning, data-parallel, distributed stochastic algorithms have drawn significant attention in the present days. While the synchronous versions of these algorithms are well
understood in terms of their convergence, the convergence analyses of their asynchronous counterparts are not widely studied.
In this paper, we propose and analyze a  distributed, asynchronous parallel SGD method in light of solving an arbitrary consistent linear system by reformulating the system into a stochastic optimization problem as
studied by Richtárik and Takáč in [35]. We compare the convergence rates of our asynchronous SGD algorithm with the
synchronous parallel algorithm proposed by Richtárik and Takáč in [35] under different choices of the hyperparameters---the stepsize, the damping factor, the number of processors, and the delay factor.
We show that our asynchronous parallel SGD algorithm also enjoys a global linear convergence rate, similar to the "basic method" and the synchronous parallel method in [35] for solving any arbitrary consistent
linear system via stochastic reformulation. We also show that our asynchronous parallel SGD improves upon the "basic method" with a better convergence rate when the number of processors is larger than four.
We further show that this asynchronous approach performs asymptotically better than its synchronous counterpart for certain linear systems. Moreover, for certain linear systems, we compute the minimum number of
processors required for which our asynchronous parallel SGD is better, and find that this number can be as low as two for some ill-conditioned problems.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 3, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2004.01442">"From Local SGD to Local Fixed Point Methods for Federated Learning"</a> -
joint work with Grigory Malinovsky, <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="http://elnurgasanov.me">Elnur Gasanov</a>, and <a href="https://lcondat.github.io">Laurent Condat</a>.<br>
<br>

Abstract: <i>
Most algorithms for solving optimization problems or finding saddle points of convex-concave functions are fixed point algorithms.
In this work we consider the generic problem of finding a fixed point of an average of operators, or an approximation thereof,
in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models
the computations done locally on a mobile device. We investigate two strategies to achieve such a consensus: one based on a fixed
number of local steps, and the other based on randomized computations. In both cases, the goal is to limit communication of the
locally-computed variables, which is often the bottleneck in distributed frameworks. We perform convergence analysis of both methods
and conduct a number of experiments highlighting the benefits of our approach.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 10, 2020</h3>
<h1>Area Chair for NeurIPS 2020</h1>
<br>

I will serve as an Area Chair for <a href="https://nips.cc/Conferences/2020/">NeurIPS 2020</a>, to be
held during December 6-12, 2020 in Vancouver, Canada (same location as last year). For those not in the know,
Google Scholar Metrics says that NeurIPS is the #1 conference in AI:
<br>
<br>
<a href="https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=eng_artificialintelligence"> <img alt="" src="imgs/GoogleScholarMetrics-AI.png" width="700" ></a>
<br>
<br>
 The review process has changed this year; here is a short and beautifully produced video explaining the key 5 changes:
 <br>
<br>
<a href="https://www.youtube.com/watch?v=361h6lHZGDg&feature=share"> <img alt="" src="imgs/5changes.jp2" width="700" ></a>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 9, 2020</h3>
<h1>Coronavirus at KAUST</h1>
<br>

No, Covid-19 did not catch up with anyone at KAUST yet. Still in luck. However, as in many places, its increasing omnipresence and gravitational pull is felt here as well.

<br>
<br>
For example, as of today, all KAUST lectures are moving online. And for a good reason I think: we have seen in Lombardy what the virus can do when unchecked.
I am teaching my CS 390T Federated Learning course on Sundays (yes - the work week in Saudi spans Sunday-Thursday) and Tuesdays, and hence my first online lecture will take place on Sunday March 15. I hope, at least, as I need
to decide how best to do it.
<br>
<br>

Conference travel has been limited for some time now, but the rules are even  more strict now. This seems less than necessary
as conferences drop like flies anyway. My planned travel between now and May includes a seminar talk at EPFL (Switzerland), a workshop
keynote lecture at King Faisal University (Al-Ahsa, Saudi Arabia), presentation at ICLR (Addis Ababa, Ethiopia), and SIAM Conference on Optimization (Hong Kong)
which I am helping to organize. Most of these events are cancelled, and those that survive will most probably go to sleep soon.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 27, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.12410">"On Biased Compression for Distributed Learning"</a> -
joint work with <a href="https://scholar.google.ru/citations?user=hVVJR-sAAAAJ&hl=en">Aleksandr Beznosikov</a>,
<a href="https://samuelhorvath.github.io">Samuel Horváth</a>, and <a href="https://mher-safaryan.github.io">Mher Safaryan</a>.<br>
<br>

Abstract: <i>
In the last few years, various communication  compression techniques have  emerged  as an indispensable  tool helping to alleviate the communication bottleneck in distributed learning.
However, despite the fact biased compressors often show superior performance in practice when compared to the much more studied and understood unbiased compressors,  very little
is known about them. In this work we study three classes of biased compression operators, two of which are new, and their performance when applied to  (stochastic) gradient descent
and distributed (stochastic) gradient descent.  We show for the first time that biased compressors can lead to linear convergence rates both in the single node and distributed settings.
Our distributed SGD method enjoys the ergodic rate $O \left( \frac{\delta L \exp(-K) }{\mu}  + \frac{(C + D)}{K\mu} \right)$, where $\delta$ is a compression parameter which grows when
more compression is applied, $L$ and $\mu$ are the smoothness and strong convexity constants, $C$ captures stochastic gradient noise ($C=0$ if full gradients are computed on each node)
and $D$ captures the variance of the gradients at the optimum ($D=0$ for over-parameterized models).  Further,  via a theoretical study of several synthetic and empirical distributions
of communicated gradients, we shed light on why and by how much  biased compressors outperform  their unbiased variants.  Finally, we  propose a new highly performing biased
compressor---combination of Top-$k$ and natural dithering---which in our experiments outperforms all other compression techniques.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 26, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.11364">"Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization"</a> -
joint work with <a href="https://zhizeli.github.io">Zhize Li</a>,
 <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, and <a href="https://qianxunk.github.io">Xun Qian</a>.<br>
<br>

Abstract: <i>
Due to the high communication cost in distributed and federated learning problems, methods relying on compression of communicated messages are becoming increasingly popular. While in other contexts the best performing gradient-type methods invariably rely on some form of acceleration/momentum to reduce the number of iterations, there are no methods which combine the benefits of both gradient compression and acceleration. In this paper, we remedy this situation and propose the first accelerated compressed gradient descent (ACGD) methods.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 26, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.11337">"Fast Linear Convergence of Randomized BFGS"</a> -
joint work with <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
 <a href="https://gowerrobert.github.io">Robert M. Gower</a>, and <a href="https://scholar.google.com/citations?hl=ru&user=sEjyzkgAAAAJ">Alexander Rogozin</a>.<br>
<br>

Abstract: <i>
Since the late 1950’s when quasi-Newton methods first appeared, they have become one of the most widely used and
efficient algorithmic paradigms for unconstrained optimization. Despite their immense practical success, there is
little theory that shows why these methods are so efficient. We provide a semi-local rate of convergence for the
randomized BFGS method which can be significantly better than that of gradient descent, finally giving theoretical
evidence supporting the superior empirical performance of the method.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 21, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.09526">"Stochastic Subspace Cubic Newton Method"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>,
 <a href="https://scholar.google.ru/citations?user=YNBhhjUAAAAJ&hl=en">Nikita Doikov</a> and <a href="https://en.wikipedia.org/wiki/Yurii_Nesterov">Yurii Nesterov</a>.<br>
<br>

Abstract: <i>
In this paper, we propose a new randomized second-order optimization algorithm---Stochastic Subspace Cubic Newton (SSCN)---for minimizing a high
dimensional convex function $f$. Our method can be seen both as astochastic extension of the cubically-regularized Newton method of Nesterov
and Polyak (2006), and a second-order enhancement of stochastic subspace descent of Kozak et al. (2019). We prove that as we vary the
minibatch size, the global convergence rate of SSCN interpolates between the rate of stochastic coordinate descent (CD) and the rate of cubic
regularized Newton, thus giving new insights into the connection between first and second-order methods. Remarkably, the local convergence rate
of SSCN matches the rate of stochastic subspace descent applied to the problem of minimizing the quadratic function
$\frac{1}{2} (x-x^*)^\top \nabla^2 f(x^*)(x-x^*)$, where $x^*$ is the minimizer of $f$, and hence depends on the properties of $f$ at the optimum only.
Our numerical experiments show that SSCN outperforms non-accelerated first-order CD algorithms while being competitive to their accelerated variants.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 20, 2020</h3>
<h1>New MS/PhD Student: Egor Shulgin</h1>
<br>

Egor Vladimirovich Shulgin is back at KAUST - now as an MS/PhD student. Welcome!!! <br><br>

Egor has co-authored 4 papers and a book (in Russian) entitled <a href="https://arxiv.org/abs/1907.01060">"Lecture Notes on Stochastic Processes"</a>.
Here are the papers, in reverse chronological order:<br><br>
- <a href="https://arxiv.org/abs/2002.08958">Uncertainty principle for communication compression in distributed and federated
learning and the search for an optimal compressor </a> <br>
- Adaptive catalyst for smooth convex optimization <br>
- <a href="https://arxiv.org/abs/1905.11373">Revisiting stochastic extragradient</a> (AISTATS 2020) <br>
- <a href="http://proceedings.mlr.press/v97/qian19b">SGD: general analysis and improved rates</a> (ICML 2019) <br>
<br>

Egor has a bachelor degree in Applied Mathematics from the Department of
Control and Applied Mathematics at MIPT, Dolgoprudny, Russia. He majored in
Data Analysis. His CV mentions the following as his main subjects: Probability
Theory, Random Processes, Convex Optimization, and Machine Learning.<br>
<br>


Egor’s hobbies, according to his CV, are: hiking, alpine skiing, tennis,
and judo. Notably, this list does not include table tennis. However, I
know for a fact that he is very good in it!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 20, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.08958">"Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor"</a> -
joint work with <a href="https://mher-safaryan.github.io">Mher Safaryan</a> and <a href="https://scholar.google.com/citations?user=XlmSx18AAAAJ&hl=en">Egor Shulgin</a>.<br>
<br>

Abstract: <i>
In order to mitigate the high communication cost in  distributed and federated learning, various vector compression schemes, such as
quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as
few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion
(variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of
communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more
distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators,
thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication
compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first
step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call
{\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension
independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry.
We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication
complexity improvements on previous state of the art.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 14, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.05516">"Federated Learning of a Mixture of Global and Local Models"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>.<br>
<br>

Abstract: <i>We propose a new optimization formulation for training federated learning models. The standard
  formulation has the form of an empirical risk minimization problem constructed to find a single global
  model trained from the private data stored across all participating devices. In contrast, our formulation
  seeks an explicit trade-off between this traditional global model and the local models, which can be learned
  by each device from its own private data without any communication. Further, we develop several efficient
  variants of SGD (with and without partial participation and with and without variance reduction) for solving
  the new formulation and prove communication complexity guarantees. Notably, our methods are similar but not
  identical to federated averaging / local SGD, thus shedding some light on the essence of the elusive method.
  In particular, our methods do not perform full averaging steps and instead merely take steps towards averaging.
  We argue for the benefits of this new paradigm for federated learning.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 12, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.05359">"Adaptivity of Stochastic Gradient Methods for Nonconvex Optimization"</a> -
joint work with <a href="https://samuelhorvath.github.io">Samuel Horváth</a>,
<a href="https://statistics.berkeley.edu/people/lihua-lei">Lihua Lei</a> and
<a href="https://people.eecs.berkeley.edu/~jordan/">Michael I. Jordan</a>.<br>
<br>

Abstract: <i>Adaptivity is an important yet under-studied property in modern optimization theory.
  The gap between the state-of-the-art theory and the current practice is striking in that algorithms
  with desirable theoretical guarantees typically involve drastically different settings of hyperparameters,
  such as step-size schemes and batch sizes, in different regimes. Despite the appealing theoretical results,
  such divisive strategies provide little, if any, insight to practitioners to select algorithms that work broadly
  without tweaking the hyperparameters. In this work, blending the "geometrization" technique introduced by
  Lei & Jordan 2016 and the SARAH algorithm of Nguyen et al., 2017, we propose the Geometrized SARAH
  algorithm for non-convex finite-sum and stochastic optimization. Our algorithm is proved to achieve adaptivity
  to both the magnitude of the target accuracy and the Polyak-Łojasiewicz (PL) constant if present. In addition,
  it achieves the best-available convergence rate for non-PL objectives simultaneously while outperforming
  existing algorithms for PL objectives.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 11, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.04670">"Variance Reduced Coordinate Descent with Acceleration: New Method With a
  Surprising Application to Finite-Sum Problems"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> and <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>.<br>
<br>

Abstract: <i>We propose an accelerated version of stochastic variance reduced coordinate descent -- ASVRCD. As other
variance reduced coordinate descent methods such as SEGA or SVRCD, our method can deal with problems that include a
non-separable and non-smooth regularizer, while accessing a random block of partial derivatives in each iteration only.
However, ASVRCD incorporates Nesterov's momentum, which offers favorable iteration complexity guarantees over both SEGA
and SVRCD. As a by-product of our theory, we show that a variant of Allen-Zhu (2017) is a specific case of ASVRCD,
recovering the optimal oracle complexity for the finite sum objective.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 10, 2020</h3>
<h1>Konstantin Giving a Series of Talks in the US and UK</h1>

<br>
<a href="https://konstmish.github.io/">Konstantin Mishchenko</a> is giving several talks in New York, London and Oxford. Here is his schedule:
<br>
<br>
<br>

February 10, Facebook Research, New York, "Adaptive Gradient Descent Without Descent"<br><br>
February 12, Deepmind, London, "Adaptive Gradient Descent Without Descent"<br><br>
February 12, UCL Gatsby Computational Neuroscience Unit, London, "Sinkhorn Algorithm as a Special Case of Stochastic Mirror Descent" <br><br>
February 14, Oxford University, Oxford, "Adaptive Gradient Descent Without Descent" <br><br>
February 14, Imperial College London, London, "Sinkhorn Algorithm as a Special Case of Stochastic Mirror Descent" <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 9, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.03329">"Better Theory for SGD in the Nonconvex World"</a> -
joint work with <a href="https://rka97.github.io">Ahmed Khaled</a>.<br>
<br>

Abstract: <i>Large-scale nonconvex optimization problems are ubiquitous in modern machine learning,
and among practitioners interested in solving them, Stochastic Gradient Descent (SGD)
reigns supreme. We revisit the analysis of SGD in the nonconvex setting and propose a new
variant of the recently introduced expected smoothness assumption which governs the behaviour
of the second moment of the stochastic gradient. We show that our assumption is both more general
and more reasonable than assumptions made in all prior work. Moreover, our results yield the
optimal $O(\epsilon^{-4})$ rate for finding a stationary point of nonconvex smooth functions, and
recover the optimal $O(\epsilon^{-1})$ rate for finding a global solution if the Polyak-Łojasiewicz
condition is satisfied. We compare against convergence rates under convexity and prove a theorem
on the convergence of SGD under Quadratic Functional Growth and convexity, which might be of
independent interest. Moreover, we perform our analysis in a framework which allows for a detailed
study of the effects of a wide array of sampling strategies and minibatch sizes for finite-sum
optimization problems. We corroborate our theoretical results with experiments on real and
synthetic data.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 8, 2020</h3>
<h1>Interview by Robin.ly for their Leaders in AI Platform</h1>

<br>
In December last year, while attending NeurIPS in Vancouver, I was interviewed by Robin.ly.
The video can be found here<br><br>

<a href="https://www.robinly.info/post/peter-richtarik-kaust-machine-learning-federated-learning-kaust"><img alt="" src="imgs/Leaders_in_AI_video.png" width="600"> </a><br><br>

and a podcast out of this is on soundcloud: <br><br> <br>

<a href="https://soundcloud.com/robinly/podcast-peter-richtarik"><img alt="" src="imgs/Leaders_in_AI_podcast.png" width="600"> </a><br><br>

I am in excellent company:<br><br>
<a href="https://soundcloud.com/robinly/yoshua-bengio-from-deep-learning-to-consciousness-interview-by-song-han-neurips-2019">Yoshua Bengio</a><br>
<a href="https://soundcloud.com/robinly/kai-fu-lee-the-era-of-ai-the-rise-of-china-the-future-of-work">Kai-Fu Lee</a><br>
<a href="https://soundcloud.com/robinly/the-future-of-distributed-machine-learning-max-welling-u-of-amsterdam-qualcomm-neurips-2019">Max Welling</a><br>
<a href="https://soundcloud.com/robinly/my-first-cvpr-christopher-manning-professor-director-stanford-ai-lab">Christopher Manning</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 8, 2020</h3>
<h1>AAAI in New York</h1>

<br> <a href="https://konstmish.github.io">Konstantin</a> is about to receive a Best Reviewer Award at AAAI 20 in New York. <a href="http://www.adelbibi.com">Adel</a> is presenting our paper
 <a href="https://arxiv.org/abs/1902.01272">"A stochastic derivative-free optimization method with importance sampling"</a>,
 joint work with El Houcine Bergou, Ozan Sener, Bernard Ghanem and myself, at the event.</br> <br>


 Update (May 3): Here is a <a href="https://cemse.kaust.edu.sa/vcc/news/kaust-cs-student-recognized-aaai-outstanding-program-committee-award">KAUST article about Konstantin and his achievements.</a> I am very proud.

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>February 6, 2020</h3>
<h1>ICML Deadline Today!</h1>

<br>I am a walking zombie, a being without a soul, a sleepless creature of the night. Do not approach me or
you will meet your destiny. Wait three days and I shall be alive again. </br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>February 3, 2020</h3>
<h1>Samuel in San Diego</h1>

<br><a href="https://samuelhorvath.github.io">Samuel Horváth</a>
is in San Diego, and will soon be presenting the paper <a href="https://arxiv.org/abs/1901.08689">"Don’t jump through hoops and remove those loops:
SVRG and Katyusha are better without the outer loop"</a>, joint work with Dmitry Kovalev and myself,
at <a href="http://alt2020.algorithmiclearningtheory.org">ALT 2020</a> in San Diego.<br><br> Here is the
list of <a href="http://alt2020.algorithmiclearningtheory.org/accepted-papers/">all accepted papers.</a> <br><br>
Samuel will be back at KAUST in about 10 days.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 2, 2020</h3>
<h1>Eduard Gorbunov is Visting Again</h1>

<br><a href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard Gorbunov</a> came for a research visit - this is his third time at KAUST. This time, he wil stay for about 2 months.

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>January 31, 2020</h3>

<h1>Poster for AAAI-20</h1>
<br>
Our paper <a href="https://arxiv.org/abs/1902.01272">"A stochastic derivative-free optimization
  method with importance sampling"</a>, joint work with Adel Bibi, El Houcine Bergou, Ozan Sener, and Bernard Ghanem,
  will be presented at <a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a>, which will be held in New York
  during February 7-12. <br><br>

We have just prepared a poster, here it is:<br>
<br>
<a href="posters/Poster-STP_IS-AAAI20.pdf"><img src="posters/Poster-STP_IS-AAAI20_small.png" alt="STP" border="0" height="500"></a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 20, 2019</h3>
<h1>Paper Accepted to SIMAX</h1>

<br>The paper <a href="https://arxiv.org/abs/1706.01108">"Stochastic reformulations of linear systems: algorithms and convergence theory"</a>, joint work with Martin Takáč, was accepted to <a href="https://www.siam.org/publications/journals/siam-journal-on-matrix-analysis-and-applications-simax"> SIAM Journal on Matrix Analysis and Applications.</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 18, 2020</h3>
<h1>New Intern: Alexander Rogozin</h1>

<br>A new intern arrived today: Alexander Rogozin. Alexander is a final-year MSc student in the department of Control and Applied Mathematics at the <a href="https://mipt.ru/english/">Moscow Institute of Physics and Technology (MIPT).</a> <br>

<br>Some notable achievements of Alexander so far:<br>

<br>
- Co-authored <a href="https://scholar.google.com/citations?hl=en&amp;user=sEjyzkgAAAAJ">3 papers</a> in the area of decentralized optimization over time varying networks<br>
- His GPA ranks him among the top 5% students at MIPT<br>
- Tutor of Probability Theory at MIPT, 2018-now<br>
- Finished the <i>Yandex School for Data Analysis</i> (2017-2019)<br>
- Awardee of the Russian National Physics Olympiad, 2013<br>
- Certificate of Honor at Russian National Mathematics Olympiad, 2012<br>
- Winner of Russian National Physics Olympiad, 2012<br>

<br>
In 2018, Alexander participated in the Moscow Half-Marathon.
He is a holder of 4-kyu in Judo. Having studied the piano for
11 years, Alexander participated in city, regional, national
and international musical festivals and competitions. He
performed with a symphony orchestra as a piano soloist at
festivals in his hometown.
<br>
<br>
Welcome!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>January 16, 2020</h3>
<h1>Konstantin Mishchenko Among the Best Reviewers for AAAI-20</h1>
<br> Congratulations <a href="https://konstmish.github.io/">Kostya</a>!
(AAAI Conference on Artificial Intelligence is <a
href="https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_artificialintelligence">one of the top AI conferences</a>. The email below tells the
story.)<br> <br>
<i>
Dear Konstantin, <br>
<br>
On behalf of the Association for the Advancement of
Artificial Intelligence and the AAAI-20 Program Committee,
we are pleased to inform you that you have been selected as
one of 12 Outstanding Program Committee members for 2020 in
recognition of your outstanding service on this year's
committee. Your efforts were characterized by exceptional
care, thoroughness, and thoughtfulness in the reviews and
discussions of the papers assigned to you. <br>
<br>
In recognition of your achievement, you will be
presented with a certificate by the AAAI-20 Program
Cochairs, Vincent Conitzer and Fei Sha, during the AAAI-20
Award Ceremony on Tuesday, February 11 at 8:00am. There will
also be an announcement of this honor in the program. Please
let us know (aaai20@aaai.org) if you will be present at the
award ceremony to accept your award. <br>
<br>
Congratulations, and we look forward to seeing you in New York for AAAI-20, February 7-12. <br>
<br>
Warmest regards, <br>
<br>
Carol McKenna Hamilton<br>
Executive Director, AAAI <br>
<br>
for<br>
<br>
Vincent Conitzer and Fei Sha<br>
AAAI-20 Program Cochairs <br>
</i>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 13, 2020</h3>
<h1>Konstantin Visiting Francis Bach's Group at INRIA</h1>

<br>
<a href="https://konstmish.github.io/">Konstantin Mishchenko</a>
is visiting the <a href="https://www.di.ens.fr/sierra/">SIERRA machine learning lab</a> at INRIA, Paris,
led by <a href="https://www.di.ens.fr/%7Efbach/">Francis Bach</a>. He will give a talk on January 14
entitled "Adaptive Gradient Descent Without Descent" and based on
<a href="https://arxiv.org/abs/1910.09529">this paper.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>January 9, 2020</h3>
<h1>New Intern: Aleksandr Beznosikov</h1>
<br>
A new intern arrived today: Aleksandr Beznosikov. Aleksandr is a final-year BSc student in Applied Mathematics and Physics at <a href="https://mipt.ru/english/">Moscow Institute of Physics and Technology (MIPT).</a> <br>
<br>
Some notable achievements of Aleksandr so far:<br>
<br>
- paper <a href="https://arxiv.org/abs/1911.10645">"Derivative-Free Method For Decentralized Distributed Non-Smooth Optimization"</a>, joint work with Eduard Gorbunov and Alexander Gasnikov<br>
- Increased State Academic Scholarship for 4 year bachelor and master students at MIPT, 2018-2019 <br>
- Author of problems and organizer of the student olympiad in discrete mathematics, 2018-2019 <br>
- Abramov's Scholarship for students with the best grades at MIPT, 2017-2019<br>
- First Prize at MIPT's team mathematical tournament, 2017 <br>
- Silver Medal at International Experimental Physics Olympiad,
2015<br>
- Russian President’s Scholarship for High School Students, 2014-2015 <br>
- Prize-Winner, All-Russian School Physics Olympiad, Final Round, 2014 and 2015 <br>
- Winner, All-Russian School Programming Olympiad, Region Round, 2015-2016 <br>
- Winner, All-Russian School Physics Olympiad, Region Round, 2014-2016 <br>
- Winner, All-Russian School Maths Olympiad, Region Round, 2014-2016 <br>
<br>
Welcome!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>        
        
          <h3>January 7, 2020</h3>
          <h1>Four Papers Accepted to AISTATS 2020</h1>
          <br>
          Some of the first good news of 2020: We've had four papers
          accepted to <a href="https://www.aistats.org/">The 23rd
            International Conference on Artificial Intelligence and
            Statistics (AISTATS 2020)</a>, which will be held during
          June 3-5, 2020, in Palermo, Sicily, Italy. Here they are:<br>
          <br>
          1) <a href="https://arxiv.org/abs/1905.11261">"A unified
            theory of SGD: variance reduction, sampling, quantization
            and coordinate descent"</a> - joint work with <a
            href="https://eduardgorbunov.github.io/">Eduard Gorbunov</a>
          and <a href="https://fhanzely.github.io/index.html">Filip
            Hanzely.</a> <br>
          <br>
          Abstract: <i>In this paper we introduce a unified analysis of
            a large family of variants of proximal stochastic gradient
            descent (SGD) which so far have required different
            intuitions, convergence analyses, have different
            applications, and which have been developed separately in
            various communities. We show that our framework includes
            methods with and without the following tricks, and their
            combinations: variance reduction, importance sampling,
            mini-batch sampling, quantization, and coordinate
            sub-sampling. As a by-product, we obtain the first unified
            theory of SGD and randomized coordinate descent (RCD)
            methods, the first unified theory of variance reduced and
            non-variance-reduced SGD methods, and the first unified
            theory of quantized and non-quantized methods. A key to our
            approach is a parametric assumption on the iterates and
            stochastic gradients. In a single theorem we establish a
            linear convergence result under this assumption and
            strong-quasi convexity of the loss function. Whenever we
            recover an existing method as a special case, our theorem
            gives the best known complexity result. Our approach can be
            used to motivate the development of new useful methods, and
            offers pre-proved convergence guarantees. To illustrate the
            strength of our approach, we develop five new variants of
            SGD, and through numerical experiments demonstrate some of
            their properties. </i><br>
          <br>
          2) "Tighter theory for local SGD on identical and
          heterogeneous data" - joint work with <a
            href="https://rka97.github.io">Ahmed Khaled</a> and <a
            href="https://konstmish.github.io">Konstantin Mishchenko.</a>
          <br>
          <br>
          Abstract: <i>We provide a new analysis of local SGD, removing
            unnecessary assumptions and elaborating on the difference
            between two data regimes: identical and heterogeneous. In
            both cases, we improve the existing theory and provide
            values of the optimal stepsize and optimal number of local
            iterations. Our bounds are based on a new notion of variance
            that is specific to local SGD methods with different data.
            The tightness of our results is guaranteed by recovering
            known statements when we plug $H=1$, where $H$ is the number
            of local steps. The empirical evidence further validates the
            severe impact of data&nbsp; heterogeneity on the&nbsp;
            performance of local SGD. </i><br>
          <br>
          3) <a href="https://arxiv.org/abs/1905.11373">"Revisiting
            stochastic extragradient"</a> - joint work with <a
            href="https://konstmish.github.io">Konstantin Mishchenko</a>,
          <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
          Egor Shulgin and <a
            href="https://people.epfl.ch/yurii.malitskyi">Yura Malitsky</a>.
          <br>
          <br>
          Abstract: <i>We consider a new extension of the extragradient
            method that is motivated by approximating implicit updates.
            Since in a recent work of Chavdarova et al (2019) it was
            shown that the existing stochastic extragradient algorithm
            (called mirror-prox) of Juditsky et al (2011) diverges on a
            simple bilinear problem, we prove guarantees for solving
            variational inequality that are more general than in </i><i><i>Juditsky
et al (2011)</i>. Furthermore, we illustrate numerically
            that the proposed variant converges faster than many other
            methods on the example of</i><i><i> Chavdarova et al (2019)</i>.
            We also discuss how extragradient can be applied to training
            Generative Adversarial Networks (GANs). Our experiments on
            GANs demonstrate that the introduced approach may make the
            training faster in terms of data passes, while its higher
            iteration complexity makes the advantage smaller. To further
            accelerate method's convergence on problems such as bilinear
            minimax, we combine the extragradient step with negative
            momentum Gidel et al (2018) and discuss the optimal momentum
            value. </i><br>
          <br>
          4) <a href="https://arxiv.org/abs/1906.00506">"DAve-QN: A
            distributed averaged quasi-Newton method with local
            superlinear convergence rate"</a> - work of S. Soori, <a
            href="https://konstmish.github.io">K. Mishchenko</a>, A.
          Mokhtari, M. Dehnavi, and M. Gürbüzbalaban.<br>
          <br>
          Abstract: <i>In this paper, we consider distributed
            algorithms for solving the empirical risk minimization
            problem under the master/worker communication model. We
            develop a distributed asynchronous quasi-Newton algorithm
            that can achieve superlinear convergence. To our knowledge,
            this is the first distributed asynchronous algorithm with
            superlinear convergence guarantees. Our algorithm is
            communication-efficient in the sense that at every iteration
            the master node and workers communicate vectors of size
            O(p), where p is the dimension of the decision variable. The
            proposed method is based on a distributed asynchronous
            averaging scheme of decision vectors and gradients in a way
            to effectively capture the local Hessian information of the
            objective function. Our convergence theory supports
            asynchronous computations subject to both bounded delays and
            unbounded delays with a bounded time-average. Unlike in the
            majority of asynchronous optimization literature, we do not
            require choosing smaller stepsize when delays are huge. We
            provide numerical experiments that match our theoretical
            results and showcase significant improvement comparing to
            state-of-the-art distributed algorithms. </i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h3>January 7, 2020</h3>
          <h1>Visiting ESET in Bratislava</h1>
          <br>
          I am on a visit to <a href="https://www.eset.com/int/">ESET</a>
          - a leading internet security company headquartered in
          Bratislava, Slovakia. I have given a couple of talks on
          stochastic gradient descent and have spoken to several very
          interesting people. <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h3>January 5, 2020</h3>
          <h1>Filip Visiting Francis Bach's Group at INRIA</h1>
          <br>
          <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          is visiting the <a href="https://www.di.ens.fr/sierra/">SIERRA machine learning lab</a> at INRIA, Paris, led by Francis
          Bach. He will give a talk on January 7 entitled "One method to
          rule them all: variance reduction for data, parameters and
          many new methods", and <a
            href="https://arxiv.org/abs/1905.11266">based on a paper of
            the same title</a>. <a
href="https://fhanzely.github.io/wp-content/uploads/2020/01/jacsketch.pdf">Here are his slides.</a><br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>        

          <h3>January 5, 2020</h3>
          <h1>New Intern: Grigory Malinovsky</h1>
          <br>
          A new intern arrived today: Grigory Malinovsky from <a
            href="https://mipt.ru/english/">Moscow Institute of Physics
            and Technology (MIPT).</a> Grigory wrote his BS thesis
          "Averaged Heavy Ball Method" under the supervision of Boris
          Polyak. He is now pursuing an MS degree at MIPT in Machine
          Learning.<br>
          <br>
          Among Grigory's successes belong:<br>
          <br>
          - Abramov's scholarship for students with the best grades at
          MIPT, 2016<br>
          - Participant in the final round of All-Russian Physics
          Olympiad, 2014<br>
          - Bronze medal at International Zhautykov Olympiad in Physics,
          2014<br>
          - Prize winner in the final round of All-Russian Physics
          Olympiad, 2013<br>
          <br>
          Welcome!<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          

          <h3>December 20, 2019</h3>
                   <h1>New Paper</h1>
                   <br>
                   <span class="important">New paper out: </span><a
                     href="https://arxiv.org/abs/1912.09925">"Distributed fixed
                     point methods with compressed iterates"</a> - joint work
                   with Sélim Chraibi, Ahmed Khaled, Dmitry Kovalev, Adil Salim
                   and Martin Takáč.<br>
                   <br>
                   Abstract: <i>We propose basic and natural assumptions under
                     which iterative optimization methods with compressed
                     iterates can be analyzed. This problem is motivated by the
                     practice of federated learning, where a large model stored
                     in the cloud is compressed before it is sent to a mobile
                     device, which then proceeds with training based on local
                     data. We develop standard and variance reduced methods, and
                     establish communication complexity bounds. Our algorithms
                     are the first distributed methods with compressed iterates,
                     and the first fixed point methods with compressed iterates.</i><br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>



                   <h3>December 20, 2019</h3>
                   <h1>Paper Accepted to ICLR 2020</h1>
                   <br>
                   Our paper <a href="https://arxiv.org/abs/1905.13278">"A
                     stochastic derivative free optimization method with
                     momentum"</a>, joint work with Eduard Gorbunov, Adel Bibi,
                   Ozan Sezer and El Houcine Bergou, was accepted to <a
                     href="https://iclr.cc/Conferences/2020/">ICLR 2020</a>,
                   which will be held during April 26-30 in Addis Ababa,
                   Ethiopia. The accepted version of the paper is on <a
                     href="https://openreview.net/forum?id=HylAoJSKvH">OpenReview</a>.<br>
                   <br>
                   &nbsp;We have presented the paper last week at the <a
                     href="https://optrl2019.github.io">NeurIPS 2019 Optimization
                     Foundations of Reinforcement Learning Workshop</a>. You may
                   wish to have a look at our poster:<br>
                   <br>
                   <a href="posters/Poster-SMTP.pdf"><img
                       src="posters/Poster-SMTP_small.png" alt="SMTP" border="0"
                       width="371" height="500"></a><br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>




                   <h3>December 17, 2019</h3>
                   <h1>Back at KAUST</h1>
                   <br>
                   Having spent a very intensive week at NeurIPS in Vancouver, I
                   am now back at KAUST. <br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>



                   <h3>December 16, 2019</h3>
                   <h1>Konstantin @ EPFL</h1>
                   <br>
                   <a href="https://konstmish.github.io">Konstantin Mishchenko</a>
                   is on a short visit to the <a
                     href="https://www.epfl.ch/labs/lions/">Laboratory for
                     Information and Inference Systems (LIONS)</a> at EPFL, led
                   by <a href="https://people.epfl.ch/volkan.cevher">Volkan
                     Cevher</a>. He will give a talk entitled "Stochastic Newton
                   and Cubic Newton Methods with Simple Local Linear-Quadratic
                   Rates" on December 18. The talk is based on <a
                     href="https://arxiv.org/abs/1912.01597">this paper</a>,
                   joint work with <a href="https://www.dmitry-kovalev.com">Dmitry
               Kovalev</a> and myself. You may also want to check out the
                   associated poster which we presented at the <a
                     href="https://sites.google.com/site/optneurips19/">Beyond
                     First Order Methods in ML</a> Workshop at NeurIPS last week:
                   <br>
                   <br>
                   <a href="posters/Poster-SN.pdf"><img
                       src="posters/Poster-SN_small.png" alt="Stochastic Newton
                       Methods" border="0" width="500" height="374"></a><br>
                   <br>
                   Konstantin gave a nice spotlight talk on this topic at the
                   workshop, and his EPFL talk will be a longer version of the
                   NeurIPS talk.<br>
                   <br>
                   We believe this work is a breakthrough in the area of
                   stochastic second order methods. Why do we think so? A quick
                   explanation is given on the poster and in the paper... <br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>



                   <h3>December 14, 2019</h3>
                   <h1>Filip @ EPFL</h1>
                   <br>
                   As of today, <a href="https://fhanzely.github.io/index.html">Filip
               Hanzely</a> is on a week-long visit to the <a
                     href="https://www.epfl.ch/labs/mlo/">Machine Learning and
                     Optimization Lab</a> at EPFL, led by <a
                     href="https://people.epfl.ch/martin.jaggi">Martin Jaggi</a>.
                   He will give a talk based on the paper "One Method to Rule
                   Them All: Variance Reduction for Data, Parameters and Many New
                   Methods" on December 18. The talk is based on <a
                     href="https://arxiv.org/abs/1912.01597">this paper</a>,
                   joint work with me. To get a quick idea about the paper, check
                   out this poster: <br>
                   <br>
                   <a href="posters/Poster-GJS.pdf"><img
                       src="posters/Poster-GJS_small.png" alt="One Method to Rule
                       Them All" border="0" width="500" height="375"></a><br>
                   And here are <a
                     href="https://fhanzely.github.io/wp-content/uploads/2019/11/main.pdf">Filip's
               slides.</a><br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>



                   <h3>December 13, 2019</h3>
                   <h1>Dmitry and Elnur Received Their MS Degrees Today!
                   </h1>
                   <br>
                   Congratulations to <a href="https://www.dmitry-kovalev.com">Dmitry
               Kovalev</a> and <a href="http://elnurgasanov.me">Elnur
                     Gasanov</a> for being awarded their MS degrees! Both Dmitry
                   and Elnur will continue as PhD students in my group starting
                   January 2020. <br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>



                   <h3>December 10, 2019</h3>
                   <h1>Interview by Robin.ly</h1>
                   <br>
                   I was <a
               href="https://www.linkedin.com/pulse/neurips2019-robinly-interviews-prof-yoshua-bengio-song-laffan/">interviewed
               by Margaret Laffan from Robin.ly</a> today at NeurIPS 2019
                   for their <a href="https://www.robinly.info">Leaders in AI</a>
                   video interview series and podcast. This is what they are
                   doing, in their own words:&nbsp; <br>
                   <br>
                   <i>"With almost ten thousand registered participants, NeurIPS
                     is one of the largest and most prominent AI conferences in
                     the world. This week will see the top minds in AI research
                     and industry players all converge at the Vancouver
                     Convention Centre, Canada for what promises to be an
                     incredible week of learning, networking, and gaining deep
                     insights on the bridge between AI research and
                     commercialization as we head into the next decade. </i><i>Our Robin.ly team is in attendance interviewing the top
                     academics in the world including Prof. Yoshua Bengio
                     covering a range of hot topics in AI from deep learning,
                     climate change, edge computing, imitation learning,
                     healthcare and data privacy from United States, Europe,
                     China, Canada and Saudi Arabia.</i><i>"<br>
                     <br>
                     <a href="imgs/Robin.ly_LinkedIn.png"><img
                         src="imgs/Robin.ly_LinkedIn.png" alt="Robin.ly at
                         NeurIPS 2019" border="0" width="700" height="215"></a><br>
                   </i> <br>
                   My interview is not online yet, but I was told it would appear
                   within a couple weeks. I'll throw in a link once the material
                   is online.<br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>



                   <h3>December 7, 2019</h3>
                   <h1>NeurIPS 2019</h1>
                   <br>
                   I arrived to Vancouver to attend NeurIPS. My journey took 25
                   hours door-to-door. <br>
                   <br>
                   Update: The event was very good, but super-intensive and
                   exhausting. I've spent the whole week jet-lagged, with
                   personal breakfast time progressively shifting throughout the
                   conference from 2am to 7am.<br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>




               <h3>December 4, 2019</h3>
               <h1>Our Work to be Presented at NeurIPS 2019</h1>
               <br>
               We will present several papers at NeurIPS, in the main
               conference and at some of the workshops. Here they are, listed
               by date of presentation:<br>
               <br>
               <h2>Main Conference</h2>
               1) <a href="https://arxiv.org/abs/1905.10874">RSN: Randomized
               Subspace Newton</a><br>
               Robert Mansel Gower, Dmitry Kovalev, Felix Lieder and Peter
               Richtárik<br>
               <a
               href="https://nips.cc/Conferences/2019/Schedule?showEvent=13295">schedule</a><br>
               Dec 10<br>
               <br>
               <a href="posters/Poster-RSN.pdf"><img
               src="posters/Poster-RSN_small.png" alt="RSN" border="0"
               width="500" height="377"></a><br>
               <br>
               2) <a href="https://arxiv.org/abs/1905.11768">Stochastic
               proximal Langevin algorithm: potential splitting and
               nonasymptotic rates</a><br>
               Adil Salim, Dmitry Kovalevand Peter Richtárik<br>
               <a
               href="https://nips.cc/Conferences/2019/Schedule?showEvent=15749">schedule</a><br>
               Dec 10<br>
               <br>
               <a href="posters/Poster-SPLA.pdf"><img
               src="posters/Poster-SPLA_small.png" alt="SPLA" border="0"
               width="500" height="375"></a><br>
               <br>
               3) <a href="https://arxiv.org/abs/1906.04370">Maximum mean
               discrepancy gradient flow</a><br>
               Michael Arbel, Anna Korba, Adil Salim and Arthur Gretton<br>
               <a
               href="https://nips.cc/Conferences/2019/Schedule?showEvent=13759">schedule</a><br>
               Dec 10<br>
               <br>
               4) <a href="https://arxiv.org/abs/1904.09265">SSRGD: Simple
               stochastic recursive gradient descent for escaping saddle
               points</a><br>
               Zhize Li<br>
               <a
               href="https://nips.cc/Conferences/2019/Schedule?showEvent=13368">schedule<br>
               </a>Dec 10 <br>
               <br>
               5) <a href="https://arxiv.org/abs/1905.12412">A unified
               variance-reduced accelerated gradient method for convex
               optimization</a><br>
               Zhize Li and Guanghui Lan<br>
               <a
               href="https://nips.cc/Conferences/2019/Schedule?showEvent=14058">schedule</a><br>
               Dec 11<br>
               <br>
               <h2>Beyond First Order Methods in ML (Dec 13) </h2>
               6) <a href="https://arxiv.org/abs/1912.01597">Stochastic
               Newton and cubic Newton methods with simple local
               linear-quadratic rates</a><br>
               Dmitry Kovalev, Konstantin Mishchenko and Peter Richtárik<br>
               <br>
               <a href="posters/Poster-SN.pdf"><img
               src="posters/Poster-SN_small.png" alt="Stochastic Newton
               Methods" border="0" width="500" height="374"></a><br>
               <h2>Workshop on Federated Learning for Data Privacy and
               Confidentiality (Dec 13) </h2>
               7) <a href="https://arxiv.org/abs/1909.04716">Gradient
               descent with compressed iterates</a><br>
               Ahmed Khaled and Peter Richtárik<br>
               <br>
               <a href="posters/Poster-GDCI.pdf"><img
               src="posters/Poster-GDCI_small.png" alt="GDCI" border="0"
               width="368" height="500"></a><br>
               <br>
               8) <a href="https://arxiv.org/abs/1909.04746">Better
               communication complexity for local SGD</a><br>
               Ahmed Khaled, Konstantin Mishchenko and Peter Richtárik<br>
               <br>
               <a href="posters/Poster-LocalSGD.pdf"><img
               src="posters/Poster-LocalSGD_small.png" alt="Local SGD"
               border="0" width="373" height="500"></a><br>
               <br>
               9) <a href="https://arxiv.org/abs/1909.04715">First analysis
               of local GD on heterogeneous data</a><br>
               Ahmed Khaled, Konstantin Mishchenko and Peter Richtárik<br>
               <br>
               <h2>Optimal Transport &amp; Machine Learning (Dec 13)</h2>
               10) <a href="https://arxiv.org/abs/1909.06918">Sinkhorn
               algorithm as a special case of stochastic mirror descent</a><br>
               Konstantin Mishchenko<br>
               <br>
               <a href="posters/Poster-Sinkhorn.pdf"><img
               src="posters/Poster-Sinkhorn_small.png" alt="Sinhorn"
               border="0" width="500" height="375"></a><br>
               <h2>Optimization Foundations for Reinforcement Learning (Dec
               14) <br>
               </h2>
               11) <a href="https://arxiv.org/abs/1905.13278">A stochastic
               derivative free optimization method with momentum</a> <br>
               Eduard Gorbunov, Adel Bibi, Ozan Sener, El Houcine Bergou and
               Peter Richtárik<br>
               <br>
               <a href="posters/Poster-SMTP.pdf"><img
               src="posters/Poster-SMTP_small.png" alt="SMTP" border="0"
               width="371" height="500"></a><br>
               &nbsp;<br>
               12) <a href="https://arxiv.org/abs/1905.11373">Revisiting
               stochastic extragradient</a><br>
               Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter
               Richtárik, and Yura Malitsky<br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>




          <h3>December 3, 2019</h3>
              <h1>New Paper</h1>
              <br>

              <span class="important">New paper out: </span><a
              href="https://arxiv.org/abs/1912.01597">"Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic Rates"</a> -
              joint work with <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> and
              <a href="https://konstmish.github.io">Konstantin Mishchenko.</a><br>
              <br>

              Abstract: <i>We present two new remarkably simple stochastic
              second-order methods for minimizing the average of a very
              large number of sufficiently smooth and strongly convex
              functions. The first is a stochastic variant of Newton's
              method (SN), and the second is a stochastic variant of
              cubically regularized Newton's method (SCN). We establish
              local linear-quadratic convergence results. Unlike existing
              stochastic variants of second order methods, which require
              the evaluation of a large number of gradients and/or
              Hessians in each iteration to guarantee convergence, our
              methods do not have this shortcoming. For instance, the
              simplest variants of our methods in each iteration need to
              compute the gradient and Hessian of a single randomly
              selected function only. In contrast to most existing
              stochastic Newton and quasi-Newton methods, our approach
              guarantees local convergence faster than with first-order
              oracle and adapts to the problem's curvature. Interestingly,
              our method is not unbiased, so our theory provides new
              intuition for designing new stochastic methods.</i> <br> <br>

              This work was accepted as a spotlight paper to the <a
              href="https://sites.google.com/site/optneurips19/">NeurIPS
              2019 Workshop "Beyond First Order Methods in ML"</a>, taking
              place in Vancouver on December 13, 2019. <br>

              <br>
              <br>
              <img alt="" src="imgs/fancy-line.png" width="196" height="36">
              <br>
              <br>



                  <h3>November 24, 2019</h3>
                  <h1>Machine Learning Models Gather Momentum </h1>
                  <br>
                  <img src="imgs/stickman.jpeg" alt="Learning to walk"
                    width="400" height="194"><br>
                  <br>
                  Here is a link to a <a
              href="https://discovery.kaust.edu.sa/en/article/892/machine-learning-models-gather-momentum">popular
              article from the KAUST Discovery Magazine</a> on our recent
                  work related to learning continuous control tasks via
                  designing new randomized derivative-free optimization
                  algorithms. <br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>


                  <h3>November 24, 2019</h3>
                  <h1>"Loopless" Paper Accepted to ALT 2020 </h1>
                  <br>
                  Our paper <a href="https://arxiv.org/abs/1901.08689"> "Don't
                    Jump Through Hoops and Remove Those Loops: SVRG and Katyusha
                    are Better Without the Outer Loop"</a>, joint work with <a
                    href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>
                  and <a href="https://samuelhorvath.github.io/">Samuel
                    Horváth,</a> has been accepted to <a
                    href="http://alt2020.algorithmiclearningtheory.org">ALT 2020</a>.<br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>November 23, 2019</h3>
                  <h1>KAUST-Tsinghua-Industry Workshop on Advances in AI</h1>
                  <br>
                  I am co-organizing a <a
                    href="https://kaust.edu.sa/aiworkshop/index.html">workshop
                    on Advances in AI</a> taking taking place at KAUST over the
                  next few days (November 24-26). We are hosting esteemed guests
                  from <a href="https://www.tsinghua.edu.cn">Tsinghua
                    University</a> and from Industry. The program is
                  complemented by talks of local faculty (including yours
                  truly), poster presentations by students and postdocs, panel
                  discussions and other events. <br>
                  <br>
                  The workshop was kicked-off today with a welcome reception.
                  KAUST president <a
                    href="https://en.wikipedia.org/wiki/Tony_F._Chan">Tony Chan</a>
                  (an absolutely remarkable person; we are so lucky to have
                  him!) will launch the technical part tomorrow (Nov 24) morning
                  at 8:30am with a welcome address. Here are the talks scheduled
                  for tomorrow, in order:<br>
                  <br>
                  <b>Towards Third Generation Artificial Intelligence</b><br>
                  Bo Zhang (Keynote Speaker) Fellow, Chinese Academy of
                  Sciences, Tsinghua University<br>
                  <br>
                  <b>Stochastic Gradient Descent: General Analysis and Improved
                    Rates</b><br>
                  Peter Richtarik (Speaker) Professor of Computer Science, KAUST<br>
                  <br>
                  <b>Explainability, Robustness, and User satisfaction in
                    Personalized Recommendation</b><br>
                  Min Zhang (Speaker) Associate Professor, Computer Science,
                  Tsinghua University<br>
                  <br>
                  <b>Stable Learning: The Convergence of Causal Inference and
                    Machine Learning</b><br>
                  Peng Cui (Speaker) Associate Professor, Computer Science,
                  Tsinghua University<br>
                  <br>
                  <b>Internet architecture: past, present and future</b><br>
                  Jianping Wu (Keynote Speaker) Fellow, Chinese Academy of
                  Engineering, Tsinghua University<br>
                  <br>
                  <b>Compressed Communication in Distributed Deep Learning - A
                    Systems perspective</b><br>
                  Panos Kalnis (Speaker) Professor, Computer Science, KAUST<br>
                  <br>
                  <b>Scaling Distributed Machine Learning with In-Network
                    Aggregation</b><br>
                  Marco Canini (Speaker) Associate Professor, Computer Science,
                  KAUST<br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>November 16, 2019</h3>
                  <h1>Sebastian Stich is Visiting my Lab</h1>
                  <br>
                  <a href="https://sstich.ch">Sebastian Stich</a> is visiting my
                  lab starting tomorrow, and will be around until the end of the
                  month. He is a research scientist in the <a
                    href="https://mlo.epfl.ch">Machine Learning and Optimization
                    Laboratory</a> at EPFL, led by <a
                    href="https://www.epfl.ch/labs/mlo/">Martin Jaggi</a>.<br>
                  <br>
                  Sebastian visited my lab twice before, in Fall 2017 and Fall
                  2018. We wrote a paper on both occasions (<a
              href="https://papers.nips.cc/paper/7434-accelerated-stochastic-matrix-inversion-general-theory-and-speeding-up-bfgs-rules-for-faster-second-order-optimization">first
              paper</a>, <a href="https://arxiv.org/abs/1904.05115">second paper</a>) and I am almost certain we will come up with
                  something again. <br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>November 11, 2019</h3>
                  <h1>Filip is Visiting Yurii Nesterov in Louvain</h1>
                  <br>
                  <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
                  is visiting Yurii Nesterov and his team at the <a
                    href="https://uclouvain.be/fr/node/4474">Center for
                    Operations Research and Econometrics</a>, UCLouvain,
                  Louvain-la-Neuve (I've spent two nice years as a postdoc there
                  during right after my PhD). He will give a <a
              href="https://uclouvain.be/en/research-institutes/lidam/core/events/operation-research-seminar-filip-hazely.html">seminar talk</a> on November 12, entitled "One Method to Rule Them
                  All: Variance for Data, Parameters and Many New Methods". The
                  talk is based on <a href="https://arxiv.org/abs/1905.11266">this   paper.</a><br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>November 11, 2019</h3>
                  <h1>Paper Accepted to AAAI-20</h1>
                  <br>
                  Our paper <a href="https://arxiv.org/abs/1902.01272">"A
                    stochastic derivative-free optimization method with
                    importance sampling"</a>, joint work with Adel Bibi, El
                  Houcine Bergou, Ozan Sener, and Bernard Ghanem, got accepted
                  to <a href="https://aaai.org/Conferences/AAAI-20/">AAAI-20</a>.<br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>November 10, 2019</h3>
                  <h1>Samuel is Back from his Internship at Amazon</h1>
                  <br>
                  Since June 20, <a href="https://samuelhorvath.github.io">Samuel Horváth</a> has worked as a research intern at Amazon,
                  Berlin, focusing on hyper-parameter optimization for deep
                  learning. The internship is over, and Samuel has now returned
                  to KAUST. Welcome back!<br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>November 1, 2019</h3>
                  <h1>Laurent Condat Joined as a Research Scientist</h1>
                  <br>
                  <a
                    href="https://www.gipsa-lab.grenoble-inp.fr/%7Elaurent.condat/">Dr Laurent Condat</a> just joined our team as a Research
                  Scientist. Welcome!<br>
                  <br>
                  Prior to joining KAUST, Laurent was a research scientist at
                  CNRS in Grenoble, France. Laurent got his PhD in 2006 in
                  Applied Mathematics from Grenoble Institute of Technology. He
                  then spent 2 years as a postdoc in Helmholtz Zentrum München,
                  Germany, after which he moved back to France. He has been with
                  CNRS since 2008, first in Caen, and for the past 7 years in
                  Grenoble. <br>
                  <br>
                  Dr Condat is an expert in image and signal processing, inverse
                  problems and optimization. His most well-known work in
                  optimization is his highly influential paper <a
                    href="https://link.springer.com/article/10.1007/s10957-012-0245-9">"A Primal–Dual Splitting Method for Convex Optimization
                    Involving Lipschitzian, Proximable and Linear Composite
                    Terms"</a> in which he develops what has since been known as
                  the <a href="http://fa.bianp.net/blog/2018/tos/">Condat-Vu</a>
                  algorithm. <br>
                  <br>
                  Laurent will give a <a
              href="https://cemse.kaust.edu.sa/vcc/events/event/cs-398-graduate-seminar">CS Graduate Research Seminar</a> on Dec 2.<br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>November 1, 2019</h3>
                  <h1>Dmitry Grishchenko Visiting</h1>
                  <br>
                  <a href="https://grishchenko.org">Dmitry Grishchenko</a> (PhD
                  student of Applied Mathematics at Université Grenoble Alpes)
                  is visiting me until the end of the year. He will be working
                  on distributed optimization.<br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>October 25, 2019</h3>
                  <h1>Faculty Sponsor of KAUST ACM Student Chapter </h1>
                  <br>
                  I have agreed to help the KAUST ACM Student Chapter by
                  becoming their faculty sponsor. In order to not forget what my
                  duties are, I am including a link here with a <a
                    href="https://www.acm.org/chapters/responsibilities-of-chapter-officers">list of responsibilities of all chapter officers.</a> <br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>October 25, 2019</h3>
                  <h1> New in ML @ NeurIPS 2019 </h1>
                  <br>
                  I am helping with the <a
                    href="https://nehzux.github.io/NewInML2019/">New in ML
                    initiative</a>, an event co-located with NeurIPS 2019 taking
                  place on December 9, 2019. <br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>October 24, 2019</h3>
                  <h1>Senior PC Member for IJCAI-PRICAI 2020 </h1>
                  <br>
                  I have recently accepted an invite to serve as a Senior PC
                  member for <a href="https://www.ijcai20.org">IJCAI-PRICAI
                    2020.</a> The conference will be held during July 11-17,
                  2020 in Yokohama, Japan.<br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>October 22, 2019</h3>
                  <h1> Konstantin Giving a Talk in Boris Polyak's Lab </h1>
                  <br>
                  Today, <a href="https://konstmish.github.io">Konstantin
                    Mishchenko</a> is giving a talk at the <a
                    href="http://lab7.ipu.ru/eng/people/polyak.html">V.A.
                    Trapeznikov Institute of Control Sciences of the Russian
                    Academy of Sciences</a>. Topic: <a
                    href="https://arxiv.org/abs/1909.06918">Sinkhorn Algorithm
                    as a Special Case of Stochastic Mirror Descent.</a> This is
                  Konstantin's first single-authored paper.<br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>October 18, 2019</h3>
                  <h1> Eduard Visiting Paris </h1>
                  <br>
                  <a
                    href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard Gorbunov</a> is on a visit to the <a
                    href="https://www.di.ens.fr/sierra/index.php">SIERRA Machine
                    Learning Laboratory</a> in Paris led by Francis Bach. He
                  gave a talk today based on the paper "A unified theory of SGD:
                  variance reduction, sampling, quantization and coordinate
                  descent", joint work with Filip Hanzely and myself. Here are
                  Eduard's slides:<br>
                  <br>
                  <a href="talks/TALK-unified_sgd_inria2019.pdf"><img
                      src="imgs/sigma_k-thumbnail.png" alt="Slides" border="0"
                      width="650" height="489"></a><br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>October 16, 2019</h3>
                  <h1> New Intern Arrived: Hoang H. Nguyen </h1>
                  <br>
                  Hoàng Huy Nguyễn arrived to KAUST today to start a research
                  internship in my group. He will stay until the end of the
                  year. Welcome! <br>
                  <br>
                  Hoang is studying Computer Science at the <a
                    href="https://www.minerva.kgi.edu/">Minerva Schools at KGI</a>,
                  headquartered in San Francisco. Minerva is a bold innovative
                  higher education project. If you have not heard of it, <a
                    href="https://en.wikipedia.org/wiki/Minerva_Schools_at_KGI">read about it here.</a> During 2012-2015, Hoang specialized in
                  mathematics at the <a
                    href="https://en.wikipedia.org/wiki/High_School_for_the_Gifted">High School for the Gifted</a> associted with the Vietnam
                  National University, Ho Chi Minh City. <br>
                  <br>
                  At around this time, Hoang was active in various mathematical
                  contests. In 2015, he was awarded a <a
                    href="https://www.imo-official.org/participant_r.aspx?id=26060">Silver Medal at the 56th International Mathematical Olympiad</a>
                  held in Chiang Mai, Thailand. In 2016, Hoang spent 2 weeks at
                  KAUST training 30 Saudi national team students for the Balkan
                  Mathematical Olympiad and the International Mathematical
                  Olympiad 2016. When Hoang is bored, he co-authors mathematics
                  olympiad orientation books for gifted high school students.
                  Yes, he wrote Định hướng bồi dưỡng học sinh năng
                  khiếu toán: a 400 pages long book about advanced topics in
                  mathematics olympiad. In 2016, he finished top 20 in the
                  ACM/ICPC Pacific Northwest Region Programming Contest
                  (Division 1). <br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>October 15, 2019</h3>
                  <h1>Less Chat Leads to More Work for Machine Learning</h1>
                  <br>
                  The latest issue of the KAUST Discovery Magazine features an
                  article on our recent work on distributed training of machine
                  learning models. <a
              href="https://discovery.kaust.edu.sa/en/article/880/less-chat-leads-to-more-work-for-machine-learning">Here it is.</a><br>
                  <br>
                  The basis for the piece are three papers, written in
                  collaboration with several people:<br>
                  <br>
                  [1] Horváth, S., Kovalev, D., Mishchenko, K., Richtárik, P.
                  &amp; Stich, S. U., <b>Stochastic distributed learning with
                    gradient quantization and variance reduction</b>, <a
                    href="https://arxiv.org/abs/1904.05115">arXiv:1904.05115</a>
                  (2019)<br>
                  <br>
                  [2] Mishchenko, K., Hanzely, F. &amp; Richtárik, P., <b>99%
                    of distributed optimization is a waste of time: the issue
                    and how to fix it</b>, <a
                    href="https://arxiv.org/abs/1901.09437">arxiv.org/abs/1901.09437</a>
                  (2019)<br>
                  <br>
                  [3] Mishchenko, K., Gorbunov, E., Takáč, M. &amp; Richtárik,
                  P., <b>Distributed learning with compressed gradient
                    differences</b>, <a href="https://arxiv.org/abs/1901.09269">arxiv.org/abs/1901.09269</a>
                  (2019)<br>
                  <br>
                  In these papers we develop and analyze the first variance
                  reduction techniques for the variance introduced by gradient
                  compression used in distributed optimization. As a result,
                  these are the first methods that converge to the true solution
                  (model). We show theoretically and experimentally that one can
                  safely perform very aggresive gradient compression, which
                  dramatically reduces communication cost, without any increase
                  in the number of communications needed to perform successful
                  training when compared to methods that do not perform any
                  compression. With more parallel machines available, more
                  dramatic compression can be applied, and this leads to better
                  overall performance. These methods will therefore be
                  especially useful for federated learning tasks as in this
                  regime the number of parallel workers (mobile devices) is
                  huge.<br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>


                  <h3>October 11, 2019</h3>
                  <h1> Konstantin Mishchenko Giving 2 Talks in Moscow </h1>
                  <br>
                  <a href="https://konstmish.github.io">Konstantin</a> is on a
                  research visit to Moscow. As a part of the visit, he will give
                  two talks at Moscow Institute of Physics and Technology. <br>
                  <br>
                  The first talk is today and is based on this paper: <br>
                  <br>
                  [1] Konstantin Mishchenko and Peter Richtárik<br>
                  A stochastic decoupling method for minimizing the sum of
                  smooth and non-smooth functions<br>
                  <a href="https://arxiv.org/abs/1905.11535">arXiv:1905.11535,
                    2019</a><br>
                  <br>
                  Here is the announcement: <br>
                  <br>
                  <img src="imgs/2019-10-11-Kostya-MIPT-talk.png" alt="First
                    talk" width="606" height="765"><br>
                  <br>
                  The second talk will be held on October 15; Kostya will speak
                  about some new breakthroughs we have made very recently on
                  Local GD/SGD. The talk is based on these two recent papers:<br>
                  <br>
                  [2] Ahmed Khaled, Konstantin Mishchenko and Peter Richtárik<br>
                  First analysis of local GD on heterogeneous data<br>
                  <a href="https://arxiv.org/abs/1909.04715">arXiv:1909.04715</a>,
                  2019 <br>
                  <br>
                  [3] Ahmed Khaled, Konstantin Mishchenko and Peter Richtárik<br>
                  Better communication complexity for local SGD<br>
                  <a href="https://arxiv.org/abs/1909.04746">arXiv:1909.04746</a>,
                  2019<br>
                  <br>
                  Here is the announcement for the second talk:<br>
                  <br>
                  <img src="imgs/2019-10-15-Kostya-MIPT-talk.png" alt="First
                    talk" width="606" height="771"><br>
                  <br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>














          <h3>October 10, 2019</h3>
          <h1> Video Talks </h1>
          <br>
          I've just went through the youtubes of the world and assembled
          a list of all talks I have given which have been recorded.
          Here they are:<br>
          <br>
          <p><a href="https://www.youtube.com/watch?v=a05S0kL5u30">A Guided Walk Through the ZOO of Stochastic Gradient Descent Methods</a> (5 hrs)<br>MIPT, Moscow, Russia, 9/2019<i><br>This mini-course is based on my <a href="https://iccopt2019.berlin/index.php?page=summerschool">ICCOPT 2019 Summer School lectures,</a> with a slight update. I am also teaching a graduate course at KAUST where this material is part of the syllabus.</i><i><br></i></p>
          <p><a href="https://www.youtube.com/watch?v=n1ELUphtbgg">Variance Reduction for Gradient Compression</a> (38 mins)<br>Rutgers University, 9/2019<br></p>
          <p><a href="https://www.youtube.com/watch?v=gjgEck0zU7w">Stochastic Quasi-Gradient Methods: Variance Reduction via Jacobian Sketching</a> (33 mins)<br>Simons Institute, Berkeley, 9/2018<br></p>
          <p><a href="https://www.youtube.com/watch?v=DLbZJnz04cE">Empirical Risk Minimization: Complexity, Duality, Sampling, Sparsity and Big Data</a> (85 mins)<br>Yandex, Russia, 12/2017<br></p>
          <p><a href="https://www.youtube.com/watch?v=iZc2eFqS2l4">Stochastic Primal-Dual Hybrid Gradient Algorithm with Arbitrary Sampling</a> (1 hr)<br>MIPT, Moscow, Russia, 10/2017 <br></p>
          <p>Introduction to Randomized Optimization <a href="https://comm.medias.polytechnique.fr/videos/richtarik_1_randonopti/">1</a> <a href="https://comm.medias.polytechnique.fr/videos/richtarik_2_randonoptimization/">2</a> <a href="https://comm.medias.polytechnique.fr/videos/richtarik_3_randonopti_08033/">3</a> <a href="https://comm.medias.polytechnique.fr/videos/richtarik_4_randonoptimization/">4</a> <a href="https://comm.medias.polytechnique.fr/videos/richtarik_5_randonoptimization/">5</a> (5 hrs)<br>Ecole Polytechnique, France, 8/2017<br></p>
          <p><a href="https://www.youtube.com/watch?v=RbkhWrTbrKs">Stochastic Dual Ascent for Solving Linear Systems</a> (31 mins)<br>The Alan Turing Institute, London, UK, 10/2016<br></p>
          <p><a href="https://www.youtube.com/watch?v=BS0kF4YijGc">Introduction to Big Data Optimization</a> (55 mins)<br>Portsmouth, UK, 9/2016<br></p>
          <p><a href="https://www.youtube.com/watch?v=0sHOfqhCZw0">Accelerated, Parallel and Proximal Coordinate Descent</a> (90 mins)<br>Moscow, Russia, 2/2014<br></p>
          <p><a href="https://www.youtube.com/watch?v=IQgnstB0n2E">Parallel Coordinate Descent Methods for Big Data Optimization</a> (55 mins)<br>Simons Institute, Berkeley, 10/2013<br></p>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>





          <h3>October 10, 2019</h3>
          <h1> Filip is Back at KAUST </h1>
          <br>
          <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          is now back at KAUST after having spent several months as a
          research intern at Google in New York. Welcome back! During
          his internship Filip was working on optimization for deep
          learning under the supervision of <a
          href="https://scholar.google.com/citations?user=70lgwYwAAAAJ&amp;hl=en">Sashank J. Reddi.</a><br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


<h3>October 7, 2019</h3>
<h1> I am Back at KAUST </h1>
<br>
After having spent a couple weeks traveling (Bielefeld,
Dolgoprudny, Nizhny Novgorod, Moscow) and giving talks (I have
given about 12 hours of lectures in total), I am now back at
KAUST. I've met and talked to many awesome people during my
travels, thank you all!<br>
<br>
I am teaching tomorrow morning, then meeting some students and
postdocs, and also meeting KAUST leaders (President, VP for
Research, Dean) to talk about further steps in pushing the new
AI Initiative at KAUST further. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>October 4, 2019</h3>
<h1> NeurIPS 2019 Workshop Papers</h1>
<br>
We have had several papers accepted to various NeurIPS 2019
workshops. Here they are:<br>
<br>
<h2><a href="http://federated-learning.org/fl-neurips-2019/">Workshop on Federated Learning for Data Privacy and Confidentiality</a></h2>
1. <a href="https://arxiv.org/abs/1909.04716">Gradient
Descent with Compressed Iterates</a>, by <a
href="https://rka97.github.io">Ahmed Khaled</a> and me<br>
<br>
2. <a href="https://arxiv.org/abs/1909.04746">Better
Communication Complexity for Local SGD</a>, by <a
href="https://rka97.github.io">Ahmed Khaled</a>, <a
href="https://konstmish.github.io">Konstantin Mishchenko</a>
and me<br>
<br>
3. <a href="https://arxiv.org/abs/1909.04715">First Analysis
of Local GD on Heterogeneous Data</a>, by <a
href="https://rka97.github.io">Ahmed Khaled</a>, <a
href="https://konstmish.github.io">Konstantin Mishchenko</a>
and me<br>
<br>
<h2><a href="https://sites.google.com/site/optneurips19/">Beyond First Order Methods in ML</a></h2>
4. Stochastic Newton and Cubic Newton Methods with Simple
Local Linear-Quadratic Rates, by <a
href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a
href="https://konstmish.github.io">Konstantin Mishchenko</a>
and me<br>
<br>
5. <a href="https://arxiv.org/abs/1802.09022">An Accelerated
Method for Derivative-Free Smooth Stochastic Convex
Optimization</a>, by <a
href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>,
<a href="https://www.wias-berlin.de/people/dvureche/?lang=1">Pavel Dvurechensky</a> and <a
href="http://www.mathnet.ru/eng/person27590">Alexander
Gasnikov</a><br>
<br>
<h2> <a
href="https://optrl2019.github.io/accepted_papers.html">Optimization Foundations for Reinforcement Learning</a></h2>
6. <a href="https://arxiv.org/abs/1905.13278">A Stochastic
Derivative Free Optimization Method with Momentum</a>, by <a
href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>,
<a href="http://www.adelbibi.com">Adel Bibi</a>, <a
href="http://ozansener.net">Ozan Sener</a>, <a
href="https://ehbergou.github.io">El Houcine Bergou</a> and
me<br>
<br>
<h2><a href="https://sites.google.com/view/otml2019/">Optimal
Transport &amp; Machine Learning</a></h2>
7. <a href="https://arxiv.org/abs/1909.06918">Sinkhorn
Algorithm as a Special Case of Stochastic Mirror Descent</a>,
by <a href="https://konstmish.github.io">Konstantin
Mishchenko</a><br>
<br>
<h2><a href="https://sgo-workshop.github.io">Smooth Games
Optimization and Machine Learning Workshop: Bridging Game
Theory and Deep Learning</a></h2>
8. <a href="https://arxiv.org/abs/1905.11373">Revisiting
Stochastic Extragradient</a>, by <a
href="https://konstmish.github.io">Konstantin Mishchenko</a>,
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a
href="https://scholar.google.ru/citations?user=XlmSx18AAAAJ&amp;hl=en">Egor Shulgin</a>, me, and <a
href="https://www.researchgate.net/profile/Yura_Malitsky">Yura Malitsky</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>October 3, 2019</h3>
<h1> Best Paper Award at VMV 2019</h1>
<br>
The paper <a href="https://arxiv.org/abs/1909.00145">"Stochastic convolutional sparse coding"</a>, joint work with <a
href="https://vccimaging.org/People/xiongj/">Jinhui Xiong</a>
and <a href="https://vccimaging.org/People/heidriw/">Wolfgang
Heidrich</a>, has won the 2019 Vision, Modeling and
Visualization (VMV) Best Paper Award.<br>
<br>
Update (December 2, 2019): KAUST wrote a <a
href="https://cemse.kaust.edu.sa/vcc/news/jinhui-xiong-wins-best-paper-award-vmv-2019">short article</a> about this. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 1, 2019</h3>
<h1> New Postdoc: Mher Safaryan</h1>
<br>
<a href="https://mher-safaryan.github.io">Mher Safaryan</a>
joined my group today as a postdoc. He got his PhD in 2018 in
Mathematics at Yerevan State University, Armenia, under the
supervision of <a
href="http://math.sci.am/user/grigori-karagulyan">Grigori
Karagulyan</a>. During his PhD, Mher worked on several
problems in harmonic analysis and algebra. Mher and me have
recently written a paper <a
href="http://arxiv.org/abs/1905.12938">on stochastic sign
descent methods.</a><br>
<br>
Welcome! <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 29, 2019</h3>
<h1> Nizhny Novgorod</h1>
<br>
I just took a 1hr flight from Moscow to Nizhny Novgorod. I
will stay here until October 3 and deliver four lectures:
three lectures on Oct 1 at <a
href="https://nnov.hse.ru/en/latna/conferences/ada2019">"Approximation and Data Analysis"</a> (an event organized by Moscow State
University, Higher School of Economics and Russian Academy of
Sciences), and one lecture on Oct 2 at <a
href="http://www.huawei.com/ru">Huawei.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>September 4, 2019</h3>
<h1>Three Papers Accepted to NeurIPS 2019</h1>
<br>
The long-awaited decisions just came. We've had three papers
accepted; I was involved with the first two of them. The third
is a collaboration of <a href="https://adil-salim.github.io">Adil Salim</a> with people from Gatsby:<br>
<br>
<a href="https://arxiv.org/abs/1905.10874">"RSN: Randomized
Subspace Newton"</a> - joint work with <a
href="https://gowerrobert.github.io/">Robert Mansel Gower</a>,
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>
and <a
href="http://www.opt.uni-duesseldorf.de/%7Elieder/de/inhalt.php">Felix Lieder.</a><br>
<br>
Abstract:<i> We develop a randomized Newton method capable of
solving learning problems with huge dimensional feature
spaces, which is a common setting in applications such as
medical imaging, genomics and seismology. Our method
leverages randomized sketching in a new way, by finding the
Newton direction constrained to the space spanned by a
random sketch. We develop a simple global linear convergence
theory that holds for practically all sketching techniques,
which gives the practitioners the freedom to design custom
sketching approaches suitable for particular applications.
We perform numerical experiments which demonstrate the
efficiency of our method as compared to accelerated gradient
descent and the full Newton method. Our method can be seen
as a refinement and randomized extension of the results of
Karimireddy, Stich, and Jaggi (2019).</i><br>
<br>
<a href="https://arxiv.org/abs/1905.11768">"Stochastic
proximal Langevin algorithm: potential splitting and
nonasymptotic rates"</a> - joint work with <a
href="https://adil-salim.github.io">Adil Salim</a> and <a
href="https://www.dmitry-kovalev.com">Dmitry Kovalev.</a><br>
<br>
Abstract:<i> We propose a new algorithm---Stochastic Proximal
Langevin Algorithm (SPLA)---for sampling from a log concave
distribution. Our method is a generalization of the Langevin
algorithm to potentials expressed as the sum of one
stochastic smooth term and multiple stochastic nonsmooth
terms. In each iteration, our splitting technique only
requires access to a stochastic gradient of the smooth term
and a stochastic proximal operator for each of the nonsmooth
terms. We establish nonasymptotic sublinear and linear
convergence rates under convexity and strong convexity of
the smooth term, respectively, expressed in terms of the KL
divergence and Wasserstein distance. We illustrate the
efficiency of our sampling technique through numerical
simulations on a Bayesian learning task.</i> <br>
<br>
<a href="https://adil-salim.github.io/Research/mmd19.pdf">"Maximum mean discrepancy gradient flow"</a> - work of <a
href="https://michaelarbel.github.io">Michael Arbel</a>, <a
href="https://akorba.github.io">Anna Korba</a>, <a
href="https://adil-salim.github.io">Adil Salim</a> and <a
href="http://www.gatsby.ucl.ac.uk/%7Egretton/">Arthur
Gretton.</a><br>
<br>
Abstract:<i> We construct a Wasserstein gradient flow of the
maximum mean discrepancy (MMD) and study its convergence
properties. The MMD is an integral probability metric
defined for a reproducing kernel Hilbert space (RKHS), and
serves as a metric on probability measures for a
sufficiently rich RKHS. We obtain conditions for convergence
of the gradient flow towards a global optimum, that can be
related to particle transport when optimizing neural
networks. We also propose a way to regularize this MMD flow,
based on an injection of noise in the gradient. This
algorithmic fix comes with theoretical and empirical
evidence. The practical implementation of the flow is
straightforward, since both the MMD and its gradient have
simple closed-form expressions, which can be easily
estimated with samples.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 4, 2019</h3>
<h1>Best NeurIPS 2019 Reviewer Award </h1>
<br>
I have received the following email: "Thank you for all your
hard work reviewing for NeurIPS 2019! We are delighted to
inform you that you were one of the 400 highest-scoring
reviewers this year! You will therefore be given access (for a
limited period of time) to one free registration to this
year’s conference; you will later receive additional
information by email explaining how to access your
registration." Thanks NeurIPS!<br>
<br>
<i>Update:</i> <a href="https://konstmish.github.io">Konstantin Mishchenko</a> also got this award. Congrats!<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 3, 2019</h3>
<h1>New Postdoc: Zhize Li
</h1>
<br>
<a href="https://zhizeli.github.io">Zhize Li</a> joined my
group today as a postdoc. He got his PhD in Computer Science
from Tsinghua University in July 2019, and is interested in
"theoretical computer science and machine learning, in
particular (non-)convex optimization algorithms, machine
learning, algorithms and data structures". His PhD thesis
"Simple and Fast Optimization Methods for Machine Learning"
won the <span class="important">2019 Tsinghua Outstanding
Doctoral Dissertation Award.</span> <br>
Zhize has written 12 papers, including publications in venues
such as NeurIPS, ICLR, COLT, IJCAI, SAGT, DCC and SPIRE. <br>
<br>
Welcome!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>September 26, 2019</h3>
<h1> Speeding up the Machine Learning Process </h1>
<br>
The KAUST Discovery Magazine has written a <a
href="https://discovery.kaust.edu.sa/en/article/879/speeding-up-the-machine-learning-process%C2%A0">popular article on our recent work published in ICML 2019.</a> Here
are links to the papers:<br>
<br>
<a href="http://proceedings.mlr.press/v97/qian19b">SGD:
General Analysis and Improved Rates</a><br>
<a href="http://proceedings.mlr.press/v97/horvath19a.html">Nonconvex Variance Reduced Optimization with Arbitrary Sampling</a> <br>
<a href="SAGA%20with%20Arbitrary%20Sampling">SAGA with
Arbitrary Sampling</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 26, 2019</h3>
<h1> Visiting MIPT </h1>
<br>
After a couple days in Germany, I am now traveling to Moscow
to visit Moscow Institute of Physics and Technology (MIPT).
Indeed, I am writing this onboard a flight from Frankfurt to
Domodedovo. <a
href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard</a>
will pick me up at the airport. Thanks Eduard!<br>
<br>
Alexander Gasnikov put together a nice workshop around my
visit, with excellent speakers: V. Spokoiny, E. Tyrtyshnikov,
A. Nazin, P. Dvurechensky, and A. Tremba. The workshop will
start at 2pm on the 27th of September and will take place
somewhere at MIPT. I do not know where yet - as there is no
website for the event and I was not yet informed - but I am
sure I will learn about this before the workshop starts ;-)<br>
<br>
The day after, on September 28th, I will deliver a series of
lectures for MIPT students entitled "A Guided Walk Through the
ZOO of Stochastic Gradient Descent Methods". This mini-course
is aimed to serve as the best introduction to the topic of
SGD, and is largely based on research originating from my
group at KAUST. We will start at 10:45am and finish at 6pm.
And yes, there will be breaks.<br>
<br>
<br>
<i>Update (Sept 29):</i> My visit of MIPT is over, today I
will fly to Nizhny Novgorod. <a
href="https://youtu.be/a05S0kL5u30">My mini-course was
recorded and should appear on YouTube at some point.</a>
There might have been an issue with voice recording towards
the end though... <br>
<br>
<i>Update (October 8):</i> <a
href="https://youtu.be/a05S0kL5u30">The course is up on
YouTube now!</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 24, 2019</h3>
<h1> Bielefeld </h1>
<br>
I am on my way to Bielefeld to give a talk at a <a
href="https://www.uni-bielefeld.de/mathematik/numerik/workshop50.html">numerical analysis workshop</a> associated with the celebrations of 50
years of mathematics at Bielefeld. I normally do not have a
chance to hang out with numerical PDE people; but I am glad I
did. It was a fun event. Moreover, my rather esoteric talk
(relative to the workshop theme) on stochastic Newton and
gradient methods was met with surprising enthusiasm. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 15, 2019</h3>
<h1> KAUST Professor Wins Distinguished Speaker Award </h1>
<br>
KAUST wrote a <a
href="https://www.kaust.edu.sa/en/news/kaust-professor-peter-richt%C3%A1rik-wins-distinguished-speaker-award">short article about me</a>... <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 15, 2019</h3>
<h1>Heading to DIMACS </h1>
<br>
I am on my way to Rutgers, to attend the <a
href="http://dimacs.rutgers.edu/events/details?eID=316">DIMACS Workshop on Randomized Numerical Linear Algebra, Statistics,
and Optimization,</a> which is to take place at the <a
href="http://dimacs.rutgers.edu">Center for Discrete
Mathematics and Computer Science (DIMACS)</a> during
September 16-18, 2019. <br>
<br>
My talk, entitled "Variance Reduction for Gradient
Compression", is on Monday afternoon. The talk will be
recorded and put on <a
href="https://www.youtube.com/watch?v=n1ELUphtbgg">YouTube.</a>
<br>
<br>
Abstract: <i>Over the past few years, various randomized
gradient compression (e.g., quantization, sparsification,
sketching) techniques have been proposed for reducing
communication in distributed training of very large machine
learning models. However, despite high level of research
activity in this area, surprisingly little is known about
how such compression techniques should properly interact
with first order optimization algorithms. For instance,
randomized compression increases the variance of the
stochastic gradient estimator, and this has an adverse
effect on convergence speed. While a number of
variance-reduction techniques exists for taming the variance
of stochastic gradients arising from sub-sampling in
finite-sum optimization problems, no variance reduction
techniques exist for taming the variance introduced by
gradient compression. Further, gradient compression
techniques are invariably applied to unconstrained problems,
and it is not known whether and how they could be applied to
solve constrained or proximal problems. In this talk I will
give positive resolutions to both of these problems. In
particular, I will show how one can design fast
variance-reduced proximal stochastic gradient descent
methods in settings where stochasticity comes from gradient
compression. </i><br>
<br>
This talk is based on:<br>
<br>
<i><a
href="https://papers.nips.cc/paper/7478-sega-variance-reduction-via-gradient-sketching">[1]</a>
Filip Hanzely, Konstantin Mishchenko and Peter Richtárik.
SEGA: Variance reduction via gradient sketching, NeurIPS
2018</i><i><br>
</i><i><br>
</i><i><a href="https://arxiv.org/abs/1901.09269">[2]</a>
Konstantin Mishchenko, Eduard Gorbunov, Martin Takáč and
Peter Richtárik. Distributed learning with compressed
gradient differences, arXiv:1901.09269</i><i>, 2019<br>
</i><i><br>
</i><i><a href="https://arxiv.org/abs/1901.09437">[3]</a>
Konstantin Mishchenko, Filip Hanzely and Peter Richtárik.
99% of distributed optimization is a waste of time: the
issue and how to fix it, arXiv:1901.09437</i><i>, 2019<br>
</i><i><br>
</i><i><a href="https://arxiv.org/abs/1904.05115">[4]</a>
Samuel Horváth, Dmitry Kovalev, Konstantin Mishchenko, Peter
Richtárik and Sebastian Stich. Stochastic distributed
learning with gradient quantization and variance reduction,
arXiv:1904.05115, 2019</i><br>
<br>
<br>
Update (September 16, 2019): I have given my talk today, <a
href="talks/TALK-Variance-Reduction-for-Gradient-Compression.pdf">here are the slides.</a><br>
<br>
Update (October 4, 2019): <a
href="https://www.youtube.com/watch?v=n1ELUphtbgg">My talk
is on YouTube now.</a>&nbsp; Here is a <a
href="https://www.youtube.com/watch?v=dvw-LnqfhLg&amp;list=PLKVCRT3MRed4yWkXj3CLKadDdClCvhQu1">playlist of all the other talks</a> from the event.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 12, 2019</h3>
<h1>Sciencetown Podcast<br>
</h1>
<br>
<img src="imgs/Sciencetown.png" alt="Sciencetown" width="300"
height="300"><br>
<br>
Today, I spent about an hour in <a
href="https://www.linkedin.com/in/nicholasdemille/">Nicholas
Demille's</a> podcast studio. We have chatted about machine
learning, life and and my work for about an hour. The material
will be used for the next episode of the <a
href="https://browse.entale.co/show/b1481445-70e4-47aa-b9d4-2f9d1d8fe439">Sciencetown podcast</a> Nicholas is preparing.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 12, 2019</h3>
<h1>Nicolas Loizou's PhD Thesis </h1>
<br>
Here is a <a href="papers/2019_PhD_Thesis_Nicolas_Loizou.pdf">copy of Nicolas' PhD thesis entitled "Randomized Iterative
Methods for Linear Systems: Momentum,<br>
Inexactness and Gossip"</a>. <a
href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas</a>
defended in June, and has just arrived to Montréal to take up
a postdoctoral position at <a href="https://mila.quebec/en/">MILA</a>.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 10, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1909.04746">"Better
communication complexity for local SGD"</a> - joint work
with Ahmed Khaled and <a href="https://konstmish.github.io">Konstantin Mishchenko.</a><br>
<br>
Abstract: <i>We revisit the local Stochastic Gradient Descent
(local SGD) method and prove new convergence rates. We close
the gap in the theory by showing that it works under
unbounded gradients and extend its convergence to weakly
convex functions. Furthermore, by changing the assumptions,
we manage to get new bounds that explain in what regimes
local SGD is faster that its non-local version. For
instance, if the objective is strongly convex, we show that,
up to constants, it is sufficient to synchronize $M$ times
in total, where $M$ is the number of nodes. This improves
upon the known requirement of Stich (2018) of $\sqrt{TM}$
synchronization times in total, where $T$ is the total
number of iterations, which helps to explain the empirical
success of local SGD.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 10, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1909.04716">"Gradient descent
with compressed iterates"</a> - joint work with Ahmed
Khaled.<br>
<br>
Abstract: <i>We propose and analyze a new type of stochastic
first order method: gradient descent with compressed
iterates (GDCI). GDCI in each iteration first compresses the
current iterate using a lossy randomized compression
technique, and subsequently takes a gradient step. This
method is a distillation of a key ingredient in the current
practice of federated learning, where a model needs to be
compressed by a mobile device before it is sent back to a
server for aggregation. Our analysis provides a step towards
closing the gap between the theory and practice of federated
learning, and opens the possibility for many extensions.</i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 10, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1909.04715">"First analysis of
local GD on heterogeneous data"</a> - joint work with Ahmed
Khaled and <a href="https://konstmish.github.io">Konstantin
Mishchenko.</a><br>
<br>
Abstract: <i>We provide the first convergence analysis of
local gradient descent for minimizing the average of smooth
and convex but otherwise arbitrary functions. Problems of
this form and local gradient descent as a solution method
are of importance in federated learning, where each function
is based on private data stored by a user on a mobile
device, and the data of different users can be arbitrarily
heterogeneous. We show that in a low accuracy regime, the
method has the same communication complexity as gradient
descent.</i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 7, 2019</h3>
<h1>New Visitor: Xiuxian Li</h1>
<br>
<a
href="https://scholar.google.com.hk/citations?user=dQSGQSAAAAAJ&amp;hl=zh-CN">Xiuxian Li</a> (Nanyang Technological University, Singapore) is
visiting me at KAUST for a week. He is giving a <a
href="https://cemse.kaust.edu.sa/events/event/distributed-algorithms-computing-common-fixed-point-group-nonexpansive-operators">CS
seminar talk on Monday at noon</a> entitled "Distributed
Algorithms for Computing a Common Fixed Point of a Group of
Nonexpansive Operators".<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 29, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="papers/SCSC-VMV2019.pdf">"Stochastic convolutional
sparse coding"</a> - joint work with <a
href="https://jhxiong.github.io">Jinhui Xiong</a> and <a
href="http://vccimaging.org/People/heidriw">Wolfgang
Heidrich</a>. <br>
<br>
Abstract: <i>State-of-the-art methods for Convolutional
Sparse Coding usually employ Fourier-domain solvers in order
to speed up the convolution operators. However, this
approach is not without shortcomings. For example,
Fourier-domain representations implicitly assume circular
boundary conditions and make it hard to fully exploit the
sparsity of the problem as well as the small spatial support
of the filters.</i><i> In this work, we propose a novel
stochastic spatial-domain solver, in which a randomized
subsampling strategy is introduced during the learning of
sparse codes. Afterwards, we extend the proposed strategy in
conjunction with online learning, scaling the CSC model up
to very large sample sizes. In both cases, we show
experimentally that the proposed subsampling strategy, with
a reasonable selection of the subsampling rate, outperforms
the state-of-the-art frequency-domain solvers in terms of
execution time without losing in learning quality. Finally,
we evaluate the effectiveness of the over-complete
dictionary learned from large-scale datasets, which
demonstrates an improved sparse representation of the
natural images on account of more abundant learned image
features.</i><br>
<br>
The paper was accepted to and will appear in the <a
href="https://www.vmv2019.uni-rostock.de">International
Symposium on Vision, Modeling, and Visualization 2019 (VMV
2019).</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 25, 2019</h3>
<h1>Fall 2019 Semester Started</h1>
<br>
The Fall 2019 semester has just started and I am again
teaching CS 390FF: Special Topics in Data Sciences (Big Data
Optimization). I have redesigned some key portions of the
course based on some fresh and hot research from 2018 and
2019. You can sign up for the course via <a
href="https://piazza.com/kaust.edu.sa/fall2019/cs390ff/home">Piazza.</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 22, 2019</h3>
<h1>New MS/PhD Student: Alyazeed Basyoni</h1>
<br>
<a href="https://alyazeedbasyoni.wixsite.com/blog">Alyazeed
Basyoni</a> just arrived at KAUST to start his MS/PhD
studies under my supervision. Welcome!!!<br>
<br>
In 2019, Alyazeed obtained his BS in Computer Science from
Carnegie Mellon University. Desiting to learn more, Alyazeed
ended up taking many graduate level courses, inlcuding courses
in Probability Theory, Deep Reinforcement Learning, Convex
Optimization, Machine Learning, Randomized Algorithms,
Probabilistic Combinatorics, and Measure and Integration.<br>
<br>
Alyazeed already has varied industrial experience: <br>
- At Ansatz, he implemented a fast, low cost, futures
execution engine (it was deployed)<br>
- At Dropbox, he implemented a tool that allows clients to
search, preview, select and embed content from third-party
providers into Paper. <br>
- At Petuum, he contributed to the open source Dynamic Neural
Network package, DyNet.<br>
<br>
When Alyazeed is bored, he writes OS kernels (in C, from
scratch), helps the USA mathematics olympiad team by grading
mock exams and delivering short lectures, programs <a
href="https://www.youtube.com/watch?v=sQ9w7H2rY2Y">games</a>,
and fools around with C, Python, SML, OCaml, and Go.<br>
<br>
Alyazeed has a <a
href="https://www.imo-official.org/team_r.aspx?code=SAU&amp;year=2012">Silver Medal from the 53rd International Mathematics Olympiad</a>
(held in Mar del Plata, Argentina in 2012), where he
represented Saudi Arabia. By the way, at the same Olympiad, my
student <a
href="https://www.imo-official.org/team_r.aspx?code=KAZ&amp;year=2012">Alibek Sailanbayev got a Bronze Medal</a>. What a coincidence!
Alyazeed was the first Saudi to win a Silver medal at IMO.<br>
<br>
At KAUST, you will find Alyazeed in Building 1, Level 2.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 22, 2019</h3>
<h1>New MS/PhD Student: Slavomír Hanzely</h1>
<br>
<a href="https://slavomirhanzely.wordpress.com">Slavomír
Hanzely</a> just arrived at KAUST to start his MS/PhD
studies under my supervision. Welcome!!!<br>
<br>
In 2019, Slavomír ("Slavo") obtained his BS degree in Computer
Science from Comenius University, Slovakia. This, by the way,
is also where I studied back in the day. Slavo was eager to
learn faster than the study program required, and ended up
taking many more courses than necessary - all without
sacrificing his grades. <br>
<br>
Throughout his high schools and university studies, Slavo has
been active in various mathematical and computer science
olympiads and competitions, at regional, national and
international level. Here are some highlights from his
achievements:<br>
- 2017, 8-10th Place in Vojtech Jarník International
Mathematical Competition (1st place among Czech and Slovak
contestants)<br>
- 2016, represented Slovakia at the 57th International
Mathematical Olympiad (held in Hong Kong)<br>
- 2016, 3rd Place at the Slovak National Mathematical Olympiad<br>
- 2016, 1st Place at Slovak Mathematical Olympiad, Regional
Round <br>
- 2016, 1st Place at Slovak Informatics Olympiad, Regional
Round<br>
- 2015, Bronze Medal, Middle European Mathematical Olympiad<br>
- 2015, 2nd Place at Slovak Informatics Olympiad, Regional
Round<br>
- 2014, 1st Place at Slovak Mathematical Olympiad, Regional
Round <br>
- 2013, 1st Place at Slovak Mathematical Olympiad, Regional
Round <br>
<br>
Slavo has been active with marking solutions for the Slovak
National Mathematical Olympiad, preparing the Slovak team for
the International Mathematical Olympiad, marking solutions of
various correspondence contests in mathematics and computer
science, and organizing summer camps for highly talented
Slovak pupils in mathematics and computer science.<br>
<br>
At KAUST, you will find Slavo in Building 1, Level 2.<br>
<br>
Disambiguation: Slavo's older brother <a
href="https://fhanzely.github.io/index.html">Filip</a> is
also at KAUST, studying towards his PhD in my group.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 21, 2019</h3>
<h1>2 Interviews in 1 Day</h1>
<br>
I have been interviewed twice today. First by <a
href="https://dmurphyfreelancer.wixsite.com/dmurphyfreelancer">David Murphy</a> for a KAUST article related to the "Distinguished
Speaker Award" I received at <a
href="https://iccopt2019.berlin">ICCOPT</a> earlier this
month, and then by <a
href="https://www.slovenskezahranicie.sk/sk/udalost/2955/rozhlasova-publicistka-lubica-hargasova-ocenena-zlatym-biatecom-hospodarskeho-klubu-za-projekt--nasi-a-svetovi-v-rtvs">Ľubica Hargašová</a> (who was kind enough to travel to meet me) for
her <a href="https://www.rtvs.sk">RTVS</a> (Slovak Radio and
Television) radio show <a
href="https://slovensko.rtvs.sk/relacie/nasi-a-svetovi">"Naši a Svetoví"</a> ("Ours and of the World") about Slovaks who
found success abroad. The former interview will lead to a
written piece (in English), while the latter interview was
recorded and should air at some point in September (in
Slovak).<br>
<br>
[By the way - I was officially on vacation today...]<br>
<br>
<i>Update (September 7, 2019):</i> A (short compilation from)
the interview aired today at Radio Slovensko. The recording
can be listened to <a
href="https://slovensko.rtvs.sk/relacie/nasi-a-svetovi/204797/nasi-a-svetovi-peter-richtarik">online.
</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 11, 2019</h3>
<h1>2 Postdoc Positions </h1>
<br>
I have two postdoc positions open in the area of optimization
and/or machine learning, to be filled by January 2020. If
interested, send me an email! Include your CV and explain why
you are interested. <br>
<br>
Application deadline: no deadline; positions will be open
until filled<br>
<br>
Position start: By January 2020<br>
<br>
Duration: 1 to 3 years (based on agreement)<br>
<br>
Conditions: Very competitive salary and benefits; Travel
funding and access to state-of-the-art facilities; On-campus
accommodation. The KAUST campus is home of around 7,000
people, and comprises a land area of 36 km2. Includes
restaurants, schools, shops, cinema, two private beaches,
recreation centers, supermarket, medical center, etc.<br>
<br>
Application process: Send an email to me (peter dot richtarik
at kaust dot edu dot sa), explain why you are interested in
the position, and enclose your CV. If your CV catches my
attention, I may ask for reference letters and extra
materials. Alternatively, you may instruct your letter writers
to send letters to me (by email) right away. Shortlisted
candidates will progress to a Skype interview. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 8, 2019</h3>
<h1>My Group @ ICCOPT</h1>
<br>
Many members of my (combined KAUST-Edinburgh-MIPT) group
attended ICCOPT. Here is info on their talks plus links to the
underlying papers and slides (if available):<br>
<br>
<ul>
<li><a href="https://adil-salim.github.io">Adil Salim</a></li>
<li>paper <a href="https://arxiv.org/abs/1905.11768">"Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates"</a></li>
</ul>
<ul>
<li><a href="https://qianxunk.github.io">Xun Qian</a></li>
<li>paper <a href="http://proceedings.mlr.press/v97/qian19b.html">"SGD: General Analysis and Improved Rates"</a>, ICML 2019</li>
<li>[<a href="talks/TALK-SGD-AS.pdf">slides</a>]</li>
</ul>
<ul>
<li><a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas Loizou</a><br></li>
<li>paper <a href="http://proceedings.mlr.press/v97/assran19a.html">"Stochastic Gradient Push for Distributed Deep Learning"</a>, ICML 2019</li>
<li>[<a href="talks/TALK-SGPush-ICCOPT2019-Nicolas.pdf">slides</a>]<br></li>
</ul>
<ul>
<li><a href="https://konstmish.github.io">Konstantin Mishchenko</a><br></li>
<li>paper <a href="https://arxiv.org/abs/1905.11535">"A Stochastic Decoupling Method for Minimizing the Sum of Smooth and Non-smooth Functions"</a></li>
<li>[<a href="talks/TALK-SDM-ICCOPT2019-Konstantin.pdf">slides</a>]<br></li>
<li><br></li>
<li><a href="https://samuelhorvath.github.io">Samuel Horváth</a></li>
<li>paper <a href="https://arxiv.org/abs/1904.05115">"Stochastic Distributed Learning with Gradient Quantization and Variance Reduction"</a></li>
</ul>
<ul>
<li><a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a><br></li>
<li>paper <a href="https://arxiv.org/abs/1905.11373">"Revisiting Stochastic Extragradient"</a></li>
<li>[<a href="https://www.dmitry-kovalev.com/posts/iccopt_2019/iccopt.pdf">slides</a>]<br></li>
<li><br></li>
<li><a href="https://elnurgasanov.tilda.ws">Elnur Gasanov</a></li>
<li><br></li>
<li>Dmitry Kamzolov</li>
<li>talk "Composite High-Order Method for Convex Optimization"</li>
<li><br></li>
<li>Egor Shulgin (attended the Summer School and the conference)</li>
<li><br></li>
<li>Igor Sokolov (attended the Summer School and the conference)</li>
</ul>
<ul>
<li><a href="https://richtarik.org">Peter Richtárik</a></li>
<li>paper <a href="https://papers.nips.cc/paper/7478-sega-variance-reduction-via-gradient-sketching">"SEGA: Variance Reduction via Gradient Sketching"</a>, NeurIPS 2018</li>
<li>[<a href="talks/TALK-SEGA-ICCOPT2019.pdf">slides</a>]</li>
<li><br></li>
</ul>
<br>
Several former members of my KAUST and Edinburgh groups
attended as well:<br>
<br>
<ul>
<li><a href="https://ehbergou.github.io">El Houcine Bergou</a> <br></li>
<li>paper <a href="https://arxiv.org/abs/1902.03591">"Stochastic Three Points Method for Unconstrained Smooth Minimization"</a></li>
</ul>
<ul>
<li><a href="https://www.aritradutta.com">Aritra Dutta</a><br></li>
<li>paper <a href="https://arxiv.org/abs/1707.00133">"Weighted Singular Value Thresholding and its Application to Background Estimation"</a></li>
</ul>
<ul>
<li><a href="http://mtakac.com">Martin Takáč</a><br></li>
<li>paper <a href="https://arxiv.org/abs/1901.09997">"Quasi-Newton Methods for Deep Learning: Forget the Past, Just Sample"</a></li>
<li><br></li>
<li><a href="https://gowerrobert.github.io">Robert Gower</a> <br></li>
<li>talk "Expected Smoothness is the Key to Understanding the Mini-batch Complexity of Stochastic Gradient Methods"</li>
<li>[<a href="https://arxiv.org/abs/1805.02632">paper 1</a> (JacSketch)] [<a href="http://proceedings.mlr.press/v97/qian19b.html">paper 2</a> (SGD)]  [<a href="https://gowerrobert.github.io/pdf/papers/free_SVRG.pdf">paper 3</a> (SVRG)]</li>
<li>[<a href="https://gowerrobert.github.io/pdf/talks/expected_smoothness.pdf">slides</a>]</li>
</ul>
<ul>
<li><a href="https://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a></li>
<li>talk: "Adaptive Primal-Dual Coordinate Descent Methods for Non-smooth Composite Minimization with Linear Operator"<br></li>
<li><br></li>
<li><a href="http://www.math.canterbury.ac.nz/%7Er.tappenden/">Rachael Tappenden</a></li>
<li>talk <a href="https://arxiv.org/abs/1710.03695">"Underestimate Sequences via Quadratic Averaging"</a><br></li>
<li><br></li>
<li><a href="https://researcher.watson.ibm.com/person/ie-jakub.marecek">Jakub Mareček</a></li>
<li>talk: "Time-varying Non-convex Optimisation: Three Case Studies"<br></li>
</ul>
<br>
It's 18 people in total (and I am not counting
students/postdocs of my former students)! We had a distinct
presence, and most importantly, had fun at the event!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 3, 2019</h3>
<h1>ICCOPT Summer School Slides </h1>
<br>
My ICCOPT summer school course slides are here: <br>
<br>
<a href="talks/ZOO_course_ICCOPT2019.pdf"><img src="imgs/ZOO_course_cover.png" alt="slides" border="0"
width="700" height="494"></a><br>
<br>
Here are supplementary (flashy Powerpoint) slides about <a
href="talks/ZOO_course_ICCOPT2019-SGD-SR.pdf">SGD-SR</a> and <a href="talks/ZOO_course_ICCOPT2019-SEGA.pdf">SEGA</a>.
<br>
<br>
I was pleasantly surprised to have received a "distinguished
speaker" award:<br>
<br>
<img src="imgs/golden_bear.png" alt="Bear" width="700"
height="474"><br>
<br>
The bear probably represents the speed with which I delivered
the lectures... ;-)<br>
<br>
Update (September 15, 2019): KAUST wrote a <a
href="https://www.kaust.edu.sa/en/news/kaust-professor-peter-richt%C3%A1rik-wins-distinguished-speaker-award">short article about this, and other things...
</a>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>July 30, 2019</h3>
<h1>On my way to Berlin for ICCOPT </h1>
<br>
I am on my way to Berlin to first teach in the <a
href="https://iccopt2019.berlin/index.php?page=summerschool">ICCOPT Summer School</a>, and then to attend the <a
href="https://iccopt2019.berlin/index.php">ICCOPT conference</a>.
On August 3rd I will deliver a 1 day (4 x 1.5 hours) short
course entitled "A Guided Walk Through the ZOO of Stochastic
Gradient Descent Methods". Here is what the course is going to
be about:<br>
<br>
<i>Stochastic gradient descent (SGD) in one of its many
variants is the workhorse method for training modern
supervised machine learning models. However, the world of
SGD methods is vast and expanding, which makes it hard to
understand its landscape and inhabitants. In this tutorial I
will offer a guided walk through the ZOO of SGD methods. I
will chart the landscape of this beautiful world, and make
it easier to understand its inhabitants and their
properties. In particular, I will introduce a unified
analysis of a large family of variants of proximal
stochastic gradient descent (SGD) which so far have required
different intuitions, convergence analyses, have different
applications, and which have been developed separately in
various communities. This framework includes methods with
and without the following tricks, and their combinations:
variance reduction, data sampling, coordinate sampling,
importance sampling, mini-batching and quantization. As a
by-product, the presented framework offers the first unified
theory of SGD and randomized coordinate descent (RCD)
methods, the first unified theory of variance reduced and
non-variance-reduced SGD methods, and the first unified
theory of quantized and non-quantized methods.</i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 25, 2019</h3>
<h1>NeurIPS Reviews Arrived
</h1>
<br>
NeurIPS reviews came in. As usual, most reviewers assigned to
evaluate my papers are not quite at home in my area, or simply
provide an educated guess only. This leads to many rather
meaningless and noisy reviews (this is in sharp contrast with
journal submissions in top journals where more often than not
the reviewers are knowledgeable). This is something that took
me some time to get used to back in the day... The reason for
this? A trade-off between the quality of the reviews and the
speed of the accept/reject decision. Thanks to the few
reviewers who actually understood our results and were able to
provide useful feedback! Now we have until July 31 to prepare
author response, aka a "rebuttal". <br>
<br>
An interesting innovation this year: a system was put in place
to automatically flag some papers with a common subset of
authors as potentially being a "dual submission". A dual
submission is essentially a single set of results presented as
two (usually slightly) different papers, which is a trick
aimed to increase chances of acceptance. When incentives are
high, people are inventive... Some of my work got flagged this
way, and incorrectly so. The problem I can see right away is
that some reviewers, already busy with many reviews and other
tasks, apparently consider this as a convenient excuse to
spend less time reviewing and simply taking the flag at face
value, which allows them to simply <i>claim</i> dual
submission without providing any supporting evidence. Do we
really want AI to do reviews for us as well? No, we do not!
This is a big danger to the serious researchers in the
community; and it is not at all clear to me whether this issue
was considered before the system was launched. Do the benefits
outweigh the costs? People like me who would never think of a
dual submission will be on the losing side. This would not
have to happen if the reviewers took their job seriously and
evaluated the papers properly. But perhaps this new system
will eliminate some of the genuine dual submissions - and I
have seen some in the past. What's worse, we are now forced to
compare the two papers flagged as potentially dual submission
in the rebuttal. This on its own is a great idea - but not
delivered correctly because no extra space is given to write
the author response. We already have just a single page to
respond, which I never found to be enough. Now, there is even
less space to respond to the actual review comments - which
almost by definition will lead to such papers to be rejected.
After all, the reviewer will not get a response to all
criticism, and will interpret this in the obvious way. To sum
this up: I am not happy with this new system, and the
community should not be either.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>July 19, 2019</h3>
<h1>Konstantin @ Stanford<br>
</h1>
<br>
<a href="https://konstmish.github.io">Konstantin</a> is
visiting <a href="https://web.stanford.edu/%7Eboyd/">Stephen
Boyd</a> at Stanford.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>July 15, 2019</h3>
<h1>Konstantin @ Frontiers of Deep Learning
</h1>
<br>
<a href="https://konstmish.github.io">Konstantin</a> is
attending the Simons Institute (Berkeley) workshop <a
href="https://simons.berkeley.edu/workshops/dl2019-1">Frontiers of Deep Learning.</a> The schedule and videos of the talks
will become <a
href="https://simons.berkeley.edu/workshops/schedule/10627">available here.</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>July 14, 2019</h3>
<h1>ICIAM 2019 - Valencia, Spain
</h1>
<br>
I am attending <a href="https://iciam2019.org">ICIAM 2019</a>,
the largest scientific meeting of industrial and applied
mathematicians taking place once every four years. I am giving
a 30 min talk on Wednesday in an invited session on
optimization (11am-1pm). I will be leaving Valencia on
Saturday.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>July 13, 2019</h3>
<h1>Accelerating the Grapevine Effect
</h1>
<br>
My recent work with <a
href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
Loizou</a> on randomized gossip algorithms is featured in
the KAUST Discovery magazine. You can <a
href="https://discovery.kaust.edu.sa/en/article/841/accelerating-the-grapevine-effect">read the article online here.</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 11, 2019</h3>
<h1>Martin Takáč Giving a Talk in Bratislava
</h1>
<br>
Today, my former PhD student <a href="http://www.mtakac.com">Martin Takáč</a> (and now an Assistant Professor at Lehigh
University, USA) is giving a popular science talk in
Bratislava, Slovakia. The talk is entitled: "Current trends in
big data and artificial intelligence". I understand the talk
will be delivered in Slovak language.<br>
<br>
<img src="imgs/Plagat_VvC_jul.png" alt="Plagat" width="700"
height="989"><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 9, 2019</h3>
<h1>Filip @ Google<br>
</h1>
<br>
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
started a research internship at Google, New York. He will be
back at KAUST in early October.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 6, 2019</h3>
<h1>Nature Index: KAUST #52 Globally and #4 in Western Asia
</h1>
<br>
The <a
href="https://www.nature.com/articles/d41586-019-01919-8">2019
Nature index rankings</a> were published. Here is what
Nature says about its new "fractional count" rankings, "Our
measure, fractional count (FC), is based on the share of
articles published in 82 prestigious scientific journals,
selected by an independent panel of scientists and tracked by
the Nature Index database." <a
href="https://www.nature.com/articles/d41586-019-01919-8">The full story can be found here.</a><br>
<br>
In the western Asia region, among academic institutions, and
in the "nature &amp; science" area, KAUST was ranked #4. Here
is a <a
href="https://www.natureindex.com/institution-outputs/generate/Nature%20&amp;%20Science/regions-Western%20Asia/academic/score">list of the top 20 institutions</a>:<br>
<br>
01.&nbsp;&nbsp;&nbsp; Weizmann Institute of Science (WIS)<br>
02.&nbsp;&nbsp;&nbsp; Technion-Israel Institute of Technology
(IIT)<br>
03.&nbsp;&nbsp;&nbsp; Tel Aviv University (TAU)<br>
04.&nbsp;&nbsp;&nbsp; <font color="#ff9900">King Abdullah
University of Science and Technology (KAUST)</font><br>
05.&nbsp;&nbsp;&nbsp; Hebrew University of Jerusalem (HUJI)<br>
06.&nbsp;&nbsp;&nbsp; New York University Abu Dhabi (NYUAD)<br>
07.&nbsp;&nbsp;&nbsp; Sharif University of Technology (SUT)<br>
08.&nbsp;&nbsp;&nbsp; Ben-Gurion University of the Negev (BGU)<br>
09.&nbsp;&nbsp;&nbsp; Bar-Ilan University (BIU)<br>
10.&nbsp;&nbsp;&nbsp; King Saud University (KSU)<br>
11.&nbsp;&nbsp;&nbsp; Istanbul University<br>
12.&nbsp;&nbsp;&nbsp; The University of Jordan<br>
13.&nbsp;&nbsp;&nbsp; E. A. Buketov Karaganda State University
(KSU)<br>
14.&nbsp;&nbsp;&nbsp; University of Haifa (HU)<br>
15.&nbsp;&nbsp;&nbsp; Nazarbayev University (NU)<br>
16.&nbsp;&nbsp;&nbsp; S. Toraighyrov Pavlodar State University
(PSU)<br>
17.&nbsp;&nbsp;&nbsp; University of Tehran (UT)<br>
18.&nbsp;&nbsp;&nbsp; Middle East Technical University (METU)<br>
19.&nbsp;&nbsp;&nbsp; A. A. Baitursynov Kostanay State
University<br>
20.&nbsp;&nbsp;&nbsp; Koç University (KU)<br>
<br>
Globally, also among academic institutions, KAUST ranked #52
in the area <a
href="https://www.natureindex.com/institution-outputs/generate/Nature%20&amp;%20Science/global/academic/n_article">"nature &amp; science" (article count)</a><br>
and #79 in the area <a
href="https://www.natureindex.com/institution-outputs/generate/Physical%20Sciences/global/academic/score">"physical sciences" (fractional count).</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>July 3, 2019</h3>
<h1> 2019 Shanghai Rankings </h1>
<br>
In the 2019 Shanghai rankings, KAUST was <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/computer-science-engineering.html">ranked 101-150 in Computer Science and Engineering</a>. This is
quite some achievement for a university that did not yet exist
10 years ago, and one that currently has about 150 faculty
only! We are still growing, and plan to reach full capacity in
about 5 years. <br>
<br>
Here are notable rankings in some other fields: <br>
<br>
25. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/energy-science-engineering.html">Energy Science &amp; Engineering</a><br>
32. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/nanoscience-nanotechnology.html">Nanoscience &amp; Nanotechnology </a><br>
33. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/materials-science-engineering.html">Materials Science &amp; Engineering</a><br>
33. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/mechanical-engineering.html">Mechanical Engineering</a><br>
38. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/chemical-engineering.html">Chemical Engineering</a><br>
50. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/telecommunication-engineering.html">Telecommunication  Engineering</a><br>
51-75. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/chemistry.html">Chemistry</a><br>
51-75. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/water-resources.html">Water Resources</a><br>
101-150. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/computer-science-engineering.html">Computer Science &amp; Engineering</a><br>
101-150. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/environmental-science-engineering.html">Environmental Science &amp; Engineering</a><br>
201-300. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/earth-sciences.html">Earth Sciences</a><br>
301-400. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/mathematics.html">Mathematics</a><br>
301-400. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/electrical-electronic-engineering.html">Electrical &amp; Electronic Engineering</a><br>
<br>
Overall, <a
href="http://www.shanghairanking.com/ARWU2018.html">KAUST is
ranked 201-300 globally.</a> Four years ago, when KAUST was
6 years old, our ranking was 301-400. Five years ago, KAUST
was ranked 401-500. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 1, 2019</h3>
<h1>Promotion to Full Professor
</h1>
<br>
I have been promoted to full professor. <br>
<br>
What does this mean? Some people thought about this quite a
bit [<a href="http://rtalbert.org/full-professor/">1</a>, <a
href="https://kardiagroup.com/changes-at-promotion-to-full-professor/">2</a>,
<a
href="https://www.chronicle.com/article/You-re-a-Full-Professor-Now/245403">3</a>].

In  my case, the most immediate and obvious changes are: <br>
<br>
i) I now have a 5 year rolling contract at KAUST. That means
that each year my contract gets automatically extended by one
year (until it does not - which I do not expect will happen -
at which point I will have 5 years to find another job).<br>
<br>
ii) My KAUST baseline research funding will increase (I do not
yet know by how much; but I expect a roughly 40-50% increase).
This means I can either <a
href="https://richtarik.org/i_join.html">grow</a> the <a
href="https://richtarik.org/i_team.html">group</a>, or do
more with the current group. In any case, this is an excellent
boost which will have a positive effect one way or another.<br>
<br>
iii) My salary will increase.<br>
<br>
I will reflect on this in more depth at some point in the
future. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 30, 2019</h3>
<h1>Samuel @ Amazon Internship</h1>
<br>
<a href="https://samuelhorvath.github.io/">Samuel</a> has
started his research internship in Machine Learning Science
group at Amazon, Berlin, Germany.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>June 28, 2019</h3>
<h1> Nicolas Loizou: Thesis Defense</h1>
<br>
<a
href="https://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Nicolas  Loizou</a> successfully defended his PhD thesis "Randomized
iterative methods for linear systems: momentum, inexactness
and gossip" today. Congratulations!!! Nicolas is the last
student graduating from my Edinburgh group. He will join <a
href="https://mila.quebec/">MILA</a>, Montréal, in the Fall.
<br>
<br>
<i>Update (September 12, 2019):</i> Here is his <a
href="papers/2019_PhD_Thesis_Nicolas_Loizou.pdf">PhD thesis.</a>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>June 24, 2019</h3>
<h1>Dmitry, Adil and Elnur @ DS3 2019</h1>
<br>
<a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>,
<a href="https://adil-salim.github.io/">Adil Salim</a> and <a
href="https://elnurgasanov.tilda.ws/">Elnur Gasanov</a> are
attending the <a
href="https://www.ds3-datascience-polytechnique.fr/">Data
Science Summer School (DS3) at École Polytechnique, Paris,
France</a>. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 23, 2019</h3>
<h1>Paper Accepted to JMLR
</h1>
<br>
The paper <a href="https://arxiv.org/abs/1811.12403">"New
convergence aspects of stochastic gradient algorithms"</a>,
joint work with <a
href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-lamnguyen.mltd">Lam M. Nguyen</a>, <a
href="https://scl.uconn.edu/people/ha/info.php">Phuong Ha
Nguyen</a>, <a href="https://coral.ise.lehigh.edu/katyas/">Katya Scheinberg</a>, <a href="http://mtakac.com">Martin Takáč</a>
and <a href="http://www.ee.uconn.edu/marten-van-dijk/">Marten
van Dijk</a>, was accepted to <a
href="http://www.jmlr.org/">JMLR</a>. <br>
<br>
Update: <a href="http://jmlr.org/papers/v20/18-759.html">The
paper appeared on the JMLR website.</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>June 20, 2019</h3>
<h1>Paper Accepted to SIAM Journal on Optimization </h1>
<br>
The paper <a href="https://arxiv.org/abs/1801.04873">"Randomized projection methods for convex feasibility problems:
conditioning and convergence rates"</a>, joint work with <a
href="http://acse.pub.ro/person/ion-necoara/">Ion Necoara</a>
and <a href="https://dblp.org/pers/hd/p/Patrascu:Andrei">Andrei Patrascu</a>, was accepted to <a
href="https://www.siam.org/Publications/Journals/SIAM-Journal-on-Optimization-SIOPT">SIAM Journal on Optimization</a>. <br>
<br>
Update: <a
href="https://epubs.siam.org/doi/pdf/10.1137/18M1167061">The
paper appeared on the SIOPT website.</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 17, 2019</h3>
<h1>Dmitry @ Summer School in Voronovo</h1>
<br>
<a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>
is attending <a href="https://ssopt.org/">"Control,
Information and Optimization"</a> Summer School in Voronovo,
Moscow region, Russia.<br>
<br>
<i>Update:</i> Dmitry won the Best Poster Award for his poster
describing the paper <a
href="https://arxiv.org/abs/1904.05115">"Stochastic
distributed learning with gradient quantization and variance
reduction"</a>. Congratulations!!! The paper was co-autored
by <a href="https://samuelhorvath.github.io/">Samuel Horváth</a>,
<a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>,
<a href="https://konstmish.github.io/">Konstantin Mishchenko</a>,
myself and <a href="https://sstich.ch/">Sebastian Stich</a>.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 17, 2019</h3>
<h1>Workshop at the Isaac Newton Institute, Cambridge</h1>
<br>
I am at the <a href="https://www.newton.ac.uk/">Isaac Newton
Institute for Mathematical Sciences</a> at the <a
href="https://www.cam.ac.uk/">University of Cambridge</a>,
attending the workshop <a
href="https://www.newton.ac.uk/event/ascw03">"Approximation,
Sampling, and Compression in High Dimensional Problems"</a>.
My talk is on Thursday June 20; I will speak about <a
href="https://arxiv.org/abs/1805.02632">JacSketch.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 16, 2019</h3>
<h1>Konstantin @ Bath</h1>
<br>
<a href="https://konstmish.github.io/">Konstantin Mishchenko</a>
is visiting <a href="https://mehrhardt.github.io/">Matthias
J. Ehrhardt</a> at <a href="https://www.bath.ac.uk/">University of Bath</a>, United Kingdom.<br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>
<h3>June 14, 2019</h3>
<h1>ICML Workshops Started<br>
</h1>
<br>
The main ICML conference is over; the workshops start today
and continue tomorrow.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 13, 2019</h3>
<h1>KAUST President @ ICML 2019</h1>
<br>
KAUST president, <a
href="https://en.wikipedia.org/wiki/Tony_F._Chan">Tony Chan</a>,
attended ICML yesterday. I have shown him around and we have
jointly attended a number of interesting talks and sessions.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 11, 2019</h3>
<h1>ICML 2019 Talks</h1>
<br>
We have given three talks today; one by Samuel and two by me.
Here are the slides:<br>
<br>
<a href="talks/TALK-nonconvex-AS.pdf">Slides for "Nonconvex
Variance Reduced Optimization with Arbitrary Sampling"</a>
(5 min oral)<br>
<a href="talks/TALK-SGD-AS.pdf">Slides for "SGD: General
Analysis and Improved Rates"</a> (20 min oral)<br>
<a href="talks/TALK-SAGA-AS.pdf">Slides for "SAGA with
Arbitrary Sampling"</a> (5 min oral)<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 9, 2019</h3>
<h1>ICML 2019          </h1>
<br>
I am in Los Angeles, attending <a href="https://icml.cc/">ICML 2019</a>. I am here until June 16; and will attend the
workshops as well. <a
href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas</a>,
<a href="https://konstmish.github.io/">Konstantin</a>, <a
href="http://www.ali-sa.org/">Alibek</a>, <a
href="https://samuelhorvath.github.io/">Samuel</a>, <a
href="https://adil-salim.github.io/">Adil</a>, <a
href="https://www.aritradutta.com/">Aritra</a>, and <a
href="https://ehbergou.github.io/">El Houcine</a> are here,
too. <br>
<br>
KAUST has a booth at ICML - check out booth #212! We are
hiring! We have openings for MS/PhD positions, postdocs,
research scientists, assistant professors, associate professor
and full professors.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 8, 2019</h3>
<h1>New Intern Arrived: Ahmed Khaled Ragab from Cairo</h1>
<br>
Ahmed Khaled Ragab (Cairo University) just arrived to KAUST
for a research internship. Welcome!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>June 6, 2019</h3>
<h1>ICML 2019 Posters</h1>
<br>
We have prepared posters for our ICML 2019 papers:<br>
<br>
<a href="http://proceedings.mlr.press/v97/horvath19a.html">"Nonconvex Variance Reduced Optimization with Arbitrary Sampling"</a><br>
oral talk, Tuesday June 11 @ 11:35-11:40am in Room 104 (<a
href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4756">schedule</a>)<br>
poster, Tuesday June 11 @ 6:30pm-9:00pm in Pacific Ballroom
#95 (<a
href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=3831">schedule</a>)<br>
<br>
<a href="http://proceedings.mlr.press/v97/qian19b.html">"SGD:
General Analysis and Improved Rates"</a><br>
20 min oral talk, Tuesday June 11 @ 2:40-3:00pm in Room 103 (<a
href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4669">schedule</a>)<br>
poster, Tuesday June 11 @ 6:30pm-9:00pm in Pacific Ballroom
#195 (<a
href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4155">schedule</a>)<br>
<br>
<a href="http://proceedings.mlr.press/v97/qian19a.html">"SAGA
with Arbitrary Sampling"</a><br>
oral talk, Tuesday June 11 @ 3:15-3:20pm in Room 103 (<a
href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4673">schedule</a>)<br>
poster, Tuesday June 11 @ 6:30pm-9:00pm in Pacific Ballroom
#199 (<a
href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=3969">schedule</a>)<br>
<br>
<br>
Here are the posters:<br>
<br>
<a href="posters/Poster-nonconvex_AS.pdf"><img
src="posters/Poster-nonconvex_AS_small.png" alt="Poster
nonconvex VR opt with arbitrary sampling" border="0"
width="700" height="526"></a><br>
<br>
<a href="posters/Poster-SGD_AS.pdf"><img
src="posters/Poster-SGD_AS_small.png" alt="Poster SGD with
arbitrary sampling" border="0" width="700" height="524"></a><br>
<br>
<a href="posters/Poster-SAGA_AS.pdf"><img
src="posters/Poster-SAGA_AS_small.png" alt="Poster SAGA
with Arbitrary Sampling" border="0" width="700"
height="527"></a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 4, 2019</h3>
<h1>New Paper<br>
</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1906.01481">"L-SVRG and
L-Katyusha with arbitrary sampling"</a> - joint work with <a
href="https://qianxunk.github.io/">Xun Qian</a> and <a
href="https://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu.</a><br>
<br>
Abstract: <i>We develop and analyze a new family of
nonaccelerated and accelerated loopless variance-reduced
methods for finite sum optimization problems. Our
convergence analysis relies on a novel expected smoothness
condition which upper bounds the variance of the stochastic
gradient estimation by a constant times a distance-like
function. This allows us to handle with ease arbitrary
sampling schemes as well as the nonconvex case. We perform
an in-depth estimation of these expected smoothness
parameters and propose new importance samplings which allow
linear speedup when the expected minibatch size is in a
certain range. Furthermore, a connection between these
expected smoothness parameters and expected separable
overapproximation (ESO) is established, which allows us to
exploit data sparsity as well. Our results recover as
special cases the recently proposed loopless SVRG and
loopless Katyusha methods.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 4, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1906.01474">"MISO is making a
comeback with better proofs and rates"</a> - joint work with
<a href="https://qianxunk.github.io/">Xun Qian</a>, <a
href="http://www.ali-sa.org/">Alibek Sailanbayev</a> and <a
href="https://konstmish.github.io/">Konstantin Mishchenko.</a>
<br>
<br>
Abstract: <i>MISO, also known as Finito, was one of the first
stochastic variance reduced methods discovered, yet its
popularity is fairly low. Its initial analysis was
significantly limited by the so-called Big Data assumption.
Although the assumption was lifted in subsequent work using
negative momentum, this introduced a new parameter and
required knowledge of strong convexity and smoothness
constants, which is rarely possible in practice. We
rehabilitate the method by introducing a new variant that
needs only smoothness constant and does not have any extra
parameters. Furthermore, when removing the strong convexity
constant from the stepsize, we present a new analysis of the
method, which no longer uses the assumption that every
component is strongly convex. This allows us to also obtain
so far unknown nonconvex convergence of MISO. To make the
proposed method efficient in practice, we derive
minibatching bounds with arbitrary uniform sampling that
lead to linear speedup when the expected minibatch size is
in a certain range. Our numerical experiments show that MISO
is a serious competitor to SAGA and SVRG and sometimes
outperforms them on real datasets.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 3, 2019</h3>
<h1>Elnur Visiting Grenoble</h1>
<br>
<a href="https://elnurgasanov.tilda.ws/">Elnur Gasanov</a> is
visiting <a href="https://ljk.imag.fr/membres/Jerome.Malick/">Jérôme Malick</a> and his group in Grenoble. He will stat there
until the end of June.<br>
<br>
Update (June 29): Elnur's visit was extended until until July
19.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 30, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1905.13278">"A stochastic
derivative free optimization method with momentum"</a> -
joint work with <a href="https://eduardgorbunov.github.io/">Eduard Gorbunov</a>, <a href="http://www.adelbibi.com/">Adel Bibi</a>,
Ozan Sezer and <a href="https://ehbergou.github.io/">El
Houcine Bergou.</a><br>
<br>
Abstract: <i>We consider the problem of unconstrained
minimization of a smooth objective function in R^d in
setting where only function evaluations are possible. We
propose and analyze stochastic zeroth-order method with
heavy ball momentum. In particular, we propose SMTP - a
momentum version of the stochastic three-point method (STP)
of Bergou et al (2018). We show new complexity results for
non-convex, convex and strongly convex functions. We test
our method on a collection of learning to continuous control
tasks on several MuJoCo environments with varying difficulty
and compare against STP, other state-of-the-art
derivative-free optimization algorithms and against policy
gradient methods. SMTP significantly outperforms STP and all
other methods that we considered in our numerical
experiments. Our second contribution is SMTP with importance
sampling which we call SMTP_IS. We provide convergence
analysis of this method for non-convex, convex and strongly
convex objectives.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 30, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1905.12938">"On stochastic sign
descent methods"</a> - joint work with <a
href="https://mher-safaryan.github.io">Mher Safaryan.</a><br>
<br>
Abstract: <i>Various gradient compression schemes have been
proposed to mitigate the communication cost in distributed
training of large scale machine learning models. Sign-based
methods, such as signSGD, have recently been gaining
popularity because of their simple compression rule and
connection to adaptive gradient methods, like ADAM. In this
paper, we perform a general analysis of sign-based methods
for non-convex optimization. Our analysis is built on
intuitive bounds on success probabilities and does not rely
on special noise distributions nor on the boundedness of the
variance of stochastic gradients. Extending the theory to
distributed setting within a parameter server framework, we
assure variance reduction with respect to number of nodes,
maintaining 1-bit compression in both directions and using
small mini-batch sizes. We validate our theoretical findings
experimentally. </i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 29, 2019</h3>
<h1>Tong Zhang @ KAUST</h1>
<br>
<a href="http://tongzhang-ml.org/">Tong Zhang</a> is visiting
me at KAUST. He is giving a talk at noon today in the <a
href="https://ml.kaust.edu.sa/seminar.html">ML Hub Seminar
Series.</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 28, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1905.11768">"Stochastic proximal
Langevin algorithm: potential splitting and nonasymptotic
rates"</a> - joint work with <a
href="https://adil-salim.github.io/">Adil Salim</a> and <a
href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>.<br>
<br>
Abstract: <i>We propose a new algorithm---Stochastic Proximal
Langevin Algorithm (SPLA)---for sampling from a log concave
distribution. Our method is a generalization of the Langevin
algorithm to potentials expressed as the sum of one
stochastic smooth term and multiple stochastic nonsmooth
terms. In each iteration, our splitting technique only
requires access to a stochastic gradient of the smooth term
and a stochastic proximal operator for each of the nonsmooth
terms. We establish nonasymptotic sublinear and linear
convergence rates under convexity and strong convexity of
the smooth term, respectively, expressed in terms of the KL
divergence and Wasserstein distance. We illustrate the
efficiency of our sampling technique through numerical
simulations on a Bayesian learning task. </i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 28, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1905.11692">"Direct nonlinear
acceleration"</a> - joint work with <a
href="https://www.aritradutta.com/">Aritra Dutta</a>, <a
href="https://ehbergou.github.io/">El Houcine Bergou</a>,
Yunming Xiao and <a href="https://mcanini.github.io/">Marco
Canini.</a><br>
<br>
Abstract: <i>Optimization acceleration techniques such as
momentum play a key role in state-of-the-art machine
learning algorithms. Recently, generic vector sequence
extrapolation techniques, such as regularized nonlinear
acceleration (RNA) of Scieur et al., were proposed and shown
to accelerate fixed point iterations. In contrast to RNA
which computes extrapolation coefficients by (approximately)
setting the gradient of the objective function to zero at
the extrapolated point, we propose a more direct approach,
which we call direct nonlinear acceleration (DNA). In DNA,
we aim to minimize (an approximation of) the function value
at the extrapolated point instead. We adopt a regularized
approach with regularizers designed to prevent the model
from entering a region in which the functional approximation
is less precise. While the computational cost of DNA is
comparable to that of RNA, our direct approach significantly
outperforms RNA on both synthetic and real-world datasets.
While the focus of this paper is on convex problems, we
obtain very encouraging results in accelerating the training
of neural networks. </i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 27, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1905.11535">"A stochastic
decoupling method for minimizing the sum of smooth and
non-smooth functions"</a> - joint work with <a
href="https://konstmish.github.io/">Konstantin Mishchenko.</a><br>
<br>
Abstract: <i>We consider the problem of minimizing the sum of
three convex functions: i) a smooth function $f$ in the form
of an expectation or a finite average, ii) a non-smooth
function $g$ in the form of a finite average of proximable
functions $g_j$, and iii) a proximable regularizer $R$. We
design a variance reduced method which is able
progressively&nbsp; learn the proximal operator of $g$ via
the computation of the proximal operator of a single
randomly selected function $g_j$ in each iteration only. Our
method can provably and efficiently accommodate many
strategies for the estimation of the gradient of $f$,
including via standard and variance-reduced stochastic
estimation, effectively decoupling the smooth part of the
problem from the non-smooth part. We prove a number of
iteration complexity results, including a general $O(1/t)$
rate, $O(1/t^2)$ rate in the case of strongly convex $f$,
and several linear rates in special cases, including
accelerated linear rate. For example, our method achieves a
linear rate for the problem of minimizing a strongly convex
function $f$ under linear constraints under no assumption on
the constraints beyond consistency. When combined with SGD
or SAGA estimators for the gradient of $f$, this&nbsp; leads
to&nbsp; a very efficient method for empirical risk
minimization with large linear constraints.&nbsp; Our method
generalizes several existing algorithms, including
forward-backward splitting, Douglas-Rachford splitting,
proximal SGD, proximal SAGA, SDCA, randomized Kaczmarz and
Point-SAGA. However, our method leads to many new specific
methods in special cases; for instance,&nbsp; we obtain the
first randomized variant of the Dykstra's method for
projection onto the intersection of closed convex sets. </i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 27, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1905.11373">"Revisiting
stochastic extragradient"</a> - joint work with <a
href="https://konstmish.github.io/">Konstantin Mishchenko</a>,
<a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>,
<a href="https://github.com/shulgin-egor">Egor Shulgin</a> and
<a
href="https://scholar.google.com/citations?user=GI_-KjoAAAAJ&amp;hl=en">Yura Malitsky.</a><br>
<br>
Abstract: <i>We consider a new extension of the extragradient
method that is motivated by approximating implicit updates.
Since in the recent work of Chavdarova et al (2019) it was
shown that the existing stochastic extragradient algorithm
(called mirror-prox) of Juditsky et al (2011) diverges on a
simple bilinear problem, we prove guarantees for solving
variational inequality that are more general. Furthermore,
we illustrate numerically that the proposed variant
converges faster than many other methods on the example of Chavdarova et al (2019). We also discuss how extragradient can be
applied to training Generative Adversarial Networks (GANs).
Our experiments on GANs demonstrate that the introduced
approach may make the training faster in terms of data
passes, while its higher iteration complexity makes the
advantage smaller. To further accelerate method's
convergence on problems such as bilinear minimax, we combine
the extragradient step with the negative momentum of Gidel
et al (2018) and discuss the optimal momentum value. </i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 27, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1905.11266">"One method to rule
them all: variance reduction for data, parameters and many
new methods"</a> - joint work with <a
href="https://fhanzely.github.io/index.html">Filip Hanzely.</a><br>
<br>
Abstract: <i>We propose a remarkably general variance-reduced
method suitable for solving regularized empirical risk
minimization problems with either a large number of training
examples, or a large model dimension, or both. In special
cases, our method reduces to several known and previously
thought to be unrelated methods, such as SAGA, LSVRG,
JacSketch, SEGA and ISEGA, and their arbitrary sampling and
proximal generalizations. However, we also highlight a large
number of new specific algorithms with interesting
properties. We provide a single theorem establishing linear
convergence of the method under smoothness and quasi strong
convexity assumptions. With this theorem we recover
best-known and sometimes improved rates for known methods
arising in special cases. As a by-product, we provide the
first unified method and theory for stochastic gradient and
stochastic coordinate descent type methods. </i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 27, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1905.11261">"A unified theory of
SGD: variance reduction, sampling, quantization and
coordinate descent"</a> - joint work with <a
href="https://eduardgorbunov.github.io/">Eduard Gorbunov</a>
and <a href="https://fhanzely.github.io/index.html">Filip
Hanzely.</a><br>
<br>
Abstract: <i>In this paper we introduce a unified analysis of
a large family of variants of proximal stochastic gradient
descent (SGD) which so far have required different
intuitions, convergence analyses, have different
applications, and which have been developed separately in
various communities. We show that our framework includes
methods with and without the following tricks, and their
combinations: variance reduction, importance sampling,
mini-batch sampling, quantization, and coordinate
sub-sampling.&nbsp; As a by-product, we obtain the first
unified theory of SGD and randomized coordinate descent
(RCD) methods,&nbsp; the first unified theory of variance
reduced and non-variance-reduced SGD methods, and the first
unified theory of quantized and non-quantized methods. A key
to our approach is a parametric assumption on the iterates
and stochastic gradients. In a single theorem we establish a
linear convergence result under this assumption and
strong-quasi convexity of the loss function. Whenever we
recover an existing method as a special case, our theorem
gives the best known complexity result. Our approach can be
used to motivate the development of new useful methods, and
offers pre-proved convergence guarantees. To illustrate the
strength of our approach, we develop five new variants of
SGD, and through numerical experiments demonstrate some of
their properties. </i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 27, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1905.10988">"Natural compression
for distributed deep learning"</a> - joint work with <a
href="https://samuelhorvath.github.io/">Samuel Horváth</a>,
<a href="https://www.chenyuho.com/">Chen-Yu Ho</a>, Ľudovít
Horváth, Atal Narayan Sahu and <a
href="https://mcanini.github.io/">Marco Canini.</a><br>
<br>
Abstract: <i>Due to their hunger for big data, modern deep
learning models are trained in parallel, often in
distributed environments, where communication of model
updates is the bottleneck. Various update compression (e.g.,
quantization, sparsification, dithering) techniques have
been proposed in recent years as a successful tool to
alleviate this problem. In this work, we introduce a new,
remarkably simple and theoretically and practically
effective compression technique, which we call natural
compression (NC). Our technique is applied individually to
all entries of the to-be-compressed update vector and works
by randomized rounding to the nearest (negative or positive)
power of two. NC is "natural" since the nearest power of two
of a real expressed as a float can be obtained without any
computation, simply by ignoring the mantissa. We show that
compared to no compression, NC increases the second moment
of the compressed vector by the tiny factor 9/8 only, which
means that the effect of NC on the convergence speed of
popular training algorithms, such as distributed SGD, is
negligible. However, the communications savings enabled by
NC are substantial, leading to 3-4x improvement in overall
theoretical running time. For applications requiring more
aggressive compression, we generalize NC to natural
dithering, which we prove is exponentially better than the
immensely popular random dithering technique. Our
compression operators can be used on their own or in
combination with existing operators for a more aggressive
combined effect. Finally, we show that NC is particularly
effective for the in-network aggregation (INA) framework for
distributed training, where the update aggregation is done
on a switch, which can only perform integer computations. </i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 26, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1905.10874">"Randomized Subspace
Newton"</a> - joint work with <a
href="https://gowerrobert.github.io/">Robert Mansel Gower</a>,
<a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>
and <a
href="http://www.opt.uni-duesseldorf.de/%7Elieder/de/inhalt.php">Felix Lieder. </a><br>
<br>
Abstract: <i>We develop a randomized Newton method capable of
solving learning problems with huge dimensional feature
spaces, which is a common setting in applications such as
medical imaging, genomics and seismology. Our method
leverages randomized sketching in a new way, by finding the
Newton direction constrained to the space spanned by a
random sketch. We develop a simple global linear convergence
theory that holds for practically all sketching techniques,
which gives the practitioners the freedom to design custom
sketching approaches suitable for particular applications.
We perform numerical experiments which demonstrate the
efficiency of our method as compared to accelerated gradient
descent and the full Newton method. Our method can be seen
as a refinement and randomized extension of the results of
Karimireddy, Stich, and Jaggi (2019). </i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 25, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1905.10598">"Best pair
formulation &amp; accelerated scheme for non-convex
principal component pursuit"</a> - joint work with <a
href="https://richtarik.org/i_team.html">Aritra Dutta</a>, <a
href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
and <a href="https://jliang993.github.io/">Jingwei Liang</a>.<br>
<br>
Abstract: <i>The best pair problem aims to find a pair of
points that minimize the distance between two disjoint sets.
In this paper, we formulate the classical robust principal
component analysis (RPCA) as the best pair; which was not
considered before. We design an accelerated proximal
gradient scheme to solve it, for which we show global
convergence, as well as the local linear rate. Our extensive
numerical experiments on both real and synthetic data
suggest that the algorithm outperforms relevant baseline
algorithms in the literature.</i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 26, 2019</h3>
<h1>Filip @ Berkeley</h1>
<br>
As of today, <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> is visiting <a
href="https://www.stat.berkeley.edu/%7Emmahoney/">Michael
Mahoney</a> at UC Berkeley. He will stay there until June
18. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 22, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1905.08645">"Revisiting
randomized gossip algorithms: general framework, convergence
rates and novel block and accelerated protocols"</a> - joint
work with <a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas Loizou.</a><br>
<br>
Abstract: <i>In this work we present a new framework for the
analysis and design of randomized gossip algorithms for
solving the average consensus problem. We show how classical
randomized iterative methods for solving linear systems can
be interpreted as gossip algorithms when applied to special
systems encoding the underlying network and explain in
detail their decentralized nature. Our general framework
recovers a comprehensive array of well-known gossip
algorithms as special cases, including the pairwise
randomized gossip algorithm and path averaging gossip, and
allows for the development of provably faster variants. The
flexibility of the new approach enables the design of a
number of new specific gossip methods. For instance, we
propose and analyze novel block and the first provably
accelerated randomized gossip protocols, and dual randomized
gossip algorithms. From a numerical analysis viewpoint, our
work is the first that explores in depth the decentralized
nature of randomized iterative methods for linear systems
and proposes them as methods for solving the average
consensus problem. We evaluate the performance of the
proposed gossip protocols by performing extensive
experimental testing on typical wireless network topologies.</i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 12, 2019</h3>
<h1>Nicolas @ ICASSP 2019</h1>
<br>
Nicolas Loizou is attending <a
href="https://2019.ieeeicassp.org/">ICASSP 2019</a> (2019
IEEE International Conference on Acoustics, Speech and Signal
Processing) in Brighton, UK, where is presenting the paper <a
href="https://ieeexplore.ieee.org/document/8683847">"Provably  accelerated randomized gossip algorithms"</a>, joint work
with Michael Rabbat and me.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 9, 2019</h3>
<h1>Samuel Visiting Michael Jordan @ Berkeley<br>
</h1>
<br>
Starting today, <a href="https://samuelhorvath.github.io/">Samuel Horváth</a> is visiting <a
href="https://people.eecs.berkeley.edu/%7Ejordan/">Michael
I. Jordan</a> at UC Berkeley. He will stay there for a
month.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 2, 2019</h3>
<h1>PhD Proposal Defense</h1>
<br>
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
defended his PhD proposal and progressed to PhD candidacy.
Congratulations! <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 29, 2019</h3>
<h1>PhD Proposal Defense</h1>
<br>
<a href="https://konstmish.github.io/">Konstantin Mishchenko</a>
defended his PhD proposal and progressed to PhD candidacy.
Congratulations! <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 23, 2019</h3>
<h1>Xavier Bresson @ KAUST</h1>
<br>
I invited <a href="http://www.ntu.edu.sg/home/xbresson/">Xavier Bresson</a> to KAUST; he arrived yesterday. Today he is
giving an <a href="https://ml.kaust.edu.sa/seminar.html">ML
Hub seminar talk</a> on "Convolutional Neural Networks on
Graphs". On April 24 &amp; 25 he will be teaching his <a
href="https://ml.kaust.edu.sa/tutorial.html">Industrial
Short Course on Deep Learning and Latest AI Algorithms</a>.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 22, 2019</h3>
<h1>Four Papers Accepted to ICML 2019</h1>
<br>
The long-awaited decisions just came! We've had four papers
accepted:<br>
<br>
<a href="https://arxiv.org/abs/1809.04146">"Nonconvex variance
reduced optimization with arbitrary sampling"</a> - joint
work with <a href="https://samuelhorvath.github.io/">Samuel
Horváth</a>.<br>
<br>
Abstract:<i> We provide the first importance sampling variants
of variance reduced algorithms for empirical risk
minimization with non-convex loss functions. In particular,
we analyze non-convex versions of SVRG, SAGA and SARAH. Our
methods have the capacity to speed up the training process
by an order of magnitude compared to the state of the art on
real datasets. Moreover, we also improve upon current
mini-batch analysis of these methods by proposing importance
sampling for minibatches in this setting. Surprisingly, our
approach can in some regimes lead to superlinear speedup
with respect to the minibatch size, which is not usually
present in stochastic optimization. All the above results
follow from a general analysis of the methods which works
with arbitrary sampling, i.e., fully general randomized
strategy for the selection of subsets of examples to be
sampled in each iteration. Finally, we also perform a novel
importance sampling analysis of SARAH in the convex setting.</i><br>
<br>
<a href="https://arxiv.org/abs/1901.09401">"SGD: General
analysis and improved rates"</a> - joint work with <a
href="https://gowerrobert.github.io/">Robert Mansel Gower</a>,
<a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
Loizou</a>, <a href="https://qianxunk.github.io/">Xun Qian</a>,
<a href="http://www.ali-sa.org/">Alibek Sailanbayev</a> and
Egor Shulgin.<br>
<br>
Abstract:<i> We propose a general yet simple theorem
describing the convergence of SGD under the arbitrary
sampling paradigm. Our theorem describes the convergence of
an infinite array of variants of SGD, each of which is
associated with a specific probability law governing the
data selection rule used to form mini-batches. This is the
first time such an analysis is performed, and most of our
variants of SGD were never explicitly considered in the
literature before. Our analysis relies on the recently
introduced notion of expected smoothness and does not rely
on a uniform bound on the variance of the stochastic
gradients. By specializing our theorem to different
mini-batching strategies, such as sampling with replacement
and independent sampling, we derive exact expressions for
the stepsize as a function of the mini-batch size. With this
we can also determine the mini-batch size that optimizes the
total complexity, and show explicitly that as the variance
of the stochastic gradient evaluated at the minimum grows,
so does the optimal mini-batch size. For zero variance, the
optimal mini-batch size is one. Moreover, we prove
insightful stepsize-switching rules which describe when one
should switch from a constant to a decreasing stepsize
regime.</i><br>
<br>
<a href="https://arxiv.org/abs/1901.08669">"SAGA with
arbitrary sampling"</a> - joint work with <a
href="https://qianxunk.github.io/">Xun Qian</a> and <a
href="https://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a>.<br>
<br>
Abstract:<i> We study the problem of minimizing the average of
a very large number of smooth functions, which is of key
importance in training supervised learning models. One of
the most celebrated methods in this context is the SAGA
algorithm. Despite years of research on the topic, a
general-purpose version of SAGA---one that would include
arbitrary importance sampling and minibatching
schemes---does not exist. We remedy this situation and
propose a general and flexible variant of SAGA following the
arbitrary sampling paradigm. We perform an iteration
complexity analysis of the method, largely possible due to
the construction of new stochastic Lyapunov functions. We
establish linear convergence rates in the smooth and
strongly convex regime, and under a quadratic functional
growth condition (i.e., in a regime not assuming strong
convexity). Our rates match those of the primal-dual method
Quartz for which an arbitrary sampling analysis is
available, which makes a significant step towards closing
the gap in our understanding of complexity of primal and
dual methods for finite sum problems.</i><br>
<br>
<a href="https://arxiv.org/abs/1811.10792">"Stochastic
gradient push for distributed deep learning"</a> - this is
the work of my student <a
href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
Loizou</a>, joint with his Facebook coauthors Mahmoud
Assran, Nicolas Ballas and Michael Rabbat.<br>
<br>
Abstract:<i> Distributed data-parallel algorithms aim to
accelerate the training of deep neural networks by
parallelizing the computation of large mini-batch gradient
updates across multiple nodes. Approaches that synchronize
nodes using exact distributed averaging (e.g., via
AllReduce) are sensitive to stragglers and communication
delays. The PushSum gossip algorithm is robust to these
issues, but only performs approximate distributed averaging.
This paper studies Stochastic Gradient Push (SGP), which
combines PushSum with stochastic gradient updates. We prove
that SGP converges to a stationary point of smooth,
non-convex objectives at the same sub-linear rate as SGD,
that all nodes achieve consensus, and that SGP achieves a
linear speedup with respect to the number of compute nodes.
Furthermore, we empirically validate the performance of SGP
on image classification (ResNet-50, ImageNet) and machine
translation (Transformer, WMT'16 En-De) workloads. Our code
will be made publicly available.</i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 14, 2019</h3>
<h1>Filip @ AISTATS 2019</h1>
<br>
Today, <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> is travelling to Naha, Okinawa, Japan, to attend
<a href="https://www.aistats.org">AISTATS 2019.</a> He will present our paper <a href="https://arxiv.org/abs/1809.09354">"Accelerated coordinate descent with arbitrary sampling and best rates
for minibatches"</a>. Here is the poster for the paper:<br>
<br>
<a href="posters/Poster-ACD.pdf"><img src="posters/Poster-ACD-small.png" alt="Poster - ACD"
border="0" width="300" align="middle" ></a>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 10, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a href="https://arxiv.org/abs/1904.05115">"Stochastic distributed learning with gradient quantization and variance reduction"</a> - joint work with <a href="https://samuelhorvath.github.io">Samuel Horváth,</a> <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev,</a> <a href="https://konstmish.github.io">Konstantin Mishchenko,</a> and <a href="https://sstich.ch">Sebastian Stich.</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 9, 2019</h3>
<h1>Alexey Kroshnin @ KAUST</h1>
<br>
<a href="https://www.hse.ru/en/org/persons/219293044">Alexey
Kroshnin</a> arrived at KAUST today and will stay here until
the end of April. Alexey's research interests include
fundamental theory of optimal transport, geometry of
Wasserstein spaces, Wasserstein barycenters, dynamical systems
on Wasserstein spaces, probability theory, measure theory,
functional analysis and computational complexity theory.<br>
<br>
Alexey will work with <a href="https://konstmish.github.io">Konstantin Mishchenko</a> and me on randomized methods for feasibility
problems. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 8, 2019</h3>
<h1>Nicolas Loizou @ KAUST</h1>
<br>
<a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
Loizou</a> arrived at KAUST today and will stay here until
mid-May. He is finishing writing up his PhD thesis, and plans
to defend in the Summer. Once he is done with the thesis, we
will work do some work towards NeurIPS 2019. Nicolas got
several job offers and chose to join <a
href="https://mila.quebec">MILA</a> as a postdoc in
September 2019. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 19, 2019</h3>
<h1>New Paper          </h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1903.07971">"Convergence
analysis of inexact randomized iterative methods"</a> -
joint work with <a  href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
Loizou.</a><br>
<br>
Abstract: <i>In this paper we present a convergence rate
analysis of inexact variants of several randomized iterative
methods. Among the methods studied are: stochastic gradient
descent, stochastic Newton, stochastic proximal point and
stochastic subspace ascent. A common feature of these
methods is that in their update rule a certain sub-problem
needs to be solved exactly. We relax this requirement by
allowing for the sub-problem to be solved inexactly. In
particular, we propose and analyze inexact randomized
iterative methods for solving three closely related
problems: a convex stochastic quadratic optimization
problem, a best approximation problem and its dual, a
concave quadratic maximization problem. We provide iteration
complexity results under several assumptions on the
inexactness error. Inexact variants of many popular and some
more exotic methods, including randomized block Kaczmarz,
randomized Gaussian Kaczmarz and randomized block coordinate
descent, can be cast as special cases. Numerical experiments
demonstrate the benefits of allowing inexactness.</i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 18, 2019</h3>
<h1>Dmitry in Moscow</h1>
<br>
As of today, <a
href="https://vcc.kaust.edu.sa/Pages/Kovalev.aspx">Dmitry
Kovalev</a> is visiting Moscow - he will stay there for two
weeks and will give two research talks while there (one in
Boris Polyak's group and another at MIPT).<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>March 17, 2019</h3>
<h1>Zheng Qu @ KAUST</h1>
<br>
<a href="https://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu (The
University of Hong Kong)</a> is visiting me at KAUST this
week. She will stay for a week, and will give the Machine
Learning Hub seminar on Thursday.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>March 9, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1903.06701">"Scaling distributed
machine learning with in-network aggregation"</a> - joint
work with <a
href="https://ecrc.kaust.edu.sa/Pages/Sapio.aspx">Amedeo
Sapio</a>, <a href="https://mcanini.github.io">Marco Canini</a>,
<a href="https://www.chenyuho.com">Chen-Yu Ho</a>, <a
href="https://www.microsoft.com/en-us/research/people/jacnels/">Jacob Nelson</a>, <a
href="https://www.kaust.edu.sa/en/study/faculty/panagiotis-kalnis">Panos Kalnis</a>, <a
href="https://www.linkedin.com/in/changhoon-chang-kim-b3394317/">Changhoon Kim</a>, <a
href="https://www.cs.washington.edu/people/faculty/arvind">Arvind Krishnamurthy</a>, <a
href="https://scholar.google.com/citations?user=okIQd4oAAAAJ&amp;hl=en">Masoud Moshref</a>, and <a
href="https://www.microsoft.com/en-us/research/people/dports/">Dan R. K. Ports.</a><br>
<br>
Abstract: <i>Training complex machine learning models in
parallel is an increasingly important workload. We
accelerate distributed parallel training by designing a
communication primitive that uses a programmable switch
dataplane to execute a key step of the training process. Our
approach, SwitchML, reduces the volume of exchanged data by
aggregating the model updates from multiple workers in the
network. We co-design the switch processing with the
end-host protocols and ML frameworks to provide a robust,
efficient solution that speeds up training by up to 300%,
and at least by 20% for a number of real-world benchmark
models.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 9, 2019</h3>
<h1>Ľubomír Baňas @ KAUST</h1>
<br>
Ľubomír Baňas (Bielefeld) is arriving today at KAUST for a
research visit; he will stay for a week. He will give an <a
href="https://cemse.kaust.edu.sa/events/Pages/AMCS-seminar--Numerical-approximation-of-the-stochastic-Cahn-Hilliard-equation-and-the-%28stochastic%29-Hele-Shaw-probl-125059.aspx">AMCS  seminar talk on Wednesday. </a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 4, 2019</h3>
<h1>Atal Joining KAUST as a PhD Student</h1>
<br>
My former intern, Atal Sahu (IIT Kanpur), joined KAUST as an
MS student in the group of <a
href="https://mcanini.github.io">Marco Canini</a>. <br>
<br>
Atal: Welcome back!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 23, 2019</h3>
<h1>Senior PC Member for IJCAI 2019</h1>
<br>
I have accepted an invite to serve as a Senior Program
Committee Member at the 28th International Joint Conference on
Artificial Intelligence (<a href="https://ijcai19.org">IJCAI
2019</a>). The conference will take place in Macao, China,
during August 10-16, 2019. The first IJCAI conference was held
in 1969.<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 20, 2019</h3>
<h1>I am in Vienna</h1>
<br>
I am in Vienna, visiting the <a href="https://www.esi.ac.at">Erwin Schrödinger International Institute for Mathematics and
Physics (ESI)</a> which is the hosting a program on <a
href="https://www.esi.ac.at/activities/events/2019/modern-maximal-monotone-operator-theory">Modern Maximal Monotone Operator Theory: From Nonsmooth
Optimization to Differential Inclusions</a>.<br>
<br>
On February 22 I am teaching a one-day (5 hrs) doctoral course
on randomized methods in convex optimization. I offered two
possible courses to the students, and they picked (almost
unanimously) <a
href="https://www.esi.ac.at/activities/events/2019/files/richtarik">this  one</a>.<br>
<br>
During February 25-March 1, I am attending the workshop <a
href="https://www.univie.ac.at/projektservice-mathematik/e/?event=nualnoop">Numerical Algorithms in Nonsmooth Optimization</a>. My talk is on
February 26; I am speaking about the <a
href="https://papers.nips.cc/paper/7478-sega-variance-reduction-via-gradient-sketching">"SEGA" paper (NeurIPS 2018)</a> - joint work with <a
href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
and <a href="https://konstmish.github.io">Konstantin
Mishchenko</a>. My SEGA slides are here (click on the image
to get the pdf file):<br>
<br>
<a href="talks/Talk-SEGA.pdf"><img
src="imgs/SEGA-cover-slide.png" alt="Click to download the
slides" border="2" width="600" height="447"></a>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 18, 2019</h3>
<h1>Konstantin @ EPFL</h1>
<br>
As of today, Konstantin Mishchenko is visiting Martin Jaggi's <a href="https://mlo.epfl.ch">Machine Learning and Optimization Laboratory at EPFL</a>. He will stay there for a month.<br>
<br>
<i>Update (March 17): Konstantin is back at KAUST now.</I><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 12, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1902.03591">"Stochastic three
points method for unconstrained smooth minimization"</a> -
joint work with <a
href="https://vcc.kaust.edu.sa/Pages/Bergou.aspx">El Houcine
Bergou</a> and <a
href="https://eduardgorbunov.github.io/index.html">Eduard
Gorbunov</a>.<br>
<br>
Abstract: <i>In this paper we consider the unconstrained
minimization problem of a smooth function in $R^n$ in a
setting where only function evaluations are possible. We
design a novel randomized direct search method based on
stochastic three points (STP) and analyze its complexity. At
each iteration, STP generates a random search direction
according to a certain fixed probability law. Our
assumptions on this law are very mild: roughly speaking, all
laws which do not concentrate all measure on any half-space
passing through the origin will work. For instance, we allow
for the uniform distribution on the sphere and also
distributions that concentrate all measure on a positive
spanning set. Given a current iterate $x$, STP compares the
objective function at three points: $x$, $x + \alpha s$ and $x-\alpha s$, where
$\alpha$ is a stepsize parameter and $s$ is the random search
direction. The best of these three points is the next
iterate. We analyze the method STP under several stepsize
selection schemes (fixed, decreasing, estimated through
finite differences, etc). We study non-convex, convex and
strongly convex cases. We also propose a parallel
version for STP, with iteration complexity bounds which do
not depend on the dimension n.</i><br>
<br>
<i>Comment:</i> The paper was finalized in March 2018; but we
only put it online now.<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 11, 2019</h3>
<h1>Internships Available in my Group</h1>
<br>
I always have research internships available in my group @ <a
href="https://www.kaust.edu.sa/en">KAUST</a> throughout the
year for outstanding and highly motivated students. If you are
from Europe, USA, Canada, Australia or New Zealand, you are
eligible for the <a
href="https://vsrp.kaust.edu.sa/Pages/Topics%20in%20Machine%20Learning%20and%20Optimization.aspx">Visiting Student Research Program (VSRP).</a> These internships are a
minimum 3 months and a maximum 6 months in duration. We have a
different internship program dedicated to applicants from
elsewhere. Shorter internships are possible with this program.
Drop me an email if you are interested in working with me,
explaining why you are interested, attaching your CV and
complete transcript of grades. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 8, 2019</h3>
<h1>Group Photo</h1>
<br>
This is my research group: <br>
<br>
<a href="imgs/Richtarik-group-2019-02-medium.jpeg"><img
src="imgs/Richtarik-group-2019-02-small.png" alt="My
research group (February 2019)" border="0" width="700"
height="353"></a><br>
<br>
People on the photo: <br>
<br>
<i>Postdocs:</i> Aritra Dutta, El-Houcine Bergou, Xun Qian <br>
<br>
<i>PhD students:</i> Filip Hanzely, Konstantin Mishchenko,
Alibek Sailanbayev, Samuel Horváth <br>
<br>
<i>MS/PhD students:</i> Elnur Gasanov, Dmitry Kovalev <br>
<br>
<i>interns:</i> Eduard Gorbunov, Dmitry Kamzolov, Igor
Sokolov, Egor Shulgin, Vladislav Elsukov (all belong to my
group at MIPT where I am a visiting professor), Ľudovít
Horváth (from Comenius University)<br>
<br>
Comment: Nicolas Loizou (Edinburgh) is not on the photo; we
will photoshop him in once he comes for a visit in April... <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 4, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1902.01272">"A stochastic
derivative-free optimization method with importance
sampling"</a> - joint work with <a
href="http://www.adelbibi.com">Adel Bibi</a>, <a
href="https://vcc.kaust.edu.sa/Pages/Bergou.aspx">El Houcine
Bergou</a>, <a href="http://ozansener.net">Ozan Sener</a>
and <a
href="https://www.kaust.edu.sa/en/study/faculty/bernard-ghanem">Bernard Ghanem</a>.<br>
<br>
Abstract: <i>We consider the problem of unconstrained
minimization of a smooth objective function in R^n in a
setting where only function evaluations are possible. While
importance sampling is one of the most popular techniques
used by machine learning practitioners to accelerate the
convergence of their models when applicable, there is not
much existing theory for this acceleration in the
derivative-free setting. In this paper, we propose an
importance sampling version of the stochastic three points
(STP) method proposed by Bergou et al. and derive new
improved complexity results on non-convex, convex and
λ-strongly convex functions. We conduct extensive
experiments on various synthetic and real LIBSVM datasets
confirming our theoretical results. We further test our
method on a collection of continuous control tasks on
several MuJoCo environments with varying difficulty. Our
results suggest that STP is practical for high dimensional
continuous control problems. Moreover, the proposed
importance sampling version results in a significant sample
complexity improvement.</i> <br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 27, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1901.09437">"99% of parallel
optimization is inevitably a waste of time"</a> - joint work
with <a href="https://konstmish.github.io">Konstantin
Mishchenko</a> and <a
href="https://fhanzely.github.io/index.html">Filip Hanzely</a>.<br>
<br>
Abstract: <i>It is well known that many optimization methods,
including SGD, SAGA, and Accelerated SGD for
over-parameterized models, do not scale linearly in the
parallel setting. In this paper, we present a new version of
block coordinate descent that solves this issue for a number
of methods. The core idea is to make the sampling of
coordinate blocks on each parallel unit independent of the
others. Surprisingly, we prove that the optimal number of
blocks to be updated by each of $n$ units in every iteration
is equal to $m/n$, where $m$ is the total number of blocks.
As an illustration, this means that when $n=100$ parallel
units are used, 99% of work is a waste of time. We
demonstrate that with $m/n$ blocks used by each unit the
iteration complexity often remains the same. Among other
applications which we mention, this fact can be exploited in
the setting of distributed optimization to break the
communication bottleneck. Our claims are justified by
numerical experiments which demonstrate almost a perfect
match with our theory on a number of datasets.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 26, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1901.09269">"Distributed
learning with compressed gradient differences"</a> - joint
work with <a href="https://konstmish.github.io">Konstantin
Mishchenko</a>, <a
href="https://eduardgorbunov.github.io/index.html">Eduard
Gorbunov</a> and <a href="http://mtakac.com">Martin Takáč</a>.<br>
<br>
Abstract: <i>Training very large machine learning models
requires a distributed computing approach, with
communication of the model updates often being the
bottleneck. For this reason, several methods based on the
compression (e.g., sparsification and/or quantization) of
the updates were recently proposed, including QSGD (Alistarh
et al., 2017), TernGrad (Wen et al., 2017), SignSGD
(Bernstein et al., 2018), and DQGD (Khirirat et al., 2018).
However, none of these methods are able to learn the
gradients, which means that they necessarily suffer from
several issues, such as the inability to converge to the
true optimum in the batch mode, inability to work with a
nonsmooth regularizer, and slow convergence rates. In this
work we propose a new distributed learning
method---DIANA---which resolves these issues via compression
of gradient differences. We perform a theoretical analysis
in the strongly convex and nonconvex settings and show that
our rates are vastly superior to existing rates. Our
analysis of block quantization and differences between l2
and l∞ quantization closes the gaps in theory and practice.
Finally, by applying our analysis technique to TernGrad, we
establish the first convergence rate for this method.</i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 26, 2019</h3>
<h1>Filip and Aritra @ AAAI 2019 in Hawaii</h1>
<br>
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> and <a href="https://www.aritradutta.com">Aritra Dutta</a> are on their way to <a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a>,
to be held during Jan 27-Feb 1, 2019 in Honolulu, Hawaii.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 25, 2019</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1901.09401">"SGD: general analysis and improved rates"</a> - joint work with <a
href="https://perso.telecom-paristech.fr/rgower/">Robert Mansel Gower</a>, <a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas Loizou</a>, <a href="https://qianxunk.github.io">Xun Qian</a>,
<a href="http://www.ali-sa.org">Alibek Sailanbayev</a> and <a href="https://www.linkedin.com/in/egor-shulgin-a34373127">Egor Shulgin</a>.<br>
<br>
Abstract: <i>We propose a general yet simple theorem
describing the convergence of SGD under the arbitrary
sampling paradigm. Our theorem describes the convergence of
an infinite array of variants of SGD, each of which is
associated with a specific probability law governing the
data selection rule used to form minibatches. This is the
first time such an analysis is performed, and most of our
variants of SGD were never explicitly considered in the
literature before. Our analysis relies on the recently
introduced notion of expected smoothness and does not rely
on a uniform bound on the variance of the stochastic
gradients. By specializing our theorem to different
mini-batching strategies, such as sampling with replacement
and independent sampling, we derive exact expressions for
the stepsize as a function of the mini-batch size. With this
we can also determine the mini-batch size that optimizes the
total complexity, and show explicitly that as the variance
of the stochastic gradient evaluated at the minimum grows,
so does the optimal mini-batch size. For zero variance, the
optimal mini-batch size is one. Moreover, we prove
insightful stepsize-switching rules which describe when one
should switch from a constant to a decreasing stepsize
regime.</i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 24, 2019</h3>
<h1>Two New Papers</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1901.08689">"Don’t jump through
hoops and remove those loops: SVRG and Katyusha are better
without the outer loop"</a> - joint work with Dmitry Kovalev
and <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.<br>
<br>
Abstract: <i>The stochastic variance-reduced gradient method
(SVRG) and its accelerated variant (Katyusha) have attracted
enormous attention in the machine learning community in the
last few years due to their superior theoretical properties
and empirical behaviour on training supervised machine
learning models via the empirical risk minimization
paradigm. A key structural element in both of these methods
is the inclusion of an outer loop at the beginning of which
a full pass over the training data is made in order to
compute the exact gradient, which is then used to construct
a variance-reduced estimator of the gradient. In this work
we design loopless variants of both of these methods. In
particular, we remove the outer loop and replace its
function by a coin flip performed in each iteration designed
to trigger, with a small probability, the computation of the
gradient. We prove that the new methods enjoy the same
superior theoretical convergence properties as the original
methods. However, we demonstrate through numerical
experiments that our methods have substantially superior
practical behavior.<br>
<br>
</i> <br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1901.08669">"SAGA with arbitrary
sampling"</a> - joint work with <a
href="https://qianxunk.github.io">Xun Qian</a> and <a
href="https://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a>.<br>
<br>
Abstract: <i>We study the problem of minimizing the average
of a very large number of smooth functions, which is of key
importance in training supervised learn- ing models. One of
the most celebrated methods in this context is the SAGA
algorithm of Defazio et al. (2014). Despite years of
research on the topic, a general-purpose version of SAGA—one
that would include arbitrary importance sampling and
minibatching schemes—does not exist. We remedy this
situation and propose a general and flexible variant of SAGA
following the arbitrary sampling paradigm. We perform an
iteration complexity analysis of the method, largely
possible due to the construction of new stochastic Lyapunov
functions. We establish linear convergence rates in the
smooth and strongly convex regime, and under a quadratic
functional growth condition (i.e., in a regime not assuming
strong convexity). Our rates match those of the primal-dual
method Quartz (Qu et al., 2015) for which an arbitrary
sampling analysis is available, which makes a significant
step towards closing the gap in our understanding of
complexity of primal and dual methods for finite sum
problems.</i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 15, 2019</h3>
<h1>El Houcine Moving on to a New Position</h1>
<br>
<a href="https://vcc.kaust.edu.sa/Pages/Bergou.aspx">El
Houcine Bergou's</a> 1 year postdoc contract in my group
ended; he now a postdoc in <a
href="https://www.kaust.edu.sa/en/study/faculty/panagiotis-kalnis">Panos Kalnis</a>' group here at KAUST. I am looking forward to
further collaboration with El Houcine and Panos.<br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 14, 2019</h3>
<h1>ICML 2019 Deadline Approaching</h1>
<br>
<a href="https://icml.cc/Conferences/2019">ICML</a> deadline
is upon us (on Jan 23)... Everyone in my group is working hard
towards the deadline. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 10, 2019</h3>
<h1>AI Committee Lead</h1>
<br>
I've been asked to lead an Aritificial Intelligence Committee
at KAUST whose role is to prepare a strategic plan for growing
AI research and activities at KAUST over the next 5 years.
This will be a substantial investment, and will involve a
large number of new faculty, research scientist, postdoc and
PhD and MS/PhD positions; investment into computing
infrastructure and more. (The committee started its work in
2018; I am positing the news with some delay...)<br>
<br>
Independently to this, Bernard Ghanem, Marco Canini, Panos
Kalnis and me have established the <a
href="https://ml.kaust.edu.sa">Machine Learning Hub at KAUST</a>,
with the aim to advance ML research and training activities
for the benefit of the entire KAUST community. The website is
only visible from within the KAUST network at the moment. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>January 6, 2019</h3>
<h1>Back @ KAUST: People Counting</h1>
<br>
I am back at KAUST. <a
href="http://maiage.jouy.inra.fr/?q=fr/bergou">El Houcine</a>,
<a href="https://konstmish.github.io">Konstantin</a> and <a
href="https://qianxunk.github.io">Xun</a> are here. <a
href="https://www.aritradutta.com">Aritra</a> is on his way
to <a href="http://wacv19.wacv.net">WACV 2019, Hawaii</a>. <a
href="https://samuelhorvath.github.io">Samuel</a> and <a
href="https://fhanzely.github.io/index.html">Filip</a> will
come back tomorrow. <a
href="https://vcc.kaust.edu.sa/Pages/Sailanbayev.aspx">Alibek</a>
and <a href="https://vcc.kaust.edu.sa/Pages/Gasanov.aspx">Elnur</a>
are arriving soon, too.<br>
<br>
I will have several interns/research visitors from <a
href="https://mipt.ru/science/visiting_prof/piter-rikhtarik-peter-richtarik.php">my  group at MIPT</a> visiting me at KAUST during
January-February: <br>
<br>
- <a href="https://www.researchgate.net/profile/Egor_Shulgin">Egor Shulgin</a> (Jan 6 - Feb 21) <br>
- Dmitry Kamzolov (Jan 10 - Feb 18)<br>
- Vladislav Elsukov (Jan 11 - Feb 15) <br>
- <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>
(Jan 13 - Feb 24) <br>
- Igor Sokolov (Jan 18 - Feb 25)<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 3, 2019</h3>
<h1>Visiting Rado Harman</h1>
<br>
I am visiting <a href="http://www.iam.fmph.uniba.sk/ospm/Harman/">Radoslav
Harman</a> @ Comenius University, Slovakia.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>December 22, 2018</h3>
<h1>Vacation</h1>
<br>
I am on vacation until the end of the year.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>December 22, 2018</h3>
<h1>Paper Accepted to AISTATS 2019          </h1>
<br>
The paper <a href="https://arxiv.org/abs/1809.09354">"Accelerated
coordinate      descent with arbitrary sampling and best rates for minibatches"</a>,
coauthored with <a
href="https://fhanzely.github.io/index.html">Filip Hanzely</a>,
was accepted to <a href="https://www.aistats.org">the 22nd
International Conference on Artificial Intelligence and
Statistics (AISTATS 2019)</a>. The conference will take
place in Naha, Okinawa, Japan, during April 16-18, 2019. The
acceptance email said: <i>"There were 1,111 submissions for
AISTATS this year, of which the program committee accepted
360 for presentation at the conference; among these, 28
papers were accepted for oral presentation, and 332 for
poster presentation."</i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>December 22, 2018</h3>
<h1>I will Deliver Summer School Lectures @ ICCOPT 2019</h1>
<br>
I have accepted an invite to deliver half-a-day worth of
summer school lectures on optimization in machine learning at
the <a href="https://iccopt2019.berlin">International
Conference on Continuous Optimization (ICCOPT 2019)</a>. The
Summer School and the main conference take place in Berlin in
August 2019. The Summer School precedes the main event, and
spans two days: August 3-4. The main conference runs from
August 5 until August 8.<br>
<br>
ICCOPT is the flagship conference series of the <a
href="http://www.mathopt.org">Mathematical Optimization
Society (MOS)</a> on continuous optimization, covering a
wide range of topics in the field. The individual conferences
are typically held once every three years. The last three
editions of the conference took place in <a
href="http://www.iccopt2016.tokyo">Tokyo, Japan (2016)</a>,
<a href="https://eventos.fct.unl.pt/iccopt2013/">Lisbon,
Portugal (2013)</a>, and <a
href="http://iccopt2010.cmm.uchile.cl">Santiago, Chile
(2010)</a>. I attended all three.<br>
<br>
There are two more key conferences in optimization that take
place once in three years; each runs in a differenty year, so
that one takes place every year. They are: <a
href="https://ismp2018.sciencesconf.org">ISMP (International
Symposium on Mathematical Programming)</a> and <a
href="https://www.siam.org/Conferences/CM/Main/op20">OP
(SIAM Conference on Optimization)</a>. The last ISMP took
place in Bordeaux in Summer 2018. The <a
href="https://www.siam.org/Conferences/CM/Main/op20">next OP
conference will be in Hong Kong during May 26-29, 2020.</a>
I am a member of the organizing committee for OP2020 which is
collectively responsible for the selection of invited plenary
and tutorial speakers, summer school lecturers, and the
organization of mini-symposia. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>December 14, 2018</h3>
<h1>Alibek and Samuel Graduated with MS Degrees </h1>
<br>
<a href="https://vcc.kaust.edu.sa/Pages/Sailanbayev.aspx">Alibek</a>
and <a href="https://samuelhorvath.github.io">Samuel</a>
received their MS degrees today. Congratulations! Both will
continue as PhD students in my group as of January 2019. <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>December 14, 2018</h3>
<h1>Meeting Kai-Fu Lee</h1>
<br>
Today I had the great pleasure and honor to meet with
<a href="https://en.wikipedia.org/wiki/Kai-Fu_Lee">Kai-Fu Lee</a>
(CEO of <a
href="http://www.sinovationventures.com/index.php/home/aboutus/teams.html">Sinovation Ventures</a>; former president of Google China; founder
&amp; former managing director of Microsoft Research Asia) for
a 2hr discussion about AI. I recommend that you watch some of
his videos <br>
<br>
<a
href="https://www.ted.com/talks/kai_fu_lee_how_ai_can_save_our_humanity">TED Talk 2018:  How AI Can Save Humanity</a><br>
<a href="https://www.youtube.com/watch?v=oNAFI3Lh97Y">'AI
Superpowers': A Conversation With Kai-Fu Lee</a><br>
<a href="https://www.youtube.com/watch?v=0u2qLxTK4mw">The
Future of AI with Kai-Fu Lee: Udacity Talks</a><br>
<a href="https://www.youtube.com/watch?v=9VlhMfo7mjY">The Race
for AI: Book Talk with Dr. Kai-Fu Lee</a><br>
<br>
and read his most recent book:<br>
<br>
<a
href="https://www.amazon.com/AI-Superpowers-China-Silicon-Valley/dp/132854639X">AI Superpowers:       China, Silicon Valley and the New World Order</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>December 11, 2018</h3>
<h1>Konstantin and Filip Back From Their Internships</h1>
<br>
<a href="https://konstmish.github.io">Konstantin</a> and <a
href="https://fhanzely.github.io/index.html">Filip</a> are
back (from Amazon internship / Microsoft Research visit,
respectively). They stopped by NeurIPS on their way back. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>December 10, 2018</h3>
<h1>Robert Gower @ KAUST</h1>
<br>
The final exam for CS 390FF course is today. <a
href="https://perso.telecom-paristech.fr/rgower/">Robert
Gower</a> arrived at KAUST for a research visit; he will
stay until December 20. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>December 8, 2018</h3>
<h1>Back @ KAUST</h1>
<br>
I am back at KAUST now. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



               <h3>December 2, 2018</h3>
               <h1>Attending NeurIPS 2018</h1>
               <br>
               I have arrived in Montréal to attend the <a
                 href="https://nips.cc">NeurIPS</a> (formerly known as NIPS)
               conference. I was welcome with rain, which this is a good
               thing as far as I am concerned!. Tutorials are starting
               tomorrow; after that we have three days of the main conference
               and then two days of workshops. My group is presening three
               papers accepted to the main conference (paper <a
                 href="https://nips.cc/Conferences/2018/Schedule?showEvent=11220">SEGA</a>,
               <a
                 href="https://nips.cc/Conferences/2018/Schedule?showEvent=11176">ASBFGS</a>
               and <a
                 href="https://nips.cc/Conferences/2018/Schedule?showEvent=11338">SSCD</a>)
               and <a href="https://arxiv.org/abs/1706.07636">one paper
                 accepted to a workshop</a>. <br>
               <br>
               I am using the conference Whova app; feel free to get in
               touch! I am leaving on Thursday evening, so catch me before
               then... I've posted a few job openings we have at KAUST
               through the app: internships in my lab (apply by sending me
               your cv and transcript of university grades), postdoc and
               research scientist positions (apply by sending a cv +
               motivation letter), and <a
                 href="https://apply.interfolio.com/58109">machine learning
                 faculty positions</a> at all ranks (women and junior
               applicants are particularly encouraged to apply). <br>

               <br>
               <br>
               <img alt="" src="imgs/fancy-line.png" width="196" height="36">
               <br>
               <br>



<h3>November 30, 2018</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1811.12403">"New convergence
aspects of stochastic gradient algorithms"</a> - joint work with <a href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-lamnguyen.mltd">Lam M. Nguyen</a>, <a href="https://scl.uconn.edu/people/ha/info.php">Phuong Ha
Nguyen</a>, <a href="https://coral.ise.lehigh.edu/katyas/">Katya Scheinberg</a>, <a
href="https://www.lehigh.edu/engineering/faculty/profiles/takac.html">Martin Takáč</a>, and <a
href="http://www.ee.uconn.edu/marten-van-dijk/">Marten van
Dijk</a>.<br>
<br>
Abstract: <i>The classical convergence analysis of SGD is
carried out under the assumption that the norm of the
stochastic gradient is uniformly bounded. While this might
hold for some loss functions, it is violated for cases
where the objective function is strongly convex. In Bottou
et al. (2016), a new analysis of convergence of SGD is
performed under the assumption that stochastic gradients
are bounded with respect to the true gradient norm. We
show that for stochastic problems arising in machine
learning such bound always holds; and we also propose an
alternative convergence analysis of SGD with diminishing
learning rate regime, which results in more relaxed
conditions than those in Bottou et al. (2016). We then
move on the asynchronous parallel setting, and prove
convergence of Hogwild! algorithm in the same regime in
the case of diminished learning rate. It is well-known
that SGD converges if a sequence of learning rates... ???
In other words, we extend the
current state-of-the-art class of learning rates
satisfying the convergence of SGD.</i><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>
<br>



<h3>November 28, 2018</h3>
<h1>Nicolas Loizou Looking for Jobs</h1>
<br>
<a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
Loizou</a> is on the job market; he will get is PhD in 2019.
He is looking for research positions in academia (Assistant
Prof / postdoc) and industry (Research Scientist). Nicolas
will be at <a href="https://nips.cc">NeurIPS</a> next week,
presenting his work on privacy-preserving randomized gossip
algorithms in the <a
href="https://ppml-workshop.github.io/ppml/">PPML workshop</a>.
At the moment, Nicolas is interning at <a
href="https://research.fb.com/category/facebook-ai-research/">Facebook AI Research (FAIR)</a>, where he has done some great work on
decentralized training of deep learning models, and on
accelerated decentralized gossip communication protocols. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>November 22, 2018</h3>
<h1>NeurIPS 2018 Posters</h1>
<br>
Here are the posters of our papers accepted to this year's <a
href="https://nips.cc">NeurIPS</a>: <br>
<br>
<a href="posters/Poster-ASBFGS.pdf"><img
src="imgs/thmb-ASBFGS.png" alt="ASBFGS poster" border="0"
width="750" height="563"></a><br>
[<a href="https://arxiv.org/abs/1802.04079">paper on arXiv</a>]<br>
<br>
<a href="posters/Poster-SEGA.pdf"><img
src="imgs/thmb-SEGA.png" alt="SEGA poster" border="0"
width="750" height="564"></a><br>
<a href="https://arxiv.org/abs/1809.03054">[paper on arXiv]</a><br>
<br>
<a href="posters/Poster-SSCD.pdf"><img
src="imgs/thmb-SSCD.png" alt="SSCD poster" border="0"
width="600" height="812"></a><br>
<a href="https://arxiv.org/abs/1802.03703">[paper on arXiv]</a><br>
<br>
<br>
<br>
The poster for our <a
href="https://ppml-workshop.github.io/ppml/">Privacy
Preserving Machine Learning</a> NeurIPS workshop paper was
not finalized yet. I will include a link here once it is
ready. Update (November 28): The poster is now ready:<br>
<br>
<a href="posters/Poster-PrivateGossip.pdf"><img
src="imgs/thmb-PrivateGossip.png" alt="Privacy Preserving
Randomized Gossip" border="0" width="600" height="812"></a><br>
<br>
[<a href="https://arxiv.org/abs/1706.07636">full-length paper
on arXiv</a>]<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>November 18, 2018</h3>
<h1>New Postdoc: Xun Qian</h1>
<br>
Xun QIAN just joined my <a
href="https://www.maths.ed.ac.uk/%7Eprichtar/i_team.html">group at KAUST</a> as a postdoc. He has a <a
href="https://repository.hkbu.edu.hk/etd_oa/422/">PhD in
Mathematics (August 2017) from Hong Kong Baptist University.</a>
His PhD thesis is on "Continuous methods for convex
programming and convex semidefinite programming" (<a
href="https://repository.hkbu.edu.hk/cgi/viewcontent.cgi?article=1422&amp;context=etd_oa">pdf</a>), supervised by <a href="http://www.math.hkbu.edu.hk/%7Eliliao/">Li-Zhi Liao</a>.<br>
<br>
Some of Xun's papers:<br>
<br>
H. W. Yue, Li-Zhi Liao, and Xun Qian. Two interior point
continuous trajectory models for convex quadratic programming
with bound constraints, to appear in <i>Pacific Journal on
Optimization</i><br>
<br>
Xun Qian, Li-Zhi Liao, Jie Sun and Hong Zhu. The convergent
generalized central paths for linearly constrained convex
programming, <a
href="https://epubs.siam.org/doi/10.1137/16M1104172"><i>SIAM
Journal on Optimization</i> 28(2):1183-1204, 2018</a><br>
<br>
Xun Qian and Li-Zhi Liao. Analysis of the primal affine
scaling continuous trajectory for convex programming, <a
href="http://www.ybook.co.jp/online2/oppjo/vol14/p261.html"><i>Pacific Journal on Optimization</i> 14(2):261-272, 2018</a><br>
<br>
Xun Qian and Li-Zhi Liao and Jie Sun. Analysis of some
interior point continuous trajectories for convex programming,
<a
href="https://www.tandfonline.com/doi/full/10.1080/02331934.2017.1279160"><i>Optimization</i>
66(4):589-608, 2017</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>






          <h3>November 16, 2018</h3>
                   <h1>Nicolas Visiting MILA</h1>
                   <br>
                   <a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
                     Loizou</a> is giving a talk today at <a
                     href="https://mila.quebec/en/">Mila</a>, University of
                   Montréal. He is speaking about <a
                     href="https://arxiv.org/abs/1712.09677">"Momentum and
                     Stochastic Momentum for Stochastic Gradient, Newton,
                     Proximal Point and Subspace Descent Methods".</a> <br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>



                   <h3>November 13, 2018</h3>
                   <h1>Nicolas Visiting McGill</h1>
                   <br>
                   <a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
                     Loizou</a> is giving a talk today in the Mathematics in
                   Machine Learning Seminar at McGill University. He is speaking
                   about <a href="https://arxiv.org/abs/1712.09677">"Momentum
                     and Stochastic Momentum for Stochastic Gradient, Newton,
                     Proximal Point and Subspace Descent Methods"</a>, joint work
                   with me. <br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>



                   <h3>November 12, 2018</h3>
                   <h1>Statistics and Data Science Workshop @ KAUST</h1>
                   <br>
                   Today I am giving a talk at the <a href="https://stat.kaust.edu.sa/calender/Pages/Page-2018-03-14_11-12-20-AM.aspx">Statistics and Data  Science Workshop</a> held here at KAUST. I am speaking about the <a href="https://arxiv.org/abs/1805.02632">JacSketch paper</a>. Here is a <a href="https://www.youtube.com/watch?v=gjgEck0zU7w&amp;feature=youtu.be">YouTube video</a> of the same talk, one I gave in September at the Simons Institute.<br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>



                   <h3>November 4, 2018</h3>
                   <h1>Paper Accepted to WACV 2019</h1>
                   <br>
                   The paper <a href="https://arxiv.org/abs/1712.02249">"Online
                     and batch incremental video background estimation"</a>,
                   joint work with <a href="https://www.aritradutta.com">Aritra
                     Dutta</a>, has just been accepted to IEEE Winter Conference
                   on Applications of Computer Vision (<a
                     href="http://wacv19.wacv.net">WACV 2019</a>). The conference
                   will take place during January 7-January 11, 2019 in Honolulu,
                   Hawaii. <br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>



                   <h3>November 4, 2018</h3>
                   <h1>Back @ KAUST</h1>
                   <br>
                   I am back from annual leave. <br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>




                   <h3>November 3, 2018</h3>
                   <h1>Paper Accepted to PPML 2018</h1>
                   <br>
                   The paper "A Privacy Preserving Randomized Gossip Algorithm
                   via Controlled Noise Insertion", joint work with Nicolas
                   Loizou, Filip Hanzely, Jakub Konečný and Dmitry Grishchenko,
                   has been accepted to the NIPS Workshop on Privacy-Preserving
                   Machine Learning (PPML 2018). The full-length paper, which
                   includes a number of additional algorithms and results, <a
                     href="https://arxiv.org/abs/1706.07636">can be found on
                     arXiv here</a>.<br>
                   <br>
                   The acceptance email said: "We received an astonishing number
                   of high quality submissions to the Privacy Preserving Machine
                   Learning workshop and we are delighted to inform you that your
                   submission <i>A Privacy Preserving Randomized Gossip
                     Algorithm via Controlled Noise Insertion</i> (57) was
                   accepted to be presented at the workshop."<br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>



                   <h3>November 1, 2018</h3>
                   <h1>New Paper</h1>
                   <br>
                   <span class="important">New paper out: </span><a
                     href="https://arxiv.org/abs/1810.13387">"A stochastic
                     penalty model for convex and nonconvex optimization with big
                     constraints"</a> - joint work with <a
                     href="https://konstmish.github.io">Konstantin Mishchenko</a>.<br>
                   <br>
                   Abstract: <i>The last decade witnessed a rise in the
                     importance of supervised learning applications involving <u>big  data</u> and <u>big models</u>. Big data refers to
                     situations where the amounts of training data available and
                     needed causes difficulties in the training phase of the
                     pipeline. Big model refers to situations where large
                     dimensional and over-parameterized models are needed for the
                     application at hand. Both of these phenomena lead to a
                     dramatic increase in research activity aimed at taming the
                     issues via the design of new sophisticated optimization
                     algorithms. In this paper we turn attention to the <u>big
                       constraints</u> scenario and argue that elaborate machine
                     learning systems of the future will necessarily need to
                     account for a large number of real-world constraints, which
                     will need to be incorporated in the training process. This
                     line of work is largely unexplored, and provides ample
                     opportunities for future work and applications. To handle
                     the big constraints regime, we propose a stochastic penalty
                     formulation which reduces the problem to the well understood
                     big data regime. Our formulation has many interesting
                     properties which relate it to the original problem in
                     various ways, with mathematical guarantees. We give a number
                     of results specialized to nonconvex loss functions, smooth
                     convex functions, strongly convex functions and convex
                     constraints. We show through experiments that our approach
                     can beat competing approaches by several orders of magnitude
                     when a medium accuracy solution is required.</i><br>

                   <br>
                   <br>
                   <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                   <br>
                   <br>



          <h3>November 1, 2018</h3>
          <h1>Aritra and El Houcine @ 2018 INFORMS Annual Meeting</h1>
          <br>
          <a href="https://www.aritradutta.com">Aritra Dutta</a> and <a href="https://vcc.kaust.edu.sa/Pages/Bergou.aspx">El Houcine Bergou</a> are on their way to Phoenix, Arizona, to give talks at the <a
      href="https://www.informs.org/Meetings-Conferences/INFORMS-Conference-Calendar/2018-INFORMS-Annual-Meeting-Phoenix">2018 INFORMS  Annual Meeting</a>. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>October 31, 2018</h3>
                  <h1>New Paper</h1>
                  <br>
                  <span class="important">New paper out: </span><a
                    href="https://arxiv.org/abs/1810.13084">"Provably
                    accelerated randomized gossip algorithms"</a> - joint work
                  with <a href="https://arxiv.org/abs/1810.13084">Nicolas
                    Loizou</a> and <br>
                  <a href="https://research.fb.com/people/rabbat-mike/">Michael
                    G. Rabbat</a>.<br>
                  <br>
                  Abstract: <i>In this work we present novel provably
                    accelerated gossip algorithms for solving the average
                    consensus problem. The proposed protocols are inspired from
                    the recently developed accelerated variants of the
                    randomized Kaczmarz method - a popular method for solving
                    linear systems. In each gossip iteration all nodes of the
                    network update their values but only a pair of them exchange
                    their private information. Numerical experiments on popular
                    wireless sensor networks showing the benefits of our
                    protocols are also presented.</i><br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>


                  <h3>October 31, 2018</h3>
                  <h1>Paper Accepted to AAAI 2019</h1>
                  <br>
                  The paper <a href="https://arxiv.org/abs/1805.07962">"A
                    nonconvex projection method for robust PCA"</a>, joint work
                  with Aritra Dutta and Filip Hanzely, has been accepted to the
                  Thirty-Third AAAI Conference on Artificial Intelligence (<a
                    href="https://aaai.org/Conferences/AAAI-19/">AAAI-19)</a>.
                  The conference will take place during January 27-February 1,
                  2019, in Honolulu, Hawaii, USA.<br>
                  <br>
                  The acceptance email said: "We had a record number of over
                  7,700 submissions this year. Of those, 7,095 were reviewed,
                  and due to space limitations we were only able to accept 1,150
                  papers, yielding an acceptance rate of 16.2%. There was
                  especially stiff competition this year because of the number
                  of submissions, and you should be proud of your success."<br>
                  <br>
                  Abstract:<i> Robust principal component analysis (RPCA) is a
                    well-studied problem with the goal of decomposing a matrix
                    into the sum of low-rank and sparse components. In this
                    paper, we propose a nonconvex feasibility reformulation of
                    RPCA problem and apply an alternating projection method to
                    solve it. To the best of our knowledge, we are the first to
                    propose a method that solves RPCA problem without
                    considering any objective function, convex relaxation, or
                    surrogate convex constraints. We demonstrate through
                    extensive numerical experiments on a variety of
                    applications, including shadow removal, background
                    estimation, face detection, and galaxy evolution, that our
                    approach matches and often significantly outperforms current
                    state-of-the-art in various ways.</i> <br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>October 30, 2018</h3>
                  <h1>Paper Accepted to JASA</h1>
                  <br>
                  The paper <a href="https://arxiv.org/abs/1801.05661">"A
                    randomized exchange algorithm for computing optimal
                    approximate designs of experiments"</a>, joint work with <a
                    href="http://www.iam.fmph.uniba.sk/ospm/Harman/">Radoslav
                    Harman</a> and <a
                    href="http://www.iam.fmph.uniba.sk/ospm/Filova/">Lenka
                    Filová</a>, has been accepted to
                  <meta charset="utf-8">
                  Journal of the American Statistical Association (JASA).<br>
                  <br>
                  Abstract: <i>We propose a class of subspace ascent methods
                    for computing optimal approximate designs that covers both
                    existing as well as new and more efficient algorithms.
                    Within this class of methods, we construct a simple,
                    randomized exchange algorithm (REX). Numerical comparisons
                    suggest that the performance of REX is comparable or
                    superior to the performance of state-of-the-art methods
                    across a broad range of problem structures and sizes. We
                    focus on the most commonly used criterion of D-optimality
                    that also has applications beyond experimental design, such
                    as the construction of the minimum volume ellipsoid
                    containing a given set of data-points. For D-optimality, we
                    prove that the proposed algorithm converges to the optimum.
                    We also provide formulas for the optimal exchange of weights
                    in the case of the criterion of A-optimality. These formulas
                    enable one to use REX for computing A-optimal and I-optimal
                    designs.</i><br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>October 25, 2018</h3>
                  <h1>Annual Leave</h1>
                  <br>
                  I am about to go on an annual leave to an island in the Indian
                  ocean. I will likely have no functioning internet, and will
                  not be reading my emails (maybe I'll read one or two *if* I
                  get internet over there, but do not expect me to respond as
                  the purpose of annual leave is to relax and recharge). I will
                  be back at KAUST and operational on November 4, teaching at
                  9am. <br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>

                  <h3>October 22, 2018</h3>
                  <h1>Sebastian Stich @ KAUST</h1>
                  <br>
                  <a href="https://sstich.ch">Sebastian Stich</a> is visiting me
                  at KAUST. He will stay here for three weeks, and will give a
                  CS seminar talk on November 12. <br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>October 20, 2018</h3>
                  <h1>Filip @ MSR</h1>
                  <br>
                  <a href="https://vcc.kaust.edu.sa/Pages/Hanzely.aspx">Filip Hanzely</a> is visiting <a href="https://www.microsoft.com/en-us/research/people/lixiao/">Lin Xiao</a> at Microsoft Research in Redmond, Washington. He will be back roughly in a month. While in the US, he will also drop by Phoenix to give a talk at the <a
              href="https://www.informs.org/Meetings-Conferences/INFORMS-Conference-Calendar/2018-INFORMS-Annual-Meeting-Phoenix">2018 INFORMS  Annual Meeting</a>.<br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>



                  <h3>October 15, 2018</h3>
                  <h1>Filip Received NIPS Travel award</h1>
                  <br>
                  Congratulations to <a
                    href="https://vcc.kaust.edu.sa/Pages/Hanzely.aspx">Filip
                    Hanzely</a> for receiving a <a
                    href="https://nips.cc/Conferences/2018/Dates">NIPS</a>
                  Travel Award ($1,500). Filip is the coauthor of 2 papers
                  accepted to NIPS this year: <br>
                  <br>
                  <a href="https://arxiv.org/abs/1802.04079">"Accelerated
                    stochastic matrix inversion: general theory and speeding up
                    BFGS rules for faster second-order optimization"</a> - joint
                  work with Robert M. Gower, me, and Sebastian Stich.<br>
                  <br>
                  <a href="https://arxiv.org/abs/1809.03054">"SEGA: Variance
                    reduction via gradient sketching"</a> - joint work with
                  Konstantin Mishchenko and me.<br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>


                  <h3>October 8, 2018</h3>
                  <h1>Paper published in SIAM Journal on Optimization</h1>
                  <br>
                  The paper <i>"Stochastic Primal-Dual Hybrid Gradient
                    Algorithm with Arbitrary Sampling and Imaging Applications"</i>
                  (<a href="https://arxiv.org/abs/1706.04957">arXiv preprint
                    here</a>), coauthored with Antonin Chambolle , Matthias J.
                  Ehrhardt, and Carola-Bibiane Schönlieb, was <a
                    href="https://epubs.siam.org/doi/abs/10.1137/17M1134834">just published  by the SIAM Journal on Optimization</a>.<br>
                  <br>
                  Here are related <a href="talks/TALK-SPDHG.pdf">slides</a>, <a
              href="https://www.maths.ed.ac.uk/%7Eprichtar/posters/Poster-SPDHG.pdf">poster</a>,
                  <a href="https://github.com/mehrhardt/spdhg">GitHub code</a>
                  and a <a href="https://www.youtube.com/watch?v=iZc2eFqS2l4">YouTub talk</a>.<br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36"><br>
                  <br>
                  <br>


                  <h3>October 3, 2018</h3>
                  <h1>Filip back from Amazon internship </h1>
                  <br>
                  <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
                  is now back from an internship at the Amazon Scalable Machine
                  Learning group in Berlin. While there, he was working on
                  Bayesian optimization for deep learning. <br>

                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>


                  <h3>September 25, 2018</h3>
                  <h1>New paper</h1>
                  <br>
                  <span class="important">New paper out: </span><a
                    href="https://arxiv.org/abs/1809.09354">"Accelerated
                    coordinate descent with arbitrary sampling and best rates
                    for minibatches"</a> - joint work with <a
                    href="https://fhanzely.github.io/index.html">Filip Hanzely.</a>
                  <br>
                  <br>
                  Abstract: <i>Accelerated coordinate descent is a widely
                    popular optimization algorithm due to its efficiency on
                    large-dimensional problems. It achieves state-of-the-art
                    complexity on an important class of empirical risk
                    minimization problems. In this paper we design and analyze
                    an accelerated coordinate descent (ACD) method which in each
                    iteration updates a random subset of coordinates according
                    to an arbitrary but fixed probability law, which is a
                    parameter of the method. If all coordinates are updated in
                    each iteration, our method reduces to the classical
                    accelerated gradient descent method AGD of Nesterov. If a
                    single coordinate is updated in each iteration, and we pick
                    probabilities proportional to the square roots of the
                    coordinate-wise Lipschitz constants, our method reduces to
                    the currently fastest coordinate descent method NUACDM of
                    Allen-Zhu, Qu, Richt\'{a}rik and Yuan. While mini-batch
                    variants of ACD are more popular and relevant in practice,
                    there is no importance sampling for ACD that outperforms the
                    standard uniform mini-batch sampling. Through insights
                    enabled by our general analysis, we design new importance
                    sampling for mini-batch ACD which significantly outperforms
                    previous state-of-the-art minibatch ACD in practice. We
                    prove a rate that is at most O(√</i><i><i>τ</i>) times worse
                    than the rate of minibatch ACD with uniform sampling, but
                    can be O(n/τ) times better, where τ is the minibatch size.
                    Since in modern supervised learning training systems it is
                    standard practice to choose τ ≪ n, and often τ=O(1), our
                    method can lead to dramatic speedups. Lastly, we obtain
                    similar results for minibatch nonaccelerated CD as well,
                    achieving improvements on previous best rates.</i><br>


                  <br>
                  <br>
                  <img alt="" src="imgs/fancy-line.png" width="196" height="36">
                  <br>
                  <br>




          <h3>September 23, 2018</h3>
          <h1>Visiting Simons Institute, UC Berkeley</h1>
          <br>
          I am at the Simons Institute, UC Berkeley, attending the
          workshop <a
            href="https://simons.berkeley.edu/data-science-2018-1">"Randomized Numerical   Linear Algebra and Applications"</a>. This workshop is a part of
          the semester-long program <a
            href="https://simons.berkeley.edu/programs/datascience2018">"Foundations of      Data Science"</a>. <br>
          <br>
          My talk is on Tuesday, Sept 25, at 9:30am, PST. I will be
          talking about <a href="https://arxiv.org/abs/1805.02632">"Stochastic Quasi-Gradient Methods: Variance Reduction via Jacobian Sketching"</a> (joint work
          with R.M. Gower and F. Bach). All <a
            href="https://simons.berkeley.edu/workshops/schedule/6681">talks are live-streamed</a> and recorded, and will be uploaded
          onto YouTube. <br>
          <br>
          Update (Sept 25): <a
      href="https://www.youtube.com/watch?v=gjgEck0zU7w&amp;list=PLgKuh-lKre13E9dXSsif4KsGoFp4bjubd&amp;index=9">My talk is on YouTube.</a><br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>September 17, 2018</h3>
          <h1>Area chair for ICML 2019</h1>
          <br>
          I have accepted an invite to serve as an Area Chair for <a
            href="https://icml.cc/Conferences/2019">The 36th
            International Conference on Machine Learning (ICML 2019)</a>.
          The event will be held in Long Beach, California, June 10-15,
          2019.<br>
          <br>
          Submission deadline: January 23, 2019<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>September 14, 2018</h3>
          <h1>Paper published in JMLR</h1>
          <br>
          The paper <a href="http://jmlr.org/papers/v19/16-241.html">"Importance sampling for minibatches"</a>, coauthored with Dominik Csiba, was just
          published by the Journal of Machine Learning Research.<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>August 13, 2018</h3>
          <h1>New Paper &amp; Best Poster Award</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1809.04146">"Nonconvex variance
            reduced optimization with arbitrary sampling"</a> - joint
          work with <a href="https://samuelhorvath.github.io">Samuel
            Horváth.</a> <br>
          <span style="font-style: italic;"></span> <br>
          Abstract: <i>We provide the first importance sampling
            variants of variance reduced algorithms for empirical risk
            minimization with non-convex loss functions. In particular,
            we analyze non-convex versions of SVRG, SAGA and SARAH. Our
            methods have the capacity to speed up the training process
            by an order of magnitude compared to the state of the art on
            real datasets. Moreover, we also improve upon current
            mini-batch analysis of these methods by proposing importance
            sampling for minibatches in this setting. Surprisingly, our
            approach can in some regimes lead to superlinear speedup
            with respect to the minibatch size, which is not usually
            present in stochastic optimization. All the above results
            follow from a general analysis of the methods which works
            with arbitrary sampling, i.e., fully general randomized
            strategy for the selection of subsets of examples to be
            sampled in each iteration. Finally, we also perform a novel
            importance sampling analysis of SARAH in the convex setting.</i><br>
          <br>
          A poster based on the results of this paper won a Best Poster
          Prize. Below I am recycling an earlier blog post I made about
          this in June (the paper was not available online at that
          time):<br>
          <br>
          <span class="important">Best DS3 Poster Award</span>
            for Samuel Horváth<br>
          <br>
          <a
            href="https://vcc.kaust.edu.sa/Pages/Horvath.aspx">Samuel
            Horváth</a> attended the <a
            href="http://www.ds3-datascience-polytechnique.fr">Data
            Science Summer School</a>, which took place during June
          25-29, 2018, at École Polytechnique in Paris, France. Based on
          the <a href="http://www.ds3-datascience-polytechnique.fr">event website</a>, the event gathered 500 participants from 34
          countries and 6 continents, out of which 290 were MS and PhD
          students and postdocs, and 110 professionals. Selected
          guest/speaker names (out of 41): Cédric Villani, Nicoló
          Cesa-Bianchi, Mark Girolami, Yann Lecun, Suvrit Sra,
          Jean-Philippe Vert, Adrian Weller, Marco Cuturi, Arthur
          Gretton, and Andreas Krause.<br>
          <br>
          The event also
          included a best-poster competition, with an impressive total
          of <a
            href="http://www.ds3-datascience-polytechnique.fr/posters/">170 posters</a>. Samuel's poster won the <span
            class="important">Best DS Poster Award.</span> The poster, entitled <i>Nonconvex
            variance reduced optimization with arbitrary sampling</i>,
          is based on a paper of the same title, joint work with me, and
          currently under review.<br>
          <br>
          Here is the poster:<br>
          <br>
          <a href="http://www.ds3-datascience-polytechnique.fr/wp-content/uploads/2018/06/DS3-342.pdf"><img
              src="posters/Poster-DS3-small.png" alt="Poster" border="2"
              width="700" height="526"></a><br>
          <br>
          And here is the award:<br>
          <br>
          <img src="docs/best_poster_DS3.jpg" alt="" width="600"
            height="348"><br>
          <br>
          This first prize carries a 500 EUR cash award.<br>
          <br>
          Samuel: Congratulations!!! <br>
          <br>
          <br>
          Update (October 11, 2018): Here is a <a
      href="https://www.kaust.edu.sa/en/news/Pages/best-poster-award-at-Data-Science-Summer-School.aspx">KAUST News article</a> about this. <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>September 5, 2018</h3>
          <h1>Three papers accepted to NIPS 2018</h1>
          <br>
          The long-awaited decisions arrived today! We've had three
          papers accepted to the Thirty-second Annual Conference on
          Neural Information Processing Systems (NIPS 2018):<br>
          &nbsp;<br>
          <a href="https://arxiv.org/abs/1802.03703">"Stochastic
            spectral and conjugate descent methods"</a> - joint work
          with Dmitry Kovalev, <a
            href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard Gorbunov</a> and Elnur Gasanov.<br>
          <br>
          Abstract: <i>The state-of-the-art methods for solving
            optimization problems in big dimensions are variants of
            randomized coordinate descent (RCD). In this paper we
            introduce a fundamentally new type of acceleration strategy
            for RCD based on the augmentation of the set of coordinate
            directions by a few spectral or conjugate directions. As we
            increase the number of extra directions to be sampled from,
            the rate of the method improves, and interpolates between
            the linear rate of RCD and a linear rate independent of the
            condition number. We develop and analyze also inexact
            variants of these methods where the spectral and conjugate
            directions are allowed to be approximate only. We motivate
            the above development by proving several negative results
            which highlight the limitations of RCD with importance
            sampling.</i><br>
          <br>
          <br>
          <a href="https://arxiv.org/abs/1802.04079"> "Accelerated
            stochastic matrix inversion: general theory and speeding up
            BFGS rules for faster second-order optimization"</a> - joint
          work with <a
            href="https://perso.telecom-paristech.fr/rgower/">Robert M.
            Gower</a>, <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> and <a href="https://sstich.ch">Sebastian Stich</a>.
          <br>
          <br>
          Abstract: <i>We present the first accelerated randomized
            algorithm for solving linear systems in Euclidean spaces.
            One essential problem of this type is the matrix inversion
            problem. In particular, our algorithm can be specialized to
            invert positive definite matrices in such a way that all
            iterates (approximate solutions) generated by the algorithm
            are positive definite matrices themselves. This opens the
            way for many applications in the field of optimization and
            machine learning. As an application of our general theory,
            we develop the first accelerated (deterministic and
            stochastic) quasi-Newton updates. Our updates lead to
            provably more aggressive approximations of the inverse
            Hessian, and lead to speed-ups over classical
            non-accelerated rules in numerical experiments. Experiments
            with empirical risk minimization show that our rules can
            accelerate training of machine learning models.</i><i><br>
          </i><i> </i><br>
          <br>
          <a href="https://arxiv.org/abs/1809.03054">"SEGA: Variance
            reduction via gradient sketching"</a> - joint work with <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          and <a href="https://konstmish.github.io">Konstantin
            Mishchenko</a>.<br>
          <br>
          Abstract: <i>We propose a novel randomized first order
            optimization method—SEGA (SkEtched GrAdient method)—which
            progressively throughout its iterations builds a
            variance-reduced estimate of the gradient from random linear
            measurements (sketches) of the gradient provided at each
            iteration by an oracle. In each iteration, SEGA updates the
            current estimate of the gradient through a
            sketch-and-project operation using the information provided
            by the latest sketch, and this is subsequently used to
            compute an unbiased estimate of the true gradient through a
            random relaxation procedure. This unbiased estimate is then
            used to perform a gradient step. Unlike standard subspace
            descent methods, such as coordinate descent, SEGA can be
            used for optimization problems with a non-separable proximal
            term. We provide a general convergence analysis and prove
            linear convergence for strongly convex objectives. In the
            special case of coordinate sketches, SEGA can be enhanced
            with various techniques such as importance sampling,
            mini-batching and acceleration, and its rate is up to a
            small constant factor identical to the best-known rate of
            coordinate descent.</i><br>
          <br>
          <br>
          As an added bonus, I got a free NIPS registration as one of
          the top-ranked reviewers this year. Thanks NIPS!<br>
          <br>
          The conference will take place during December 3-8, 2018 in
          Montreal, Canada.<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>September 3, 2018</h3>
          <h1>Two new MS/PhD students</h1>
          <br>
          Two new students just joined my group at KAUST:<br>
          <br>
          - Dmitry Kovalev (from Moscow Institute of Physics and
          Technology)<br>
          <br>
          - Elnur Gasanov (from Moscow Institute of Physics and
          Technology)<br>
          <br>
          Welcome!<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>August 29, 2018</h3>
          <h1>People away on internships</h1>
          <br>
          Several people from my group are away on internships. <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          has been with Amazon Scalable Machine Learning group in
          Berlin, Germany, since June and will stay until the end of
          September. <a href="https://konstmish.github.io">Konstantin
            Mishchenko</a> is with Amazon, Seattle, USA since August,
          and will stay there until the end of November. <a
            href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a> is with <a
            href="https://research.fb.com/category/facebook-ai-research/">FAIR</a>
          at Facebook in Montreal, Canada since August and will stay
          there for four months.<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>August 26, 2018</h3>
          <h1>Fall semester started</h1>
          <br>
          The Fall semester is starting at KAUST today. I am teaching
          CS390FF: "Selected Topics in Data Sciences" (Sundays and
          Tuesdays, 9-10:30am in Bldg 9: 4125). <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>August 12, 2018</h3>
          <h1>Attending a workshop on Optimization in Machine Learning @ Lehigh</h1>
          <br>
          I am on my way to Bethlehem, Pennsylvania, to give a talk at the <a href="http://coral.ie.lehigh.edu/%7Emopta/">DIMACS Workshop on Optimization in Machine Learning</a>, taking place at <a href="https://en.wikipedia.org/wiki/Lehigh_University">Lehigh University</a> during August 13-15, 2018. The workshop is part of a larger event which also includes the MOPTA conference (Aug 15-17) and the TRIPODS Summer School for PhD students (Aug 10-12). <br>
          <br>
          I am giving a talk on Tuesday entitled "Stochastic
          quasi-gradient methods: variance reduction via Jacobian
          sketching", joint work with Robert M. Gower and Francis Bach.
          <a href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas Loizou</a> is attending as well; he is presenting a poster
          on Tuesday and giving a talk on Thursday, both on the same
          topic: "Revisiting randomized gossip algorithms", and based on
          these two papers: [<a href="http://arxiv.org/abs/1610.04714">GlobalSIP2016</a>],  [<a href="papers/acc_gossip.pdf">Allerton2018</a>]. <br>
          <br>
          The <a href="http://coral.ie.lehigh.edu/%7Emopta/program">speaker line-up</a> is excellent. On the other hand, the weather in
          Bethlehem does not seem to be particularly welcoming:<br>
          <br>
          <img src="imgs/weather-Lehigh.png" alt="Weather forecast"
            width="680" height="519"><br>
          <br>
          Meanwhile, this is what we are supposed to have at KAUST
          during the same week:<br>
          <br>
          <img src="imgs/weather-KAUST.png" alt="weather forecast"
            width="680" height="520"><br>
          <br>
          I'd welcome a convex combination of the two instead ;-)<br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>August 10, 2018</h3>
          <h1>New paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1808.03045">"Accelerated Bregman
            proximal gradient methods for relatively smooth convex
            optimization"</a> - joint work with <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          and <a
            href="https://www.microsoft.com/en-us/research/people/lixiao/">Lin Xiao</a>. <br>
          <span style="font-style: italic;"></span> <br>
          Abstract: <i>We consider the problem of minimizing the sum of
            two convex functions: one is differentiable and relatively
            smooth with respect to a reference convex function, and the
            other can be nondifferentiable but simple to optimize. The
            relatively smooth condition is much weaker than the standard
            assumption of uniform Lipschitz continuity of the gradients,
            thus significantly increases the scope of potential
            applications. We present accelerated Bregman proximal
            gradient (ABPG) methods that employ the Bregman distance of
            the reference function as the proximity measure. These
            methods attain an O(1/k^</i><i><i>γ</i>) convergence rate in
            the relatively smooth setting, where γ ∈ [1,2] is determined
            by a triangle scaling property of the Bregman distance. We
            develop adaptive variants of the ABPG method that
            automatically ensure the best possible rate of convergence
            and argue that the O(1/k^2) rate is attainable in most
            cases. We present numerical experiments with three
            applications: D-optimal experiment design, Poisson linear
            inverse problem, and relative-entropy nonnegative
            regression. In all experiments, we obtain numerical
            certificates showing that these methods do converge with the
            O(1/k^2) rate.</i><br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>August 7, 2018</h3>
          <h1>Paper accepted to SIAM Journal on Optimization</h1>
          <br>
          The paper <a href="https://arxiv.org/abs/1706.04957">"Stochastic primal-dual  hybrid gradient algorithm with arbitrary sampling and imaging applications"</a>, joint work with Antonin Chambolle,
          Matthias J. Ehrhardt, and Carola-Bibiane Schönlieb, was
          accepted to <i>SIAM Journal on Optimization</i>.<br>
          <br>
          Here is a <a
            href="https://www.youtube.com/watch?v=iZc2eFqS2l4">YouTube
            video</a> of a talk I gave on this topic. Here is the <a
            href="https://github.com/mehrhardt/spdhg">SPDHG GitHub code.</a><br>

              <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>August 4, 2018</h3>
          <h1>Paper accepted to Allerton</h1>
          <br>
          The paper <a href="papers/acc_gossip.pdf">"Accelerated gossip via stochastic heavy ball method"</a>, joint work with <a
            href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas Loizou</a>, was accepted to Allerton (<i>56th Annual Allerton Conference on Communication, Control, and Computing</i>, 2018).  <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>July 27, 2018</h3>
          <h1>NIPS rebuttals</h1>
          <br>
          Working on NIPS author feedback... The deadline is on August 1st.<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>July 23, 2018</h3>
          <h1>Paper published by Linear Algebra and its Applications</h1>
          <br>
          The paper "The complexity of primal-dual fixed point methods
          for ridge regression", coauthored with <a  href="https://docs.ufpr.br/%7Eademir.ribeiro/">Ademir Alves  Ribeiro</a>, just appeared online in <a
      href="https://www.sciencedirect.com/science/article/pii/S0024379518303434"><i>Linear Algebra  and its Applications</i></a>.<br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>July 22, 2018</h3>
          <h1>Plenary talk in Brazil</h1>
          <br>
          I am in Foz do Iguacu, Brazil, attending the conference <a href="http://www.foz2018.com/default">Mathematics and it
            Applications</a> and <a
            href="http://www.foz2018.com/continuous-optimization">XII
            Brazilian Workshop on Continuous Optimization</a>. I will
          give a plenary talk on Thursday.<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>July 16, 2018</h3>
          <h1>New paper</h1>
          <br>
          <span class="important"></span><span class="important">New
            paper out: </span><a href="papers/MACO-highlights.pdf">"Matrix completion  under interval uncertainty: highlights"</a><a> </a>
          <meta http-equiv="Content-Type" content="text/html;
            charset=UTF-8">
          - joint work with <a
            href="https://researcher.watson.ibm.com/person/ie-jakub.marecek">Jakub Mareček</a> and <a href="http://mtakac.com">Martin Takáč.</a>
          To appear in <a href="http://www.ecmlpkdd2018.org"><i>ECML-PKDD</i>
            2018</a>.<br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>July 10, 2018</h3>
          <h1>Most-read paper in Optimization Methods and Software in 2017</h1>

          <br>
          The paper <a href="https://www.tandfonline.com/doi/full/10.1080/10556788.2016.1278445">Distributed Optimization with Arbitrary Local Solvers</a>, joint work with Chenxin Ma, Jakub Konečný, Martin Jaggi, Virginia Smith, Michael I Jordan,
          and Martin Takáč, was the most-read paper in the <a href="https://www.tandfonline.com/toc/goms20/current">OMS journal</a> in year 2017. <br>

          <br>
          This is how I know: I clicked on the "Read our most-read article of 2017 for free here" link available <a href="https://www.tandfonline.com/toc/goms20/current">on this website</a>, and got a nice surprise.<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


 <h3>July 9, 2018</h3>
<h1>New paper</h1>
<br>

<span class="important">New paper out:</span> <a href="papers/acc_gossip.pdf">"Accelerated gossip via stochastic heavy ball method"</a><a> </a>- joint work with <a href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas Loizou.</a> <br>

<br>Abstract: <i>In this paper we show how the stochastic heavy ball method (SHB)—a popular method for solving stochastic convex and non-convex optimization problems—operates as a randomized gossip algorithm. In particular, we focus on two special cases of SHB: the Randomized Kaczmarz method with momentum and its block variant. Building upon a recent framework for the design and analysis of randomized gossip algorithms [19] we interpret the distributed nature of the proposed methods. We present novel protocols for solving the average consensus problem where in each step all nodes of the network update their values but only a subset of them exchange their private values. Numerical experiments on popular wireless sensor networks showing the benefits of our protocols are also presented.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




          <h3>July 3, 2018</h3>
          <h1>Editor @ OMS</h1>
          <br>
          <span class="important"></span>I have joined the Editorial
          Board of <a
            href="https://www.tandfonline.com/toc/goms20/current">Optimization Methods  and Software</a>.
	<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>July 1, 2018</h3>
          <h1>23rd International Symposium on Mathematical Programming</h1>
          <br>
          I am on my way to Bordeaux, to attend <a
            href="https://ismp2018.sciencesconf.org">ISMP (23rd
            International Symposium on Mathematical Programming)</a>.
          With Alexandre d’Aspremont, Olivier Beaumont, and Suvrit Sra,
          we have organized stream 4a: "Learning: Machine Learning, Big
          Data, Cloud Computing, and Huge-Scale Optimization". Here is
          the schedule of talks which are based on papers I am co-author
          of (highlighted in red):<br>
          <br>
          <meta http-equiv="Content-Type" content="text/html;
            charset=UTF-8">
          <p class="MsoNormal"><b style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt">Coordinate Descent and Randomized Direct Search Methods (Continuous Optimization</span></b><b style="mso-bidi-font-weight:
      normal"><span style="font-size:10.0pt;mso-ansi-language:EN-GB" lang="EN-GB">)<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"><br>RandomM - Mo 3:15pm-4:45pm, Format: 3x30 min <br>Room: Salle KC6 Building: K, Intermediate 1, Zone: 10 <br>Invited Session 211<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"><br>Organizer: Martin Takáč, Lehigh University, US<o:p></o:p></span></b></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">1 - When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent<o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Asu Ozdaglar, MIT, US, talk 1486<o:p></o:p></span><span style="font-size:10.0pt"><br>Co-Authors: Mert Gurbuzbalaban, Nuri Vanli, Pablo Parrilo <o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">2 - Random direct search method for unconstrained smooth minimization<o:p></o:p></span><span style="font-size:10.0pt;color:red">
      Speaker: El houcine Bergou, KAUST-INRA, SA, talk 421<o:p></o:p></span><span style="font-size:10.0pt;color:red">
      Co-Authors: Peter Richtárik, Eduard Gorbunov<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">3 - Active Metric Learning for Supervised Classification<o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Dimitri Papageorgiou, ExxonMobil, US, talk 1275 <o:p></o:p></span><span style="font-size:10.0pt">
      Co-Authors: Krishnan Kumaran, Martin Takáč <o:p></o:p></span>
      </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p><b>Machine learning and sparse optimisation (Continuous Optimization)
      NLP - Tu 8:30am-10:30am, Format: 4x30 min <br>Room: Salle 05 Building: Q, 1st floor, Zone: 11<br>Invited Session 109<br>Organizer: Coralia Cartis, University of Oxford, GB </b> </o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">1 - Condition numbers and weak average-case complexity in optimization<o:p></o:p></span><span style="font-size:10.0pt"><br>Speaker: Martin Lotz, The University of Manchester, GB, talk 957 <o:p></o:p></span><span style="font-size:10.0pt"><br>Co-Authors: Dennis Amelunxen, Jake Walvin<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">2 - A Long (Random) Walk Solves All Your (Linear) Problems <o:p></o:p></span><span style="font-size:10.0pt;color:red"><br>Speaker: Armin Eftekhari, Alan Turing Institute, GB, talk 1199<o:p></o:p></span><span style="font-size:10.0pt;color:red">
      Co-Authors: Martin Lotz, Peter Richtárik<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">3 - Manifold lifting: problems and methods <o:p></o:p></span><span style="font-size:10.0pt"><br>Speaker: Florentin Goyens, Oxford University, GB, talk 1182 <o:p></o:p></span><span style="font-size:10.0pt">
      Co-Authors: Armin Eftekhari, Coralia Cartis, Greg Ongie<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">4 - Sparse non-negative super-resolution: simplified and stabilized<o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Jared Tanner, University of Oxford, GB, talk 1462
      Co-Authors: Armin Eftekhari, Andrew Thompson, Bogdan Toader, Hemant Tyagi <o:p></o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><b style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt"><o:p></o:p></span></b><b style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt">Recent advances in first-order algorithms for non-smooth optimization (Continuous Optimization)<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
      NonSmooth - We 8:30am-10:30am, Format: 4x30 min<br>Room: Salle LC4 Building: L, Intermediate 1, Zone: 9 <o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
      Invited Session 198<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
      Organizer: Thomas Pock, Graz University of Technology, AT <o:p></o:p></span></b> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">1 - Non-smooth Non-convex Bregman Minimization: Unification and New Algorithms<o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Peter Ochs, Saarland University, DE, talk 134 <o:p></o:p></span><span style="font-size:10.0pt">
      Co-Authors: Jalal Fadili <o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">2 - Primal-dual algorithm for linearly constrained optimization problem<o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Yura Malitsky, Univeristy of Göttingen, DE, talk 155 <o:p></o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">3 - Stochastic PDHG with Arbitrary Sampling and Applications to Medical Imaging<o:p></o:p></span><span style="font-size:10.0pt;color:red">
      Speaker: Matthias Ehrhardt, University of Cambridge, GB, talk 127 <o:p></o:p></span><span style="font-size:10.0pt;color:red">
      Co-Authors: Carola Schoenlieb, Peter Richtárik, Antonin Chambolle<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">4 - Acceleration and global convergence of the NL- PDHGM<o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Stanislav Mazurenko, University of Liverpool, GB, talk 1655 <o:p></o:p></span><span style="font-size:10.0pt">
      Co-Authors: Tuomo Valkonen, C. Clason<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"><o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">Fast Converging Stochastic Optimization Algorithms (Continuous Optimization)<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
      RandomM - We 3:15pm-4:45pm, Format: 3x30 min <br>Room: Salle KC6 Building: K, Intermediate 1, Zone: 10 <o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
      Invited Session 213<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
      Organizer: Francis Bach, INRIA - ENS, FR&nbsp;</span></b> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">1 - Bridging the Gap between Constant Step Size SGD and Markov Chains<o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Aymeric Dieuleveut, EPFL, CH, talk 1102 <o:p></o:p></span><span style="font-size:10.0pt">
      Co-Authors: Alain Durmus, Francis Bach <o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">2 - Stochastic Optimization for Large Scale Optimal Transport</span><span style="font-size:10.0pt">
      Speaker: Aude Genevay, ENS, FR, talk 888<o:p></o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">3 - Variance Reduced Methods via Sketching&nbsp;</span><span style="font-size:10.0pt;color:red">
      Speaker: Robert Gower, Telecom Paristech, FR, talk 859 <o:p></o:p></span><span style="font-size:10.0pt;color:red">
      Co-Authors: Peter Richtárik, Francis Bach<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"><o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">Non-Convex and Second-order Methods in Machine Learning (Continuous Optimization)<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
      RandomM - We 5:00pm-6:30pm, Format: 4x20 min <br>Room: Salle KC6 Building: K, Intermediate 1, Zone: 10 <o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b>
      <b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">Invited Session 33</span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
      Organizer: Martin Takáč, Lehigh University, US <o:p></o:p></span></b> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">1 - Escaping Saddles with Stochastic Algorithms <o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Aurelien Lucchi, ETH Zurich, CH, talk 531<o:p></o:p></span></p>
          <p class="MsoNormal" style="mso-outline-level:1"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">2 - Convergence Rate of Expectation-Maximization <o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Reza Babanezhad, UBC, CA, talk 1135 <o:p></o:p></span><span style="font-size:10.0pt">
      Co-Authors: Raunak Kumar, Mark Schmidt <o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">3 - Parameter-free nonsmooth convex stochastic optimization through coin betting<o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Francesco Orabona, Stony Brook University, US, talk 1108<o:p></o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">4 - SGD and Hogwild! Convergence Without the Bounded Gradients Assumption<o:p></o:p></span><span style="font-size:10.0pt;color:red">
      Speaker: Martin Takáč, Lehigh University, US, talk 1342 <o:p></o:p></span><span style="font-size:10.0pt;color:red">
      Co-Authors: Lam Nguyen, Phuong Nguyen, Marten van Dijk, Peter Richtárik, Katya Scheinberg <o:p></o:p></span> </p>
          <p class="MsoNormal"><b style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt">First-order methods for large-scale convex problems (Specific Models, Algorithms, and Software Learning) <o:p></o:p></span></b><b style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt">
      Th 8:30am-10:30am, Format: 4x30 min <br>Room: FABRE Building: J, Ground Floor, Zone: 8 <o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"><br>Invited Session 316<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
      Organizer: Stephen Vavasis, University of Waterloo, CA <o:p></o:p></span></b></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">1 - A single potential governing convergence of CG, AG and geometric descent<o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Stephen Vavasis, University of Waterloo, CA, talk 582 <o:p></o:p></span><span style="font-size:10.0pt">
      Co-Authors: Sahar Karimi<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">2 - Robust Accelerated Gradient Method<o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Mert Gurbuzbalaban, Rutgers University, US, talk 1106<o:p></o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">3 - Randomized methods for convex feasibility problems and applications to ML<o:p></o:p></span><span style="font-size:10.0pt;color:red">
      Speaker: Peter Richtárik, KAUST, SA, talk 385<o:p></o:p></span><span style="font-size:10.0pt;color:red">
      Co-Authors: Ion Necoara, Andrei Patrascu<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">4 - Bregman Divergence for Stochastic Variance Reduction<o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Yaoliang Yu, University of Waterloo, CA, talk 937 <o:p></o:p></span><span style="font-size:10.0pt">
      Co-Authors: Xinhua Zhang, Zhan Shi<o:p></o:p></span><span style="font-size:10.0pt"><o:p> </o:p></span><b><span style="font-size:10.0pt"></span><span style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt"></span></span></b></p>
          <p class="MsoNormal"><b><span style="mso-bidi-font-weight:
      normal"><span style="font-size:10.0pt">Recent Advances in Coordinate Descent and Constrained Problems (Continuous Optimization)<br>RandomM - Fr 9:00am-10:30am, Format: 3x30 min
      Room: Salle KC6 Building: K, Intermediate 1, Zone: 10
      Invited Session 208
      Organizer: Ion Necoara, Univ. Politehnica Bucharest, RO</span></span></b><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red"></span> </p>
          <span style="font-size:10.0pt; color:red"></span>
          <p class="MsoNormal"><span style="font-size:10.0pt; color:red">1 - Convergence Analysis of Inexact Randomized Iterative Methods<o:p></o:p></span><span style="font-size:10.0pt;color:red">
      Speaker: Nicolas Loizou, University of Edinburgh, GB, talk 835 <o:p></o:p></span><span style="font-size:10.0pt;color:red">
      Co-Authors: Peter Richtárik<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">2 - A Stochastic Penalty Model for Optimization with Many Convex Constraints<o:p></o:p></span><span style="font-size:10.0pt;color:red">
      Speaker: Konstantin Mishchenko, KAUST, SA, talk 1264
      Co-Authors: Peter Richtárik, Ion Necoara<o:p></o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">3 - Random coordinate descent methods for linearly constrained convex optimization<o:p></o:p></span><span style="font-size:10.0pt">
      Speaker: Ion Necoara, Univ. Politehnica Bucharest, RO, talk 809
      </span><span style="font-size:10.0pt">Co-Authors: Martin Takáč </span></p>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>June 29, 2018</h3>
          <h1> Best DS3 poster award for Samuel Horváth</h1>
          <br>
          <span class="important"></span><a
            href="https://vcc.kaust.edu.sa/Pages/Horvath.aspx">Samuel
            Horváth</a> attended the <a
            href="http://www.ds3-datascience-polytechnique.fr">Data
            Science Summer School</a> (DS3), which took place during June
          25-29, 2018, at École Polytechnique in Paris, France. Based on
          the <a href="http://www.ds3-datascience-polytechnique.fr">event website</a>, the event gathered 500 participants from 34
          countries and 6 continents, out of which 290 were MS and PhD
          students and postdocs, and 110 professionals. Selected
          guest/speaker names (out of 41): Cédric Villani, Nicoló
          Cesa-Bianchi, Mark Girolami, Yann Lecun, Suvrit Sra,
          Jean-Philippe Vert, Adrian Weller, Marco Cuturi, Arthur
          Gretton, and Andreas Krause.<br>
          <br>

         The event also
          included a best-poster competition, with an impressive total
          of <a
            href="http://www.ds3-datascience-polytechnique.fr/posters/">170 posters</a>. Samuel's poster won the <span
            class="important">Best DS Poster Award.</span>

          The poster, entitled <i>Nonconvex
            variance reduced optimization with arbitrary sampling</i>,
          is based on a paper of the same title, joint work with me, and
          currently under review.<br>
          <br>
          Here is the poster:<br>
          <br> <a  href="http://www.ds3-datascience-polytechnique.fr/wp-content/uploads/2018/06/DS3-342.pdf"><img
              src="posters/Poster-DS3-small.png" alt="Poster" border="2"
              width="700" height="526"></a><br>
          <br>
          And here is the award:<br>
          <br>
          <img src="docs/best_poster_DS3.jpg" alt="" width="600"
            height="348"><br>
          <br>
          This first prize carries a 500 EUR cash award.<br>
          <br>
          Samuel: Congratulations!!! <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>June 18, 2018</h3>
          <h1>I am visiting Edinburgh</h1>
          <br>
          I am now in Edinburgh for a week. On Tuesday, I am giving a talk in the <a href="http://www.anc.ed.ac.uk/events/aggregator">ANC Seminar</a> (School of Informatics), and on Wednesday I am giving the same talk in the <a href="http://www.maths.ed.ac.uk/ERGO/abstracts/2018-06-richtarik.html">ERGO Seminar</a> (School of Mathematics). <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>June 15, 2018</h3>
          <h1>New paper</h1>
          <br>
          <span class="important">New paper out: </span><a href="https://arxiv.org/abs/1806.05633">"Improving SAGA via
            a probabilistic interpolation with gradient descent"</a> - joint work with <a href="http://www.adelbibi.com">Adel Bibi</a>,
          <a href="https://vcc.kaust.edu.sa/Pages/Sailanbayev.aspx">Alibek Sailanbayev</a>, <a href="http://www.bernardghanem.com">Bernard Ghanem</a> and <a href="https://perso.telecom-paristech.fr/rgower/">Robert Mansel Gower</a>.<br>
          <br>
          Abstract:<span style="font-style: italic;"> We develop and
            analyze a new algorithm for empirical risk minimization,
            which is the key paradigm for training supervised machine
            learning models. Our method---SAGD---is based on a
            probabilistic interpolation of SAGA and gradient descent
            (GD). In particular, in each iteration we take a gradient
            step with probability $q$ and a SAGA step with probability
            $1−q$. We show that, surprisingly, the total expected
            complexity of the method (which is obtained by multiplying
            the number of iterations by the expected number of gradients
            computed in each iteration) is minimized for a non-trivial
            probability $q$. For example, for a well conditioned problem
            the choice $q=1/(n−1)^2$, where $n$ is the number of data
            samples, gives a method with an overall complexity which is
            better than both the complexity of GD and SAGA. We further
            generalize the results to a probabilistic interpolation of
            SAGA and minibatch SAGA, which allows us to compute both the
            optimal probability and the optimal minibatch size. While
            the theoretical improvement may not be large, the practical
            improvement is robustly present across all synthetic and
            real data we tested for, and can be substantial. Our
            theoretical results suggest that for this optimal minibatch
            size our method achieves linear speedup in minibatch size,
            which is of key practical importance as minibatch
            implementations are used to train machine learning models in
            practice. This is the first time linear speedup in minibatch
            size is obtained for a variance reduced gradient-type method
            by directly solving the primal empirical risk minimization
            problem.</span><br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>June 10, 2018</h3>
          <h1>10th traditional youth school in control, information
            &amp; optimization</h1>
          <br>
          I am in Voronovo, Russia, attending the Traditional Youth School in <a
            href="https://cs.hse.ru/tradschool/2018/">"Control,
            Information and Optimization"</a> organized by <a
            href="http://lab7.ipu.ru/eng/people/polyak.html">Boris
            Polyak</a>&nbsp; and Elena Gryazina. This is the 10th
          edition of the school. I will be teaching a 3h module on
          stochastic methods in optimization and machine learning.<br>
          <br>
          <i>Update 1: </i>Slides from my two talks: <a href="talks/2018%20Voronovo%20-%20TALK%201.pdf">TALK 1</a>,
          <a href="talks/2018%20Voronovo%20-%20TALK%202.pdf">TALK 2</a>.<br>
            <br>
          <i>Update 2:</i> <a href="https://www.hse.ru/en/staff/nikitadoikov">Nikita
            Doikov</a> won the Best Talk Award for the paper <a
            href="https://arxiv.org/abs/1802.04084">"Randomized Block Cubic Newton Method"</a>, to appear in ICML 2018.<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>June 1, 2018</h3>
          <h1>Jingwei Liang @ KAUST</h1>
          <br>
          <span class="important"></span><a href="http://www.damtp.cam.ac.uk/people/jl993/">Jingwei Liang</a> (Cambridge) is visiting me at KAUST.<br>

         <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>May 21, 2018</h3>
          <h1>Adil Salim @ KAUST</h1>
          <br>
          <a href="https://adil-salim.github.io">Adil Salim</a> (Télécom
          ParisTech) is visiting me at KAUST this week.<br>

         <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>May 21, 2018</h3>
          <h1>New paper</h1>
          <br>
          <span class="important"></span><span class="important">New
            paper out: </span><a
            href="https://arxiv.org/abs/1805.07962">"A nonconvex
            projection method for robust PCA"</a> - joint work with <a
            href="https://vcc.kaust.edu.sa/Pages/Dutta.aspx">Aritra
            Dutta</a> and <a
            href="https://vcc.kaust.edu.sa/Pages/Hanzely.aspx">Filip
            Hanzely</a>.<br>
          <br>
          Abstract:<span style="font-style: italic;"> Robust principal
            component analysis (RPCA) is a well-studied problem with the
            goal of decomposing a matrix into the sum of low-rank and
            sparse components. In this paper, we propose a nonconvex
            feasibility reformulation of RPCA problem and apply an
            alternating projection method to solve it. To the best of
            our knowledge, we are the first to propose a method that
            solves RPCA problem without considering any objective
            function, convex relaxation, or surrogate convex
            constraints. We demonstrate through extensive numerical
            experiments on a variety of applications, including shadow
            removal, background estimation, face detection, and galaxy
            evolution, that our approach matches and often significantly
            outperforms current state-of-the-art in various ways.</span><br>


         <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>May 19, 2018</h3>
          <h1> NIPS deadline over!</h1>
          <br>
          The NIPS deadline is over now. Me and my group members will probably spend a few days  sleeping...<br>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>May 11, 2018</h3>
          <h1> Two papers accepted to ICML</h1>
          <br>

          We have got two papers accepted to ICML 2018:<br>

          <br>
          1) <a href="https://arxiv.org/abs/1802.04084"> Randomized block cubic Newton method</a> (with Nikita
          Doikov)<br>

          <br>
          2) <a href="https://arxiv.org/abs/1802.03801"> SGD and Hogwild! convergence without the bounded
            gradients assumption</a> (with Lam M. Nguyen, Phuong Ha Nguyen, Marten van Dijk, Katya Scheinberg and Martin Takáč)<br>
            <br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>May 1, 2018</h3>
          <h1> New paper</h1>
          <br>
          <span class="important"></span><span class="important">New
            paper out: </span><a
            href="https://arxiv.org/abs/1805.02632">"Stochastic
            quasi-gradient methods: variance reduction via Jacobian
            sketching"</a> - joint work with <a
            href="https://perso.telecom-paristech.fr/rgower/">Robert
            Gower</a> and <a href="http://www.di.ens.fr/%7Efbach/">Francis Bach.</a><br>
          <br>
          Abstract:<span style="font-style: italic;"> We develop a new
            family of variance reduced stochastic gradient descent
            methods for minimizing the average of a very large number of
            smooth functions. Our method---JacSketch---is motivated by
            novel developments in randomized numerical linear algebra,
            and operates by maintaining a stochastic estimate of a
            Jacobian matrix composed of the gradients of individual
            functions. In each iteration, JacSketch efficiently updates
            the Jacobian matrix by first obtaining a random linear
            measurement of the true Jacobian through (cheap) sketching,
            and then projecting the previous estimate onto the solution
            space of a linear matrix equation whose solutions are
            consistent with the measurement. The Jacobian estimate is
            then used to compute a variance-reduced unbiased estimator
            of the gradient, followed by a stochastic gradient descent
            step. Our strategy is analogous to the way quasi-Newton
            methods maintain an estimate of the Hessian, and hence our
            method can be seen as a stochastic quasi-gradient method.
            Indeed, quasi-Newton methods project the current Hessian
            estimate onto a solution space of a linear equation
            consistent with a certain linear (but non-random)
            measurement of the true Hessian. Our method can also be seen
            as stochastic gradient descent applied to a controlled
            stochastic optimization reformulation of the original
            problem, where the control comes from the Jacobian
            estimates.<br>
            <br>
            We prove that for smooth and strongly convex functions,
            JacSketch converges linearly with a meaningful rate dictated
            by a single&nbsp; convergence theorem which applies to
            general sketches. We also provide a refined convergence
            theorem which applies to a smaller class of sketches,
            featuring a novel proof technique based on a stochastic
            Lyapunov function. This enables us to obtain sharper
            complexity results for variants of JacSketch with importance
            sampling. By specializing our general approach to
            specific&nbsp; sketching strategies, JacSketch reduces to
            the celebrated stochastic average gradient (SAGA) method,
            and its several existing and many new minibatch, reduced
            memory, and importance sampling variants. Our rate for SAGA
            with importance sampling is the current best-known rate for
            this method, resolving a conjecture by Schmidt et al (2015).
            The rates we obtain for minibatch SAGA are also superior to
            existing rates. Moreover, we obtain the first minibatch SAGA
            method with importance sampling.</span><br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>April 29, 2018</h3>
          <h1> Seminar talks at University of Birmingham and Warwick</h1>
          <br>
          <span class="important"></span>I am on my way to Birmingham,
          and then Coventry. I will be giving a talk at the <a
      href="https://warwick.ac.uk/fac/cross_fac/dimap/seminars/#010518Richtarik">DIMAP</a>
          seminar (DIMAP = Centre for Discrete Mathematics and its
          Applications), <a href="https://warwick.ac.uk">University of
            Warwick</a>, on "Stochastic Quasi-Gradient Methods: Variance
          Reduction via Jacobian Sketching". The talk is based on joint
          work with <a href="https://perso.telecom-paristech.fr/rgower/">Robert M.
            Gower</a> and <a href="http://www.di.ens.fr/%7Efbach/">Francis Bach</a>.<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>April 25, 2018</h3>
          <h1>Teaching Saudi math olympiad contestants</h1>
          <br>
          Today and tomorrow I am teaching a mini-course on "Optimization for Machine Learning" for students from
          various Saudi universities who were previously contestants in <a href="https://en.wikipedia.org/wiki/KFUPM_mathematics_olympiad">Saudi National  Mathematical Olympiad</a><span style="font-style: italic;"> </span>and/or
          <a href="https://www.imo-official.org/country_individual_r.aspx?code=SAU">IMO</a>. Several current contestants are
          attending as well. <br>
          <br>  This is a collaborative effort with <a href="https://www.kaust.edu.sa/en/study/faculty/diogo-gomes">Diogo Gomes</a>, who is teaching a "Mathematica" mini-course. <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>April 18, 2018</h3>
          <h1>New paper</h1>
          <br>
          <span class="important">New
            paper out: </span><a
            href="https://arxiv.org/abs/1804.06252">"Weighted low-rank
            approximation of matrices and background modeling"</a> -
          joint work with <a
            href="https://vcc.kaust.edu.sa/Pages/Dutta.aspx">Aritra
            Dutta</a> and <a
            href="https://sciences.ucf.edu/math/people/li-xin-2/">Xin Li</a>.<br>
          <br>
          Abstract:<span style="font-style: italic;"> We primarily study
            a special a weighted low-rank approximation of matrices and
            then apply it to solve the background modeling problem. We
            propose two algorithms for this purpose: one operates in the
            batch mode on the entire data and the other one operates in
            the batch-incremental mode on the data and naturally
            captures more background variations and computationally more
            effective. Moreover, we propose a robust technique that
            learns the background frame indices from the data and does
            not require any training frames. We demonstrate through
            extensive experiments that by inserting a simple weight in
            the Frobenius norm, it can be made robust to the outliers
            similar to the L1 norm. Our methods match or outperform
            several state-of-the-art online and batch background
            modeling methods in virtually all quantitative and
            qualitative measures.</span><br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>April 16, 2018</h3>
          <h1>I am giving a seminar talk @ KAUST</h1>
          <br>
          I am giving a talk today at the CS Graduate Seminar at KAUST. I will be talking about <a href="https://arxiv.org/abs/1801.04873">"Randomized Methods for Convex Feasibility Problems"</a>. This is joint work
          with Ion Necoara and Andrei Patrascu.<br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>April 5, 2018</h3>
          <h1>Postdoc and research scientist vacancies</h1>
          <br>
          My lab has openings for&nbsp;<span class="important">postdoc</span>
          (straight after PhD, or a few years after PhD) and&nbsp;<span
            class="important">research scientist</span> (several to many
          years after PhD; similar to a RS position at big data
          companies such as Google, Microsoft Research, Amazon, Baidu,
          Tencent, Facebook) positions. <br>
          <br>
          <span style="font-weight: bold;">Relevant areas:</span>
          machine learning theory, optimization, algorithms, high
          performance computing, deep learning, randomized and
          stochastic algorithms, federated learning, computer vision,
          machine learning systems, data science, applied mathematics,
          theoretical computer science. Contact me by email if
          interested. Please send your CV (including publication
          record), a brief statement of interest, 3 reference letters
          (and PhD transcript for postdoc applicants).<br>
          <br>
          <span style="font-weight: bold;">Place of work:</span> <a
            href="https://www.kaust.edu.sa/en">KAUST</a>. Outstanding
          working conditions.<br>
          <br>
          <span style="font-weight: bold;">Starting date:</span> Fall
          2018 (flexible). <br>
          <br>
          <span style="font-weight: bold;">Contract duration:</span>
          based on agreement (e.g., 1-3 years).<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>April 2, 2018</h3>
          <h1> Dominik's PhD thesis online</h1>
          <br>
          <a href="http://www.dominikcsiba.com">Dominik Csiba's</a> PhD
          thesis <a href="https://arxiv.org/abs/1804.00437">"Data
            sampling strategies in stochastic algorithms for empirical
            risk minimization"</a> is online now. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>March 2, 2018</h3>
          <h1>Vacation</h1>
          <br>
          I am on vacation this week. <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>March 23, 2018</h3>
          <h1>Konstantin and Filip @ INFORMS Opt</h1>
          <br>
          <a href="https://konstmish.github.io">Konstantin</a> and <a
            href="https://vcc.kaust.edu.sa/Pages/Hanzely.aspx">Filip</a>
          are attending the <a
      href="https://www.informs.org/Meetings-Conferences/INFORMS-Conference-Calendar/2018-INFORMS-Optimization-Society-Conference">2018
      INFORMS   Optimization Society Conference</a> in Denver, Colorado. <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>March 21, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important"></span><span class="important">New
            paper out: </span><a
            href="https://arxiv.org/abs/1803.07374">"Fastest rates for
            stochastic mirror descent methods"</a> - joint work with <a
            href="https://vcc.kaust.edu.sa/Pages/Hanzely.aspx">Filip
            Hanzely</a>.<br>
          <br>
          Abstract:<span style="font-style: italic;"> Relative
            smoothness - a notion introduced by Birnbaum et al. (2011)
            and rediscovered by Bauschke et al. (2016) and Lu et al.
            (2016) - generalizes the standard notion of smoothness
            typically used in the analysis of gradient type methods. In
            this work we are taking ideas from well studied field of
            stochastic convex optimization and using them in order to
            obtain faster algorithms for minimizing relatively smooth
            functions. We propose and analyze two new algorithms:
            Relative Randomized Coordinate Descent (relRCD) and Relative
            Stochastic Gradient Descent (relSGD), both generalizing
            famous algorithms in the standard smooth setting. The
            methods we propose can be in fact seen as variants of
            stochastic mirror descent. One of them, relRCD is the first
            stochastic mirror descent algorithm with a linear
            convergence rate. </span> <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>March 18, 2018</h3>
          <h1> A Student from TUM Doing her MS Thesis Under my
            Supervision</h1>
          <br>
          <span class="important"></span>Sarah Sachs, a master student
          from <a href="https://www.tum.de/en/homepage/">Technical
            University Munich (TUM)</a>, arrived at KAUST today. She
          will spend six months at KAUST (until early September) as a
          visiting student in my group, and will write her master's
          thesis under my supervision. In her thesis she is focusing on
          randomized optimization algorithms. Welcome!<br>
          <br>
          Sarah's bachelor thesis at TUM focused on approximation of the
          infimal convolution for non-convex functions. She&nbsp;
          previously worked on finding efficiently computable stopping
          criteria for ADMM and the Chambolle-Pock algorithm applied to
          LP relaxations of ILPs with integral extreme points. She is
          generally interested in optimization with applications to
          computer vision.<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>March 4, 2018</h3>
          <h1>Konstantin @ Cambridge &amp; Vatican<br>
          </h1>
          <br>
          <span class="important"></span><a
            href="https://konstmish.github.io">Konstantin Mishchenko</a>
          is visiting the Cambridge Image Analysis group of <a
            href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">Carola-Bibiane Schönlieb</a> at the University of Cambridge. During March
          8-11 he is participating in <a href="https://vhacks.org">VHacks</a>,
          the first ever hackathon at the Vatican. <br>
          <br>
          Aritra and El Houcine are also travelling. <br>
          <br>
          Update (March 19): Konstantin, El Houcine and Aritra are back.<br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>February 25, 2018</h3>
          <h1> Visiting "Matfyz"</h1>
          <br>
          <span class="important"></span>I am on my way to Bratislava,
          Slovakia. Tomorrow, I am giving a <a
      href="https://fmph.uniba.sk/detail-novinky/back_to_page/fakulta-matematiky-fyziky-a-informatiky-uk/article/seminar-z-matematickej-statistiky-peter-richtarik-2622018/">statistics
      seminar  talk at "Matfyz"</a> - School of Mathematics, Physics and
          Informatics, Comenius<span style="font-style: italic;"> </span>University.<span
            style="font-style: italic;"><br>
            <br>
          </span>Title: On stochastic algorithms in linear algebra,
          optimization and machine learning<br>
          Place: FMFI UK, M/XII<span style="font-style: italic;"><br>
          </span>Date:<span style="font-style: italic;"> </span>Monday,
          February 26, 2018<span style="font-style: italic;"><br>
          </span>Time:<span style="font-style: italic;"> </span>09:50am<span
            style="font-style: italic;"><br>
            <br>
          </span>If anyone is interested in MS / PhD / postdocs /
          research scientist positions at <a
            href="https://www.kaust.edu.sa/en">KAUST</a>, I will be
          available to talk to you after the talk.<br>
          <span style="font-style: italic;"> <br>
          </span> <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>February 20, 2018</h3>
          <h1>Optimization &amp; Big Data 2018: Videos are Online</h1>
          <br>
          Videos of the talks from the <a
            href="https://obd.kaust.edu.sa">KAUST Research Workshop on
            Optimization and Big Data</a> are now available. <a
      href="http://mediasite.kaust.edu.sa/Mediasite/Catalog/catalogs/mediasiteadmin-kaust-research-workshop-on-optimization-and-big-data---2018">They can  be found here.</a><br>
          <br>
          Comment: At the moment the videos are accessible to KAUST
          community only, they will soon be available globally.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 13, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1802.03801">"SGD and Hogwild!
            convergence without the bounded gradients assumption"</a> -
          joint work with Lam M. Nguyen, Phuong Ha Nguyen, Marten van
          Dijk, Katya Scheinberg and Martin Takáč.<br>
          <br>
          Abstract:<span style="font-style: italic;"> Stochastic
            gradient descent (SGD) is the optimization algorithm of
            choice in many machine learning applications such as
            regularized empirical risk minimization and training deep
            neural networks. The classical analysis of convergence of
            SGD is carried out under the assumption that the norm of the
            stochastic gradient is uniformly bounded. While this might
            hold for some loss functions, it is always violated for
            cases where the objective function is strongly convex. In
            (Bottou et al., 2016) a new analysis of convergence of SGD
            is performed under the assumption that stochastic gradients
            are bounded with respect to the true gradient norm. Here we
            show that for stochastic problems arising in machine
            learning such bound always holds. Moreover, we propose an
            alternative convergence analysis of SGD with diminishing
            learning rate regime, which is results in more relaxed
            conditions that those in (Bottou et al., 2016). We then move
            on the asynchronous parallel setting, and prove convergence
            of the Hogwild! algorithm in the same regime, obtaining the
            first convergence results for this method in the case of
            diminished learning rate. </span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>February 12, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1802.04079">"Accelerated
            stochastic matrix inversion: general theory and speeding up
            BFGS rules for faster second-order optimization"</a> - joint
          work with Robert M. Gower, Filip Hanzely and Sebastian Stich.<br>
          <br>
          Abstract:<span style="font-style: italic;"> We present the
            first accelerated randomized algorithm for solving linear
            systems in Euclidean spaces. One essential problem of this
            type is the matrix inversion problem. In particular, our
            algorithm can be specialized to invert positive definite
            matrices in such a way that all iterates (approximate
            solutions) generated by the algorithm are positive definite
            matrices themselves. This opens the way for many
            applications in the field of optimization and machine
            learning. As an application of our general theory, we
            develop the first accelerated (deterministic and stochastic)
            quasi-Newton updates. Our updates lead to provably more
            aggressive approximations of the inverse Hessian, and lead
            to speed-ups over classical non-accelerated rules in
            numerical experiments. Experiments with empirical risk
            minimization show that our rules can accelerate training of
            machine learning models. </span> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>February 10, 2018</h3>
          <h1>New Paper</h1>
          <span class="important"><br>
            New paper out: </span><a
            href="https://arxiv.org/abs/1802.04084">"Randomized block
            cubic Newton method"</a> - joint work with Nikita Doikov.<br>
          <br>
          Abstract:<span style="font-style: italic;"> We study the
            problem of minimizing the sum of three convex functions: a
            differentiable, twice-differentiable and a non-smooth term
            in a high dimensional setting. To this effect we propose and
            analyze&nbsp; a randomized block&nbsp; cubic Newton (RBCN)
            method, which in each iteration builds a model of the
            objective function formed as the sum of the {\em natural}
            models of its three components: a linear model with a
            quadratic regularizer for the differentiable term, a
            quadratic model with a cubic regularizer for the twice
            differentiable term, and perfect (proximal)&nbsp; model for
            the nonsmooth term. Our method in each iteration minimizes
            the model over a random subset of&nbsp; blocks of the search
            variable. RBCN is the first algorithm with these properties,
            generalizing several existing methods, matching the best
            known bounds in all special cases. We establish ${\cal
            O}(1/\epsilon)$, ${\cal O}(1/\sqrt{\epsilon})$ and ${\cal
            O}(\log (1/\epsilon))$ rates under different assumptions on
            the component functions. Lastly, we show numerically that
            our method outperforms the state-of-the-art on a variety of
            machine learning problems, including cubically regularized
            least-squares, logistic regression with constraints, and
            Poisson regression. </span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>February 10, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1802.03703">"Stochastic spectral
            and conjugate descent methods"</a> - joint work with Dmitry
          Kovalev, <a
            href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard Gorbunov</a> and Elnur Gasanov.<br>
          <br>
          Abstract:<span style="font-style: italic;"> The
            state-of-the-art methods for solving optimization problems
            in big dimensions are variants of randomized coordinate
            descent (RCD). In this paper we introduce a fundamentally
            new type of acceleration strategy for RCD based on the
            augmentation of the set of coordinate directions by a few
            spectral or conjugate directions. As we increase the number
            of extra directions to be sampled from, the rate of the
            method improves, and interpolates between the linear rate of
            RCD and a linear rate independent of the condition number.
            We develop and analyze also inexact variants of these
            methods where the spectral and conjugate directions are
            allowed to be approximate only. We motivate the above
            development by proving several negative results which
            highlight the limitations of RCD with importance sampling.</span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>February 5, 2018</h3>
          <h1><span class="important"></span>Optimization &amp; Big Data
            2018 Started</h1>
          <span class="important"><br>
            OBD 2018 is starting!</span> The <a
            href="https://obd.kaust.edu.sa">KAUST Workshop on
            Optimization and Big Data</a> just started. We have 19
          amazing speakers and 21 deluxe e-posters lined up. <br>
          <br>
          Update (February 12): Thanks for all who participated in the
          workshop, thanks you to this was an excellent event! Group
          photos:<br>
          <br>
          <img style="width: 750px; height: 438px;" alt="KAUST Research
            Workshop on Optimization and Big Data"
            src="imgs/OBD2018a.jpg"><br>
          <br>
          <img style="width: 750px; height: 438px;" alt="KAUST Research
            Workshop on Optimization and Big Data"
            src="imgs/OBD2018b.jpg"><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>February 4, 2018</h3>
          <h1> Optimization &amp; Big Data 2018</h1>
          <br>
          <a href="https://obd.kaust.edu.sa">KAUST Research Workshop on Optimization and Big Data</a> is starting tomorrow! We have 19 amazing speakers, and 21 deluxe poster talks and <a
            href="http://epostersonline.com/obd2018/">ePoster presentations</a>. <br>
          <br>
          This year, <a href="https://coral.ise.lehigh.edu/terlaky/">Tamás Terlaky (Lehigh)</a> is the keynote speaker. <br>
          <br>
          Thanks to the KAUST Office for Sponsored Research, The Alan Turing Institute and KICP.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>February 1, 2018</h3>
          <h1> Nicolas @ KAUST</h1>
          <br>
          <a href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a> is back at KAUST on a research visit. Welcome! <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>January 28, 2018</h3>
          <h1>Aritra, Alibek and Samuel @ EPFL</h1>
          <br>
          Aritra Dutta (postdoc), Alibek Sailanbayev (MS/PhD student)
          and Samuel Horvath (MS/PhD student) are attending <a
            href="https://www.appliedmldays.org">Applied Machine
            Learning Days</a> at EPFL, Lausanne, Switzerland. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 27, 2018</h3>
          <h1> Two new MS Students and a new Intern</h1>
          <br>
          Let me welcome Dmitry Kovalev and Elnur Gasanov (master
          students visiting from MIPT, Moscow) and Slavomír Hanzely
          (undergraduate student at Comenius University), who arrived at
          KAUST about a week ago and are working with me as interns.
          They will be here for about a month. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>January 18, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a href="https://arxiv.org/abs/1801.05661">"A randomized  exchange algorithm for computing optimal approximate designs of experiments"</a> - joint work with <a
            href="http://www.iam.fmph.uniba.sk/ospm/Harman/index-sk.htm">Radoslav Harman</a> and <a
            href="http://www.iam.fmph.uniba.sk/ospm/Filova/index-sk.htm">Lenka Filová</a>.<br><br>

           Abstract:<span style="font-style: italic;"> We propose a class
            of subspace ascent methods for computing optimal approximate
            designs that covers both existing as well as new and more
            efficient algorithms. Within this class of methods, we
            construct a simple, randomized exchange algorithm (REX).
            Numerical comparisons suggest that the performance of REX is
            comparable or superior to the performance of
            state-of-the-art methods across a broad range of problem
            structures and sizes. We focus on the most commonly used
            criterion of D-optimality that also has applications beyond
            experimental design, such as the construction of the minimum
            volume ellipsoid containing a given set of datapoints. For
            D-optimality, we prove that the proposed algorithm converges
            to the optimum. We also provide formulas for the optimal
            exchange of weights in the case of the criterion of
            A-optimality. These formulas enable one to use REX for
            computing A-optimal and I-optimal designs.</span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>January 16, 2018</h3>
          <h1>New Intern, Visitor and Postdoc</h1>
          <br>
          I was traveling and am back at KAUST now. <br>
          <br>
          Let me welcome <a
            href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard Gorbunov</a> (a master's student visiting from MIPT, Moscow;
          will be here until Feb 8), <a
            href="http://www.damtp.cam.ac.uk/user/me404/">Matthias
            Ehrhardt</a> (visiting from Cambridge, UK, until February
          10) and <a href="http://maiage.jouy.inra.fr/?q=fr/bergou">Elhoucine  Bergou</a> (new postdoc in my group, starting today). <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




<h3>January 15, 2018</h3>
<h1>New Paper</h1>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1801.04873">"Randomized
projection methods for convex feasibility problems:
conditioning and convergence rates"</a> - joint work with <a
href="http://acse.pub.ro/person/ion-necoara/">Ion Necoara</a>
and <a
href="https://www.researchgate.net/profile/Andrei_Patrascu2">Andrei Patrascu</a>.<br>
<br>
Abstract: <span style="font-style: italic;">Finding a point
in the intersection of a collection of closed convex sets,
that is the convex feasibility problem, represents the main
modeling strategy for many computational problems. In this
paper we analyze new stochastic reformulations of the convex
feasibility problem in order to facilitate the development
of new algorithmic schemes. We also analyze the conditioning
problem parameters using certain (linear) regularity
assumptions on the individual convex sets. Then, we
introduce a general random projection algorithmic framework,
which extends to the random settings many existing
projection schemes, designed for the general convex
feasibility problem. Our general random projection algorithm
allows to project simultaneously on several sets, thus
providing great flexibility in matching the implementation
of the algorithm on the parallel architecture at hand. Based
on the conditioning parameters, besides the asymptotic
convergence results, we also derive explicit sublinear and
linear convergence rates for this general algorithmic
framework.</span><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>December 22, 2017</h3>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1712.09677">"Momentum and
stochastic momentum for stochastic gradient, Newton,
proximal point and subspace descent methods"</a> - joint
work with Nicolas Loizou.<br>
<br>
Abstract:<span style="font-style: italic;"> In this paper we
study several classes of stochastic optimization algorithms
enriched with heavy ball momentum. Among the methods studied
are: stochastic gradient descent, stochastic Newton,
stochastic proximal point and stochastic dual subspace
ascent. This is the first time momentum variants of several
of these methods are studied. We choose to perform our
analysis in a setting in which all of the above methods are
equivalent. We prove global nonassymptotic linear
convergence rates for all methods and various measures of
success, including primal function values, primal iterates
(in L2 sense), and dual function values. We also show that
the primal iterates converge at an accelerated linear rate
in the L1 sense. This is the first time a linear rate is
shown for the stochastic heavy ball method (i.e., stochastic
gradient descent method with momentum). Under somewhat
weaker conditions, we establish a sublinear convergence rate
for Cesaro averages of primal iterates. Moreover, we propose
a novel concept, which we call stochastic momentum, aimed at
decreasing the cost of performing the momentum step. We
prove linear convergence of several stochastic methods with
stochastic momentum, and show that in some sparse data
regimes and for sufficiently small momentum parameters,
these methods enjoy better overall complexity than methods
with deterministic momentum. Finally, we perform extensive
numerical testing on artificial and real datasets, including
data coming from average consensus problems.</span><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>December 15, 2017</h3>
<br>
I am now in Havana, Cuba, attending the <a
href="https://www.wias-berlin.de/workshops/oms2017/">4th
Conference on Optimization and Software</a>. I am speaking
about a stochastic version of the Chambolle-Pock algorithm [<a
href="https://arxiv.org/abs/1706.04957">1</a>, <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/PET-SPDHGM.pdf">2</a>].
Filip Hanzely is here, too - speaking about randomized and
accelerated methods for minimizing relatively smooth
functions.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>December 12, 2017</h3>
<br>
A few random updates: <br>
<br>
<a href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas</a>
and <a href="http://jakubkonecny.com">Jakub</a> attended
NIPS. Filip and me will soon fly to Cuba to give talks at the
<a href="https://www.wias-berlin.de/workshops/oms2017/">4th
Conference on Optimization Methods and Software.</a> <a
href="http://www.cs.ubc.ca/%7Eschmidtm/">Mark Schmidt</a> is
joining us in the same session. The Fall 2017 semester at
KAUST is over - I have had some fantastic students in my
CS390FF (Big Data Optimization) class. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>November 30, 2017</h3>
<br>
<span class="important"></span>Video recordings of the <a
href="http://www.ds3-datascience-polytechnique.fr">Data Science Summer School</a> (held at Ecole Polytechnique in
Aug/Sept 2017) lectures are now <a
href="https://comm.medias.polytechnique.fr/channels/#data-science-summer-school-2017_17697">online</a>.<br>
<br>
Lecturers: <br>
- <a href="http://www.iro.umontreal.ca/%7Ebengioy/yoshua_en/">Joshua Bengio (Montreal)</a>: Deep Learning <br>
- <a href="http://www.cs.cmu.edu/%7Epradeepr/">Pradeep Ravikumar (CMU)</a>: Graphical Models <br>
- Peter Richtarik (KAUST/Edinburgh): Randomized Optimization Methods<br>
- <a href="https://sites.ualberta.ca/%7Eszepesva/">Csaba Szepesvári (Alberta/Google DeepMind)</a>: Bandits<br>
<br>
I have given a 5 hr course on Randomized Optimization Methods; the videos are here:<br>
<br>
<a href="https://comm.medias.polytechnique.fr/videos/richtarik_1_randonopti/">Video Lecture 1</a><br>
<a href="https://comm.medias.polytechnique.fr/videos/richtarik_2_randonopti/">Video Lecture 2</a><br>
<a href="https://comm.medias.polytechnique.fr/videos/richtarik_3_randonopti_08033/">Video Lecture 3</a><br>
<a href="https://comm.medias.polytechnique.fr/videos/richtarik_4_randonoptimization/">Video Lecture 4</a><br>
<a href="https://comm.medias.polytechnique.fr/videos/richtarik_5_randonoptimization/">Video Lecture 5</a><br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>November 25, 2017</h3>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1712.02249">"Online and batch
supervised background estimation via L1 regression"</a> -
joint work with Aritra Dutta.<br>
<br>
Abstract: <span style="font-style: italic;">We propose a
surprisingly simple model for supervised video background
estimation. Our model is based on L1 regression. As existing
methods for L1 regression do not scale to high-resolution
videos, we propose several simple and scalable methods for
solving the problem, including iteratively reweighted least
squares, a homotopy method, and stochastic gradient descent.
We show through extensive experiments that our model and
methods match or outperform the state-of-the-art online and
batch methods in virtually all quantitative and qualitative
measures.</span><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>November 24, 2017</h3>
<br>
<span class="important"></span> <a
href="http://www.dominikcsiba.com">Dominik Csiba</a> defended his PhD thesis "Data sampling strategies in
stochastic algorithms for empirical risk minimization" today. Congratulations!<br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>November 16, 2017</h3>
<br>
<span class="important"></span> <a
href="https://perso.telecom-paristech.fr/rgower/">Robert Gower</a> is visiting me at KAUST for 2 weeks. He will give two
talks during his visit, one at my <a
href="https://sites.google.com/site/allhandsoptimization/">research group seminar </a>on Nov 21, and one at the <a
href="https://cemse.kaust.edu.sa/events/Pages/CS-Graduate-Seminar-Robert-Gower.aspx">CS graduate seminar</a> on Nov 27.<br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>November 9, 2017</h3>
<br>
<a href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas Loizou</a> is visiting me at KAUST. He will stay until early
December, after which he is heading off for NIPS, to present our work on <a href="https://arxiv.org/abs/1710.10737">
"Linearly convergent stochastic heavy ball method for minimizing generalization error".</a><br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>October 27, 2017</h3>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1710.10737">"Linearly convergent
stochastic heavy ball method for minimizing generalization
error"</a> - joint work with Nicolas Loizou.<br>
<span class="important"> <br>
</span>Abstract: <span style="font-style: italic;">In this
work we establish the first linear convergence result for
the stochastic heavy ball method. The method performs SGD
steps with a fixed stepsize, amended by a heavy ball
momentum term. In the analysis, we focus on minimizing the
expected loss and not on finite-sum minimization, which is
typically a much harder problem. While in the analysis we
constrain ourselves to quadratic loss, the overall objective
is not necessarily strongly convex.</span><br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>October 25, 2017</h3>
<br>
I am on my way to Moscow, where I will kick-start a research
project funded at <a href="https://mipt.ru/english/">MIPT</a>
by giving a talk at a workshop entitled <a
href="https://mipt.ru/education/departments/fpmi/events/optimization_at_work">Optimization at Work</a>. As a part of this project, several MIPT
students will join my team. The first three members are:<br>
<br>
Dmitry Kovalev<br>
<a
  href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard Gorbunov</a><br>
Elnur Gasanov<br>
<br>
There are two more spots to be filled. If you are an
exceptionally strong mathematics or computer science student
of MIPT, get in touch with me.<br>
<br>
<a href="https://konstmish.github.io">Konstantin Mishchenko</a>
and Eduard Gorbunov are giving talks, too.<br>
<br>
Update (Oct 27, 2017): I just gave my talk; I talked about <a
  href="https://arxiv.org/abs/1706.04957">stochastic
  Chambolle-Pock algorithm</a> (joint work with A. Chambolle,
M. J. Ehrhardt, and C.-B. Schoenlieb). See also follow up work
on an application to <a
  href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/PET-SPDHGM.pdf">PET reconstruction</a> (Proceedings of SPIE, 2017). <br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>October 9, 2017</h3>
<br>
<a href="http://faculty.skoltech.ru/people/nikitadoikov">Nikita Doikov (Higher School of Economics, Moscow)</a> is visiting
me at KAUST. He will stay until late November. Nikita is a PhD student working under the supervision
of <a href="https://www.hse.ru/en/org/persons/150293981">Yurii Nesterov</a>. <br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>October 5, 2017</h3>
<br>
<a href="http://sstich.ch">Sebastian Stich (EPFL)</a> is visiting me at KAUST; he will stay for a couple weeks.<br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>October 1, 2017</h3>
<br>
<a href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
Loizou</a> is visiting Berkeley for 10 days. He is attending
the <a
href="https://simons.berkeley.edu/workshops/optimization2017-2">Fast Iterative Methods in Optimization workshop</a> held at the <a
href="https://simons.berkeley.edu">Simons Institute</a>. The
workshop is a part of the program <a
href="https://simons.berkeley.edu/programs/optimization2017">Bridging Continuous and Discrete Optimization</a>. On October 5,
Nicolas will give a seminar talk in the Statistics department
at Berkeley entitled "'Stochastic and doubly stochastic dual
heavy ball methods for quadratic optimization with low-rank
Hessian". This talk is based on a new joint paper with me
which will be posted on arXiv soon.<br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>September 24, 2017</h3>
<br>
Aritra Dutta is attending a sectional meeting of the American
Mathematical Society in Orlando, Florida. He is giving a talk
based on the paper <a href="https://arxiv.org/abs/1707.00281">"A Batch-Incremental Video Background Estimation Model using
Weighted Low-Rank Approximation of Matrices",</a> co-authored with Xin Li and myself, in a <a
href="http://www.ams.org/meetings/sectional/2246_program_ss21.html#title">"Special Session on Advances in Dirac Equations, Variational
Inequalities, Sequence Spaces and Optimization"</a>. He will give the same talk at <a href="http://iccv2017.thecvf.com/">ICCV</a>
/ <a href="http://rsl-cv.univ-lr.fr/2017/">RSL-CV</a> in Venice, Italy on October 28.<br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>September 23, 2017</h3>
<br>
I'd like to welcome five new students who joined my group at
KAUST in August/September:<br>
<br>
Filip Hanzely (PhD student) - coming from Comenius University
/ University of Edinburgh [<a
href="https://scholar.google.com/citations?user=_X0AKjEAAAAJ&amp;hl=en">Scholar</a>]<br>
Samuel Horváth (MS/PhD student) - coming from Comenius
University <br>
Viktor Lukáček (PhD student) - coming from Comenius University<br>
<a href="https://konstmish.github.io/"> Konstantin Mishchenko</a>
(PhD student) - coming from MIPT / ENS Cachan / Paris Dauphine
[<a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/docs/cv_mishchenko.pdf">CV</a>]<br>
Alibek Sailanbayev (MS/PhD student) - coming from Nazarbayev
University [<a
href="https://scholar.google.com/citations?user=BgBr19sAAAAJ&amp;hl=en">Scholar</a>]<br>
<br>
Filip Hanzely transfered to KAUST from Edinburgh where he
spent 1 year as a PhD student under my supervision. He wrapped
up his 1 year spent at Edinburgh with an MSc degree. Filip
co-authored two papers during his time in Edinburgh: one on <a
href="https://arxiv.org/abs/1706.07636">privacy-preserving
gossip methods</a>, and one on randomized algorithms for
minimizing relatively smooth functions (this is the subject of
his MSc thesis @ Edinburgh, the paper will be soon posted onto
arXiv). Filip gave a talk on the latter paper at the <a
href="http://www.siam.org/meetings/op17/">SIAM Conference on
Optimization</a> in Vancouver, and presented a poster at the
AN70 meeting at the <a
href="http://www.fields.utoronto.ca/activities/17-18/AN70">Fields Institute </a>in Toronto. Before coming to Edinburgh, Filip
wrote a paper on <a
href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.94.052203">testing
for causality in reconstructed state spaces</a>. Alibek
wrote two papers during his undergraduate studies: one on <a
href="https://hal.inria.fr/hal-01186339/">pattern structures
for structured attribute sets</a> and one on <a
href="https://link.springer.com/chapter/10.1007/978-3-319-19545-2_3">data
analysis in biomedical studies</a>. Konstantin is writing
two papers on distributed optimization based on results
obtained during his Summer 2017 internship in Grenoble with
Frank Iutzeler and Jerome Malick. He presented these results
as a poster entitled&nbsp; "An asynchronous distributed
proximal gradient algorithm" in a <a
href="https://team.inria.fr/magnet/workshop-on-decentralized-machine-learning-optimization-and-privacy/">workshop
on Decentralized Machine Learning, Optimization and Privacy</a>
held recently in Lille, France.<br>
<br>
Filip, Samuel, Viktor, Konstantin and Alibek were all active
in various national and international mathematical and
computing competitions for high school and university
students. Here is a list of some of their achievements:<br>
<br>
2017 (Horváth), 37th Place, Vojtech Jarnik International
Mathematical Competition, Ostrava, Czech Republic<br>
2016 (Horváth), 36th Place, Vojtech Jarnik International
Mathematical Competition, Ostrava, Czech Republic<br>
2016 (Horváth), 3rd Prize, Int. Mathematical Competition for
University Students, Blagoevgrad, Bulgaria<br>
2016 (Sailanbayev), Semifinal, Programming contest ACM ICPC in
NEERC region, Almaty, Kazakhstan<br>
2015 (Sailanbayev), 2nd Prize, Int. Mathematical Competition
for University Students, Blagoevgrad, Bulgaria<br>
2015 (Mishchenko), 1st Prize, HSE Olympiad in Applied
Mathematics and Informatics, Moscow, Russia<br>
2014 (Mishchenko), 3rd Prize, MIPT Student Mathematical
Olympiad, Moscow, Russia<br>
2014 (Horváth), 18th Place, National Mathematical Olympiad,
Bratislava, Slovakia<br>
2014 (Horváth), 1st Place, Nitra District Mathematical
Olympiad, Category A, Slovakia<br>
2014 (Sailanbayev), 2nd Prize, Int. Mathematical Competition
for University Students, Blagoevgrad, Bulgaria<br>
2014 (Hanzely), 2nd Prize, Int. Mathematical Competition for
University Students, Blagoevgrad, Bulgaria<br>
2014 (Hanzely), 9th Place, Vojtech Jarnik International
Mathematical Competition, Ostrava, Czech Republic<br>
2014 (Lukáček), 26th Place, Vojtech Jarnik International
Mathematical Competition, Ostrava, Czech Republic<br>
2013 (Sailanbayev), Silver Medal, International Mathematical
Olympiad, Santa Marta, Colombia<br>
2013 (Hanzely), Bronze Medal, International Mathematical
Olympiad, Santa Marta, Colombia<br>
2013 (Sailanbayev), 1st Place, National Mathematical Olympiad,
Kazachstan<br>
2013 (Hanzely), 1st Place, National Mathematical Olympiad,
Kosice, Slovakia<br>
2013 (Sailanbayev), Gold Medal, International Zhautykov
Olympiad, Almaty, Kazakhstan<br>
2013 (Lukáček), 20th Place, Vojtech Jarnik International
Mathematical Competition, Ostrava, Czech Republic<br>
2012 (Lukáček), 3rd Prize, Int. Mathematical Competition for
University Students, Blagoevgrad, Bulgaria<br>
2012 (Mishchenko), 1st Prize, Moscow Mathematical Olympiad,
Moscow, Russia<br>
2012 (Mishchenko), 1st Prize, PhysTech International Olympiad
in Mathematics<br>
2012 (Hanzely), Bronze Medal, Middle European Mathematical
Olympiad, Solothurn, Switzerland<br>
2012 (Sailanbayev), Bronze Medal, International Mathematical
Olympiad, Mar del Plata, Argentina<br>
2012 (Sailanbayev), Silver Medal, Balkan Mathematical
Olympiad, Antalya, Turkey<br>
2012 (Lukáček), 2nd Place, International Correspondence
Seminar in Mathematics (iKS)<br>
2011 (Lukáček), Bronze Medal (26th Place), Middle European
Mathematical Olympiad, Varaždin, Croatia<br>
<br>
It's exciting to have you all here, welcome!<br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>September 13, 2017</h3>
<br>
I am back at KAUST now. The <a
href="https://team.inria.fr/magnet/workshop-on-decentralized-machine-learning-optimization-and-privacy/">Lille workshop</a> was very nice: excellent talks, great group of
people.<br>
<br>
I will soon start inviting speakers for the Optimization and Big Data workshop which will take place at KAUST during
February 5-7, 2018.<br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>September 12, 2017</h3>
<br>
<span class="important">New paper out: </span><a
href="https://arxiv.org/abs/1709.03014">"Global convergence
of arbitrary-block gradient methods for generalized
Polyak-Łojasiewicz functions"</a> - joint work with <a
href="http://www.dominikcsiba.com">Dominik Csiba</a>.<br>
<span class="important"> <br>
</span>Abstract: <span style="font-style: italic;">In this
paper we introduce two novel generalizations of the theory
for gradient descent type methods in the proximal setting.
First, we introduce the proportion function, which we
further use to analyze all known (and many new)
block-selection rules for block coordinate descent methods
under a single framework. This framework includes randomized
methods with uniform, non-uniform or even adaptive sampling
strategies, as well as deterministic methods with batch,
greedy or cyclic selection rules. Second, the theory of
strongly-convex optimization was recently generalized to a
specific class of non-convex functions satisfying the
so-called Polyak-Łojasiewicz condition. To mirror this
generalization in the weakly convex case, we introduce the
Weak Polyak-Łojasiewicz condition, using which we give
global convergence guarantees for a class of non-convex
functions previously not considered in theory. Additionally,
we establish (necessarily somewhat weaker) convergence
guarantees for an even larger class of non-convex functions
satisfying a certain smoothness assumption only. By
combining the two abovementioned generalizations we recover
the state-of-the-art convergence guarantees for a large
class of previously known methods and setups as special
cases of our general framework. Moreover, our frameworks
allows for the derivation of new guarantees for many new
combinations of methods and setups, as well as a large class
of novel non-convex objectives. The flexibility of our
approach offers a lot of potential for future research, as a
new block selection procedure will have a convergence
guarantee for all objectives considered in our framework,
while a new objective analyzed under our approach will have
a whole fleet of block selection rules with convergence
guarantees readily available. </span><br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>September 10, 2017</h3>
<br>
I am on my way to Lille, France, to attend a workshop on <a
href="https://team.inria.fr/magnet/workshop-on-decentralized-machine-learning-optimization-and-privacy/">Decentralized Machine Learning, Optimization and Privacy</a> (Sept 11-12,
2017). <br>
<br>
<span style="font-style: italic;">Update (Sept 11):</span> I
have given my talk "Privacy preserving randomized gossip
algorithms" today. The talk is based on <a
href="https://arxiv.org/abs/1706.07636">this paper</a>, and
here are <a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/talks/TALK-private-gossip.pdf">the
slides</a>. <br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>September 9, 2017</h3>
<br>
<a href="http://www.dominikcsiba.com/">Dominik Csiba</a>
submitted his PhD thesis entitled "Data Sampling Strategies in
Stochastic Algorithms for Empirical Risk Minimization" a
couple weeks ago. The thesis consist of 6 chapters; I include
links to the papers the chaopters are based on.<br>
<br>
1. Introduction <br>
2. <a href="http://proceedings.mlr.press/v37/csiba15.pdf">Stochastic Dual Coordinate Ascent with Adaptive Probabilities</a><br>
3. <a href="https://arxiv.org/abs/1506.02227">Primal Method
for ERM with Flexible Mini-batching Schemes and Non- convex
Losses</a><br>
4. <a href="https://arxiv.org/abs/1602.02283">Importance
Sampling for Minibatches</a><br>
5. <a href="https://arxiv.org/abs/1605.08982">Coordinate
Descent Faceoff: Primal or Dual?</a><br>
6. <a href="https://arxiv.org/abs/1709.03014">Global
Convergence of Arbitrary-Block Gradient Methods for
Generalized Polyak-Lojasiewicz Functions</a> <br>
<br>
His defense/viva will take place at some point in the Fall; a
pdf of the thesis will be made public afterwards.<br>
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

          <h3>September 8, 2017</h3>
          <br>
          I am co-organizing the workshop "Sparse Approximation and
          Sampling" which is to be held in London sometime in May or
          June 2019. The precise dates will be fixed soon. This is a
          joint event of the <a href="https://www.newton.ac.uk/">Isaac
            Newton Institute</a> and <a
            href="https://www.turing.ac.uk/">The Alan Turing Institute</a>.
          This is one of three workshops which are part of a 6 month
          programme on "Approximation, sampling and compression in high
          dimensional problems" held at the Isaac Newton Institute
          during January-June 2019.
          <ul>
            <br>
            <li>Workshop organizers: <a
                href="http://ece.duke.edu/faculty/robert-calderbank">Robert










































                Calderbank</a> (Duke, USA), <a
                href="http://www.damtp.cam.ac.uk/research/afha/anders/">Anders










































                Hansen</a> (Cambridge, UK),<br>
              <a href="http://www.maths.ed.ac.uk/%7Eprichtar/">Peter
                Richtarik</a> (KAUST, KSA - Edinburgh, UK - The Alan
              Turing Institute, UK), <a
                href="https://www.ee.ucl.ac.uk/%7Euceemrd/">Miguel
                Rodrigues</a> (UCL, UK).<br>
            </li>
          </ul>
          <ul>
            <li>Programme Scientific Advisory Committee: <a
                href="http://ece.duke.edu/faculty/robert-calderbank">Robert










































                Calderbank</a> (Duke, USA), <a
                href="https://statweb.stanford.edu/%7Ecandes/">Emmanuel
                Candes</a> (Stanford, USA), <a
                href="http://www.math.tamu.edu/%7Erdevore/">Ronald
                DeVore</a> (Texas A&amp;M, USA), <a
                href="https://math.duke.edu/people/ingrid-daubechies">Ingrid












                Daubechies</a> (Duke, USA), <a
href="http://www.damtp.cam.ac.uk/user/ai/Arieh_Iserles/Arieh_Iserles.html">Arieh












                Iserles</a> (Cambridge, UK)</li>
          </ul>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 1, 2017</h3>
          <br>
          I am now back at <a href="https://www.kaust.edu.sa/en">KAUST</a>.
          The <a href="https://en.wikipedia.org/wiki/Eid_al-Adha">Eid
            al-Adha</a> holiday started yesterday. I am looking forward
          to a bit of rest (or "stayvacation", as spending vacation at
          KAUST as opposed to somewhere else is called).<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 29, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/papers/pdfixedpoint.pdf">"The


















            complexity of primal-dual fixed point methods for ridge
            regression"</a> - joint work with <a
            href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">Ademir















































            Ribeiro</a> (Federal University of Paraná).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">We study
            the ridge regression ($L_2$ regularized least squares)
            problem and its dual, which is also a ridge regression
            problem. We observe that the optimality conditions
            describing the primal and dual optimal solutions can be
            formulated in several different but equivalent ways. The
            optimality conditions we identify form a linear system
            involving a structured matrix depending on a single
            relaxation parameter which we introduce for regularization
            purposes. This leads to the idea of studying and comparing,
            in theory and practice, the performance of the fixed point
            method applied to these reformulations. We compute the
            optimal relaxation parameters and uncover interesting
            connections between the complexity bounds of the variants of
            the fixed point scheme we consider. These connections follow
            from a close link between the spectral properties of the
            associated matrices. For instance, some reformulations
            involve purely imaginary eigenvalues; some involve real
            eigenvalues and others have all eigenvalues on the complex
            circle. We show that the deterministic Quartz method--which
            is a special case of the randomized dual coordinate ascent
            method with arbitrary sampling recently developed by Qu,
            Richtarik and Zhang--can be cast in our framework, and
            achieves the best rate in theory and in numerical
            experiments among the fixed point methods we study. </span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 28, 2017</h3>
          <br>
          I have arrived to Paris. I am attending the <a
            href="http://www.ds3-datascience-polytechnique.fr">Data
            Science Summer School (DS3)</a> organized by <a
            href="http://www.ds3-datascience-polytechnique.fr/organizers/">Ecole















































            Polytechnique.</a> I am giving a 5 hour minicourse on
          Randomized Optimization Methods (<a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/talks/RandOpt-DS3.pdf">here


















            are the slides</a>).<br>
          <br>
          Some event stats (copy pasted from the event website):<br>
          <br>
          400 participants<br>
          220 students (MSc, PhD) &amp; postdocs, 100 professionals<br>
          16 experts (speakers, guests)<br>
          30 countries<br>
          6 continents<br>
          200 institutions<br>
          50 companies<br>
          6 sponsors<br>
          120 posters<br>
          female : male ratio = 3 : 10<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 20, 2017</h3>
          <br>
          The first day of the Fall 2017 semester at KAUST is today. I
          am teaching CS390FF: Selected Topics in Data Sciences (Big
          Data Optimization). <br>
          <br>
          <span style="font-style: italic;">Update (Sept 8):</span> 26
          students are enrolled in the course.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 13, 2017</h3>
          <br>
          I am back at <a href="https://www.kaust.edu.sa/en">KAUST</a>
          now. The Fall 2017 semester is starting in a week.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 10, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/papers/PET-SPDHGM.pdf">"Faster


















            PET reconstruction with a stochastic primal-dual hybrid
            gradient method"</a> - joint work with <a
            href="http://www.cmap.polytechnique.fr/%7Eantonin/">Antonin
            Chambolle</a> (Ecole Polytechnique), <a
            href="http://www.damtp.cam.ac.uk/user/me404/">Matthias J.
            Ehrhardt</a> (Cambridge), and <a
            href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">Carola-Bibiane















































            Schoenlieb</a> (Cambridge).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">Image
            reconstruction in positron emission tomography (PET) is
            computationally challenging due to Poisson noise,
            constraints and potentially non-smooth priors—let alone the
            sheer size of the problem. An algorithm that can cope well
            with the first three of the aforementioned challenges is the
            primal-dual hybrid gradient algorithm (PDHG) studied by
            Chambolle and Pock in 2011. However, PDHG updates all
            variables in parallel and is there- fore computationally
            demanding on the large problem sizes encountered with modern
            PET scanners where the number of dual variables easily
            exceeds 100 million. In this work, we numerically study the
            usage of SPDHG—a stochastic extension of PDHG—but is still
            guaranteed to converge to a solution of the deterministic
            optimization problem with similar rates as PDHG. Numerical
            results on a clinical data set show that by introducing
            randomization into PDHG, similar results as the
            deterministic algorithm can be achieved using only around
            10% of operator evaluations. Thus, making significant
            progress towards the feasibility of sophisticated
            mathematical models in a clinical setting.</span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 5, 2017</h3>
          <br>
          I have just arrived in Sydney, Australia - I am attending <a
            href="https://2017.icml.cc">ICML</a>. Looking forward to the
          excellent program!<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 10, 2017</h3>
          <br>
          I am reviewing <a href="https://nips.cc">NIPS</a> papers this
          week.<br>
          <br>
          <span style="font-style: italic;">Update (after rebuttal): </span>It's















































          never a good strategy for authors to deny obvious issues
          raised by the reviewers simply do not exist. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 3, 2017</h3>
          <br>
          <span class="important"></span><a
            href="http://jakubkonecny.com">Jakub's</a> PhD thesis is <a
            href="https://arxiv.org/abs/1707.01155">now on arXiv.</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 3, 2017</h3>
          <br>
          <span class="important"></span>I am on my way to the Fields
          Institute, Toronto, to attend a workshop entitled <a
            href="http://www.fields.utoronto.ca/activities/17-18/AN70">"Modern















































            Convex Optimization and Applications: AN70"</a>. This event
          is organized in honour of Arkadi Nemirovski's 70th birthday.
          Arkadi is one of the most influential individuals in
          optimization, directly resposible for the existence of several
          of its most important and most beautiful subfields. Here is a
          <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/keynote.html">very















































            brief profile</a> of this giant of our beloved field, taken
          from the website of a workshop I co-organized in Edinburgh in
          2015.<br>
          <br>
          Update (July 5, 2017): I have given my talk today, <a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/talks/TALK-2017-FieldsInstitute-AN70.pdf">here


















            are the slides.</a> <br>
          <br>
          Update (July 7, 2017): Filip delivered his pitch talk and
          presented his poster "Extending the Reach of Big Data
          Optimization: Randomized Algorithms for Minimizing Relatively
          Smooth Functions".<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 2, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1707.00281">"A batch-incremental
            video background estimation model using weighted low-rank
            approximation of matrices"</a> - joint work with Aritra
          Dutta (KAUST) and Xin Li (University of Central Florida).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">Principal
            component pursuit (PCP) is a state-of-the-art approach for
            background estimation problems. Due to their higher
            computational cost, PCP algorithms, such as robust principal
            component analysis (RPCA) and its variants, are not feasible
            in processing high definition videos. To avoid the curse of
            dimensionality in those algorithms, several methods have
            been proposed to solve the background estimation problem in
            an incremental manner. We propose a batch-incremental
            background estimation model using a special weighted
            low-rank approximation of matrices. Through experiments with
            real and synthetic video sequences, we demonstrate that our
            method is superior to the state-of-the-art background
            estimation algorithms such as GRASTA, ReProCS, incPCP, and
            GFL. </span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 27, 2017</h3>
          <br>
          <span class="important">IMA Fox Prize (2nd Prize) for Robert
            Mansel Gower <br>
            <br>
          </span><a href="http://www.di.ens.fr/%7Ergower/">Robert M.
            Gower<span style="font-style: italic;"></span></a> was
          awarded a Leslie Fox Prize (2nd Prize) by the <a
            href="https://ima.org.uk">Institute of Mathematics and its
            Applications (IMA)</a> for the paper <a
            href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">Randomized















































            Iterative Methods for Linear Systems (SIAM J. Matrix Anal.
            &amp; Appl., 36(4), 1660–1690, 2015)</a>, coauthored with
          me. The list of finalists can be found <a
            href="http://people.maths.ox.ac.uk/wathen/fox/shortlist.php">here.</a>
          <br>
          <br>
          The Leslie Fox Prize for Numerical Analysis of the Institute
          of Mathematics and its Applications (IMA) is a biennial prize
          established in 1985 by the IMA in honour of mathematician
          Leslie Fox (1918-1992). The prize honours "young numerical
          analysts worldwide" (any person who is less than 31 years
          old), and applicants submit papers for review. A committee
          reviews the papers, invites shortlisted candidates to give
          lectures at the Leslie Fox Prize meeting, and then awards
          First Prize and Second Prizes based on "mathematical and
          algorithmic brilliance in tandem with presentational skills".<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 26, 2017</h3>
          <br>
          <a href="http://jakubkonecny.com">Jakub Konečný</a> defended
          his PhD thesis <span style="font-style: italic;">"Stochastic,
            Distributed and Federated Optimization for Machine Learning"</span>
          today. Congratulations!<br>
          <br>
          Jakub joined my group as a PhD student in August 2013. His PhD
          was in his first year supported by the Principal's Career
          Development Scholarship, and in the subsequent years by a
          Google Europe Doctoral Fellowship in Optimization Algorithms.
          Jakub has co-authored 13 papers during his PhD (<a
href="https://scholar.google.com/citations?hl=en&amp;user=4vq7eXQAAAAJ&amp;view_op=list_works">links















































            to the papers can be found here</a>). He has worked on
          diverse topics such as distributed optimization, machine
          learning, derivative-free optimization, federated learning,
          gesture recognition, semi-stochastic methods and
          variance-reduced algorithms for empirical risk minimization.
          He is joining Google Seattle in August 2017 as a research
          scientist.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 21, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1706.07636">"Privacy Preserving
            Randomized Gossip Algorithms"</a> - joint work with <a
            href="http://filiphanzely.com">Filip Hanzely</a>
          (Edinburgh), <a href="http://jakubkonecny.com">Jakub Konečný</a>
          (Edinburgh), <a href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas















































            Loizou</a> (Edinburgh) and Dmitry Grishchenko (Higher School
          of Economics, Moscow).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">In this
            work we present three different randomized gossip algorithms
            for solving the average consensus problem while at the same
            time protecting the information about the initial private
            values stored at the nodes. We give iteration complexity
            bounds for all methods, and perform extensive numerical
            experiments. </span> <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 18, 2017</h3>
          <br>
          I am now in Slovakia, visiting <a
            href="http://www.iam.fmph.uniba.sk/ospm/Harman/index.htm">Radoslav
















            Harman</a> at <a href="http://uniba.sk/en/">Comenius
            University</a>. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 15, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1706.04957">"Stochastic
            primal-dual hybrid gradient algorithm with arbitrary
            sampling and imaging applications"</a> - joint work with <a
            href="http://www.cmap.polytechnique.fr/%7Eantonin/">Antonin
            Chambolle</a> (Ecole Polytechnique), <a
            href="http://www.damtp.cam.ac.uk/user/me404/">Matthias J.
            Ehrhardt</a> (Cambridge), and <a
            href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">Carola-Bibiane
















            Schoenlieb</a> (Cambridge).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">We propose
            a stochastic extension of the primal-dual hybrid gradient
            algorithm studied by Chambolle and Pock in 2011 to solve
            saddle point problems that are separable in the dual
            variable. The analysis is carried out for general
            convex-concave saddle point problems and problems that are
            either partially smooth / strongly convex or fully smooth /
            strongly convex. We perform the analysis for arbitrary
            samplings of dual variables, and obtain known deterministic
            results as a special case. Several variants of our
            stochastic method significantly outperform the deterministic
            variant on a variety of imaging tasks.</span> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 4, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1706.01108">"Stochastic
            reformulations of linear systems: algorithms and convergence
            theory"</a> - joint work with <a href="http://mtakac.com">Martin















































            Takáč</a> (Lehigh).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">We develop
            a&nbsp; family of reformulations of an arbitrary consistent
            linear system into&nbsp; a stochastic problem. The
            reformulations are governed by two user-defined parameters:
            a positive definite matrix defining a norm, and an arbitrary
            discrete or continuous distribution over random matrices.
            Our reformulation has several equivalent interpretations,
            allowing for researchers from various communities to
            leverage their domain specific insights. In particular, our
            reformulation can be equivalently seen as a stochastic
            optimization problem, stochastic linear system, stochastic
            fixed point problem and a probabilistic intersection
            problem. We prove sufficient, and necessary and sufficient
            conditions for the reformulation to be exact. <br>
            <br>
            Further, we propose and analyze three stochastic algorithms
            for solving the reformulated problem---basic, parallel and
            accelerated methods---with global linear convergence rates.
            The rates can be interpreted as&nbsp; condition numbers of a
            matrix which depends on the system matrix and on the
            reformulation parameters. This gives rise to a new
            phenomenon&nbsp; which we call stochastic preconditioning,
            and which refers to the problem of finding parameters
            (matrix and distribution) leading to a sufficiently small
            condition number. Our basic method can be equivalently
            interpreted as&nbsp; stochastic gradient descent, stochastic
            Newton method, stochastic proximal point method, stochastic
            fixed point method, and stochastic projection method,&nbsp;
            with fixed stepsize (relaxation parameter), applied to the
            reformulations. </span><br>
          <br>
          Comment: I have taught a course at the University of Edinburgh
          in Spring 2017 which was largely based on the results in this
          paper. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 3, 2017</h3>
          <br>
          I am now back at KAUST to welcome <a
            href="https://scholar.google.com/citations?user=vquoiHsAAAAJ&amp;hl=en">Aritra















































            Dutta</a> who just joined my group at KAUST as a postdoc.
          Aritra: welcome! <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 26, 2017</h3>
          <br>
          I am in Edinburgh now - I'll be here until May 30. I am then
          giving a talk at Plymouth University on May 31 and at Cardiff
          University on June 1st. I'll be in London on June 2nd.<br>
          <br>
          Update (June 4): <a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/papers/stochastic_reformulations.pdf">This


















            is the paper</a> I talked about in Plymouth and Cardiff. <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 20, 2017</h3>
          <br>
          I am in Vancouver as of today, attending the <a
            href="http://www.siam.org/meetings/op17/">SIAM Conference on
            Optimization</a>. I am giving a talk on Monday, May 22
          (stochastic reformulations of linear systems), and so is <a
            href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas Loizou</a>
          (stochastic heavy ball method) and <a
            href="http://filiphanzely.com">Filip Hanzely</a> (randomized
          methods for minimizing relatively smooth functions).
          Strangely, none of these three papers are online yet! <a
            href="http://www.di.ens.fr/%7Ergower/">Robert Gower</a> is
          giving his talk on Tuesday (sketch and project: a tool for
          designing stochastic quasi-Newton methods and stochastic
          variance reduced gradient methods). The first part of the talk
          is based on <a href="https://arxiv.org/abs/1602.01768">this</a>
          and <a href="http://proceedings.mlr.press/v48/gower16.html">this</a>
          paper, the variance reduction part is also new and not online
          yet. <a href="http://jakubkonecny.com">Jakub Konečný</a> on
          Wednesday (<a href="https://arxiv.org/abs/1608.06879">AIDE:
            fast and communication efficient distributed optimization</a>).<br>
          <br>
          Update (June 4): <a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/papers/stochastic_reformulations.pdf">This


















            is the paper</a> I talked about in Vancouver. Here are the
          talk slides.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>May 15, 2017</h3>
          <br>
          <a href="http://mtakac.com">Martin Takáč</a> is visiting me at
          <a href="https://www.kaust.edu.sa/en">KAUST</a> this week.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>May 10, 2017</h3>
          <br>
          <span class="important">New Approach to AI: Federated Learning<br>
            <br>
          </span>Standard machine learning approaches require
          centralizing the training data on one machine or in a
          datacenter. For models trained from user interaction with
          mobile devices, a new approach was just released by Google, a
          result of collaboration between Google, <a
            href="http://jakubkonecny.com">Jakub Konečný</a> and myself.
          <br>
          <br>
          The new approach is called Federated Learning; it is described
          in the following four paper:<br>
          <br>
          [1] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio
          Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage,
          Aaron Segal and Karn Seth<br>
          <a href="http://eprint.iacr.org/2017/281">Practical Secure
            Aggregation for Privacy Preserving Machine Learning</a>
          (3/2017)<br>
          <br>
          [2] Jakub Konečný, H. Brendan McMahan, Felix X. Yu, Peter
          Richtárik, Ananda Theertha Suresh, Dave Bacon<br>
          <a href="https://arxiv.org/abs/1610.05492">Federated Learning:
            Strategies for Improving Communication Efficiency</a>
          (10/2016)<br>
          <br>
          [3] Jakub Konečný, H. Brendan McMahan, Daniel Ramage, Peter
          Richtárik<br>
          <a href="https://arxiv.org/abs/1610.02527">Federated
            Optimization: Distributed Machine Learning for On-Device
            Intelligence</a> (10/2016)<br>
          <br>
          [4] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth
          Hampson, Blaise Agüera y Arcas<br>
          <a href="https://arxiv.org/abs/1602.05629">Communication-Efficient















































            Learning of Deep Networks from Decentralized Data</a>
          (2/2016)<br>
          <br>
          Federated Learning enables mobile phones to collaboratively
          learn a shared prediction model while keeping all the training
          data on device, decoupling the ability to do machine learning
          from the need to store the data in the cloud. This goes beyond
          the use of local models that make predictions on mobile
          devices by bringing model training to the device as well. The
          technology is now in use by around 1 billion Android devices.<br>
          <br>
          The CEO of Google, Sundar Pichai, <a
            href="http://avondaleam.com/alphabet-1q17-earnings-call-notes/">said:</a>
          <br>
          <br>
          <span style="font-style: italic;">“… we continue to set the
            pace in machine learning and AI research. We introduced a
            new technique for training deep neural networks on mobile
            devices called&nbsp;Federated Learning. This technique
            enables people to run a shared machine learning model, while
            keeping the underlying data stored locally on mobile
            phones." </span><br>
          <br>
          The&nbsp;new technology&nbsp;is described in a <a
href="https://research.googleblog.com/2017/04/federated-learning-collaborative.html">Google















































            Research Blog, dated April 2017,</a> to a lay audience.
          Selected media coverage:<br>
          <br>
          <ul>
            <li><a
href="https://www.forbes.com/sites/kevinmurnane/2017/04/11/google-makes-your-smartphone-smarter-with-federated-learning/#7c1d9eb25a0f">Forbes</a></li>
            <li><a
href="https://www.theverge.com/2017/4/10/15241492/google-ai-user-data-federated-learning">The










































                Verge</a></li>
            <li><a
href="https://qz.com/952986/google-goog-may-be-using-your-phones-keyboard-to-train-its-massive-ai-brain/?utm_source=dlvr.it&amp;utm_medium=twitter">Quartz</a></li>
            <li><a
href="http://www.techrepublic.com/article/how-google-wants-to-crowdsource-machine-learning-with-smartphones-and-federated-learning/">TechRepublic</a></li>
            <li><a
href="http://www.zdnet.com/article/smarter-android-ai-powered-google-services-will-get-better-as-you-use-them/">ZDNet</a></li>
            <li><a
href="http://www.cbronline.com/news/mobility/smartphones/google-wants-make-android-smarter-ai-algorithms-phone/">Computer










































                Business Review</a></li>
            <li><a
href="https://motherboard.vice.com/en_us/article/google-thinks-it-can-solve-artificial-intelligences-privacy-problem">Motherboard










































                Vice</a></li>
            <li><a
href="http://www.infoworld.com/article/3188430/artificial-intelligence/android-gboard-smartens-up-with-federated-machine-learning.html">Infoworld</a></li>
            <li><a href="http://www.silicon.co.uk">Silicon.co.uk</a><br>
            </li>
            <li><a
href="https://venturebeat.com/2017/04/06/following-apple-google-tests-differential-privacy-in-gboard-for-android/">Venturebeat</a></li>
            <li><a
href="https://www.engadget.com/2017/04/07/gboard-studies-your-behavior-without-sending-details-to-google/">Engadget</a></li>
            <li><a
href="http://www.technarratives.com/2017/04/06/google-develops-federated-machine-learning-method-which-keeps-personal-data-on-devices/">Tech










































                Narratives</a><br>
            </li>
            <li><a
href="https://android.gadgethacks.com/news/google-introduces-ai-for-its-android-services-learns-from-you-without-compromising-privacy-0177003/">GadgetHacks</a><br>
            </li>
            <li><a
href="http://bgr.com/2017/04/10/googles-new-ai-doesnt-need-to-talk-to-the-cloud/">BGR</a></li>
            <li><a
href="http://www.androidauthority.com/google-machine-learning-privacy-federated-learning-762978/">AndroidAuthority</a></li>
            <li><a
href="https://www.androidheadlines.com/2017/04/google-intros-ai-solution-based-on-federated-learning.html">AndroidHeadlines</a></li>
            <li><a
href="http://www.tomsguide.com/us/google-federated-learning-autocorrect-improvements,news-24853.html">Tom's










































                Guide</a></li>
            <li><a
href="https://www.digitaltrends.com/mobile/google-federated-learning-android/">Digital










































                Trends</a><br>
            </li>
            <li><a
href="https://www.getrevue.co/profile/azeem/issues/publishing-in-the-platform-globalisation-google-s-deep-learning-chip-panpsychism-elephants-poo-graphene-108-52456">The










































                Exponential View</a></li>
            <li><a href="http://www.vvcat.com/view/187072.html">vvcat</a><br>
            </li>
            <li><a
href="https://9to5google.com/2017/04/06/google-testing-new-differential-privacy-strategy-with-gboard-for-android/">9to5google</a><br>
            </li>
            <li><br>
            </li>
          </ul>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <br>
          <h3>May 9, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1705.02005">"Parallel stochastic
            Newton method"</a> - joint work with Mojmír Mutný (ETH
          Zurich).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">We propose
            a parallel stochastic Newton method (PSN) for minimizing
            smooth convex functions. We analyze the method in the
            strongly convex case, and give conditions under which
            acceleration can be expected when compared to it serial
            stochastic Newton. We show how PSN can be applied to the
            empirical risk minimization problem, and demonstrate the
            practical efficiency of the method through numerical
            experiments and models of simple matrix classes.</span><br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <br>
          <h3>May 6, 2017</h3>
          <br>
          I am hosting two interns from <a href="http://iitk.ac.in">Indian















































            Institute of Technology, Kanpur</a> this Summer at <a
            href="https://www.kaust.edu.sa/en">KAUST</a>; they just
          arrived: <a
            href="https://www.linkedin.com/in/aashutosh-tiwari/?ppe=1">Aashutosh















































            Tiwari</a> and <a
            href="https://www.linkedin.com/in/atal-narayan-a517ba104/">Atal















































            Narayan</a>. Welcome!<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <br>
          <h3>May 2, 2017</h3>
          <br>
          <span class="important">Most Downloaded SIMAX Paper</span><br>
          <br>
          The paper <a
            href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">"Randomized















































            Iterative Methods for Linear Systems"</a>, coauthored with <a
            href="http://www.di.ens.fr/%7Ergower/">Robert M. Gower</a>
          and published in 2015, is now the <a
href="http://epubs.siam.org/action/showMostReadArticles?journalCode=sjmael&amp;">Most















































            Downloaded Paper in the SIAM Journal on Matrix Analysis and
            Applications</a>. <br>
          <br>
          The paper was the Second Most Downloaded Paper since at least
          August 2016 when Robert noticed this and brought it to my
          attention. We have just noticed it climbed up to the #1
          position. Thanks for all the interest in our work!<br>
          <br>
          For those who want to pursue the line of work initiated in our
          paper, we recommend looking at the following follow-up papers
          where we address several extensions and obtain various
          additional insights and improvements:<br>
          <br>
          1) Stochastic dual ascent for solving linear systems, <a
            href="https://arxiv.org/abs/1512.06890">arXiv:1512.06890</a><br>
          <br>
          Here we lift the full rank assumption from our original SIMAX
          paper. In doing so, we discover a particularly beautiful
          duality theory behind the method. This also leads to the
          design of a novel stochastic method in optimization, which we
          call Stochastic Dual Ascent (SDA). With a bit of hindsight -
          we should have called it Stochastic Dual Subspace Ascent
          (SDSA).<br>
          <br>
          2) Randomized quasi-Newton updates are linearly convergent
          matrix inversion algorithms, <a
            href="https://arxiv.org/abs/1602.01768">arXiv:1602.01768</a><br>
          <br>
          The basic idea behind this paper is to apply a similar
          methodology we used to solve linear systems in the SIMAX paper
          to the problem of computing an inverse of a (large) matrix.
          I'll simply copy-paste the abstract here:<br>
          <br>
          <span style="font-style: italic;">We develop and analyze a
            broad family of stochastic/randomized algorithms for
            inverting a matrix. We also develop specialized variants
            maintaining symmetry or positive definiteness of the
            iterates. All methods in the family converge globally and
            linearly (i.e., the error decays exponentially), with
            explicit rates. In special cases, we obtain stochastic block
            variants of several quasi-Newton updates, including bad
            Broyden (BB), good Broyden (GB), Powell-symmetric-Broyden
            (PSB), Davidon-Fletcher-Powell (DFP) and
            Broyden-Fletcher-Goldfarb-Shanno (BFGS). Ours are the first
            stochastic versions of these updates shown to converge to an
            inverse of a fixed matrix. Through a dual viewpoint we
            uncover a fundamental link between quasi-Newton updates and
            approximate inverse preconditioning. Further, we develop an
            adaptive variant of randomized block BFGS, where we modify
            the distribution underlying the stochasticity of the method
            throughout the iterative process to achieve faster
            convergence. By inverting several matrices from varied
            applications, we demonstrate that AdaRBFGS is highly
            competitive when compared to the well established
            Newton-Schulz and minimal residual methods. In particular,
            on large-scale problems our method outperforms the standard
            methods by orders of magnitude. Development of efficient
            methods for estimating the inverse of very large matrices is
            a much needed tool for preconditioning and variable metric
            optimization methods in the advent of the big data era.</span><br>
          <br>
          3) Stochastic block BFGS: squeezing more curvature out of
          data, <a href="http://proceedings.mlr.press/v48/gower16.html">ICML















































            2016</a><br>
          <br>
          In this work we apply the stochastic block BFGS method
          developed in the above paper to empirical risk minimization.
          Of course, much more is needed than just a straightforward
          application - but this is the initial idea behind the work.<br>
          <br>
          4) Linearly convergent randomized iterative methods for
          computing the pseudoinverse, <a
            href="https://arxiv.org/abs/1612.06255">arXiv:1612.06255</a><br>
          <br>
          Here we show that after suitable insights and modifications,
          the iterative sketching framework for inverting matrices from
          the "quasi-Newton" paper above can be used to compute the
          Moore-Penrose pseudoinverse of arbitrary (rectangular) real
          matrices. Extension to the complex setting is possible, but we
          did not do it.<br>
          <br>
          5) Soon I will post a new paper on ArXiv which will go much
          deeper than the SIMAX paper - this work will represent what is
          to the best of my knowledge the deepest insight into the
          sketch-and-project we have at the moment. <br>
          <br>
          <span style="font-style: italic;">Update (18.6.2017):</span>
          The paper I mentioned in item 5 above is <a
            href="https://arxiv.org/abs/1706.01108">now on arXiv</a>. <br>
          <br>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 19, 2017</h3>
          <br>
          <a href="http://www.dominikcsiba.com">Dominik Csiba</a> is
          giving a talk entitled "The role of optimization in machine
          learning" at the <a href="http://www.mlmu.sk/program/">Machine















































            Learning MeetUp (MLMU) in Bratislava today (at 7pm)</a>. If
          you are around and interested in machine learning and/or
          optimization, I recommend you attend!<br>
          <br>
          <span style="font-style: italic;">Update (May 26, 2017):</span>
          A <a href="https://youtu.be/fCzxhe66kKc">video recording of
            Dominik's talk is now online</a>.<br>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 17, 2017</h3>
          <br>
          <a
            href="https://www.semanticscholar.org/author/Alibek-Sailanbayev/2476427">Alibek















































            Sailanbayev</a> is visiting me at <a
            href="https://www.kaust.edu.sa/en">KAUST</a> this week.<br>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 10, 2017</h3>
          <br>
          <div align="left">I am giving 2 talks this week. Today, I am
            giving a talk on stochastic Chambolle-Pock algorithm at the
            <a
              href="https://vcc.kaust.edu.sa/Conference-2017/Pages/default.aspx">Visual















































              Computing Conference</a> held at KAUST. My PhD students <a
              href="http://www.jakubkonecny.com">Jakub</a>, <a
              href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas</a>
            and <a href="http://filiphanzely.com">Filip </a>are
            presenting posters tomorrow. On Thursday (April 13), I am
            giving a talk on randomized algorithms in linear algebra at
            the <a
href="https://cemse.kaust.edu.sa/events/Pages/AMCS-Graduate-Peter-Richtarik.aspx">AMCS















































              Graduate Seminar</a> at KAUST.<br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 9, 2017</h3>
          <br>
          <div align="left">I have several visitors at KAUST at the
            moment. <a href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas















































              Loizou</a> arrived in late March and is staying until
            early may. <a href="http://filiphanzely.com">Filip Hanzely</a>
            and <a href="http://jakubkonecny.com">Jakub Konečný</a>
            arrived yesterday; Jakub will stay for a couple weeks and
            Filip will stay until late May, after which all of will go
            to Vancouver for <a
              href="http://www.siam.org/meetings/op17/">SIAM Conference
              on Optimization</a>. <a
              href="https://angel.co/konstantin-mishchenko">Konstantin
              Mishchenko</a> (Paris Dauphine / Ecole Normale Superieure)
            visited for a few days recently, and is now attending the <a
href="http://ljk.imag.fr/membres/Jerome.Malick/osl2017/people.html">OSL
              2017 workshop</a> in Les Houches, France. Dmitry I.
            Grishchenko (Higher School of Economics) arrived yesterday
            and is staying for 2 weeks.<br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 6, 2017</h3>
          <br>
          <div align="left"><span class="important">#1 Trending Paper in
              Mathematical Programming (Series A and B)</span><br>
            <br>
            About a week ago I have received an email (see a screenshot
            below) in which Springer notifies the optimization community
            about the top 5 trending articles in the Mathematical
            Programming (Series A and B) journals. It was a great
            surprise (and pleasure) to learn that our paper <a
              href="http://link.springer.com/article/10.1007/s10107-015-0901-6">Parallel















































              coordinate descent methods for big data optimization</a>
            (coauthored with <a href="http://www.mtakac.com">Martin
              Takáč</a>) is <span class="important">#1 on the list!</span><br>
            <br>
            <img style="width: 700px; height: 947px;" alt="Top 5
              trending articles in Mathematical Programming, Series A
              and B"
src="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/imgs/2017-MAPR-trending-papers.png"><br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>March 29, 2017</h3>
          <br>
          <div align="left"><a href="http://www.di.ens.fr/%7Ergower/">Robert















































              Gower's</a> paper <a
              href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">Randomized
Iterative















































              Methods for Linear Systems (SIAM Journal on Matrix
              Analysis and Applications 36(4):1660-1690, 2015)</a>,
            written in collaboration with me, was shortlisted for the <a
href="http://people.maths.ox.ac.uk/wathen/fox/shortlist.php">18th Leslie
              Fox Prize in Numerical Analysis</a>. <br>
            <br>
            This paper has been the <a
href="http://epubs.siam.org/action/showMostReadArticles?journalCode=sjmael&amp;">2nd
most















































              downloaded SIMAX paper</a> since (at least) August 2016.
            The first most downloaded paper was published in year 2000.<br>
            <br>
            The Leslie Fox Prize for Numerical Analysis of the <a
              href="https://ima.org.uk">Institute of Mathematics and its
              Applications (IMA)</a> is a biennial prize established in
            1985 by the IMA in honour of mathematician <a
              href="https://en.wikipedia.org/wiki/Leslie_Fox">Leslie Fox
              (1918-1992)</a>. The prize honours "young numerical
            analysts worldwide" (any person who is less than 31 years
            old), and applicants submit papers for review. A committee
            reviews the papers, invites shortlisted candidates to give
            lectures at the Leslie Fox Prize meeting, and then awards
            First Prize and Second Prizes based on "mathematical and
            algorithmic brilliance in tandem with presentational
            skills".<br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>March 17, 2017</h3>
          <br>
          <div align="left">As of today, <a
              href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
              Loizou</a> is visiting me at <a
              href="https://www.kaust.edu.sa/en">KAUST</a>. He will stay
            until early May.<br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>March 14, 2017</h3>
          <br>
          <div align="left"><a href="http://filiphanzely.com/">Filip
              Hanzely</a> is giving a talk in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big















































              data optimization seminar</a> today. He will speak on <a
              href="https://arxiv.org/abs/1611.01146">"Finding
              Approximate Local Minima for Nonconvex Optimization in
              Linear Time"</a>.<br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>March 13, 2017</h3>
          <br>
          <div align="left">I will be giving a tutorial on randomized
            optimization methods at the <a
              href="http://www.ds3-datascience-polytechnique.fr">Data
              Science Summer School</a>, held at CMAP, École
            Polytechnique, during August 28 - September 1, 2017. <br>
            <br>
            Full list of tutorials:<br>
            <br>
            - <a
              href="http://www.iro.umontreal.ca/%7Ebengioy/yoshua_en/index.html">Yoshua















































              Bengio</a> (Montreal): Deep Learning<br>
            - <a href="https://www.cs.cmu.edu/%7Epradeepr/">Pradeep
              Ravikumar</a> (Carnegie Mellon): Graphical Models<br>
            - Peter Richtarik (Edinburgh &amp; KAUST): Randomized
            Optimization Methods<br>
            - <a href="https://sites.ualberta.ca/%7Eszepesva/">Csaba
              Szepesvari</a> (Alberta): Bandits<br>
            <br>
            Plenary speakers:<br>
            <br>
            - <a href="https://www.linkedin.com/in/carchambeau/">Cedric
              Archambeau</a> (Amazon)<br>
            - <a
              href="https://research.google.com/pubs/OlivierBousquet.html">Olivier















































              Bousquet</a> (Google)<br>
            - <a href="http://blogs.ulg.ac.be/damien-ernst/">Damien
              Ernst</a> (Liege)<br>
            - <a href="https://who.rocq.inria.fr/Laura.Grigori/">Laura
              Grigori</a> (INRIA)<br>
            - <a href="http://www.meyn.ece.ufl.edu">Sean Meyn</a>
            (Florida)<br>
            - <a href="http://www.nowozin.net/sebastian/">Sebastian
              Nowozin</a> (Microsoft Research)<br>
            - <a href="https://people.eecs.berkeley.edu/%7Erussell/">Stuart















































              Russell</a> (Berkeley)<br>
            <br>
            The <a
              href="http://www.ds3-datascience-polytechnique.fr/program/">full
program















































              can be found here.</a><br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>March 7, 2017</h3>
          <br>
          <div align="left"><a href="http://www.macs.hw.ac.uk/%7Emp71/">Marcelo















































              Pereyra</a> is giving a talk in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big















































              data optimization seminar today.</a> He is talking about
            his recent paper "Efficient Bayesian computation by proximal
            Markov chain Monte Carlo: when Langevin meets Moreau".<br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>February 28, 2017</h3>
          <br>
          <div align="left"><a href="http://jakubkonecny.com">Jakub
              Konečný</a> is giving a talk in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big















































              data optimization seminar today.</a><br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>February 26, 2017</h3>
          <br>
          <div align="left">I am on a leave from Edinburgh as of today.
            I have taken up an Associate Professor position at <a
              href="https://www.kaust.edu.sa/en">King Abdullah
              University of Science and Technology (KAUST). </a>I am
            affiliated with the <a
              href="https://cs.kaust.edu.sa/Pages/Home.aspx">Computer
              Science (CS)</a> and <a
              href="https://amcs.kaust.edu.sa/Pages/Home.aspx">Applied
              Mathematics &amp; Computational Science (AMCS)</a>
            programs. I am also affiliated with the <a
              href="https://ecrc.kaust.edu.sa/Pages/Home.aspx">Extreme
              Computing Research Center (ECRC)</a> and the <a
              href="https://vcc.kaust.edu.sa/Pages/Home.aspx">Visual
              Computing Center (VCC)</a>. I have several positions open,
            contact me if interested!<br>
            <br>
            PS: I am pleasantly surprised to see that the weather at
            KAUST is great at this time of the year! <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>February 21, 2017</h3>
          <br>
          <div align="left"><a
              href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
              Loizou</a> is giving a talk in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big















































              data optimization seminar today.</a><br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>February 14, 2017</h3>
          <br>
          <div align="left"><a
              href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=506">Kostas















































              Zygalakis</a> is giving a talk in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big















































              data optimization seminar today.</a><br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>February 7, 2017</h3>
          <br>
          <div align="left"><a href="http://personal.lse.ac.uk/veghl/">László















































              Végh</a> is visiting me for a couple days. He is also
            giving a talk in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big















































              data optimization seminar tomorrow.</a><br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>January 29, 2017</h3>
          <br>
          <div align="left"> Between January 29 and February 3, I am in
            Villars-sur-Ollon, Switzerland, attending the <a
              href="http://www.baspfrontiers.org"> BASP Frontiers 2017
              workshop.</a><br>
            <br>
          </div>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>January 25, 2017</h3>
          <br>
          <div align="left"> Due to certain administrative reasons,
            interviews for the 2 postdoc posts I have open will happen a
            bit later than anticipated. Originally I expected the
            interviews to happen this week - there will be some delay
            with this.<br>
          </div>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>January 23, 2017</h3>
          <span class="important"><br>
          </span>
          <div align="left">I have two visitors this week: <a
              href="http://acse.pub.ro/person/ion-necoara/">Ion Necoara
              (Bucharest)</a> and <a
              href="http://maiage.jouy.inra.fr/?q=fr/node/516">Elhoucine
              Bergou (INRA)</a>.<br>
            <br>
            <br>
          </div>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>January 19, 2017</h3>
          <span class="important"><br>
          </span>
          <div align="left">I am in London today and tomorrow. Today I
            am discussing research with people at the <a
              href="https://www.turing.ac.uk/">Alan Turing Institute</a>
            (we managed to start a new project today and prove a lemma
            to kick it off), and tomorrow I am giving a seminar talk in
            the <a href="http://www.imperial.ac.uk/computing">Department














































              of Computing at Imperial College</a>.<br>
            <br>
            <br>
          </div>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>January 16, 2017</h3>
          <span class="important"><br>
          </span>
          <div align="left">As of this week, I am teaching an intensive
            course (6 hours per week) entitled "Modern Optimization
            Methods for Big Data Problems". This is a rigorous course
            covering some of the fundamentals of randomized algorithms
            for optimization problems described by very large quantities
            of data. It is open to anyone interested (the current
            composition of the class includes PhD students, Master's
            students, a few undergraduate students and even some
            faculty).<br>
            <br>
          </div>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>January 10, 2017</h3>
          <span class="important"><br>
          </span>
          <div align="left">We have several Lectureships (i.e.,
            positions equivalent to Assistant Professorships in the US)
            open in the School of Mathematics:<br>
            <br>
            <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/jobs/lectureship-in-industrial-mathematics">Lectureship
in















































              Industrial Mathematics</a>, application deadline: February
            1, 2017 (5pm UK time)<br>
            <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/jobs/lectureships-in-statistics-and-data-science">Two
Lectureships















































              in Statistics and Data Science</a>, application deadline:
            February 7, 2017 (5pm UK time)<br>
            <br>
          </div>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>January 7, 2017</h3>
          <span class="important"><br>
          </span>
          <div align="left">I am in Slovakia at the moment, and it's
            been crazy cold the last few days. Today we have -15 degrees
            Celsius, but it feels like -25. This is when your eyebrows
            freeze. Anyway, I am returning back to Edinburgh tomorrow.<br>
            <br>
            <i>Some news:</i> <a href="http://www.dominikcsiba.com/">Dominik















































              Csiba</a> is now based in Slovakia. He will be finishing
            off his PhD from there; and is expected to submit his thesis
            in the Summer of 2017. He will be picking up some
            optimization/machine learning related activities in Slovakia
            - do talk to him if you get a chance! For instance, on
            February 15, <a href="http://www.dominikcsiba.com/">Dominik</a>
            will give a talk at a <a href="http://www.mlmu.sk/">Slovak
              Machine Learning Meetup</a> (MLMU). Further, Dominik is a
            mentor in a Data Science <a
              href="http://www.basecamp.ai/student/#about">BaseCamp</a>.
            Here is a blurb from their website: "BaseCamp is an
            immersive full-time 8-week program for prospective data
            scientists. During 8 weeks you will deepen your theoretical
            knowledge, enhance your practical skills and become a
            qualified data scientist ready for your exciting data
            science career." <br>
            <br>
            <br>
          </div>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>December 21, 2016</h3>
          <span class="important"><br>
          </span>
          <div align="left"><span class="important">!!! 2 postdoc posts:
            </span>I have two 1-year postdoctoral associate positions
            open, ideally starting on March 1, 2017 (Feb 1 or April 1
            are also possible starting dates). If interested, please get
            in touch with me! <br>
            <br>
            Areas: big data optimization, machine learning, randomized
            numerical linear algebra.<br>
            <br>
            <i>Update: </i>Application deadline: January 23, 2017 (5pm
            UK time)<br>
            <br>
            For more information on the position and the required
            application files, and to get to the online application
            form, <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=038477">click


















              here</a>.<br>
            <br>
          </div>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>December 20, 2016</h3>
          <span class="important"><br>
          </span><span class="important">New paper out:</span> <a
            href="https://arxiv.org/abs/1612.06255">Linearly convergent
            randomized iterative methods for computing the pseudoinverse</a>,
          joint with <a href="http://www.di.ens.fr/%7Ergower/">Robert
            M. Gower.</a><br>
          <br>
          Abstract: <i>We develop the first stochastic incremental
            method for calculating the Moore-Penrose pseudoinverse of a
            real rectangular matrix. By leveraging three alternative
            characterizations of pseudoinverse matrices, we design three
            methods for calculating the pseudoinverse: two general
            purpose methods and one specialized to symmetric matrices.
            The two general purpose methods are proven to converge
            linearly to the pseudoinverse of any given matrix. For
            calculating the pseudoinverse of full rank matrices we
            present additional two specialized methods which enjoy
            faster convergence rate than the general purpose methods. We
            also indicate how to develop randomized methods for
            calculating approximate range space projections, a much
            needed tool in inexact Newton type methods or quadratic
            solvers when linear constraints are present. Finally, we
            present numerical experiments of our general purpose methods
            for calculating pseudoinverses and show that our methods
            greatly outperform the Newton-Schulz method on large
            dimensional matrices.&nbsp;</i> <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>December 5, 2016</h3>
          <span class="important"><br>
          </span>
          <div align="left"><span class="important"> </span>This week,
            everyone is away. <a href="http://jakubkonecny.com/">Jakub</a>,
            <a href="http://www.dominikcsiba.com/">Dominik</a> and <a
              href="http://filiphanzely.com/">Filip</a> are at <a
              href="https://nips.cc/">NIPS in Barcelona</a>. <a
              href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas</a>
            is at <a href="http://www.ieeeglobalsip.org/">GlobalSip</a>
            in Greater Washington, D.C. Because of this, we are not
            having the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">Big

















              Data Optimization Seminar</a> this week. Next week, we
            have an invited speaker from Imperial College giving a talk
            in the seminar: <a href="http://www.doc.ic.ac.uk/%7Epp500/">Panos

















              Parpas</a>. <br>
          </div>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 30, 2016</h3>
          <span class="important"><br>
          </span>
          <div align="left"><span class="important"> </span><span
              class="important">New poster: </span><a
              href="https://arxiv.org/abs/1611.07555">Federated
              learning: strategies for improving communication
              efficiency</a>, to be presented by <a
              href="http://jakubkonecny.com/">Jakub Konečný</a> at the <a
              href="https://pmpml.github.io/PMPML16/">NIPS Private
              Multi-Party Machine Learning Workshop</a> in Barcelona.
            The underlying paper is <a
              href="https://pmpml.github.io/PMPML16/papers/PMPML16_paper_20.pdf">here</a>.<br>
          </div>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 30, 2016</h3>
          <span class="important"><br>
          </span>For the rest of the week I am in Moscow, visiting <a
            href="http://www.skoltech.ru/en/">SkolTech</a>, <a
            href="https://mipt.ru/en/">MIPT</a> and <a
            href="https://www.yandex.ru/">Yandex</a> (Russian search
          engine company). I am giving a talk at SkolTech on Thursday
          and at Yandex/MIPT on Friday.<br>
          <br>
          <i>Update (December 15):</i> Here is a <a
            href="https://events.yandex.ru/lib/talks/4294/">video
            recording of my Yandex talk</a>. The talk was mostly based
          on the papers <a
            href="http://link.springer.com/article/10.1007%2Fs11590-015-0916-1">NSync</a>
          (Optimization Letters 2015) and <a
href="http://papers.nips.cc/paper/5926-quartz-randomized-dual-coordinate-ascent-with-arbitrary-sampling">Quartz</a>
          (NIPS 2015), with a few slides mentioning <a
            href="http://www.jmlr.org/papers/v17/15-001.html">Hydra</a>
          (JMLR 2016) and <a
href="http://ieeexplore.ieee.org/document/6958862/?tp=&amp;arnumber=6958862&amp;url=http:%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Ftp%3D%26arnumber%3D6958862">Hydra^2</a>
          (IEEE MLSP 2014).<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 25, 2016</h3>
          <span class="important"><br>
          </span>Today I am giving a talk at Telecom ParisTech, in a <a
href="http://machinelearningforbigdata.telecom-paristech.fr/fr/article/workshop-friday-novembre-25-distributed-machine-learning">Workshop
on


















            Distributed Machine Learning</a>. The other two speakers are
          <a href="http://researchers.lille.inria.fr/abellet/">Aurélien
            Bellet (INRIA, Lille)</a> and <a
            href="https://people.kth.se/%7Emikaelj/">Mikael Johansson
            (KTH, Stockholm)</a>.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 24, 2016</h3>
          <span class="important"><br>
          </span>I am in Paris for the next couple days. Today I was an
          external examiner for a PhD thesis of <a
            href="http://perso.telecom-paristech.fr/%7Ecolin/">Igor
            Colin</a> at Telecom ParisTech. Igor is supervised by <a
            href="http://www.josephsalmon.eu/">Joseph Salmon</a> and <a
href="http://perso.telecom-paristech.fr/%7Eclemenco/Home.html">Stephan
            Clemenson</a>. Igor's thesis, entitled "Adaptation des
          méthodes d’apprentissage aux U-statistiques", is an in-depth
          exploration of several important aspects of machine learning
          involving U-statistics. Igor first develops strong statistical
          learning guarantees for ERM (empirical risk minimization) with
          incomplete U-statistics, then moves to solving the problem of
          computing/estimating U-statistics in a distributed environment
          via a gossip-like method, and finally develops a decentralized
          dual averaging optimization method for solving an ERM problem
          with pairwise functions. The results in the thesis are very
          strong, and the work is beautifully written. Needless to say,
          Igor defended easily.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 22, 2016</h3>
          <span class="important"><br>
          </span><span class="important">New paper out:</span> <a
            href="https://arxiv.org/abs/1611.07555">Randomized
            distributed mean estimation: accuracy vs communication</a>,
          joint with <a href="http://jakubkonecny.com/">Jakub Konečný.</a><br>
          <br>
          Abstract: <i>We consider the problem of estimating the
            arithmetic average of a finite collection of real vectors
            stored in a distributed fashion across several compute nodes
            subject to a communication budget constraint. Our analysis
            does not rely&nbsp; on any statistical assumptions about the
            source of the vectors. This problem arises as a subproblem
            in many applications, including reduce-all operations within
            algorithms for distributed and federated optimization and
            learning. We propose a flexible family of randomized
            algorithms exploring the trade-off between expected
            communication cost and estimation error. Our family contains
            the full-communication and zero-error method on one extreme,
            and an $\epsilon$-bit communication and ${\cal
            O}\left(1/(\epsilon n)\right)$ error method on the opposite
            extreme. In the special case where we&nbsp; communicate, in
            expectation, a single bit per coordinate of each vector, we
            improve upon existing results by obtaining
            $\mathcal{O}(r/n)$ error, where $r$ is the number of bits
            used to represent a floating point value.</i> <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 22, 2016</h3>
          <span class="important"><br>
          </span>Today we had <a
            href="http://www.maths.ed.ac.uk/%7Elszpruch/">Lukasz Szpruch</a>
          giving a talk in our <a
            href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">Big


















            Data Optimization Seminar</a>. He spoke about optimization,
          stochastic differential equations and consensus-based global
          optimization. Double thanks as he was able to make time
          despite just becoming a father. Congratulations!&nbsp; <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 21, 2016</h3>
          <span class="important"><br>
          </span>I traveled a bit last week. I first visited the Alan
          Turing Institute on November 16 and had some nice discussions
          before and over lunch with <a
            href="https://www.turing.ac.uk/research-fellows/">Armin
            Eftekhari and Hemant Tyagi</a>. Later on the same day I gave
          a <a
href="http://www.lse.ac.uk/maths/Seminars/Seminar-on-Combinatorics-Games-and-Optimisation.aspx">talk
at


















            London School of Economics</a>, and subsequently had a nice
          discussion with <a href="http://personal.lse.ac.uk/veghl/">Laszlo


















            Vegh</a> who is working on some problems similar to those
          I've been working on recently. The next day I took a train
          down to Southampton, where I gave a <a
href="http://www.southampton.ac.uk/cormsis/news/events/2016/Nov-Dec/seminar-richtarik.page">talk
on


















            SDNA in the CORMSIS seminar</a>. Thanks to <a
            href="http://www.southampton.ac.uk/maths/about/staff/abz1e14.page">Alain</a>,
          <a
            href="http://www.southampton.ac.uk/maths/about/staff/hx.page?">Xuifu</a>,
          <a
            href="http://www.southampton.ac.uk/maths/about/staff/tn6g10.page?">Tri-Dung</a>,
          and <a
            href="http://www.southampton.ac.uk/maths/about/staff/sc2r15.page?">Stefano</a>
          for fun discussions about mathematics, life, travel and
          politics! &nbsp;&nbsp;&nbsp; <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 9, 2016</h3>
          <span class="important"><br>
          </span>I am at Warwick today, giving a talk in the <a
            href="http://warwicksiam.yolasite.com/stream-1.php">2016
            Warwick SIAM Annual Conference on Machine Learning and
            Statistics</a>. <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 8, 2016</h3>
          <span class="important"><br>
          </span>We have had three interesting talks in our Big Data
          Optimization seminar series (aka "All Hands Meetings on Big
          Data Optimization") over the past three weeks. This is an
          informal reading seminar, covering recent papers in the field.
          <br>
          <br>
          On October 25, <a href="http://www.dominikcsiba.com/">Dominik
            Csiba</a> talked about "Linear Coupling", a framework for
          unifying gradient and mirror descent proposed in 2015 by
          Allen-Zhu and Orecchia. The week after, <a
            href="http://filiphanzely.com/">Filip Hanzely</a> talked
          about variance reduction methods for nonconvex stochastic
          optimization. Yesterday, <a
            href="http://www.maths.ed.ac.uk/%7Eateckent/">Aretha
            Teckentrup</a> talked about scaling up Gaussian process
          regression via doubly stochastic gradient descent. <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 4, 2016</h3>
          <span class="important"><br>
          </span>My <a
            href="https://www.youtube.com/watch?v=BS0kF4YijGc">OR58
            plenary talk on "Introduction to Big Data Optimization" is
            now on YouTube</a>. This is a very introductory talk,
          delivered at a slow pace, touching on topics such as gradient
          descent, handling nonsmoothness, acceleration, and
          randomization. <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>October 25, 2016</h3>
          <span class="important"><br>
          </span>My Alan Turing Institute <a
            href="https://www.youtube.com/watch?v=RbkhWrTbrKs">talk on
            "Stochastic Dual Ascent for Solving Linear Systems" is now
            on YouTube.</a> <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>October 14, 2016</h3>
          <span class="important"><br>
          </span>At 12:00 today, I am giving a short talk at <a
            href="http://www.icms.org.uk/">ICMS</a> in a seminar
          organized by the PhD students in the <a
            href="http://www.maxwell.ac.uk/migsaa">MIGSAA programme (The
            Maxwell Institute Graduate School in Analysis &amp; its
            Applications).</a> The students invite MIGSAA affiliated
          faculty of their choosing to speak about some of their recent
          work, chosen by the students. <br>
          <br>
          The full schedule of the event today:<br>
          <br>
          Peter Richtarik (12:00 – 12:30) <br>
          Empirical Risk Minimization: Complexity, Duality, Sampling,
          Sparsity and Big Data<br>
          &nbsp;<br>
          Lyonell Boulton (12:30 – 13:00)<br>
          Analytical and computational spectral theory<br>
          &nbsp;<br>
          Martin Dindos (13:00 – 13:30)<br>
          Elliptic and Parabolic PDEs with coefficients of minimal
          smoothness<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>October 13, 2016</h3>
          &nbsp; <span class="important"><br>
          </span>Today, I am giving a short talk at the <a
            href="https://turing.ac.uk/">Alan Turing Institute</a> in
          London. The talks in this series are recorded and will be put
          on YouTube. I will speak about "Stochastic Dual Ascent for
          Solving Linear Systems"; the content is based on two papers
          written jointly with Robert M. Gower [<a
            href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">paper


















            1</a>, <a href="http://arxiv.org/abs/1512.06890">paper 2</a>].<br>
          <br>
          If you are interested in stochastic optimization or fast
          algorithms for solving empirical risk minimization (ERM)
          problems in machine learning, this talk can be seen as a very
          good introduction into these areas, in a somewhat simplified
          setting. <br>
          <br>
          The methods I will talk about fit the ERM framework, and are
          both primal and dual in nature, simultaneously. They are
          variance-reduced (if you have followed recent research on
          variance-reduced methods for minimizing finite sums, you know
          what I am talking about) by default, and have linear
          convergence rate despite lack of strong convexity. The duality
          here is simpler than standard ERM duality, and hence stronger
          claims can be made. The dual problem is an unconstrained
          concave (but not strongly concave) quadratic maximization
          problem. The dual method is a randomized subspace ascent
          algorithm: the update to the dual vector is selected greedily
          from a random subspace. That is, in each iteration, one picks
          the update from this subspace which maximizes the dual
          function value. If the subspace is one-dimensional, and
          aligned with coordinate axes, one recovers randomized
          coordinate ascent. However, the random direction does not have
          to be aligned with the coordinate axes: one can pick it, say,
          from a Gaussian distribution, or any other continuous or
          discrete distribution. If the subspace is more than
          1-dimensional, the dual algorithm can be seen as a randomized
          variant of Newton's method. This variant has close connections
          with a machine learning / optimization technique known as
          minibatching. <br>
          <br>
          The primal method arises as an affine image of the dual
          method. That is, the dual iterates are simply mapped via a
          fixed affine mapping to the primal iterates, defining the
          primal method. The primal method can be seen from several
          different yet equivalent perspectives. It can be seen as
          stochastic gradient descent (SGD) with fixed stepsize applied
          to a particular stochastic (and not necessarily finite-sum!)
          objective. Surprisingly, it can also be seen as a Stochastic
          Newton Method (SNM), applied to the same objective. However,
          it can also be seen as a stochastic fixed point method and as
          a stochastic projection method ("sketch-and-project"). The
          method can be made parallel, and can be accelerated in the
          sense of Nesterov. <br>
          <br>
          The point I am making here is that in this setup, many key
          concepts and algorithms from stochastic optimization/machine
          learning coalesce into a unified framework, making it an ideal
          introduction into modern stochastic methods in optimization /
          machine learning. While I will only be able to introduce some
          of these connections in the short talk, instead of scratching
          the surface, my aim in the talk is to provide a thorough and
          understandable introduction into the area.&nbsp; <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>October 12, 2016</h3>
          <span class="important"><br>
          </span><a href="http://www.dominikcsiba.com/">Dominik Csiba</a>
          won a Postgraduate Essay Prize for his essay on <a
            href="http://www.dominikcsiba.com/docs/essay.pdf">Sampling
            Strategies for Empirical Risk Minimization.</a> The prize is
          given to the best 2-page-long essay(s) written by a PhD
          student in the School of Mathematics, based on his/her recent
          research, for a general mathematical audience.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3> </h3>
          <h3>October 8, 2016</h3>
          <span class="important"><br>
          </span><span class="important">New paper out:</span> <a
            href="https://arxiv.org/abs/1610.02527">Federated
            optimization: distributed machine learning for on-device
            intelligence</a>, joint with <a
            href="http://jakubkonecny.com/">Jakub Konečný</a>, and two
          Google coauthors: <a
            href="http://research.google.com/pubs/author35837.html">H.
            Brendan McMahan</a> and <a
            href="http://nlp.stanford.edu/dramage/">Daniel Ramage</a>. <br>
          <br>
          Update (Oct 19): The paper is now on <a
            href="https://arxiv.org/abs/1610.05492">arXiv</a>.<br>
          <br>
          Abstract: <i>We introduce a new and increasingly relevant
            setting for distributed optimization in machine learning,
            where the data defining the optimization are unevenly
            distributed over an extremely large number of nodes. The
            goal is to train a high-quality centralized model. We refer
            to this setting as Federated Optimization. In this setting,
            communication efficiency is of the utmost importance and
            minimizing the number of rounds of communication is the
            principal goal.<br>
            <br>
            A motivating example arises when we keep the training data
            locally on users’ mobile devices instead of logging it to a
            data center for training. In federated optimization, the
            devices are used as compute nodes performing computation on
            their local data in order to update a global model. We
            suppose that we have extremely large number of devices in
            the network — as many as the number of users of a given
            service, each of which has only a tiny fraction of the total
            data available. In particular, we expect the number of data
            points available locally to be much smaller than the number
            of devices. Additionally, since different users generate
            data with different patterns, it is reasonable to assume
            that no device has a representative sample of the overall
            distribution.<br>
            <br>
            We show that existing algorithms are not suitable for this
            setting, and propose a new algorithm which shows encouraging
            experimental results for sparse convex problems. This work
            also sets a path for future research needed in the context
            of federated optimization.</i><br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>October 6, 2016</h3>
          <span class="important"><br>
          </span><span class="important">New paper out:</span> <a
            href="papers/federated_communication_NIPS16.pdf">Federated
            learning: strategies for improving communication efficiency</a>,
          joint with <a href="http://jakubkonecny.com/">Jakub Konečný</a>,
          and four Google coauthors: <a
            href="http://research.google.com/pubs/author35837.html">H.
            Brendan McMahan</a>, <a
            href="http://research.google.com/pubs/FelixYu.html">Felix Yu</a>,
          Ananda Theertha Suresh and <a
            href="https://scholar.google.com/citations?user=KebzzVQAAAAJ&amp;hl=en">Dave


















            Bacon</a>. <br>
          <br>
          The paper was accepted to the <a
            href="https://nips.cc/Conferences/2016/Schedule?showEvent=6250"><i>2016
NIPS


















              Private Multi-Party Machine Learning Workshop</i></a>.<br>
          <br>
          Abstract:<i> </i><i> </i> <i> Federated learning is a
            machine learning setting where the goal is to train a
            high-quality centralized model with training data
            distributed over a large number of clients each with
            unreliable and relatively slow network connections. We
            consider learning algorithms for this setting where on each
            round, each client independently computes an update to the
            current model based on its local data, and communicates this
            update to a central server, where the client-side updates
            are aggregated to compute a new global model. The typical
            clients in this setting are mobile phones, and communication
            efficiency is of utmost importance. In this paper, we
            propose two ways to reduce the uplink communication costs.
            The proposed methods are evaluated on the application of
            training a deep neural network to perform image
            classification. Our best approach reduces the upload
            communication required to train a reasonable model by two
            orders of magnitude.&nbsp; </i> <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>October 5, 2016</h3>
          <span class="important"><br>
          </span>This week, I am in the Netherlands, attending the <a
href="https://wsc.project.cwi.nl/woudschoten-conferences/2016-woudschoten-conference">41st
Woudschoten


















            Conference</a> (annual conference of Dutch-Flemish Numerical
          Analysis Communities). I am giving a series of two keynote
          lectures in the theme "Numerical Methods for Big Data
          Analytics". The other keynote speakers are <a
            href="http://glaros.dtc.umn.edu/">George Karypis</a>
          (Minnesota), <a href="http://web.maths.unsw.edu.au/%7Efkuo/">Frances

















            Kuo</a> (New South Wales), <a
            href="https://www.nottingham.ac.uk/mathematics/people/michael.tretyakov">Michael


















            Tretyakov</a> (Nottingham), <a
            href="https://www.ima.umn.edu/%7Earnold/">Douglas N. Arnold</a>
          (Minnesota), and <a href="http://www-dimat.unipv.it/boffi/">Daniele


















            Boffi</a> (Pavia).<br>
          <br>
          Update (Oct 8): Here are the slides from my talks: <a
            href="talks/TALK-Woudschoten2016-Lecture1.pdf">Lecture 1</a>
          and <a href="talks/TALK-Woudschoten2016-Lecture2.pdf">Lecture
            2</a>.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>October 4, 2016</h3>
          <span class="important"><br>
          </span>The <a
            href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big


















            data optimization seminar</a> (aka "all hands meetings on
          big data optimization") is restarting with new academic year.
          We'll be meeting on Tuesdays, at 12:15, in JCMB 6207 -
          everybody interested in the field is welcome! There is very
          little room for excuses not to attend as we are running this
          during lunchtime, with lunch being provided! <br>
          <br>
          <a href="http://dominikcsiba.com/">Dominik Csiba</a> kicked
          the seminar off last week with an introduction to online ad
          allocation via online optimization; work Dominik coauthored
          with colleagues from Amazon. <a
            href="http://jakubkonecny.com/"> </a><a
            href="http://jakubkonecny.com/">Jakub Konečný</a> is
          speaking today about differentially private empirical risk
          minimization. Next week, we have <a
            href="http://www.maths.ed.ac.uk/people/show?person=479">Nicolas


















            Loizou</a> covering a recent paper of Nutini et al entitled
          " Convergence rates for greedy Kaczmarz algorithms, and faster
          randomized Kaczmarz rules using the orthogonality graph".<br>
          <br>
          Thanks to generous support from the <a
            href="http://datascience.inf.ed.ac.uk/apply/">CDT in Data
            Science</a>, this year we have extra funding to invite a few
          external (to Edinburgh) speakers. <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>September 23, 2016</h3>
          <span class="important"></span><br>
          <span class="important">CoCoA+</span> is now the default
          linear optimizer in <a href="https://www.tensorflow.org/">TensorFlow</a>!
          TensorFlow is an Open Source Software Library for Machine
          Intelligence. It was originally developed by Google, and is
          used extensively for deep learning. CoCoA+ was developed in
          the following two papers:<br>
          <br>
          [1] <a
            href="http://jmlr.org/proceedings/papers/v37/mab15.html">Ma,
            Smith, Jaggi, Jordan, Richtarik and Takac. Adding vs
            Averaging in Distributed Primal-Dual Optimization, ICML, pp.
            1973-1982, 2015</a><br>
          <br>
          [2] <a href="http://arxiv.org/abs/1512.04039">Ma, Konecny,
            Jaggi, Smith, Jordan, Richtarik and Takac. Distributed
            optimization with arbitrary local solvers, 2015</a><br>
          <br>
          The algorithm previously won the <a
href="http://mlconf.com/mlconf-industry-impact-student-research-award-winners/">2015
MLConf


















            Industry Impact Student Research Award</a>. The recipient of
          the award was Virginia Smith.<br>
          <br>
          Our adaptive SDCA+ method, called <span class="important">AdaSDCA+</span>,
          has also been implemented in TensorFlow (by Google)! This
          method was developed and analyzed in the following paper:<br>
          <br>
          [3] <a
            href="http://jmlr.org/proceedings/papers/v37/csiba15.html">Csiba,
Qu


















            and Richtarik. Stochastic dual coordinate ascent with
            adaptive probabilities. ICML, pp. 674-683, 2015</a><br>
          <br>
          This paper previously won a best contribution award at
          Optimization and Big Data 2015 (2nd place). Committee: A
          Nemirovski (GeorgiaTech) and R. Jenatton (Amazon). D Csiba was
          the recipient of the award.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>September 21, 2016</h3>
          <span class="important"><br>
          </span>Today I am attending (and giving a talk at) an event
          held at the Royal Society of Edinburgh: <br>
          <br>
          <a
href="http://www.ambafrance-uk.org/Franco-Scottish-Seminar-2016-Linear-Algebra-and-Parallel-Computing-at-the-Heart">Franco–Scottish
Science


















            Seminar: Linear Algebra and Parallel Computing at the Heart
            of Scientific Computing</a><br>
          <p><br>
            The event is organized by <a
              href="https://en.wikipedia.org/wiki/Iain_S._Duff">Iain
              Duff</a>.<br>
            <br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>September 12, 2016</h3>
          <span class="important"><br>
          </span>I am at Google, Seattle, on an invite by Google,
          attending the <a
            href="https://sites.google.com/site/learningprivacymobiledata/home">Learning,
Privacy


















            and Mobile Data workshop</a>. <a
            href="http://jakubkonecny.com/">Jakub Konecny</a> is
          attending, too. <br>
          <h3><br>
          </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>September 8, 2016</h3>
          <span class="important"><br>
          </span>Here are the slides from my closing plenary talk <a
            href="talks/TALK-OR58.pdf">"Introduction to Big Data
            Optimization"</a> at <a
            href="http://www.theorsociety.com/Pages/Conferences/OR58/OR58.aspx">OR58</a>.
          <br>
          <h3><br>
          </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>September 7, 2016</h3>
          <span class="important"><br>
          </span>This week, I am simultaneously attending and giving
          talks at two conferences (while this was bound to happen at
          some point, I am not planning to repeat this as I missed
          important talks at both events...). <br>
          <br>
          Today, I am speaking at the <a
href="http://www.ima.org.uk/conferences/conferences_calendar/5th_ima_conference_on_numerical_linear_algebra_and_optimisation.cfm.html">5th
IMA


















            Conference on Numerical Linear Algebra and Optimization</a>,
          where I am co-organizing 2 minisymposia with Nicolas Loizou
          and Jakub Konecny (randomized numerical linear algebra and big
          data optimization). I am speaking about <a
            href="http://arxiv.org/abs/1512.06890">"Stochastic dual
            ascent for solving linear systems"</a>; the talk is based on
          a joint paper with Robert M. Gower. Several other people from
          my group are attending and giving talks as well. <a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=417">Dominik


















            Csiba</a> (who is now back from an internship at Amazon)
          will speak about <a href="http://arxiv.org/abs/1602.02283">"Importance


















            sampling for minibatches"</a>. Jakub Konecny (who is now
          back from an internship at Google) will speak about <a
            href="https://arxiv.org/abs/1608.06879">"AIDE: Fast and
            communication-efficient distributed optimization"</a>. <a
            href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>
          (who moved to INRIA a month ago) will speak about <a
            href="http://arxiv.org/abs/1602.01768">"Randomized
            quasi-Newton methods are linearly convergent matrix
            inversion algorithms"</a>. <a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Nicolas


















            Loizou</a> will give a talk entitled "Randomized gossip
          algorithms: complexity, duality and new variants", based on <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/gossip.pdf">this
            paper</a>, which will appear in GlobalSip 2016. <a
            href="http://filiphanzely.com/">Filip Hanzely</a> is also
          attending. <br>
          <br>
          Tomorrow, I am giving the closing plenary talk at <a
            href="http://www.theorsociety.com/Pages/Conferences/OR58/OR58.aspx">OR58</a>
          - the annual conference of the OR Society - entitled
          "Introduction to Big Data Optimization". Update (Nov 4, 2016):
          The talk is now on <a
            href="https://www.youtube.com/watch?v=BS0kF4YijGc">YouTube</a>.<br>
          <h3><br>
          </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>September 6, 2016</h3>
          <br>
          <span class="important">PhD position Open in my group<br>
            <br>
          </span>I have a PhD position open in my group. Starting date:
          as soon as possible, but not later than January 1, 2017.
          Please fill out the <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/studying-here/pgr/phd-application">online


















            application</a> (apply for PhD in OR &amp; Optimization),
          and possibly also send me an email once you do. <br>
          <br>
          There is no application deadline. Applications will be
          reviewed as they arrive, and the position will be open and
          advertised until a suitable candidate is found and the post is
          filled. <br>
          <br>
          The position is funded by the School of Mathematics, and is
          associated with the <a
            href="http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/N005538/1">EPSRC
Grant


















            "Randomized Algorithms for Extreme Convex Optimization"</a>.<br>
          <h3><br>
          </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>August 13, 2016</h3>
          <br>
          <span class="important">Highly Downloaded SIOPT Paper<br>
            <br>
          </span>The paper <a
            href="http://epubs.siam.org/doi/abs/10.1137/130949993">"Accelerated,
parallel


















            and proximal coordinate descent"</a> (SIOPT, 2015) (joint
          with <a href="http://perso.telecom-paristech.fr/%7Eofercoq/">Olivier


















            Fercoq</a>) is the <a
href="http://epubs.siam.org/action/showMostReadArticles?journalCode=sjope8">2nd
most


















            downloaded SIOPT paper</a>. The downloads are counted over
          the last 12 months, and include all SIOPT papers. The most
          downloaded paper is <a
            href="http://epubs.siam.org/doi/abs/10.1137/080738970">"A
            singular value thresholding algorithm for matrix completion"</a>
          by J. Cai, E. Candes and Z. Shen (SIOPT, 2010). The third most
          downloaded paper is <a
            href="http://epubs.siam.org/doi/abs/10.1137/100802001">"Efficiency
of


















            coordinate descent methods on huge-scale optimization
            problems"</a> (SIOPT, 2012) by Yu. Nesterov. The fourth in
          the list is <a
            href="http://epubs.siam.org/doi/abs/10.1137/S1052623400366802">"Global
optimization


















            with polynomials and the problem of moments"</a> (SIOPT,
          2001) by J.B. Lasserre.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>August 9, 2016</h3>
          <br>
          <span class="important">Highly Downloaded SIMAX Paper<br>
            <br>
          </span>The paper <a
            href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">"Randomized
Iterative


















            Methods for Linear Systems"</a> (joint with <a
            href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert M.
            Gower</a>) is the <a
href="http://epubs.siam.org/action/showMostReadArticles?journalCode=sjmael&amp;">2nd
most


















            downloaded SIMAX paper</a>. The downloads are counted over
          the last 12 months, and include all SIMAX papers. The first
          and third papers in the ranking are from 2000, the fourth was
          written in 1998 - nearly 20 years ago.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>August 8, 2016</h3>
          <br>
          <p>I am in Tokyo, at the <a
              href="http://www.iccopt2016.tokyo/">5th International
              Conference on Continuous Optimization</a>. It's warm and
            humid, but the food I just had was great! I am giving my
            talk on Tuesday, August&nbsp; 9.<br>
          </p>
          <p>Update (August 9): the conference dinner was fabulous!<br>
          </p>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>July 1, 2016</h3>
          <br>
          <p><span class="important">New paper out: </span><a
              href="papers/gossip.pdf">A new perspective on randomized
              gossip algorithms</a>, joint with <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/people/show?person=479">Nicolas














              Loizou</a>.<br>
          </p>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>July 28, 2016</h3>
          <br>
          <a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Nicolas


















            Loizou</a> is visiting Microsoft Research in Seattle this
          week.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>July 4, 2016</h3>
          <br>
          A belated message: Since about mid-May, and until
          mid/late-August, <a
            href="http://www.maths.ed.ac.uk/people/show?person=417">Dominik


















            Csiba</a> and <a href="http://jakubkonecny.com/">Jakub
            Konečný</a> are doing industrial internships. Dominik is
          with the Scalable Machine Learning Lab at Amazon, Berlin; and
          Jakub is with Google, Seattle. Nicolas Loizou is participating
          in the <a href="https://pcmi.ias.edu/program-index/2016">PCMI
            26th Annual PCMI Session on "The Mathematics of Data"</a>.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>June 26, 2016</h3>
          <br>
          I am on my way to Beijing to participate in the <a
            href="http://lsec.cc.ac.cn/%7Emoa2016/index.html">2016
            International Workshop on Modern Optimization and
            Applications (MOA 2016)</a>.<!--[if gte mso 9]><xml>
 <o:DocumentProperties>
  <o:Revision>0</o:Revision>
  <o:TotalTime>0</o:TotalTime>
  <o:Pages>1</o:Pages>
  <o:Words>10</o:Words>
  <o:Characters>59</o:Characters>
  <o:Company>University of Edinburgh</o:Company>
  <o:Lines>1</o:Lines>
  <o:Paragraphs>1</o:Paragraphs>
  <o:CharactersWithSpaces>68</o:CharactersWithSpaces>
  <o:Version>14.0</o:Version>
 </o:DocumentProperties>
 <o:OfficeDocumentSettings>
  <o:AllowPNG/>
 </o:OfficeDocumentSettings>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <w:WordDocument>
  <w:View>Normal</w:View>
  <w:Zoom>0</w:Zoom>
  <w:TrackMoves/>
  <w:TrackFormatting/>
  <w:PunctuationKerning/>
  <w:ValidateAgainstSchemas/>
  <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>
  <w:IgnoreMixedContent>false</w:IgnoreMixedContent>
  <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>
  <w:DoNotPromoteQF/>
  <w:LidThemeOther>EN-US</w:LidThemeOther>
  <w:LidThemeAsian>JA</w:LidThemeAsian>
  <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript>
  <w:Compatibility>
   <w:BreakWrappedTables/>
   <w:SnapToGridInCell/>
   <w:WrapTextWithPunct/>
   <w:UseAsianBreakRules/>
   <w:DontGrowAutofit/>
   <w:SplitPgBreakAndParaMark/>
   <w:EnableOpenTypeKerning/>
   <w:DontFlipMirrorIndents/>
   <w:OverrideTableStyleHps/>
   <w:UseFELayout/>
  </w:Compatibility>
  <m:mathPr>
   <m:mathFont m:val="Cambria Math"/>
   <m:brkBin m:val="before"/>
   <m:brkBinSub m:val="&#45;-"/>
   <m:smallFrac m:val="off"/>
   <m:dispDef/>
   <m:lMargin m:val="0"/>
   <m:rMargin m:val="0"/>
   <m:defJc m:val="centerGroup"/>
   <m:wrapIndent m:val="1440"/>
   <m:intLim m:val="subSup"/>
   <m:naryLim m:val="undOvr"/>
  </m:mathPr></w:WordDocument>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <w:LatentStyles DefLockedState="false" DefUnhideWhenUsed="true"
  DefSemiHidden="true" DefQFormat="false" DefPriority="99"
  LatentStyleCount="276">
  <w:LsdException Locked="false" Priority="0" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Normal"/>
  <w:LsdException Locked="false" Priority="9" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="heading 1"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 2"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 3"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 4"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 5"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 6"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 7"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 8"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 9"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 1"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 2"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 3"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 4"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 5"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 6"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 7"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 8"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 9"/>
  <w:LsdException Locked="false" Priority="35" QFormat="true" Name="caption"/>
  <w:LsdException Locked="false" Priority="10" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Title"/>
  <w:LsdException Locked="false" Priority="1" Name="Default Paragraph Font"/>
  <w:LsdException Locked="false" Priority="11" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Subtitle"/>
  <w:LsdException Locked="false" Priority="22" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Strong"/>
  <w:LsdException Locked="false" Priority="20" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Emphasis"/>
  <w:LsdException Locked="false" Priority="59" SemiHidden="false"
   UnhideWhenUsed="false" Name="Table Grid"/>
  <w:LsdException Locked="false" UnhideWhenUsed="false" Name="Placeholder Text"/>
  <w:LsdException Locked="false" Priority="1" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="No Spacing"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading Accent 1"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List Accent 1"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid Accent 1"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1 Accent 1"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2 Accent 1"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1 Accent 1"/>
  <w:LsdException Locked="false" UnhideWhenUsed="false" Name="Revision"/>
  <w:LsdException Locked="false" Priority="34" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="List Paragraph"/>
  <w:LsdException Locked="false" Priority="29" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Quote"/>
  <w:LsdException Locked="false" Priority="30" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Intense Quote"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2 Accent 1"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1 Accent 1"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2 Accent 1"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3 Accent 1"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List Accent 1"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading Accent 1"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List Accent 1"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid Accent 1"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading Accent 2"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List Accent 2"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid Accent 2"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1 Accent 2"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2 Accent 2"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1 Accent 2"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2 Accent 2"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1 Accent 2"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2 Accent 2"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3 Accent 2"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List Accent 2"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading Accent 2"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List Accent 2"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid Accent 2"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading Accent 3"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List Accent 3"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid Accent 3"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1 Accent 3"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2 Accent 3"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1 Accent 3"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2 Accent 3"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1 Accent 3"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2 Accent 3"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3 Accent 3"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List Accent 3"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading Accent 3"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List Accent 3"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid Accent 3"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading Accent 4"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List Accent 4"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid Accent 4"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1 Accent 4"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2 Accent 4"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1 Accent 4"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2 Accent 4"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1 Accent 4"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2 Accent 4"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3 Accent 4"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List Accent 4"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading Accent 4"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List Accent 4"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid Accent 4"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading Accent 5"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List Accent 5"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid Accent 5"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1 Accent 5"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2 Accent 5"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1 Accent 5"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2 Accent 5"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1 Accent 5"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2 Accent 5"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3 Accent 5"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List Accent 5"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading Accent 5"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List Accent 5"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid Accent 5"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading Accent 6"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List Accent 6"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid Accent 6"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1 Accent 6"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2 Accent 6"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1 Accent 6"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2 Accent 6"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1 Accent 6"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2 Accent 6"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3 Accent 6"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List Accent 6"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading Accent 6"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List Accent 6"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid Accent 6"/>
  <w:LsdException Locked="false" Priority="19" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Subtle Emphasis"/>
  <w:LsdException Locked="false" Priority="21" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Intense Emphasis"/>
  <w:LsdException Locked="false" Priority="31" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Subtle Reference"/>
  <w:LsdException Locked="false" Priority="32" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Intense Reference"/>
  <w:LsdException Locked="false" Priority="33" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Book Title"/>
  <w:LsdException Locked="false" Priority="37" Name="Bibliography"/>
  <w:LsdException Locked="false" Priority="39" QFormat="true" Name="TOC Heading"/>
 </w:LatentStyles>
</xml><![endif]--> Update: <a
            href="talks/TALK-2016-06-SDNA-Beijing.pdf">my slides.</a><br>
          <br>
          <i>Speakers: </i><br>
          <br>
          <i>Day 1:</i><br>
          <br>
          <a href="http://web.stanford.edu/%7Eyyye/">Yinyu Ye</a>
          (Stanford)<br>
          <a href="http://coral.ie.lehigh.edu/%7Eted/">Ted Ralphs</a>
          (Lehigh)<br>
          <a href="http://www.caam.rice.edu/%7Ezhang/">Yin Zhang</a>
          (Rice)<br>
          <a href="http://www.zib.de/members/conrad">Tim Conrad</a>
          (ZIB)<br>
          <a href="http://people.ece.umn.edu/%7Eluozq/">Zhi-Quan (Tom)
            Luo</a> (Minnesota)<br>
          <br>
          <i>Day 2:</i><br>
          <br>
          Peter Richtarik (Edinburgh) [<a
            href="talks/TALK-2016-06-SDNA-Beijing.pdf">slides</a>]<br>
          <a
            href="https://www.microsoft.com/en-us/research/people/lixiao/">Lin


















            Xiao</a> (Microsoft Research)<br>
          <a href="http://www.zib.de/koch/">Thorsten Koch</a> (ZIB &amp;
          TU Berlin)<br>
          <a href="http://people.sutd.edu.sg/%7Enannicini/">Giacomo
            Nannicini</a> (IBM &amp; SUTD)<br>
          <a href="https://www.polyu.edu.hk/ama/staff/xjchen/ChenXJ.htm">Xiaojun


















            Chen</a> (Hong Kong Polytechnic)<br>
          <a href="http://www1.se.cuhk.edu.hk/%7Esqma/">Shiqian Ma</a>
          (Chinese University of Hong Kong)<br>
          <br>
          <i>Day 3:</i><br>
          <br>
          Zongben Xu<br>
          <a href="http://www1.se.cuhk.edu.hk/%7Emanchoso//">Anthony
            Man-Cho So</a> (Chinese University of Hong Kong)<br>
          <a href="http://ise.tamu.edu/people/faculty/butenko/">Sergiy
            Butenko</a> (Texas A&amp;M)<br>
          <a href="http://www.ie.uh.edu/faculty/peng">Jiming Peng</a>
          (Houston)<br>
          <a
            href="https://scholar.google.nl/citations?user=5jfgJR8AAAAJ&amp;hl=nl">Deren


















            Han</a> (Nanjing)<br>
          Naihua Xiu<br>
          <a href="http://lsec.cc.ac.cn/%7Eyyx/">Ya-xiang Yuan</a>
          (Chinese Academy of Sciences)<br>
          <a href="http://research.baidu.com/big-data-lab/">Tong Zhang</a>
          (Baidu)<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>June 12, 2016</h3>
          <br>
          I am attending the <a
            href="http://meetings2.informs.org/wordpress/2016international/">INFORMS
International


















            Conference</a>, held in Hawaii. I am giving an invited talk
          in the Large-Scale Optimization II session on Wednesday. Other
          speakers in the session: <a
            href="http://www.math.ucla.edu/%7Ewotaoyin/">Wotao Yin</a>
          and <a href="http://www.math.ucla.edu/%7Edamek/">Damek Davis</a>.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>May 29, 2016</h3>
          <p><span class="important">New paper out: </span><a
              href="https://arxiv.org/abs/1605.08982">Coordinate descent
              face-off: primal or dual?</a>, joint with <a
              href="http://www.dominikcsiba.com/">Dominik Csiba</a>.<br>
          </p>
          <p>Abstract:<br>
          </p>
          <p><i>Randomized coordinate descent (RCD) methods are
              state-of-the-art algorithms for training linear predictors
              via minimizing regularized empirical risk. When the number
              of examples ($n$) is much larger than the number of
              features ($d$), a common strategy is to apply RCD to the
              dual problem. On the other hand, when the number of
              features is much larger than the number of examples, it
              makes sense to apply RCD directly to the primal problem.
              In this paper we provide the first joint study of these
              two approaches when applied to L2-regularized ERM. First,
              we show through a rigorous analysis that for dense data,
              the above intuition is precisely correct. However, we find
              that for sparse and structured data, primal RCD can
              significantly outperform dual RCD even if $d \ll n$, and
              vice versa, dual RCD can be much faster than primal RCD
              even if $n \ll d$. Moreover, we show that, surprisingly, a
              single sampling strategy minimizes both the (bound on the)
              number of iterations and the overall expected complexity
              of RCD. Note that the latter complexity measure also takes
              into account&nbsp; the average cost of the iterations,
              which depends on the structure and sparsity of the data,
              and on the sampling strategy employed.&nbsp; We confirm
              our theoretical predictions using extensive experiments
              with both synthetic and real data sets. </i><br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>May 29, 2016</h3>
          <br>
          Today I am giving a seminar talk in the <a
href="https://cemse.kaust.edu.sa/events/Pages/CEMSE-Seminar-Peter-Richtarik.aspx">CEMSE


















            seminar</a> at <a href="https://www.kaust.edu.sa/en">KAUST</a>.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>May 27, 2016</h3>
          <br>
          <a href="docs/CV_Ziteng_Wang.pdf">Ziteng Wang</a> will join my
          group as a PhD student starting in October 2016. He will be
          affiliated with the <a
            href="https://turing.ac.uk/jobs/2016-doctoral-studentships/">Alan
Turing


















            Institute</a> and the School of Mathematics here in
          Edinburgh. Ziteng has an MS in Pattern Recognition and Machine
          Learning from <a href="http://english.pku.edu.cn/">Peking
            University</a> and a BS in Mathematics from <a
            href="http://www.scu.edu.cn/en/">Sichuan University</a>. He
          subsequently spent half a year as a research assistant at <a
            href="https://www.cse.ust.hk/">Hong Kong University of
            Science and Technology</a>. Ziteng has written 4 papers [<a
href="http://papers.nips.cc/paper/5670-fast-second-order-stochastic-backpropagation-for-variational-inference">1</a>,
          <a href="people.duke.edu/%7Ekf96/docs/jmlrDP.pdf">2</a>, <a
            href="http://arxiv.org/abs/1401.0987">3</a>, <a
href="http://papers.nips.cc/paper/5011-efficient-algorithm-for-privately-releasing-smooth-queries">4</a>],
two


















          of which appeared in NIPS, and one in JMLR. Ziteng: welcome to
          the group!<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>May 19, 2016</h3>
          <br>
          I am back in Edinburgh now. There is <a
            href="https://nips.cc/Conferences/2016/CallForPapers">NIPS
            deadline</a> tomorrow, still some stuff to do...<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>May 13, 2016</h3>
          <br>
          Tomorrow is an interesting day as almost everybody in my group
          is traveling away from Edinburgh (despite the fact that we are
          blessed with amazing weather these days!), including me. <a
            href="http://www.dominikcsiba.com/">Dominik Csiba</a> is
          starting his Amazon (Scalable Machine Learning group in
          Berlin) internship next week. <a
            href="http://jakubkonecny.com/">Jakub Konecny</a> is
          starting his Google (Seattle) internship also next week. I am
          visiting Stanford next week. All three of us are leaving
          Edinburgh tomorrow... ;-) To add to this, <a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Nicolas


















            Loizou</a> is also away, attending the <a
            href="https://learning.mpi-sws.org/mlss2016/">Machine
            Learning Summer School</a> in Cadiz, Spain (May 11-21). <a
            href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>
          is hanging around though, which is great, as he can take care
          of my visitor <a
            href="http://www.lix.polytechnique.fr/%7Evu/">Vu Khac Ky</a>.
          The three of us have started an interesting project earlier
          this week. If you are in Edinburgh then you might want to
          attend Ky's <a
            href="http://www.maths.ed.ac.uk/ERGO/seminars.html">ERGO
            seminar talk</a> on Wednesday next week (the website has not
          yet been updated -&nbsp; but it will soon include his talk).<br>
          <br>
          On another subject: we have just had <span class="important">two
papers


















            accepted</span> to <i>Optimization Methods and Software</i>:<br>
          <br>
          <a href="http://arxiv.org/abs/1412.8060">Coordinate descent
            with arbitrary sampling I: algorithms and complexity</a> (by
          <a href="http://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a> and
          myself)
          <p><a href="http://arxiv.org/abs/1412.8063">Coordinate descent
              with arbitrary sampling II: expected separable
              overapproximation</a> (by <a
              href="http://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a> and
            myself)<br>
          </p>
          <br>
          And now some relatively belated news: <a
            href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert</a> <span
            class="important">defended his PhD thesis </span>on Friday
          April 29. His PhD committee, composed of <a
            href="http://www.ma.man.ac.uk/%7Ehigham/">Nick Higham</a>
          (external examiner) and <a
            href="http://kac.maths.ed.ac.uk/%7Ebl/">Ben Leimkuhler</a>
          (internal examiner), suggested that his thesis should be
          nominated for the Householder Prize (for " best dissertation
          in numerical algebra"). I'd be delighted to do the nomination!
          Robert will join the <a href="http://www.di.ens.fr/sierra/">SIERRA

















            group</a> as a postdoc in August.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>May 10, 2016</h3>
          <br>
          <a href="http://www.lix.polytechnique.fr/%7Evu/">Vu Khac Ky</a>
          (LIX, Ecole Polytechnique) is visiting me until May 20.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>May 9, 2016</h3>
          <br>
          Today I am attending "Networking Workshop on Mathematical
          Optimization and Applications" taking place here in Edinburgh
          (room JCMB 4325a; if you are around and feel like
          attending...). In fact, I have just given my talk, and
          Sotirios Tsaftaris is speaking at the very moment I am writing
          this.<br>
          <br>
          <i>Speakers (listed in the order they deliver their talks):</i>
          Nickel, myself, Konecny, Tsaftaris, Giuffrida, Menolascina,
          Hall, Garcia Quiles, Kalcsics, van der Weijde, Gunda, Wallace,
          Joyce, Herrmann, Etessami, Buke, Francoise.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>May 5, 2016</h3>
          <br>
          I accepted an invite to give the closing plenary talk at <a
            href="http://www.theorsociety.com/Pages/Conferences/OR58/OR58.aspx">OR58</a>
          - the 58th Annual Conference of the Operational Research
          Society ("OR Society"). The conference will take place in
          Portsmouth, UK, during September 6-8, 2016. Established in
          1948, The OR Society is the oldest learned society in the
          field of OR.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>May 4, 2016</h3>
          <br>
          <span class="important">New poster out - </span><a
            href="posters/Poster-Gossip.pdf">Randomized Gossip
            Algorithms: New Insights</a>. Nicolas Loizou will present
          this poster on <a
            href="https://learning.mpi-sws.org/mlss2016/poster-sessions/">May

















            16</a> at the <a
            href="https://learning.mpi-sws.org/mlss2016/">Machine
            Learning Summer School</a> (MLSS) in Cádiz, Spain, which he
          is attending.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>May 4, 2016</h3>
          <br>
          We have had <span class="important">two minisymposia accepted</span>
          in the <a
href="http://www.ima.org.uk/conferences/conferences_calendar/5th_ima_conference_on_numerical_linear_algebra_and_optimisation.cfm.html">5th
IMA


















            Conference on Numerical Linear Algebra and Optimization</a>,
          to be held in Birmingham during September 7-9, 2016. The
          minisymposia are:<br>
          <br>
          1) <b>Randomized Numerical Linear Algebra</b><br>
          <br>
          <i>Organizers: </i><a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Loizou</a>,
          <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Gower</a>,
          myself<br>
          <br>
          <i>Speakers:</i><br>
          <br>
          <a href="https://www.lri.fr/%7Ebaboulin/">Marc Baboulin</a>
          (Paris-Sud), The Story of the Butterflies<br>
          <a href="https://ei.is.tuebingen.mpg.de/person/sbartels">Simon
            Bartels</a> (Max Planck Institute), TBA<br>
          <a href="https://www.cs.purdue.edu/homes/dgleich/">David
            Gleich</a> (Purdue), TBA<br>
          <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>
          (INRIA), Stochastic Quasi-Newton Methods and Matrix Inversion<br>
          <a href="https://www.maths.ox.ac.uk/people/raphael.hauser">Raphael


















            Hauser</a> (Oxford), TBA<br>
          <a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Nicolas


















            Loizou</a> (Edinburgh), Randomized Gossip Methods:
          Complexity, Duality and New Variants<br>
          Peter Richtárik (Edinburgh), Stochastic Dual Ascent for
          Solving Linear Systems<br>
          <a href="http://www.wisdom.weizmann.ac.il/%7Eshamiro/">Ohad
            Shamir</a> (Weizmann Institute), A Stochastic SVD and PCA
          Algorithm with Linear Convergence Rate<br>
          <br>
          2) <b>Optimization Methods in Machine Learning</b><br>
          <br>
          <i>Organizers:</i> <a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Loizou</a>,
          <a href="http://jakubkonecny.com/">Konečný</a>, myself<br>
          <br>
          <i>Speakers: </i><br>
          <br>
          <a href="http://www.dominikcsiba.com/">Dominik Csiba</a>
          (Edinburgh), Importance Sampling for Minibatches<br>
          <a href="https://people.epfl.ch/volkan.cevher?lang=en">Volkan
            Cevher</a> (EPFL), TBA<br>
          <a href="http://people.kth.se/%7Ehamidrez/">Hamid Reza
            Feyzmahdavian</a> (KTH), TBA<br>
          <a href="http://jakubkonecny.com/">Jakub Konečný</a>
          (Edinburgh), Federated Optimization: Distributed Optimization
          Beyond the Datacenter<br>
          <a href="http://lear.inrialpes.fr/people/mairal/">Julien
            Mairal</a> (INRIA Grenoble), A Universal Catalyst for First
          Order Optimization<br>
          <a href="http://www.doc.ic.ac.uk/%7Epp500/">Panos Parpas </a>(Imperial),


















          TBA<br>
          <a href="http://josephsalmon.eu/">Joseph Salmon</a> (Telecom
          ParisTech), GAP Safe Screening Rules for Sparsity Enforcing
          Penalties<br>
          <a href="http://www.wisdom.weizmann.ac.il/%7Eshamiro/">Ohad
            Shamir</a> (Weizmann Institute), Without Replacement for
          Stochastic Gradient Methods<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>May 3, 2016</h3>
          <br>
          We have <a
            href="http://www-syscom.univ-mlv.fr/%7Epesquet/index.htm">Jean
Christophe


















            Pesquet</a> (Universite Paris-Est) leading the <a
            href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big


















            data seminar</a> today. Prof Pesquet is a leading researcher
          in the area of inverse problems, and optimization for signal
          and image processing.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 25, 2016</h3>
          <br>
          I am visiting Stanford this week.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 24, 2016</h3>
          <p>We've just had <span class="important">3 papers accepted</span>
            to <a href="http://icml.cc/2016/">ICML 2016:</a><br>
          </p>
          <p>- <a href="http://arxiv.org/abs/1603.09649">Stochastic
              block BFGS: squeezing more curvature out of data</a> (<a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Gower</a>, <a
              href="http://www.columbia.edu/%7Egoldfarb/">Goldfarb</a>
            and R)<br>
            - <a href="http://arxiv.org/abs/1512.09103">Even faster
              accelerated coordinate descent using non-uniform sampling</a>
            (<a href="http://people.csail.mit.edu/zeyuan/">Allen-Zhu</a>,
            <a href="http://hkumath.hku.hk/%7Ezhengqu/">Qu</a>, R and <a
              href="http://www.callowbird.com/">Yuan</a>)<br>
            - <a href="http://arxiv.org/abs/1502.02268">SDNA:
              Stochastic dual Newton ascent for empirical risk
              minimization</a> (<a
              href="http://hkumath.hku.hk/%7Ezhengqu/">Qu</a>, R, <a
              href="http://mtakac.com/">Takáč</a> and <a
              href="http://perso.telecom-paristech.fr/%7Eofercoq/">Fercoq</a>)</p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 22, 2016</h3>
          <p>Today I am giving a talk at the <a
              href="http://www.ecmath.de/research/ECMathColloquium">ECMath














              Colloquium</a> in Berlin. I am speaking about "Empirical
            Risk Minimization: Complexity, Duality, Sampling, Sparsity
            and Big Data". </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 21, 2016</h3>
          <p><a
              href="http://math.mit.edu/directory/profile.php?pid=1640">Haihao
(Sean)














              Lu</a> (MIT) is visiting me this week. On Tuesday he led
            the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big














              data seminar</a>.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 14, 2016</h3>
          <p>Together with <a
              href="http://perso.telecom-paristech.fr/%7Eofercoq/">Olivier














              Fercoq</a>, a former postdoc and now an Assistant
            Professor at Telecom ParisTech, we are to receive the <span
              class="important">SIGEST Award</span> of the <a
              href="http://www.siam.org/">Society for Industrial and
              Applied Mathematics (SIAM)</a> for the paper “Accelerated,
            parallel and proximal coordinate descent”.<br>
            <br>
            The paper first appeared as a preprint on <i>arXiv</i> in
            December 2013 (<a href="http://arxiv.org/abs/1312.5799">arXiv:1312.5799</a>)
            before it was published in the <i><a
                href="http://epubs.siam.org/doi/abs/10.1137/130949993">SIAM














                Journal on Optimization (SIOPT)</a></i> in 2015. In
            addition to <i>SIOPT</i>, SIAM publishes further 16
            scholarly <a href="http://www.siam.org/journals/">journals</a>:<br>
            <br>
            &nbsp;&nbsp;&nbsp; Multiscale Modeling and Simulation<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Applied Algebra and
            Geometry<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Applied Dynamical Systems<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Applied Mathematics<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Computing<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Control and Optimization<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Discrete Mathematics<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Financial Mathematics <br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Imaging Sciences <br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Mathematical Analysis<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Matrix Analysis and
            Applications<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Numerical Analysis<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Optimization<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Scientific Computing<br>
            &nbsp;&nbsp;&nbsp; SIAM/ASA Journal on Uncertainty
            Quantification<br>
            &nbsp;&nbsp;&nbsp; SIAM Review<br>
            &nbsp;&nbsp;&nbsp; Theory of Probability and Its
            Applications<br>
            <br>
            The paper will be reprinted in the SIGEST section of <a
              href="http://www.siam.org/journals/sirev.php"><i>SIAM
                Review</i></a> (Volume 58, Issue 4, 2016), the flagship
            journal of the society. A paper from SIOPT is chosen for the
            SIGEST award about once every two or three years. The award
            will be conferred at the <a
              href="http://www.siam.org/meetings/an17/">SIAM Annual
              Meeting in Pittsburgh in July 2017</a>.<br>
            <br>
            According to C. T. Kelley, editor-in-chief of SIAM Review,<i>
              “SIGEST highlights a recent paper from one of SIAM’s
              specialized research journals, chosen on the basis of
              exceptional interest to the entire SIAM community… The
              purpose of SIGEST is to make the 10,000+ readers of SIAM
              Review aware of exceptional papers published in SIAM's
              specialized journals. In each issue of SIAM Review, the
              SIGEST section contains an outstanding paper of general
              interest that has previously appeared in one of SIAM's
              specialized research journals; the issues rotate through
              the journals. We begin the selection by asking the
              editor-in-chief of the appropriate journal to send a short
              list of nominees, usually nominated by the associate
              editors. Then, the SIAM Review section editors make the
              final selection.”</i><i><br>
            </i><i><br>
            </i>Kelley further writes: <i>“In this case, your paper was
              recommended by members of the SIOPT editorial board and
              the editor in chief of the journal, and was selected by
              the SIREV editors for the importance of its contributions
              and topic, its clear writing style, and its accessibility
              for the SIAM community.”</i><br>
          </p>
          <p>The same paper also recently won the <a
              href="http://www.numerical.rl.ac.uk/people/nimg/fox/">17th
              Leslie Fox Prize in Numerical Analysis (2nd Prize, 2015)</a>,
            awarded biennially by the <a href="http://www.ima.org.uk/">Institute













              of Mathematics and its Applications (IMA)</a>.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 11, 2016</h3>
          <p><a href="http://www.uclouvain.be/sebastian.stich">Sebastian
              Stich</a> (Louvain) and <a
              href="http://www.damtp.cam.ac.uk/people/me404/">Matthias
              Ehrhardt</a> (Cambridge) are visiting me this week
            (Matthias is staying until Tuesday next week). Sebastian
            will lead the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big














              data seminar</a> tomorrow and Matthias will give an <a
              href="http://www.maths.ed.ac.uk/ERGO/seminars.html">ERGO
              seminar</a> talk on Wednesday.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 9, 2016</h3>
          <p>We have a <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/jobs/lectureship-in-the-mathematics-of-data-science">Lectureship</a>
            (= Tenured Assistant Professorship) and a <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/jobs/chancellor-s-fellow-in-the-mathematics-of-data-sci">Chancellor's














              Fellowship</a> (= Tenure Track Assistant Professorship)
            position available in “Mathematics of Data Science” at the
            University of Edinburgh. Mathematics of Data Science
            includes but is not limited to areas such as Mathematical
            Optimization, Mathematics of Machine Learning, Operations
            Research, Statistics, Mathematics of Imaging and Compressed
            Sensing. <br>
            <br>
            Application deadline for the Lectureship post: May 9, 2016 <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/news?nid=669">[more














              info]</a><br>
          </p>
          <p>Application deadline for the Chancellor's Fellowship post:
            April 25, 2016 @ 5pm <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/jobs/chancellor-s-fellow-in-the-mathematics-of-data-sci">[more














              info]</a><br>
          </p>
          <p>Starting data: August 1, 2016 (or by mutual agreement)<br>
          </p>
          <p>We also have a <a
href="We%20also%20have%20a%20Chancellor%27s%20Fellowship%20post%20in%20Industrial%20Mathematics.">Chancellor's
Fellowship














              post in Industrial Mathematics.</a></p>
          <p>These positions are part of a larger activity in Edinburgh
            aimed at growing Data Science.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 8, 2016</h3>
          <p><a
href="http://www.maths.ed.ac.uk/school-of-mathematics/people/show?person=417">Dominik














              Csiba</a> is teaching a course entitled <a
              href="http://www.dominikcsiba.com/MML/index.html">Mathematics














              of Machine Learning</a> at Comenius University, Slovakia
            as of today; the course lasts until April 17th. Dominik
            developed the course himself and is offering it informally
            to anyone interested in the subject, free of charge! The
            first half of the material is based on Shai's book
            "Understanding Machine Learning: from Theory to Algorithms";
            and the second half is based on certain parts of my course
            "Modern Optimization Methods for Big Data Problems". His
            slides are in English, and the course is delivered in
            Slovak. A video recording of the course will be put online
            in due time.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 5, 2016</h3>
          <p>I've just learned that I am one of two people shortlisted
            for the <a
href="https://www.eusa.ed.ac.uk/representation/campaigns/teachingawards/nominees/">EUSA
Best














              Research or Dissertation Supervisor Award</a>. I had no
            clue I was nominated in the first place, so this came as a
            pleasant surprise! Thanks to those who nominated me! <br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>April 1, 2016</h3>
          <p>The list of the <a
              href="https://turing.ac.uk/faculty-fellows/">Faculty
              Fellows of the Alan Turing Institute</a> is live now. I am
            on the list. <br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>March 17, 2016</h3>
          <p><span class="important">New paper out: </span><a
              href="http://arxiv.org/abs/1602.02283"> </a><a
              href="http://arxiv.org/abs/1603.09649">Stochastic block
              BFGS: squeezing more curvature out of data</a>, joint with
            <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert M.
              Gower</a> and <a
              href="http://www.columbia.edu/%7Egoldfarb/">Donald
              Goldfarb</a>.<br>
          </p>
          <p>Abstract: <i>We propose a novel limited-memory stochastic
              block BFGS update for incorporating enriched curvature
              information in stochastic approximation methods. In our
              method, the estimate of the inverse Hessian matrix that is
              maintained by it, is updated at each iteration using a
              sketch of the Hessian, i.e., a randomly generated
              compressed form of the Hessian. We propose several
              sketching strategies, present a new quasi-Newton method
              that uses stochastic block BFGS updates combined with the
              variance reduction approach SVRG to compute batch
              stochastic gradients, and prove linear convergence of the
              resulting method. Numerical tests on large-scale logistic
              regression problems reveal that our method is more robust
              and substantially outperforms current state-of-the-art
              methods</i>.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>March 17, 2016</h3>
          <p>Results: 3 year postdoctoral position in big data
            optimization<br>
          </p>
          <p>I have received 61 applications for the 3 year postdoctoral
            position in big data optimization. Our of these, 13 were
            shortlisted and invited for an interview. One of the
            shortlisted applicants declined due to prior acceptance of
            another offer. Twelve excellent applicants were interviewed
            over a period of 2 days. An offer was recently made to the
            #1 ranked applicant (based on the application files,
            recommendation letters and performance in the interview),
            who accepted the post. <br>
          </p>
          <p>It is my pleasure to announce that <a
              href="http://people.kth.se/%7Ehamidrez/index.html">Dr
              Hamid Reza Feyzmahdavian</a> will join the group as of
            September 1, 2016, as a postdoc. Hamid has PhD from the
            Royal Institute of Technology (KTH), Sweden, supervised by <a
              href="http://people.kth.se/%7Emikaelj/">Prof Mikael
              Johansson</a>. His research spans several topics in
            control and optimization. In optimization, for instance, his
            past work spans topics such as analysis of the heavy ball
            method; development and analysis of randomized, asynchronous
            and mini-batch algorithms for regularized stochastic
            optimization; dual coordinate ascent for multi-agent
            optimization; asynchronous contractive iterations, and
            delayed proximal gradient method. <br>
          </p>
          <p>I wish to thank all unsuccessful applicants for expressing
            their interest in the position, and to shortlisted
            candidates for participating in the interview. Very hard
            decisions had to be made even at the shortlisting stage as
            many highly qualified applicants did not make it through due
            to necessary constraints on how many candidates it is
            feasible for us to interview. The situation was tougher yet
            at the interview stage. If I had more funding, I would be
            absolutely delighted to offer posts to several of the
            shortlisted candidates! Thank you all again, and I wish you
            best of luck in job search.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>March 10, 2016</h3>
          <p>According to <a
href="https://www.timeshighereducation.com/world-university-rankings/best-universities-in-europe-2016">2016
Times














              Higher Education World University Rankings</a>, The
            University of Edinburgh is the 7th best university in
            Europe.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>March 10, 2016</h3>
          <p>I am in Oberwolfach as of Sunday last week until tomorrow,
            attending the <a
              href="http://www.mfo.de/occasion/1610/www_view">Computationally
and














              Statistically Efficient Inference for Complex Large-Scale
              Data</a> workshop. Many of the talks so far were extremely
            interesting, and some downright entertaining! <a
              href="http://stat.ethz.ch/%7Ebuhlmann/">Peter Buhlmann</a>
            is a true master of ceremony ;-) <br>
          </p>
          <p>On Tuesday, I talked about stochastic methods for solving
            linear systems and inverting matrices; the talk is based on
            a trio of recent papers written in collaboration with <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert M
              Gower</a>:<br>
          </p>
          <p><a href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">Randomized
iterative














              methods for linear systems</a><br>
            <a href="http://arxiv.org/abs/1512.06890">Stochastic dual
              ascent for solving linear systems</a><br>
            <a href="http://arxiv.org/abs/1602.01768">Randomized
              quasi-Newton updates are linearly convergent matrix
              inversion algorithms</a><br>
          </p>
          <p>I mostly talked about the first two papers; but managed to
            spend a bit of time at the end on matrix inversion as well.
            Here are the <a href="talks/TALK-2016-03-Oberwolfach.pdf">slides</a>.<br>
          </p>
          <a href="http://epubs.siam.org/doi/abs/10.1137/15M1025487"> </a>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>March 1, 2016</h3>
          <p>As of today, we have a new group member. <a
              href="https://www.researchgate.net/profile/Tie_Ni">Tie Ni</a>
            is an Associate Professor at the Liaoning Technical
            University, China. He will stay in Edinburgh for a year as a
            postdoc. Tie obtained his PhD in mathematics from Tianjin
            University in 2010.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>February 29, 2016</h3>
          <p><a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert M.
              Gower</a> submitted his PhD thesis <i>"Sketch and
              Project: Randomized Iterative Methods for Linear Systems
              and Inverting Matrices"</i> today. As of tomorrow, he will
            become a postdoc in my group; I am looking forward to
            working with him for the next 4 months. After that, Robert
            will join the <a
              href="http://www.di.ens.fr/sierra/index.php">SIERRA team</a>.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>February 24, 2016</h3>
          <p>Shortlisting for the 3-year postdoc post will be finalized
            this week. We shall get in touch with the shortlisted
            candidates by the end of the week (Sunday). <i><br>
            </i></p>
          <p><i>Update (February 28):</i> All shortlisted candidates
            have now been notified via email.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>February 16, 2016</h3>
          <p>We are continuing with the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














              Hands Meetings in Big Data Optimization</a> this semester,
            thanks to funding from the <a
              href="http://www.maths.ed.ac.uk/%7Eigordon/">Head of
              School</a>. We already had two meetings, the third one is
            today at 12:15 in JCMB 5323. <a
              href="http://jakubkonecny.com/">Jakub Konečný</a> will
            speak about the NIPS 2015 paper <a
href="http://papers.nips.cc/paper/5717-taming-the-wild-a-unified-analysis-of-hogwild-style-algorithms.pdf">Taming
the














              wild: a unified analysis of Hogwild!-style algorithms</a>
            by De Sa, Zhang, Olukotun and Re. </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>February 15, 2016</h3>
          <p>I am visiting Cambridge this week.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>January 11, 2016</h3>
          <br>
          <a href="http://www.di.ens.fr/%7Easpremon/Houches/index.html">Workshop</a>
          group photo:<br>
          <br>
          <p> </p>
          <p> <a href="imgs/LesHouches2016.jpg"><img
                src="imgs/LesHouches2016_small.png" alt="Les Houches
                2016" width="600" border="0" height="400"></a> </p>
          <p> </p>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3>February 9, 2016</h3>
          <p><span class="important">New paper out: </span><a
              href="http://arxiv.org/abs/1602.02283">Importance sampling
              for minibatches</a>, joint with <a
              href="http://www.dominikcsiba.com/">Dominik Csiba</a>.<br>
          </p>
          <p>Abstract:<br>
          </p>
          <p> <i>Minibatching is a very well studied and highly popular
              technique in supervised learning, used by practitioners
              due to its ability to accelerate training through better
              utilization of parallel processing power and reduction of
              stochastic variance. Another popular technique is
              importance sampling -- a strategy for preferential
              sampling of more important examples also capable of
              accelerating the training process. However, despite
              considerable effort by the community in these areas, and
              due to the inherent technical difficulty of the problem,
              there is no existing work combining the power of
              importance sampling with the strength of minibatching. In
              this paper we propose the first importance sampling for
              minibatches and give simple and rigorous complexity
              analysis of its performance. We illustrate on synthetic
              problems that for training data of certain properties, our
              sampling can lead to several orders of magnitude
              improvement in training time. We then test the new
              sampling on several popular datasets, and show that the
              improvement can reach an order of magnitude. </i> </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>February 7, 2016</h3>
          <p>As of today, I am in Les Houches, France, attending the <a
              href="http://www.di.ens.fr/%7Easpremon/Houches/index.html">"Optimization
without














              Borders"</a> workshop. <a
              href="http://www.dominikcsiba.com/">Dominik</a>, <a
              href="http://www.jakubkonecny.com/">Jakub</a> and <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert</a>
            are here, too (Robert beat me in table tennis 1:2 tonight;
            he is so getting beaten tomorrow). The workshop is dedicated
            to the 60th birthday of Yurii Nesterov - my former postdoc
            advisor.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>February 4, 2016</h3>
          <p><span class="important">New paper out: </span><a
              href="http://arxiv.org/abs/1602.01768">Randomized
              quasi-Newton updates are linearly convergent matrix
              inversion algorithms</a>, joint with <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>.<br>
          </p>
          <p>Abstract:<br>
          </p>
          <p><i>We develop and analyze a broad family of
              stochastic/randomized algorithms for inverting a matrix.
              We also develop a specialized variant which maintains
              symmetry or positive definiteness of the iterates. All
              methods in the family converge globally and linearly
              (i.e., the error decays exponentially), with explicit
              rates. <br>
            </i></p>
          <p><i>In special cases, we obtain stochastic block variants of
              several quasi-Newton updates, including bad Broyden (BB),
              good Broyden (GB), Powell-symmetric-Broyden (PSB),
              Davidon-Fletcher-Powell (DFP) and
              Broyden-Fletcher-Goldfarb-Shanno (BFGS). Ours are the
              first stochastic versions of these updates shown to
              converge to an inverse of a fixed matrix. <br>
            </i></p>
          <p><i>Through a dual viewpoint we uncover a fundamental link
              between quasi-Newton updates and approximate inverse
              preconditioning. Further, we develop an adaptive variant
              of randomized block BFGS, where we modify the distribution
              underlying the stochasticity of the method throughout the
              iterative process to achieve faster convergence. <br>
            </i></p>
          <p><i>By inverting several matrices from varied applications,
              we demonstrate that AdaRBFGS is highly competitive when
              compared to the well established Newton-Schulz and minimal
              residual methods. In particular, on large-scale problems
              our method outperforms the standard methods by orders of
              magnitude. <br>
            </i></p>
          <p><i>Development of efficient methods for estimating the
              inverse of very large matrices is a much needed tool for
              preconditioning and variable metric methods in the advent
              of the big data era.</i><br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>January 22, 2016</h3>
          <p>Today I am examining a Machine Learning PhD thesis at the <a
              href="http://www.ed.ac.uk/informatics">School of
              Informatics</a>.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>January 19, 2016</h3>
          <p><span class="important"></span>Today, I am giving a talk on
            randomized iterative methods for solving linear systems (see
            papers [<a
              href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">1</a>,
            <a href="http://arxiv.org/abs/1512.06890">2</a>]) in our <a
href="http://www.maths.ed.ac.uk/events/nbp/working-probability">Working
              Probability Seminar</a>. Next Tuesday, <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>
            will speak about randomized iterative methods for inverting
            very large matrices (a preprint of this work should be
            available on arXiv by the end of January).&nbsp;&nbsp; <br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>January 17, 2016</h3>
          <p><span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1512.09103">Even faster
              accelerated coordinate descent using non-uniform sampling</a>,
            joint with <a href="http://people.csail.mit.edu/zeyuan/">Zeyuan














              Allen-Zhu</a>, <a
              href="http://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a> and
            <a href="http://www.callowbird.com/">Yang Yuan</a>. </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>January 14, 2016</h3>
          <p><a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert
              Gower</a> is visiting Cambridge and giving a talk today or
            tomorrow...<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>January 14, 2016</h3>
          <p>Today and tomorrow I am in Stockholm, Sweden, as an
            external examiner (opponent) for a PhD thesis at <a
              href="https://www.kth.se/en">KTH Royal Institute of
              Technology</a>.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>January 12, 2016</h3>
          <span class="important">Open Position: Postdoctoral Research
            Associate in Big Data Optimization </span>
          <p>A postdoctoral position in big data optimization is
            available at the School of Mathematics, University of
            Edinburgh, under the supervision of Dr Peter Richtarik. The
            post is funded through the <a
              href="http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/N005538/1">EPSRC
Fellowship














              "Randomized Algorithms for Extreme Convex Optimization”.</a>
          </p>
          <p>PhD in optimization, computer science, applied mathematics,
            engineering, operations research, machine learning or a
            related discipline is required. Strong research track record
            is essential.</p>
          <p> Duration: 3 years<br>
            Starting date: August or September 2016<br>
            Research travel budget <br>
            Application closing date: January 29, 2016<br>
          </p>
          More information and online application form:<br>
          <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=034907"
            target="_blank">https://www.vacancies.ed.ac.<wbr>uk/pls/corehrrecruit/erq_<wbr>jobspec_version_4.jobspec?p_<wbr>id=034907</a><br>
          &nbsp;<br>
          The University of Edinburgh is a founding partner of the Alan
          Turing Institute -- the national data science institute.
          Edinburgh is the home of Archer, the national supercomputing
          facility.<br>
          &nbsp;<br>
          For informal inquiries, send me an email.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>January 10, 2016</h3>
          <p> <span class="important">Prize: </span> <a
              href="http://mtakac.com/">Martin Takáč</a> is the winner
            of the <span class="important">2014 OR Society Doctoral
              Prize.</span> This is an annual award of the <a
              href="https://en.wikipedia.org/wiki/Operational_Research_Society">OR














              Society</a> for "<i>the most distinguished body of
              research leading to the award of a doctorate in the field
              of Operational Research</i>”. A cash prize of £1500 is
            attached to the award. Martin's thesis, "Randomized
            coordinate descent methods for big data optimization",
            defended in 2014, <a
href="http://mtakac.com/data/_uploaded/file/papers/2014/takac_phd_thesis_final.pdf">can
be














              found here</a>.&nbsp; </p>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3>January 8, 2016</h3>
          <br>
          <a href="http://www.maths.ed.ac.uk/%7Es1065527/index.html">Robert


















            Gower</a> gave a seminar talk in Paris (<a
            href="http://www.di.ens.fr/sierra/people.php">SIERRA team</a>).<br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36"><br>
          <h3>January 8, 2016</h3>
          <br>
          <a href="http://mtakac.com/">Martin Takáč</a>'s "traditional"
          Christmas cookies:<br>
          <br>
          <p> <a href="imgs/cookies.jpg"><img alt="cookies"
                src="imgs/cookies.jpg" width="500" border="0"
                height="401"></a> </p>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3>December 21, 2015 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1512.06890">Stochastic dual
              ascent for solving linear systems</a>, joint with <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>
          </p>
          Abstract:
          <p> <i>We develop a new randomized iterative
              algorithm---stochastic dual ascent (SDA)---for finding the
              projection of a given vector onto the solution space of a
              linear system. The method is dual in nature: with the dual
              being a non-strongly concave quadratic maximization
              problem without constraints. In each iteration of SDA, a
              dual variable is updated by a carefully chosen point in a
              subspace spanned by the columns of a random matrix drawn
              independently from a fixed distribution. The distribution
              plays the role of a parameter of the method. </i></p>
          <i> </i>
          <p><i> Our complexity results hold for a wide family of
              distributions of random matrices, which opens the
              possibility to fine-tune the stochasticity of the method
              to particular applications. We prove that primal iterates
              associated with the dual process converge to the
              projection exponentially fast in expectation, and give a
              formula and an insightful lower bound for the convergence
              rate. We also prove that the same rate applies to dual
              function values, primal function values and the duality
              gap. Unlike traditional iterative methods, SDA converges
              under no additional assumptions on the system (e.g., rank,
              diagonal dominance) beyond consistency. In fact, our lower
              bound improves as the rank of the system matrix drops. </i></p>
          <p><i> Many existing randomized methods for linear systems
              arise as special cases of SDA, including randomized
              Kaczmarz, randomized Newton, randomized coordinate
              descent, Gaussian descent, and their variants. In special
              cases where our method specializes to a known algorithm,
              we either recover the best known rates, or improve upon
              them. Finally, we show that the framework can be applied
              to the distributed average consensus problem to obtain an
              array of new algorithms. The randomized gossip algorithm
              arises as a special case.</i> </p>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3>December 15, 2015</h3>
          <p>I have accepted an invite to give a keynote talk (actually,
            a combo of two 1hr talks: one to a general audience, and one
            more specialized) at the 41st Woudschoten Conference, to be
            held during 5-7 October, 2016 in Zeist, Netherlands. Here is
            a <a
              href="https://wsc.project.cwi.nl/woudschoten/2015/conferentieE.php">link
to














              the website of the 2015 edition</a>.<br>
          </p>
          <p>For the 2016 conference, the following three themes have
            been selected:</p>
          <p>1.&nbsp; Numerical methods for big data analytics <br>
            2.&nbsp; Monte Carlo methods for partial and stochastic
            differential equations <br>
            3.&nbsp; Mixed finite element methods <br>
            <br>
            I have been invited to be a keynote lecturer within the
            theme "Numerical methods for big data analytics".<br>
            <br>
          </p>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3>December 13, 2015 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1512.04039">Distributed
              optimization with arbitrary local solvers,</a> joint with
            <a href="http://mtakac.com/"> </a><a
              href="https://ise.lehigh.edu/content/chenxin-ma">Chenxin
              Ma</a> (Lehigh) <a href="http://jakubkonecny.com/">Jakub
              Konečný</a> (Edinburgh), <a
              href="http://people.inf.ethz.ch/jaggim/">Martin Jaggi</a>
            (ETH), <a href="http://www.cs.berkeley.edu/%7Evsmith/">Virginia














              Smith</a> (Berkeley), <a
              href="http://www.cs.berkeley.edu/%7Ejordan/">Michael I.
              Jordan</a> (Berkeley) and <a href="http://mtakac.com/">Martin














              Takáč</a> (Lehigh).<br>
          </p>
          <p>Abstract: <i>With the growth of data and necessity for
              distributed optimization methods, solvers that work well
              on a single machine must be re-designed to leverage
              distributed computation. Recent work in this area has been
              limited by focusing heavily on developing highly specific
              methods for the distributed environment. These
              special-purpose methods are often unable to fully leverage
              the competitive performance of their well-tuned and
              customized single machine counterparts. Further, they are
              unable to easily integrate improvements that continue to
              be made to single machine methods. To this end, we present
              a framework for distributed optimization that both allows
              the flexibility of arbitrary solvers to be used on each
              (single) machine locally, and yet maintains competitive
              performance against other state-of-the-art special-purpose
              distributed methods. We give strong primal-dual
              convergence rate guarantees for our framework that hold
              for arbitrary local solvers. We demonstrate the impact of
              local solver selection both theoretically and in an
              extensive experimental comparison. Finally, we provide
              thorough implementation details for our framework,
              highlighting areas for practical performance gains.</i><br>
            <br>
          </p>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3>December 10, 2015</h3>
          <br>
          The workshop <a
            href="http://icms.org.uk/workshop.php?id=391#programme">"Mathematical
aspects


















            of big data"</a>, which I am co-organizing, has just
          started. This is a joint meeting of the <a
            href="https://www.lms.ac.uk/">London Mathematical Society</a>
          (LMS) and the <a href="http://www.ems.ac.uk/">Edinburgh
            Mathematical Society</a> (EMS). This event marks the end of
          the 150th anniversary celebrations of the LMS.<br>
          <br>
          <i>The mathematical aspects of the analysis of big data cut
            across pure mathematics, applied mathematics, and
            statistics. The invited speakers at this workshop will
            include a broad range of international experts in
            mathematics, statistics, and computer science, whose
            research covers fields that are inspired by, or have
            applications to, big data analysis.</i><i> The workshop is
            aimed at an audience of general mathematicians but is open
            to all and attendance is free of charge. It will cover
            current trends and developments, and will hopefully enable
            participants to discover or imagine new connections between
            their own research and this rapidly growing subject.</i><br>
          <br>
          Speakers:<br>
          <br>
          Jacek Brodzki, University of Southampton<br>
          Coralia Cartis, University of Oxford<br>
          Ronald Coifman, Yale University <br>
          Ilias Diakonikolas, University of Edinburgh<br>
          Colin McDiarmid, University of Oxford<br>
          Sofia Olhede, University College London<br>
          Igor Rivin, University of St. Andrews<br>
          Marian Scott, University of Glasgow<br>
          Eva Tardos, Cornell University <br>
          <br>
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>December 2, 2015</h3>
          <p>The <a href="http://arxiv.org/abs/1409.1458">CoCoA</a>
            [NIPS 2014] / <a
              href="http://www.jmlr.org/proceedings/papers/v37/mab15.html">CoCoA+</a>
            [ICML 2015] distributed optimization algorithm developed in
            a duo of papers with two co-authors from Edinburgh (<a
              href="http://mtakac.com/">Martin Takáč</a>, myself) has
            won the <a
href="http://mlconf.com/mlconf-industry-impact-student-research-award-winners/">MLconf
Industry














              Impact Student Research Award</a>. The award goes to our
            coauthor <a href="http://www.cs.berkeley.edu/%7Evsmith/">Virginia
Smith














              (UC Berkeley)</a>. Other co-authors: <a
              href="http://people.inf.ethz.ch/jaggim/">M. Jaggi</a> (ETH
            Zurich), <a href="http://www.cs.berkeley.edu/%7Ejordan/">M.I.














              Jordan</a> (Berkeley), <a
              href="https://ise.lehigh.edu/content/chenxin-ma">C. Ma</a>
            (Lehigh), <a
              href="http://statistics.berkeley.edu/people/jonathan-terhorst">J.














              Terhorst</a> (UC Berkeley), <a
              href="https://www.ocf.berkeley.edu/%7Esanjayk/">S.
              Krishnan</a> (UC Berkeley), <a
              href="http://www.da.inf.ethz.ch/people/ThomasHofmann/">T.
              Hofmann</a> (ETH Zurich).<br>
          </p>
          <p><i>About the award:</i> <span style="font-weight: 400;">"This














              year, we started a new award program called the MLconf
              Industry Impact Student Research Award, which is sponsored
              by Google. This fall, our committee of distinguished ML
              professionals reviewed several nominations sent in from
              members of the MLconf community. There were several great
              researchers that were nominated and the committee arrived
              at awarding 2 students whose work, they believe, has the
              potential to disrupt the industry in the future. The two
              winners that were announced at MLconf SF 2015 are UC
              Irvine Student, Furong Huang and UC Berkeley Student,
              Virginia Smith. Below are summaries of their research.
              We’ve invited both researchers to present their work at
              upcoming MLconf events."<br>
            </span></p>
          <p class="entry-title"><span style="font-weight: 400;"><i>The
                citation:</i> "</span><span style="font-weight: 400;"> <span
                style="font-weight: 400;">Virginia Smith’s research
                focuses on distributed optimization for large-scale
                machine learning. The main challenge in many large-scale
                machine learning tasks is to solve an optimization
                objective involving data that is distributed across
                multiple machines. In this setting, optimization methods
                that work well on single machines must be re-designed to
                leverage parallel computation while reducing
                communication costs. This requires developing new
                distributed optimization methods with both competitive
                practical performance and strong theoretical convergence
                guarantees. Virginia’s work aims to determine policies
                for distributed computation that meet these
                requirements, in particular through the development of a
                novel primal-dual framework, CoCoA, which is written on
                Spark. The theoretical and practical development of
                CoCoA is an important step for future data scientists
                hoping to deploy efficient large-scale machine learning
                algorithms.</span>"<br>
            </span></p>
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>December 2, 2015</h3>
          <p><span class="important">Alan Turing Workshop:</span><span
              class="important"> Theoretical and computational
              approaches to large scale inverse problems</span> </p>
          The <a
            href="http://icms.org.uk/workshops/largescaleinverseproblems">workshop</a>
          starts today. We have a line-up of excellent speakers and
          exciting topics. Most importantly, this workshop will inform a
          part of the future research activity of the newly established
          <a href="https://turing.ac.uk/">Alan Turing Institute:</a>
          UK's national research centre for <a
            href="https://en.wikipedia.org/wiki/Data_science">Data
            Science</a>.
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 28, 2015</h3>
          <p><span class="important">2 POSITIONS starting in September
              2016: 3-year postdoc post + PhD post. </span> I am
            looking for 2 highly talented and motivated people to join
            my <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_team.html">big














              data optimization team</a>. </p>
          <p> The closing date for applications for the postdoctoral
            post is on January 29, 2016. <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=034907">Apply
online














              here.</a> To work with me, you may also wish to apply for
            funding through the <a
href="https://turing.ac.uk/content/uploads/2015/06/ATI_fellows_advertfinal131115-NTv02.docx">Alan
Turing














              Fellowship Programme</a>. </p>
          <p> To apply for the PhD post, <a
              href="http://www.maths.ed.ac.uk/studying-here/pgr/phd-application/apply">click














              here</a> and choose the "Operational Research and
            Optimization" PhD programme. Apply as soon as possible. You
            may also wish to apply to our <a
              href="http://datascience.inf.ed.ac.uk/apply/">PhD
              programme in Data Science</a> - this is another way how
            you can get a funded post to work with me. The closing date
            for applications is also January 29, 2016 (for applicants
            from outside the UK/EU, the deadline is December 11, 2015).
          </p>
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 25, 2015</h3>
          <p><span class="important">Alan Turing Workshop: Distributed
              Machine Learning and Optimization<br>
            </span></p>
          The <a href="http://www.icms.org.uk/workshop.php?id=367">workshop</a>
          starts today, I am looking forward to seeing you all there! We
          have a line-up of excellent speakers and exciting topics. Most
          importantly, this workshop will inform a part of the future
          research activity of the newly established <a
            href="https://turing.ac.uk/">Alan Turing Institute:</a> UK's
          national research centre for <a
            href="https://en.wikipedia.org/wiki/Data_science">Data
            Science</a>. <br>
          <br>
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 17, 2015</h3>
          <p><span class="important">Alan Turing Fellowships</span></p>
          <p><a href="https://turing.ac.uk/">The Alan Turing Institute</a>
            is the UK's new national data science institute, established
            to bring together world-leading expertise to provide
            leadership in the emerging field of <a
              href="https://en.wikipedia.org/wiki/Data_science">data
              science</a>. The Institute has been founded by the
            universities of Cambridge, Edinburgh, Oxford, UCL and
            Warwick and EPSRC.<br>
            <br>
            Fellowships for Early Career Researchers are available for 3
            years with the potential for an additional 2 years of
            support following interim review. Fellows will pursue
            research based at the Institute hub in the British Library,
            London. Fellowships will be awarded to individual candidates
            and fellows will be employed by a joint<br>
            venture partner university (Cambridge, Edinburgh, Oxford,
            UCL or Warwick).</p>
          <p>The closing date for applications is 20 December 2015.<br>
            Key requirements: Successful candidates are expected to have
            <br>
            &nbsp; i) a PhD in a data science (or adjacent) subject (or
            to have submitted their doctorate before taking up the
            post), <br>
            &nbsp; ii) an excellent publication record and/or
            demonstrated excellent research potential such as via
            preprints, <br>
            &nbsp; iii) a novel and challenging research agenda that
            will advance the strategic objectives of the Institute, and
            <br>
            &nbsp; iv) leadership potential. Fellowships are open to all
            qualified applicants regardless of background.<br>
            <br>
            Alan Turing Fellowship applications can be made in all data
            science research areas. The Institute’s research roadmap is
            available at <a href="https://turing.ac.uk/#the-vision">https://turing.ac.uk/#the-vision</a>.
            <br>
          </p>
          <p>In addition to this open call, there are two specific
            fellowship programmes:<br>
            <br>
            <span class="important">Fellowships addressing data-centric
              engineering</span></p>
          <p><a href="http://www.lrfoundation.org.uk/">The Lloyd’s
              Register Foundation (LRF)</a> / Alan Turing Institute
            programme to support data-centric engineering is a 5-year,
            £10M global programme, delivered through a partnership
            between LRF and the Alan Turing Institute. This programme
            will secure high technical standards (for example the
            next-generation algorithms and analytics) to enhance the
            safety of life and property around the major infrastructure
            upon which modern society relies. For further information on
            data-centric engineering, see LRF’s Foresight Review of Big
            Data. Applications for Fellowships under this call, which
            address the aims of the LRF/Turing programme, may also be
            considered for funding under the data-centric engineering
            programme. Fellowships awarded under this programme may vary
            from the conditions given above; for more details contact
            fellowship@turing.ac.uk.<br>
            <br>
            Fellowships addressing data analytics and high-performance
            computing Intel and the Alan Turing Institute will be
            supporting additional Fellowships in data analytics and
            high-performance computing. Applications for Fellowships
            under this call may also be considered for funding under the
            joint Intel-Alan Turing Institute programme. Fellowships
            awarded under this joint programme may vary from the
            conditions given above; for more details contact
            fellowship@turing.ac.uk.<br>
            <br>
            <a
href="https://turing.ac.uk/content/uploads/2015/06/ATI_fellows_advertfinal131115-NTv02.docx">Download
full














              information on the Turing fellowships</a>.<br>
            <br>
            Diversity and equality are promoted in all aspects of the
            recruitment and career management of our researchers. In
            keeping with the principles of the Institute, we especially
            encourage applications from female researchers.<br>
          </p>
          <p>I would be happy to closely work with successful applicants
            interested in working in the areas of big data optimization
            / machine learning / numerical linear algebra. If you have a
            strong background, are considering to apply and want to chat
            about this, send me an email.<br>
          </p>
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 17, 2015</h3>
          <p><span class="important">Tenured Position in my School:</span>
            <a
href="http://www.maths.ed.ac.uk/news/2015/lectureship-in-mathematical-sciences">Assistant
Professor














              or Associate Professor post in Mathematical Sciences.</a>
            Preference may be given to candidates who strengthen
            existing research interests in the School or connections
            between them. I would welcome strong applicants in
            Optimization, Operational Research, Statistical Learning and
            Data Science. Starting date: Aug 1, 2016 or by agreement.
            Apply by December 9, 2015.<br>
          </p>
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3>November 16, 2015</h3>
          <p>This week, I am again in Louvain-la-Neuve, Belgium,
            teaching the course <a
              href="http://sites.uclouvain.be/socn/Courses/Courses2015-2">Randomized
algorithms














              for big data optimization </a>within the <a
              href="http://sites.uclouvain.be/socn/">SOCN Graduate
              School in Systems, Optimization, Control and Networks</a>.
            Course material for this week: <a href="docs/SOCN-Lec4.pdf">Lecture













              4</a>, <a href="docs/SOCN-Lab4.ipynb">Lab 4</a>, <a
              href="docs/SOCN-Lec5.pdf">Lecture 5</a>, <a
              href="docs/SOCN-Lab5.ipynb">Lab 5</a>. </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="36"></h3>
          <h3>October 27, 2015</h3>
          <p>Today I gave a seminar talk at Universite catholique de
            Louvain, Belgium.<br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="36"></h3>
          <h3>October 26, 2015</h3>
          <p>This week, I am in Louvain-la-Neuve, Belgium, teaching the
            course <a
              href="http://sites.uclouvain.be/socn/Courses/Courses2015-2">Randomized
algorithms














              for big data optimization </a>within the <a
              href="http://sites.uclouvain.be/socn/">SOCN Graduate
              School in Systems, Optimization, Control and Networks</a>.
            Course material: <a href="docs/SOCN-Lec1.pdf">Lecture 1</a>,
            <a href="docs/SOCN-Lab1.ipynb">Lab 1</a>, <a
              href="docs/SOCN-Lec2.pdf">Lecture 2 (and more)</a>, <a
              href="docs/SOCN-Lab2.ipynb">Lab 2</a>, <a
              href="docs/SOCN-Lec3.pdf">Lecture 3</a>, <a
              href="docs/SOCN-Lab3.zip">Lab 3</a>.<br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="36"></h3>
          <h3>October 23, 2015</h3>
          <p>Jakub Kisiala, an MSc student I had the pleasure to teach
            in my Optimization Methods in Finance class and whose <a
href="http://www.maths.ed.ac.uk/%7Erichtarik/docs/Kisiala_Dissertation.pdf">MSc














              Dissertation</a> I supervised has won the <a
              href="http://msc.maths.ed.ac.uk/or/students/prize">Prize
              for Best Performance on the Operational Research MSc</a>
            in the 2014-2015 academic year.<br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="36"></h3>
          <h3>October 22, 2015</h3>
          <p><a href="http://jakubkonecny.com/">Jakub Konečný </a>is
            visiting Comenius University this week; he gave a <a
href="http://www.fmph.uniba.sk/index.php?id=186&amp;no_cache=1&amp;tx_ttnews%5Btt_news%5D=2203&amp;tx_ttnews%5BbackPid%5D=17&amp;cHash=9f366814dd">seminar














              talk</a> there yesterday. <a
              href="http://www.dominikcsiba.com/">Dominik Csiba</a> is
            on a research visit in <a
              href="http://lear.inrialpes.fr/people/mairal/">Julien
              Mairal</a>'s group in Grenoble. He gave a talk on AdaSDCA
            on Monday in the <a
              href="https://sites.google.com/site/smileingrenoble/seminar-ogre">OGre














              seminar</a>. I am giving a guest lecture today, on
            Randomized Methods for Linear Systems (based on <a
              href="http://arxiv.org/abs/1506.03296">this paper</a>), in
            the <a
              href="http://www.inf.ed.ac.uk/teaching/courses/irds/">"Introduction














              to Research in Data Science"</a> doctoral course to the
            students in our <a href="http://datascience.inf.ed.ac.uk/">Data














              Science PhD programme.</a><br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="36"></h3>
          <h3>October 21, 2015</h3>
          <p>Having been away from internet for a week (I am behind my
            email; so if you are expecting a response from me, I hope to
            be able to take care of all of the backlog in the next few
            weeks), I am now in Paris at the <a
              href="http://dsaa2015.lip6.fr/">2015 IEEE International
              Conference on Data Science and Advanced Analytics</a>.
            Today I am giving a tutorial entitled "Randomized Methods
            for Big Data: from Linear Systems to Optimization". Update:
            <a href="talks/TALK-2015-DSAA-tutorial.pdf">the slides are
              here</a>. </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="36"></h3>
          <h3>October 6, 2015</h3>
          <p>I am visiting <a href="http://www.ox.ac.uk/">Oxford </a>for














            a few days. (Just arrived at the train station and heading
            to my place in a taxi. Passed by a huge crowd of happy
            freshmen hoping to get into a club or bar or something.
            Judging by the numbers, it looked quite hopeless, although
            this did not diminish their enthusiasm so maybe something
            else was going on over there...). Tomorrow I am serving as
            an external examiner for a PhD thesis at the <a
              href="https://www.maths.ox.ac.uk/">Mathematical Institute</a>
            and the day after I am giving a <a
              href="https://www.maths.ox.ac.uk/node/14919">seminar talk</a>.
            If anyone of you locals wants to meet with me, I am staying
            at the <a
href="http://www.chem.ox.ac.uk/oxfordtour/exetercollege/Map-Exeter-College---Landscape-Colour.jpg">Exeter














              College</a>.<br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="36"></h3>
          <h3>September 30, 2015</h3>
          <p>Our paper <a href="http://arxiv.org/abs/1506.03296">Randomized
iterative














              methods for linear systems</a> (joint with <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert M
              Gower</a>) was accepted to SIAM Journal on Matrix Analysis
            and Applications. Here are the <a
              href="talks/TALK-2015-09-Linear_Systems.pdf">slides</a>
            from a recent talk I gave about this work.<br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="36"></h3>
          <h3>September 28, 2015 </h3>
          <p>I am in <a href="http://www.cms.cam.ac.uk/">Cambridge</a>
            as of today, attending <a href="https://turing.ac.uk/">The
              Alan Turing Institute</a> (TATI) scoping workshop on <a
              href="https://atiworkshopstatscs.wordpress.com/">"Statistical














              and Computational Challenges in Large-Scale Data Analysis"</a>.
            This is the 2nd of <a
              href="https://turing.ac.uk/#data-summits-workshops">several
scoping














              workshops</a> taking place between September and December
            2015, aimed at shaping the research agenda of TATI for the
            years to come. I am co-organizing two TATI scoping workshops
            in Edinburgh later this year: one focusing on <a
              href="http://www.icms.org.uk/workshop.php?id=367">distributed














              achine learning &amp; optimization</a> and the other one
            on <a
              href="http://icms.org.uk/workshops/largescaleinverseproblems">large-scale
inverse














              problems</a>. <br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="36"></h3>
          <h3>September 21, 2015 </h3>
          <p> Today I am giving a talk at the <a
href="http://www.damtp.cam.ac.uk/user/cbs31/LMS_Inverse_Day_Edinburgh/Home.html">LMS
Inverse














              Day on "Large-scale and nonlinear inverse problems"</a>. I
            do not have to travel far for this as the event is taking
            place on my campus. I will be speaking about a recent joint
            work with <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert













              M Gower</a> on <a href="http://arxiv.org/abs/1506.03296">randomized














              iterative methods for linear systems</a>. My talk
            certainly does not belong to the "nonlinear" category, but
            fortunately it does belong to the "large-scale" category
            which allowed me to sneak it in ;-) <br>
          </p>
          <p>If you want to see how methods such as randomized Kaczmarz,
            randomized coordinate descent, randomized Newton and
            randomized Gaussian descent (and many others) all arise as
            special cases of a single unifying method that admits
            complexity analysis in its general form, you may wish to
            have a brief look at the <a
              href="http://arxiv.org/abs/1506.03296">paper</a> or skim
            through the <a href="TALK-2015-09-Linear_Systems">slides</a>
            (I will only cover a subset of these slides in the
            workshop).<br>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>September 14, 2015 </h3>
          <p> I am in Toulouse this week, lecturing in a <a
              href="http://www.irit.fr/cimi-machine-learning/node/1">Machine
Learning














              Summer School.</a> This is part of a larger event,<a
              href="http://www.irit.fr/cimi-machine-learning/"> Machine
              Learning Thematic Trimester</a>, which also includes
            several workshops which will be run throughout the year. My
            course is an introduction to optimization for machine
            learning. Here are the <a
              href="talks/2015-09-Toulouse-Summer-School-Optimization.pdf">slides</a>.
            Julia code for the practical session (based on <a
              href="https://www.juliabox.org/">JuliaBox</a>) is <a
              href="code/dfSDCA.zip">here.</a> <br>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>September 8, 2015 </h3>
          <p> I am returning back to Edinburgh today after a week-long
            visit in Austria. I first attended a conference in Vienna,
            and then visited <a href="https://ist.ac.at/">IST Austria</a>,
            where I gave a <a
href="https://ist.ac.at/events/lectures-talks/seminar-talks/2015/09/sdna-stochastic-dual-newton-ascent-for-empirical-risk-minimization/date/462/">talk</a>
            yesterday. I had nice discussions throughout my stay with <a
href="https://ist.ac.at/research/research-groups/barton-group/">Nick
              Barton</a>, <a
href="https://ist.ac.at/research-groups-pages/barton-group/team/katka-bodova/">Katka














              Bodova</a>, <a
              href="https://ist.ac.at/research/research-groups/henzinger-group/">Thomas














              Henzinger</a>, <a
              href="http://pub.ist.ac.at/%7Eaklimova/">Anna Klimova</a>,
            <a
              href="https://ist.ac.at/research/research-groups/kolmogorov-group/">Vladimir














              Kolmogorov</a>, <a
              href="https://ist.ac.at/research/research-groups/lampert-group/">Christoph














              Lampert</a> and <a
              href="https://ist.ac.at/research/research-groups/uhler-group/">Caroline














              Uhler</a>. Thanks!</p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>September 5, 2015 </h3>
          <p> Our paper ``Quartz: Randomized Dual Coordinate Ascent with
            Arbitrary Sampling'', joint with <a
              href="http://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a> and
            <a href="http://www.stat.rutgers.edu/home/tzhang/">Tong
              Zhang,</a> has been <a
              href="https://nips.cc/Conferences/2015/AcceptedPapers">accepted
to














              NIPS.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>September 2, 2015 </h3>
          <p> I am in Vienna this week, attending the <a
              href="https://or2015.univie.ac.at/home/">"OR2015: Optimal
              Decisions and Big Data"</a> conference. I have given a
            talk on <a href="http://arxiv.org/abs/1502.02268">SDNA</a>
            today. <a
              href="http://www.maths.ed.ac.uk/people/show?person=417">Dominik</a>
            is here, too - he talked about the <a
              href="http://www.jmlr.org/proceedings/papers/v37/csiba15.pdf">AdaSDCA</a>
            algorithm. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>August 26, 2015 </h3>
          <p> I have been awarded a <a
              href="http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/N005538/1">5-year
EPSRC














              Fellowship,</a> starting in January 2016. The project
            title is: <span class="important">Randomized Algorithms for
              Extreme Convex Optimization.</span> </p>
          <p> A total of <a
href="http://gow.epsrc.ac.uk/NGBOViewPanelROL.aspx?PanelId=1-2VB991&amp;RankingListId=1-2VB998">5
              Fellowships</a> were awarded this year, out of a total of
            <a
href="http://gow.epsrc.ac.uk/NGBOViewPanelROL.aspx?PanelId=1-2Q1DOC&amp;RankingListId=1-2Q1DPX">43














              proposals</a> across all areas of mathematics and all
            levels of seniority (established career, early career and
            postdoctoral fellowships). It is clear that many excellent
            proposals had to be turned down, which is quite unfortunate
            for the mathematical community. I wish there were more funds
            to fund these! </p>
          <p> <span class="important">!!! Postdoc Position:</span> I
            will be hiring a postdoc to work on the project; the
            position will start in <span class="important">September
              2016</span> (however, there is some flexibility with the
            staring date). The position is initially for <span
              class="important">2 years</span>; with a possible
            extension for a third year (to be decided by the end of the
            1st year). The position has not yet been formally advertised
            - but I encourage strong potential applicants to contact me
            by email!<br>
          </p>
          <p><span class="important">!!! PhD Position:</span> I will
            also be hiring a PhD student to work on the project.
            Starting date: by September 2016. If you are interested,
            apply via our <a
              href="http://www.maths.ed.ac.uk/studying-here/pgr/phd-application/apply">online














              system</a> (to the OR &amp; Optimization programme) and
            then drop me an email.<br>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>July 31, 2015 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1507.08322">Distributed
              mini-batch SDCA,</a> joint with <a
              href="http://mtakac.com/">Martin Takáč</a> and <a
              href="http://ttic.uchicago.edu/%7Enati/">Nati Srebro.</a>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>July 16, 2015 </h3>
          <p>I am in Pittsburgh this week, attending <a
              href="http://www.ismp2015.org/">ISMP 2015.</a> <a
              href="http://jakubkonecny.com/">Jakub</a>, <a
href="http://researcher.watson.ibm.com/researcher/view.php?person=ie-jakub.marecek">Jakub</a>,
            <a href="http://www.mtakac.com/CV">Martin</a>, <a
              href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier</a>,
            <a href="http://www.maths.ed.ac.uk/%7Ertappend/index.html">Rachael</a>,
            <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert</a>
            and <a href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng</a>
            are here, too. I am giving a talk tomorrow. Update: Here are
            <a href="TALK-SDNA-ISMP2015.pdf">the slides.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>July 8, 2015 </h3>
          <p> I am at <a href="http://icml.cc/2015/">ICML in Lille</a>
            this week. The ICML brochure we were all given visualizes
            the hot topics at this year's conference. Notice just how
            central optimization is to machine learning: </p>
          <p> <a href="imgs/ICML2015-cover.jpg"><img
                src="imgs/ICML2015-cover.jpg" align="absmiddle"
                width="600"></a> </p>
          <p> I gave a tutorial on "Modern Convex Optimization Methods
            for Large-scale Empirical Risk Minimization", jointly with <a
              href="http://www.cs.ubc.ca/%7Eschmidtm/">Mark Schmidt</a>,
            on Monday. The slides are <a
              href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_I.pdf">here
(part














              I)</a> and <a
              href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_II.pdf">here
(part














              II)</a>. [Unfortunately, there were some serious technical
            issues with the setup during my talk.] </p>
          <p> We have had two papers accepted to ICML this year, both
            were presented on Tuesday: <a
              href="http://jmlr.org/proceedings/papers/v37/mab15.pdf">Adding














              vs. Averaging in Distributed Primal-Dual Optimization
              (Chenxin Ma, Virginia Smith, Martin Jaggi, Michael Jordan,
              Peter Richtarik, Martin Takac) </a> and <a
              href="http://jmlr.org/proceedings/papers/v37/csiba15.pdf">
              Stochastic Dual Coordinate Ascent with Adaptive
              Probabilities (Dominik Csiba, Zheng Qu, Peter Richtarik)</a>.
          </p>
          <p> Here is a photo of Dominik presenting his poster: </p>
          <p> <a href="imgs/ICML2015_Dominik-fullsize.jpg"><img
                src="imgs/ICML2015_Dominik.png" align="absmiddle"></a> </p>
          Dominik is in the upper right corner of the room...
          <p> <a href="imgs/ICML2015poster_session-fullsize.jpg"><img
                src="imgs/ICML2015poster_session.png" align="absmiddle"></a>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>June 29, 2015 </h3>
          <p> <a href="http://jakubkonecny.com/">Jakub Konečný</a> is
            spending the summer at Google as an intern. He has been
            there for a month already, and will be there until the end
            of August. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>June 22, 2015 </h3>
          <p> I have attended the <a
              href="http://www.numerical.rl.ac.uk/people/nimg/fox/programme.php">IMA
Fox














              Prize meeting</a> in Glasgow today. All the talks were
            great, and the research inspiring. </p>
          <p> <a href="http://perso.telecom-paristech.fr/%7Eofercoq/">Olivier














              Fercoq</a> --- a former postdoc in my group and now an
            Assistant Professor at Telecom ParisTech --- received the
            17th IMA Leslie Fox Prize (2nd Prize) with his paper: <a
              href="http://arxiv.org/abs/1312.5799">Accelerated,
              parallel and proximal coordinate descent</a>, coathored
            with me. </p>
          <p>The Leslie Fox Prize for Numerical Analysis of the
            Institute of Mathematics and its Applications (IMA) is a
            biennial prize established in 1985 by the IMA in honour of
            mathematician Leslie Fox (1918-1992). The prize honours
            "young numerical analysts worldwide" (any person who is less
            than 31 years old), and applicants submit papers for review.
            A committee reviews the papers, invites shortlisted
            candidates to give lectures at the Leslie Fox Prize meeting,
            and then awards First Prize and Second Prizes based on
            "mathematical and algorithmic brilliance in tandem with
            presentational skills" </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>June 16, 2015 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1506.03296">Randomized
              iterative methods for linear systems,</a> joint work with
            <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert
              Gower</a> </p>
          <p> <em> We develop a novel, fundamental and surprisingly
              simple randomized iterative method for solving consistent
              linear systems. Our method has five different but
              equivalent interpretations: sketch-and-project,
              constrain-and-approximate, random intersect, random linear
              solve and ran- dom fixed point. By varying its two
              parameters—a positive definite matrix (defining geometry),
              and a random matrix (sampled in an i.i.d. fashion in each
              iteration)—we recover a comprehensive array of well known
              algorithms as special cases, including the randomized
              Kaczmarz method, randomized Newton method, randomized
              coordinate descent method and random Gaussian pursuit. We
              naturally also obtain variants of all these methods using
              blocks and importance sampling. However, our method allows
              for a much wider selection of these two parameters, which
              leads to a number of new specific methods. We prove
              exponential convergence of the expected norm of the error
              in a single theorem, from which existing complexity
              results for known vari- ants can be obtained. However, we
              also give an exact formula for the evolution of the
              expected iterates, which allows us to give lower bounds on
              the convergence rate. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>June 7, 2015 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1506.02227">Primal method for
              ERM with flexible mini-batching schemes and non-convex
              losses,</a> joint work with Dominik Csiba. </p>
          <p> <em>Abstract: In this work we develop a new algorithm for
              regularized empirical risk minimization. Our method
              extends recent techniques of Shalev-Shwartz [02/2015],
              which enable a dual-free analysis of SDCA, to arbitrary
              mini-batching schemes. Moreover, our method is able to
              better utilize the information in the data defining the
              ERM problem. For convex loss functions, our complexity
              results match those of QUARTZ, which is a primal-dual
              method also allowing for arbitrary mini-batching schemes.
              The advantage of a dual-free analysis comes from the fact
              that it guarantees convergence even for non-convex loss
              functions, as long as the average loss is convex. We
              illustrate through experiments the utility of being able
              to design arbitrary mini-batching schemes. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>June 1, 2015 </h3>
          <p> Today I gave a talk at <a
              href="https://www.math.ucdavis.edu/">UC Davis</a>, on an
            invitation by <a
              href="https://www.math.ucdavis.edu/%7Empf/">Michael
              Friedlander</a>. I've talked about <a
              href="http://arxiv.org/abs/1502.02268">SDNA: Stochastic
              Dual Newton Ascent for empirical risk minimization.</a></p>
          <p> Trivia: First time I used Amtrak in my life (liked it!),
            first time I lost a T-shirt, first time I thought I was
            supposed to give talk X when in fact I agreed to give talk
            Y, discussed a new and interesting joint research idea
            during the visit (a pleasant surprise), walked 1hr to the
            train station and 1hr back. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>May 27, 2015 </h3>
          <p> Today I am giving a seminar talk at AMPLab, UC Berkeley.
            Coordinates: 465H Soda Hall, Time: noon. I'll be talking
            about <a href="http://arxiv.org/abs/1502.02268">SDNA:
              Stochastic Dual Newton Ascent for empirical risk
              minimization.</a></p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>May 26, 2015 </h3>
          <p> Totday, <a href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng
              Qu</a> is giving a talk on our Quartz algorithms (<a
              href="http://arxiv.org/abs/1411.5873">here is the paper</a>)
            at the <a href="http://kac.maths.ed.ac.uk/%7Ebl/mmmds/">Mathematical














              Methods for Massive Data Sets</a> workshop. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>May 24, 2015 </h3>
          <p> I am visiting UC Berkeley during for the next couple
            weeks.</p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>May 8, 2015 </h3>
          <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/">Optimization
and


















            Big Data 2015:</a> The award committee consisting of Arkadi
          Nemirovski (Georgia Institute of Technology) and Rodolphe
          Jenatton (Amazon Berlin) announced the Best Contribution Award
          Winners:
          <p> <span class="important">Winner:</span> Rodrigo
            Mendoza-Smith (University of Oxford) <br>
            for "Expander l0 decoding" <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/slides/08-Mendoza-Smith.pdf">[slides]</a>
            <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/posters/Mendoza-Smith.pdf">[poster]</a>
            [paper] <br>
            The first prize carries a 500 EUR cash award, sponsored by
            Amazon Berlin </p>
          <p> <img
src="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/files/Award-Mendoza-Smith.png"
              alt="UoE" width="400"> </p>
          <p> <span class="important">Runner-up:</span> Dominik Csiba
            (University of Edinburgh) <br>
            for "Stochastic dual coordinate ascent with adaptive
            probabilites" <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/slides/02-Csiba.pdf">[slides]</a>
            <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/posters/Csiba.pdf">[poster]</a>
            <a href="http://arxiv.org/abs/1502.08053">[paper]</a> </p>
          <p> <img
src="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/files/Award-Csiba.png"
              alt="UoE" width="400"> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>May 6, 2015 </h3>
          <p> <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/">Optimization
and














              Big Data 2015</a> is starting today! </p>
          <p> <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/files/OBD2015.pdf"><img
src="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/files/booklet-thmb.png"
                alt="Optimization and Big Data 2015 booklet"></a> </p>
          <p> We have an amazing lineup of <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/schedule.html">speakers</a>;
            I am looking forward to all the talks and to the discussions
            during the rest of the week. </p>
          <p> A message to all participants: Welcome to Edinburgh and
            enjoy the event! </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>April 25, 2015 </h3>
          <p><span class="important">Two papers accepted</span> to <a
              href="http://icml.cc/2015/">ICML 2015:</a> </p>
          <p><a href="http://arxiv.org/abs/1502.08053">Stochastic dual
              coordinate ascent with adaptive probabilities</a> (code:
            AdaSDCA)<br>
            joint with: Dominik Csiba and Zheng Qu <br>
          </p>
          <p><a href="http://arxiv.org/abs/1502.03508">Adding vs.
              averaging in distributed primal-dual optimization</a>
            (code: CoCoA+)<br>
            joint with: Chenxin Ma, Virginia Smith, Martin Jaggi,
            Michael I. Jordan and Martin Takáč<br>
          </p>
          <p>The ICML decisions were announced today. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>April 20, 2015</h3>
          <p><span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1504.04407">Mini-Batch
              Semi-Stochastic Gradient Descent in the Proximal Setting,</a>
            joint work with <a href="http://jakubkonecny.com/">Jakub
              Konečný</a>, <a href="http://jieliucn.weebly.com/">Jie
              Liu</a> and <a href="http://www.mtakac.com/CV">Martin
              Takáč</a>. This is the full-size version of the following
            <a href="http://arxiv.org/abs/1410.4744">short paper</a>
            which was presented at the NIPS Optimization workshop. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>April 20, 2015</h3>
          <p> Today I am giving a talk at the <a
              href="http://www.macs.hw.ac.uk/%7Efd78/miapr15">Maxwell
              Institute Probability Day.</a> I will be talking about
            randomized optimization methods with ``arbitrary sampling''.
          </p>
          <p>This is a line of work which we started with my former PhD
            student <a href="http://www.mtakac.com/CV">Martin Takac</a>
            in <a href="http://arxiv.org/abs/1310.3438">this work on
              the NSync algorithm</a>, and continued in various falvours
            and settings in a sequence of papers with Zheng Qu, Martin
            Takac and Tong Zhang and Olivier Fercoq: <a
              href="http://arxiv.org/abs/1411.5873">QUARTZ</a>
            (primal-dual setup for empirical risk minimization), <a
              href="http://arxiv.org/abs/1412.8060">ALPHA</a>
            (non-accelerated and accelerated coordinate descent), <a
              href="http://arxiv.org/abs/1412.8063">ESO</a> (theory of
            expected separable overapproximation enabling the
            computation of closed form formulae for certain stepsize
            parameters), <a href="http://arxiv.org/abs/1502.02268">SDNA</a>
            (arbitrary sampling + second order information). In the
            workshop today I will focus on NSync and QUARTZ. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>April 17, 2015</h3>
          <p> The early bird deadline and abstract submission deadline
            for <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/">Optimization
and














              Big Data 2015</a> is tomorrow (April 18, 2015).</p>
          <p> <a
href="http://www.eventbrite.com/e/optimization-and-big-data-2015-tickets-15692485647?ref=ebtnebregn"
              target="_blank"><img
                src="https://www.eventbrite.co.uk/custombutton?eid=15692485647"
                alt="Eventbrite - Optimization and Big Data 2015"></a> </p>
          <p> For a list of participants already registered, <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/registration.html">click














              here.</a> For a list of contributions already accepted (we
            were doing this on a rolling basis), <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/schedule.html">look














              here.</a> The list of invited speakers and their talk
            titles <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/index.html">is














              here.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>April 12, 2015</h3>
          <p> The paper <a
              href="http://link.springer.com/article/10.1007/s10107-015-0901-6">Parallel
coordinate














              descent methods for big data optimization</a>, joint with
            <a href="http://www.mtakac.com/CV">Martin Takac</a>, has now
            appeared (online) in Mathematical Programming, Series A. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3>March 28, 2015</h3>
          <p> <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert
              Gower</a> has joined my <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_team.html">team</a>
            as a PhD student effective yesterday. He started his PhD in
            2012, and has until now been working under the supervision
            of <a href="http://www.maths.ed.ac.uk/%7Egondzio/">Jacek
              Gondzio.</a> Robert's past work is on automatic
            differentiation <a
href="http://www.tandfonline.com/doi/abs/10.1080/10556788.2011.580098?journalCode=goms20#.VRbZOkaPKpp">[1]</a>,
            <a
              href="http://dl.acm.org/citation.cfm?doid=2594412.2490254">[2]</a>,
            <a
              href="http://link.springer.com/article/10.1007%2Fs10107-014-0827-4">[3]</a>
            and quasi-Newton methods <a
              href="http://arxiv.org/abs/1412.8045"> [4]</a>. Robert:
            welcome to the group! </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> March 17, 2015 </h3>
          <p> <a href="http://homepages.inf.ed.ac.uk/gsanguin/">Guido
              Sanguinetti</a> and <a
              href="https://sites.google.com/site/tommayoresearch/cv-publications">Tom














              Mayo</a> talked today in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">Big














              Data Seminar</a> about their work in the field of
            neuroinformatics and how it relates to big data and
            optimization.</p>
          <p> <a href="http://www.mtakac.com/CV">Martin Takac</a> has
            put together (and presented in New York) a poster about our
            <a href="http://arxiv.org/abs/1502.02268">SDNA paper.</a> <a
              href="posters/Poster-SDNA.pdf">Here it is.</a></p>
          <a href="posters/Poster-SDNA.pdf"> <img
              src="imgs/fancy-line.png" align="absmiddle" height="36">
            <h3> March 16, 2015 </h3>
          </a>
          <p><a href="posters/Poster-SDNA.pdf"> We have </a><a
              href="http://www.maths.ed.ac.uk/news/2015/lecturers_ds_or">2
              Lectureships</a> (Lecturer in the UK= tenured Assistant
            Professor in the USA) open in the School of Mathematics: <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=032764">Lectureship
in














              the Mathematics of Data Science</a> and <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=032763">Lectureship
in














              Operational Research.</a> </p>
          <span class="important">Application deadline: April 14, 2015 </span>
          <br>
          <span class="important">Starting date: August 1, 2015 </span>
          <p> The University of Edinburgh, alongside Oxford, Cambridge,
            Warwick and UCL, is a partner in the <a
              href="http://en.wikipedia.org/wiki/Alan_Turing_Institute">Alan














              Turing Institute</a>, which is being formed at the moment.
            This constitutes a major investment by the UK government
            (£42 million) into <span class="important">Big Data</span>
            research and <span class="important">Algorithms.</span> The
            successful candidates will benefit from the vibrant
            community of the Alan Turing Institute. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> March 15, 2015 </h3>
          <p><a href="http://www.maths.ed.ac.uk/people/show?person=417">Dominik














              Csiba</a> was selected to participate at the <a
              href="http://www.cs.rpi.edu/%7Edrinep/G2S3_RandNLA_2015/index.htm">Gene
Golub














              SIAM Summer School on Randomization in Numerical Linear
              Algebra (RandLNA),</a> to be held in June 2015 in Delphi,
            Greece. </p>
          <p> He was also selected to take part in the <a
              href="http://mlss.tuebingen.mpg.de/2015/index.html">2015
              Machine Learnig Summer School,</a> which will be held in
            July 2015 at the <a href="http://www.is.tuebingen.mpg.de/">Max













              Planck Institute for Intelligent Systems, Germany.</a> The
            selection procedure was highly competitive, only 20% of the
            applicants were offered a place. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> March 12, 2015 </h3>
          <p><span class="important">New paper:</span> <a
              href="http://arxiv.org/abs/1503.03033">On the Complexity
              of Parallel Coordinate Descent,</a> joint work with <a
              href="http://www.maths.ed.ac.uk/%7Ertappend/index.html">
              Rachael Tappenden </a> and <a href="http://mtakac.com/">Martin














              Takáč.</a> </p>
          <p> <em>Abstract: In this work we study the parallel
              coordinate descent method (PCDM) proposed by Richtarik and
              Takac [26] for minimizing a regularized convex function.
              We adopt elements from the work of Xiao and Lu [39], and
              combine them with several new insights, to obtain sharper
              iteration complexity results for PCDM than those presented
              in [26]. Moreover, we show that PCDM is monotonic in
              expectation, which was not confirmed in [26], and we also
              derive the first high probability iteration complexity
              result where the initial levelset is unbounded. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> March 10, 2015 </h3>
          <p> In today's <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">Big














              Data Optimization meeting</a> we have <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu</a>
            covering a <a
href="http://www.uclouvain.be/cps/ucl/doc/core/documents/coredp2015_3web.pdf">recent
paper














              of Yurii Nesterov</a> on primal dual Frank-Wolfe type
            methods. These methods have attracted a considerable
            attention in the recent years, and were for instance
            featured in a <a
              href="https://sites.google.com/site/frankwolfegreedytutorial/home">2014
ICML














              tutorial by Jaggi and Harchaoui.</a></p>
          <p> An unrelated announcement: Dominik Csiba is away this week
            and next; attending the <a
              href="http://www.siam.org/meetings/cse15/index.php">SIAM
              Conference on Computational Science and Engineering</a> in
            Salt Lake City.</p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> March 9, 2015 </h3>
          <p> <a href="http://perso.telecom-paristech.fr/%7Eofercoq/">Olivier














              Fercoq</a> --- a former postdoc in my group and now a
            postdoc at Telecom Paris Tech --- is a <a
              href="http://www.numerical.rl.ac.uk/people/nimg/fox/shortlist.php">finalist
for














              the 17th IMA Leslie Fox Prize</a> with his paper: <a
              href="http://arxiv.org/abs/1312.5799">Accelerated,
              parallel and proximal coordinate descent</a>, coathored
            with me. The paper will appear in the SIAM Journal on
            Optimization. Also shortlisted is <a
              href="http://www.maths.ed.ac.uk/people/show?person=372">John














              Pearson</a>, who too was a postdoc in the School recently,
            with his paper <a
              href="http://epubs.siam.org/doi/abs/10.1137/120892003">Fast














              iterative solution of reaction-diffusion control problems
              arising from chemical processes,</a> which he wrote prior
            to joining Edinburgh.</p>
          <p>A First and a number of Second Prizes will be awarded on
            June 22, 2015 in Glasgow, at the Leslie Fox Prize meeting
            collocated with the <a
              href="http://numericalanalysisconference.org.uk/">26th
              Biennial Conference on Numerical Analysis.</a></p>
          <p>The Leslie Fox Prize for Numerical Analysis of the
            Institute of Mathematics and its Applications (IMA) is a
            biennial prize established in 1985 by the IMA in honour of
            mathematician Leslie Fox (1918-1992). The prize honours
            "young numerical analysts worldwide" (any person who is less
            than 31 years old), and applicants submit papers for review.
            A committee reviews the papers, invites shortlisted
            candidates to give lectures at the Leslie Fox Prize meeting,
            and then awards First Prize and Second Prizes based on
            "mathematical and algorithmic brilliance in tandem with
            presentational skills" </p>
          <p> Two years ago, a Second Prize was awarded to <a
              href="http://www.mtakac.com/CV">Martin Takáč.</a> A
            complete <a
              href="http://www.numerical.rl.ac.uk/people/nimg/fox/winners.php%20">list
of














              past winners is here.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> March 5, 2015 </h3>
          <p> <a href="http://people.ufpr.br/%7Eademir.ribeiro/">Ademir
              Ribeiro</a> (Federal University of Parana and University
            of Edinburgh) gave a talk today in our ERGO seminar. The
            talk is based on paper we are writing. Title: The Complexity
            of Primal-Dual Fixed Point Methods for Ridge Regression. The
            <a
              href="http://www.maths.ed.ac.uk/ERGO/abstracts/2015-03-ribeiro.html">abstract
can














              be found here.</a></p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> March 4, 2015 </h3>
          <p> Today, <a href="http://jakubkonecny.com/">Jakub Konečný</a>
            is giving (actually it seems it already happened, due to the
            time difference) a <a
href="http://esd.sutd.edu.sg/ai1ec_event/jakub-konecny-university-edinburgh-semi-stochastic-gradient-descent-methods/">talk
at














              the Singapore University of Technology and Design.</a></p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> March 3, 2015 </h3>
          <p> I have several news items packed into a single entry
            today:</p>
          <p> <a href="https://gol.dinfo.unifi.it/lab/en/node/5521">Luca














              Bravi</a> is visiting my group starting this week, he will
            stay for four months until the end of June. Luca is a PhD
            student at the University of Florence, supervised by <a
              href="https://gol.dsi.unifi.it/users/sciandrone/">Marco
              Sciandrone.</a> If you see a new face again and again at
            the All Hands and ERGO seminars, that's probably him. Take
            him to lunch. </p>
          <p> After being lost in a jungle in Australia last week, and
            then finding his way back again, apparently still with
            enough blood left (leeches take their toll), <a
              href="http://jakubkonecny.com/">Jakub Konecny</a> is now
            visiting <a
              href="http://istd.sutd.edu.sg/faculty/ngai-man-man-cheung/">Ngai-Man














              Cheung</a> and <a
              href="http://esd.sutd.edu.sg/faculty/selin-damla-ahipasaoglu/">Selin
Damla














              Ahipasaoglu</a> at the <a href="http://www.sutd.edu.sg/">Singapore
University














              of Technology and Design (SUTD).</a> I am wondering what
            will happen to him there ;-) </p>
          <p> We had two very interesting talks in the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














              Hands Meetings on Big Data Optimization</a> in the past
            two weeks. Last Tuesday, <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>
            spoke about <a href="http://arxiv.org/abs/1412.8045">"Action
constrained














              quasi-Newton methods"</a>. Today, <a
              href="http://www.maths.ed.ac.uk/%7Ekfount/">Kimon
              Fountoulakis</a> talked about a recent paper from
            Stanford/Berkeley about equipping <a
              href="http://arxiv.org/abs/1502.03571"> stochastic
              gradient descent with randomized preconditioning.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> February 20, 2015 </h3>
          <p><span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1502.08053">Stochastic Dual
              Coordinate Ascent with Adaptive Probabilities,</a> joint
            work with <a
              href="http://www.maths.ed.ac.uk/people/show?person=417">Dominik














              Csiba</a><a> and </a><a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu.</a> </p>
          <p> <em>Abstract: This paper introduces AdaSDCA: an adaptive
              variant of stochastic dual coordinate ascent (SDCA) for
              solving the regularized empirical risk minimization
              problems. Our modification consists in allowing the method
              adaptively change the probability distribution over the
              dual variables throughout the iterative process. AdaSDCA
              achieves provably better complexity bound than SDCA with
              the best fixed probability distribution, known as
              importance sampling. However, it is of a theoretical
              character as it is expensive to implement. We also propose
              AdaSDCA+: a practical variant which in our experiments
              outperforms existing non-adaptive methods. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> February 16, 2015 </h3>
          <p>As of today, <a href="http://jakubkonecny.com/">Jakub
              Konečný</a> is attending <a
              href="http://nicta.com.au/research/machine_learning/mlss2015">Machine
Learning














              Summer School</a> in Australia, Sydney. The school runs
            between Feb 16 and Feb 25. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> February 13, 2015 </h3>
          <p><span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1502.03508">Adding vs.
              Averaging in Distributed Primal-Dual Optimization,</a>
            joint work with <a
              href="http://ise.lehigh.edu/content/chenxin-ma">Chenxin Ma</a>,
            <a href="http://www.cs.berkeley.edu/%7Evsmith/">Virginia
              Smith</a>, <a href="http://people.inf.ethz.ch/jaggim/">Martin














              Jaggi</a>, <a
              href="http://www.cs.berkeley.edu/%7Ejordan/">Michael I.
              Jordan</a>, and <a href="http://mtakac.com/">Martin
              Takáč.</a> </p>
          <p> <em>Abstract: Distributed optimization algorithms for
              large-scale machine learning suffer from a communication
              bottleneck. Reducing communication makes t he efficient
              aggregation of partial work from different machines more
              challenging. In this paper we present a novel
              generalization of the recent communication efficient
              primal-dual coordinate ascent framework (CoCoA). Our
              framework, CoCoA+, allows for additive combination of
              local updates to the global parameters at each iteration,
              whereas previous schemes only allowed conservative
              averaging. We give stronger (primal-dual) convergence rate
              guarantees for both CoCoA as well as our new variants, and
              generalize the theory for both methods to also cover
              non-smooth convex loss functions. We provide an extensive
              experimental comparison on several real-world distributed
              datasets, showing markedly improved performance,
              especially when scaling up the number of machines. </em>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> February 10, 2015 </h3>
          <p>Today we have had <a
              href="http://homepages.inf.ed.ac.uk/ckiw/">Chris Williams</a>
            give a talk in the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">Big














              Data Optimization Seminar.</a> The topic was ``linear
            dynamical systems applied to condition monitoring''. </p>
          <p> <em>Abstract: We develop a Hierarchical Switching Linear
              Dynamical System (HSLDS) for the detection of sepsis in
              neonates in an intensive care unit. The Factorial
              Switching LDS (FSLDS) of Quinn et al. (2009) is able to
              describe the observed vital signs data in terms of a
              number of discrete factors, which have either
              physiological or artifactual origin. We demonstrate that
              by adding a higher-level discrete variable with semantics
              sepsis/non-sepsis we can detect changes in the
              physiological factors that signal the presence of sepsis.
              We demonstrate that the performance of our model for the
              detection of sepsis is not statistically different from
              the auto-regressive HMM of Stanculescu et al. (2013),
              despite the fact that their model is given "ground truth"
              annotations of the physiological factors, while our HSLDS
              must infer them from the raw vital signs data. Joint work
              with Ioan Stanculescu and Yvonne Freer. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> February 9, 2015 </h3>
          <p><span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1502.02268">SDNA: Stochastic
              dual Newton ascent for empirical risk minimization,</a>
            joint work with <a href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng













              Qu</a>, <a href="http://mtakac.com/">Martin Takáč</a> and
            <a href="http://perso.telecom-paristech.fr/%7Eofercoq/">
              Olivier Fercoq.</a> </p>
          <p> <em>Abstract: We propose a new algorithm for minimizing
              regularized empirical loss: Stochastic Dual Newton Ascent
              (SDNA). Our method is dual in nature: in each iteration we
              update a random subset of the dual variables. However,
              unlike existing methods such as stochastic dual coordinate
              ascent, SDNA is capable of utilizing all curvature
              information contained in the examples, which leads to
              striking improvements in both theory and practice –
              sometimes by orders of magnitude. In the special case when
              an L2-regularizer is used in the primal, the dual problem
              is a concave quadratic maximization problem plus a
              separable term. In this regime, SDNA in each step solves a
              proximal subproblem involving a random principal submatrix
              of the Hessian of the quadratic function; whence the name
              of the method. If, in addition, the loss functions are
              quadratic, our method can be interpreted as a novel
              variant of the recently introduced Iterative Hessian
              Sketch. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> February 8, 2015</h3>
          <p> Congratulations to <a href="http://jakubkonecny.com/">Jakub














              Konečný</a> who is the recipient of the <a
              href="http://www.baspfrontiers.org/award.php">Best
              Contribution Prize</a> in the field of Signal Processing
            at the <a href="http://www.baspfrontiers.org/index.php">
              2015 International BASP Frontiers Workshop.</a> The prize
            carries a cash award and is given to a young scientist (a
            PhD student or a postdoc) based on the quality of their talk
            and the presented research. Jakub gave <a
              href="http://jakubkonecny.com/files/01-15%20Semi-Stochastic%20BASP.ppsx">this














              talk</a>, which is based on these papers: <a
              href="http://arxiv.org/abs/1312.1666">S2GD</a>, <a
              href="http://arxiv.org/abs/1410.4744">mS2GD</a>, <a
              href="http://arxiv.org/abs/1412.6293">S2CD</a>. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> January 29, 2015</h3>
          <p> Mojmir Mutny (Edinburgh, Physics) has worked with me last
            Summer on a research project funded by an undergraduate
            research bursary. He has generated many interesting ideas,
            and written a report on his various findings (e.g., on a
            novel optimization formulation of an imaging problem).
            However, the reason why I am writing this post is to provide
            a <a href="https://bitbucket.org/Mojusko/biglearning/src">link














              to the code</a> he wrote, implementing gradient descent,
            coordinate descent and parallel coordinate descent. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> January 28, 2015</h3>
          <p> Alongside with Cambridge, Oxford, Warwick and UCL,
            Edinburgh will lead the new <span class="important">Alan
              Turing "Big Data" Institute</span>. This great piece of
            news was <a
href="http://www.techworld.com/news/big-data/five-universities-named-by-government-lead-alan-turing-institute-3595950/">announced














              today</a> by business secretary Vincent Cable. The
            Edinburgh bid was co-led by Mathematics and Informatics, and
            I am doubly happy about the annoucement as I was one of the
            people involved in the process. I am truly excited about the
            opportunities this will bring. </p>
          <p> Update (Feb 3, 2015): <a
              href="http://www.maths.ed.ac.uk/news/2015/alan-turing-institute">The
School














              of Mathematics news article about this.</a> </p>
          <p> This seems like an excellent excuse to announce <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/">Optimization
and














              Big Data 2015</a>, a workshop which will be held in
            Edinburgh during May 6-8, 2015. This is the third event in a
            series of very successful workshops run since 2012. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> January 27, 2015 </h3>
          <p> <a href="http://www.baspfrontiers.org/program.php"> The
              International BASP Frontiers workshop</a> is running this
            week in Villars-sur-Ollon, Switzerland. There are three
            streams (Signal Processing, Astro-Imaging and Bio-Imaging),
            all composed of three sessions. I have put together the <a
href="http://www.baspfrontiers.org/program.php?session=2#day4">"Modern
              Scalable Algorithms for Convex Optimization"</a> session
            which runs tomorrow. Speakers: <a
              href="http://www-syscom.univ-mlv.fr/%7Epesquet/index.htm">JC














              Pesquet (Universite Paris-Est)</a>, <a
              href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">CB
              Schonlieb (Cambridge)</a>, <a
              href="http://iew3.technion.ac.il/Home/Users/becka.html">A
              Beck (Technion)</a>, <a href="http://jakubkonecny.com/">J
              Konecny (Edinburgh)</a>, <a
              href="http://lear.inrialpes.fr/people/mairal/index_eng.php">J
              Mairal (INRIA)</a>. Deluxe posters: <a
              href="http://lions.epfl.ch/postdocs/quoc.tran-dinh">Q
              Tran-Dinh (EPFL)</a>, <a
              href="http://www-syscom.univ-mlv.fr/%7Epirayre/">A Pirayre
              (University Paris-Est)</a>, <a
              href="http://people.epfl.ch/vassilis.kalofolias">V
              Kalofolias (EPFL)</a>, <a
              href="http://www.see.ed.ac.uk/%7Es0574225/">M Yaghoobi
              (Edinburgh)</a>. </p>
          <p> We have started the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














              Hands Meeting in Big Data Optimization</a> again this
            year. Last week we had Ilias Diakonikolas (Edinburgh
            Informatics) giving a wonderful talk about Algorithms in
            Statistics, losely based on <a
href="http://www.iliasdiakonikolas.org/papers/piecewise-poly-learning.pdf">this














              paper</a>. Today we have <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu</a>
            covering a recent paper of Alekh Agarwal and Leon Bottou on
            <a href="http://arxiv.org/abs/1410.0723">lower bounds for
              the problem of minimizing the sum of a large number of
              convex functions</a> (Alekh: I can't wait to play some
            more TT with you ;-). Next week, after his return from
            Switzerland, <a href="http://jakubkonecny.com/">Jakub
              Konecny</a> will speak about <a
              href="http://arxiv.org/abs/1312.7853">DANE (Communication
              Efficient Distributed Optimization using an Approximate
              Newton-type Method) - a recent paper of Ohad Shamir, Nati
              Srebro and Tong Zhang</a>. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> January 15, 2015 </h3>
          <p> If you wish to work with me on exciting new optimization
            algorithms and machine learning techniques applicable to big
            data problems, apply to our PhD programme in Data Science.
            The deadline for September 2016 entry is on January 30,
            2015. </p>
          <p> <a href="http://datascience.inf.ed.ac.uk/apply/"> <img
                src="imgs/Data_Science_PhD_small.png" align="absmiddle"
                height="407"> </a> </p>
          <p> You may also want to apply for PhD in <a
              href="http://www.maths.ed.ac.uk/studying-here/pgr/phd-application">Optimization
and














              Operations Research</a> and/or to the <a
              href="http://www.maxwell.ac.uk/MIGSAA.php"> Maxwell
              Institite Graduate School in Analysis and its
              Applications.</a> To apply for a PhD in MIGSAA, send your
            CV, transcript and a cover note to explain you interests to
            apply2MIGSAA@maxwell.ac.uk. I am affiliated with all three
            PhD programmes. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> January 14, 2015 </h3>
          <p> Today I gave my <a
              href="talks/TALK-2015-01-ArbitrarySampling.ppsx">talk on
              Coordinate Descent Methods with Arbitrary Sampling</a> --
            at the <a href="http://lear.inrialpes.fr/workshop/osl2015/">Optimization














              and Statistical Learning workshop (Les Houches, France).</a>
          </p>
          <p> Randomized coordinate descent methods with arbitrary
            sampling are optimization algorithms which at every
            iteration update a random subset of coordinates (i.i.d.
            throughout the iterations), with the distribution of this
            random set-valued mapping allowed to be arbitrary. It turns
            out that methods of this type work as long as every
            coordinate has a positive probability of being chosen by the
            sampling, and hence being updated. This is clearly a
            necessary condition if we want the method to converge from
            any starting point. However, it turns out it is also
            sufficient. Naturaly, certain characteristics of the
            distribution of the random set-valued mapping (or
            "sampling", for simplicity) manifest itself in the
            complexity bound. For some distributions, the bounds is
            good, for some it is bad -- which opens the possibility to
            design samplings optimizing the complexity bound. If we
            restrict our attention to the case of samplings picking a
            single coordinate at a time, the optimal distribution is
            known as <em>importance sampling.</em> Usually, the
            difference between the uniform and importance sampling in
            terms of complexity is in the replacement of the maximum of
            certain problem-dependent quantities in the bound by their
            average. If these quantities have a very nonuniform
            distribution, this is a major imporvement - and this can be
            clearly seen in practice as well. The above general setup
            opens the possibility to efficiently solve optimization
            problems arising in applications where it is more natural to
            update structured subsets of variables (e.g., overlapping
            blocks) and in situations where the sampling is implicitly
            defined by the computing environment (e.g., faulty
            processors). </p>
          <p> To the best of my knowledge, at the moment there are only
            four papers dealing with this topic. </p>
          <p> The <a href="http://arxiv.org/abs/1310.3438">first paper
              (NSync) </a> was coauthored by <a
              href="http://www.mtakac.com/CV">Martin Takac</a> and
            myself. In it we focuse on the simple case of unconstrained
            smooth minimization of a strongly convex function. The paper
            is very brief (you could end reading at the end of page 2!)
            and the complexity result compact. We show that in order to
            find an eps-solution with probability at least 1-rho, it is
            sufficient to take </p>
          <p> max_i (v_i/p_i*lambda) * log((f(x^0)-f(x^*))/(eps*rho)) </p>
          <p> iterations, where the max is taken over the coordinates, f
            is the objective function, x^0 and x^* are the starting and
            optimal points, respectively, lambda is the strong
            covnvexity parameter of f, p_i is the probability that
            coordinate i is chosen by the sampling and v_i are certain
            parameters that depend on both f and the sampling. Warning:
            we use different notation in the paper. </p>
          <p> The <a href="http://arxiv.org/abs/1411.5873">second paper
              on the topic </a> deals with a primal-dual optimization
            formulation which has received much attention due to its
            relevance to machine learning. The method we design (Quartz;
            this is joint work with <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu</a> and
            <a href="http://stat.rutgers.edu/home/tzhang/">Tong Zhang</a>)
            in each iteration updates a random subset of dual variables
            (again, arbitrary sampling is allowed). The analysis is
            directly primal dual, and the resulting complexity bounds is
            again very simple. In order to find a pair (w^t,alpha^t) of
            primal and dual vectors for which the expected duality gap
            is below eps (E [P(w^0)-D(alpha^0)]&lt;= eps), it is
            sufficient to take </p>
          <p> max_i (1/p_i + v_i/(p_i*lambda*gamma*n)) * log
            ((P(w^0)-D(alpha^0))/epsilon) </p>
          <p>iterations. The maximum is again taken over the n
            "coordinates" (dual variables), p_i, v_i and lambda have the
            same meaning as above, with gamma being a certain smoothness
            parameter associated with the loss functions in the problem
            formulation. For insatnce, if we focus on the uniform
            sampling over individual coordinates, we recover the rate of
            SDCA (with an improvement in the log factor). However, we
            now have more flexibility and can deduce importance
            sampling, introduce minibatching of various kinds, and even
            derandomize the method by choosing the sampling which always
            updates all variables. </p>
          <p> In the <a href="http://arxiv.org/abs/1412.8060">third
              paper</a> (joint with Zheng Qu) we focus on the problem of
            minimizing the sum of a smooth convex function (which is not
            strongly convex) and a separable convex regularizer (such as
            the L1 norm). We design a new method (ALPHA) which is
            remarkably general. For the deterministic sampling (i.e.,
            always picking all coordinates), for instance, ALPHA
            specializes to gradient descent and accelerated gradient
            descent, depending on how we select a certain sequence
            appearing in the method. If we focus on samplings updatinga
            single coordinate at a time, the method specializes to
            non-accelerated or accelerated coordinate descent. The
            bounds we obtain improve on the best known bounds for
            coordinate descent for this problem. For instance, in its
            accelerated variant, the complexity of ALPHA is </p>
          <p> (2/(t+1)^2) * sum_i (x^0_i - x^*_i)^2 * (v_i/p_i^2), </p>
          <p>where t is the iteration counter. </p>
          <p> In the <a href="http://arxiv.org/abs/1412.8063">fourth
              paper</a> we develop a simple calculus for computing
            constants v_i appearing in the above bounds (they are also
            needed as parameters of all the methods: NSync, Quartz and
            ALPHA). Recall that these constants depend on both the
            objective function and the sampling. In this paper we give
            closed-form expressions for these constants for a large
            class of functions and samplings. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> January 13, 2015 </h3>
          <p> Participants of the <a
              href="http://lear.inrialpes.fr/workshop/osl2015/">Optimization














              and Statistical Learning</a> workshop: </p>
          <p> <img src="imgs/LesHouches2015_small.jpg" alt="Les Houches
              2015" width="600"> </p>
          The <a href="imgs/LesHouches2015.jpg"> full size image is
            here (16 MB)</a>. The photo was taken with my super-cool <a
            href="http://www.dpreview.com/reviews/fujifilm-x100s">Fujifilm


















            x100s</a> camera. People know me as always running around
          with a DSLR - this thingy takes better shots than my old Canon
          ESO 50d and is very compact. <img src="imgs/fancy-line.png"
            align="absmiddle" height="36">
          <h3> January 12, 2015 </h3>
          <p> This week I am in Les Houches, France, attending the <a
              href="http://lear.inrialpes.fr/workshop/osl2015/program.html">Optimization
and














              Statistical Learning</a> workshop. This is a fantastic
            event in a beautiful Alpine environment. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> January 8, 2015 </h3>
          <p> I am in London, attending the <a
              href="http://www.ucl.ac.uk/bigdata-theory/">1st UCL
              workshop on the Theory of Big Data. </a> My talk is on
            Friday, I'll talk about Randomized Dual Coordinate Ascent
            with Arbitrary Sampling, based on <a
              href="http://arxiv.org/abs/1411.5873">this paper.</a>
            Other closely related work (all related to stochastic
            methods using an <em>arbitrary</em> sampling): <a
              href="http://arxiv.org/abs/1310.3438">NSync</a>, <a
              href="http://arxiv.org/abs/1412.8060">ALPHA</a> and <a
              href="http://arxiv.org/abs/1412.8063">ESO.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> December 27, 2014 </h3>
          <p><span class="important">Two new papers out:</span> </p>
          <p> <a href="papers/alpha1.pdf">Coordinate descent with
              arbitrary sampling I: algorithms and complexity,</a> joint
            with <a href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu</a>.
          </p>
          <p><em>Abstract: We study the problem of minimizing the sum of
              a smooth convex function and a convex block-separable
              regularizer and propose a new randomized coordinate
              descent method, which we call ALPHA. Our method at every
              iteration updates a random subset of coordinates,
              following an arbitrary distribution. No coordinate descent
              methods capable to handle an arbitrary sampling have been
              studied in the literature before for this problem. ALPHA
              is a remarkably flexible algorithm: in special cases, it
              reduces to deterministic and randomized methods such as
              gradient descent, coordinate descent, parallel coordinate
              descent and distributed coordinate descent -- both in
              nonaccelerated and accelerated variants. The variants with
              arbitrary (or importance) sampling are new. We provide a
              complexity analysis of ALPHA, from which we deduce as a
              direct corollary complexity bounds for its many variants,
              all matching or improving best known bounds. </em> </p>
          <p><a href="papers/alpha2.pdf">Coordinate descent with
              arbitrary sampling II: expected separable
              overapproximation,</a> joint with <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu</a>. </p>
          <p><em>Abstract: The design and complexity analysis of
              randomized coordinate descent methods, and in particular
              of variants which update a random subset (sampling) of
              coordinates in each iteration, depends on the notion of
              expected separable overapproximation (ESO). This refers to
              an inequality involving the objective function and the
              sampling, capturing in a compact way certain smoothness
              properties of the function in a random subspace spanned by
              the sampled coordinates. ESO inequalities were previously
              established for special classes of samplings only, almost
              invariably for uniform samplings. In this paper we develop
              a systematic technique for deriving these inequalities for
              a large class of functions and for arbitrary samplings. We
              demonstrate that one can recover existing ESO results
              using our general approach, which is based on the study of
              eigenvalues associated with samplings and the data
              describing the function. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> December 20, 2014 </h3>
          <p><span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1412.6293">Semi-stochastic
              coordinate descent,</a> joint with <a
              href="http://jakubkonecny.com/">Jakub Konečný</a> and <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu</a>.
            This is the full-length version of <a
              href="http://opt-ml.org/papers/opt2014_submission_22.pdf">this














              brief paper</a>, which was accepted to and presented at
            the <a href="http://opt-ml.org/papers.html">2014 NIPS
              Workshop on Optimization in Machine Learning.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> December 17, 2014 </h3>
          <p> Here are the <a href="talks/TALK-Quartz.ppsx">slides from
              my yesterday's talk</a>; I talked about the Quartz
            algorithm. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> December 15, 2014 </h3>
          <p> The <a
              href="http://www.andrew.cmu.edu/user/jfp/FoCM2014.html">continuous
optimization














              workshop at FoCM 2014</a> has been kicked off today
            through a very nice plenary lecture by <a
              href="http://pages.cs.wisc.edu/%7Eswright/">Steve Wright</a>
            on asynchronous stochastic optimization. The quality lineup
            of speakers and topics promises a very fine event; the fun
            begins. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> December 12, 2014 </h3>
          <p> I have accepted an invite to become an Associate Editor of
            Optimization in a new <a
              href="http://en.wikipedia.org/wiki/Frontiers_Media">Frontiers</a>
            journal (Frontiers in Applied Mathematics and Statistics; to
            be launched in 2014). I am now building a team of Review
            Editors. Frontiers is a 21st century open access publisher
            with an interactive online platform which goes a long way
            beyond simple publishing. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> December 10, 2014 </h3>
          <p> Tomorrow I am travelling to Montevideo, Uruguay, to
            participate at <a
              href="https://www.fing.edu.uy/eventos/focm2014/workshops.html">
              FoCM 2014.</a> In particular, I am giving a talk in the <a
              href="http://www.andrew.cmu.edu/user/jfp/FoCM2014.html">Continuous
Optimization














              workshop</a> on the <a
              href="http://arxiv.org/abs/1411.5873">Quartz algorithm
              (randomized dual coordinate ascent with arbitrary
              sampling). </a> This is joint work with <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu
              (Edinburgh)</a> and <a
              href="http://stat.rutgers.edu/home/tzhang/">Tong Zhang
              (Rutgers/Baidu).</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> December 9, 2014 </h3>
          <p> This week, <a href="http://jakubkonecny.com/">Jakub
              Konečný </a> and <a href="http://mtakac.com/">Martin
              Takáč</a> are presenting our joint work (also with Jie Liu
            and Zheng Qu) on the <a
              href="http://arxiv.org/abs/1410.4744">mS2GD algorithm
              (minibatch semistochastic gradient descent)</a> [<a
              href="posters/Poster-mS2GD.pdf">poster</a>] and the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/S2CD.pdf">S2CD
algorithm














              (semi-stochastic coordinate descent)</a> [<a
              href="posters/Poster-S2CD.pdf">poster</a>] at the <a
              href="http://www.opt-ml.org/papers.html">NIPS Optimization
              Workshop.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> December 5, 2014 </h3>
          <p> Today I gave a <a
              href="talks/Talk-Hydra_and_Hydra_2-NAIS2014.ppsx">talk</a>
            on <a href="http://arxiv.org/abs/1310.2059">Hydra</a> and <a
              href="http://arxiv.org/abs/1405.5300v2">Hydra^2</a>
            (simple and accelerated distributed coordinate descent) in a
            workshop (which I coorganized with James Madisson) on <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/NAIS2014/">Numerical
Algorithms














              and Intelligent Software.</a> The workshop was funded by <a
              href="http://www.nais.org.uk/">NAIS</a>, which helped to
            fund my research in the past 4 years, for which I am very
            thankful. The workshop was a celebration of the achievements
            of the <a href="http://www.nais.org.uk/">NAIS centre </a>
            as the grant supporting the centre expires at the end of the
            year. However, the activities of the centre continue in a
            number of follow-up projects. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> December 2, 2014 </h3>
          <p> Tomorrow I am giving a talk in <a
              href="http://www.maths.ed.ac.uk/research/psa">a local
              probability seminar</a>, on randomized coordinate descent.
            This is the second in a series of talks on stochastic
            methods in optimization; last week I talked about
            semi-stochastic gradient descent. Incidentaly, <a
              href="http://jakubkonecny.com/">Jakub Konecny</a> will be
            speaking about semi-stochastic gradient descent at <a
              href="docs/konecny-lehigh.pdf">Lehigh tomorrow.</a> He is
            there on a research visit (visiting <a
              href="http://mtakac.com/">Martin Takac</a> and his team),
            after which he will go to present some stuff at the NIPS
            Optimization workshop, after which both Jakub and Martin
            will join me at <a
              href="http://www.andrew.cmu.edu/user/jfp/FoCM2014.html">FoCM














              in Montevideo, Uruguay.</a> </p>
          <p> <a href="http://homepages.inf.ed.ac.uk/csutton/">Charles
              Sutton</a> was <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">speaking
today














              in our big data optimization meeting </a> about some of
            his work in machine learning that intersects with
            optimization. It was double pleasure for us as we had sushi
            for lunch today. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> November 27, 2014 </h3>
          <p> <a
              href="http://www.damtp.cam.ac.uk/people/t.j.m.valkonen/">Tuomo
Valkonen














              (Cambridge)</a> is visiting me today and tomorrow. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> November 26, 2014 </h3>
          <p> Today I gave a talk on semi-stochastic gradient descent in
            the <a href="http://www.maths.ed.ac.uk/research/psa">probability
group














              seminar</a> in our school. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> November 25, 2014 </h3>
          <p> In the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














              Hands Meeting on Big Data Optimization</a> today we have
            Dominik Csiba talking about Iterative Hessian Sketching. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> November 21, 2014</h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/pdf/1411.5873v1.pdf"> Randomized
              Dual Coordinate Ascent with Arbitrary Sampling,</a> joint
            with Zheng Qu (Edinburgh) and Tong Zhang (Rutgers and Baidu
            Inc). </p>
          <p> <em> Abstract: We study the problem of minimizing the
              average of a large number of smooth convex functions
              penalized with a strongly convex regularizer. We propose
              and analyze a novel primal-dual method (Quartz) which at
              every iteration samples and updates a random subset of the
              dual variables, chosen according to an {\em arbitrary
              distribution}. In contrast to typical analysis, we
              directly bound the decrease of the primal-dual error (in
              expectation), without the need to first analyze the dual
              error. Depending on the choice of the sampling, we obtain
              efficient serial, parallel and distributed variants of the
              method. In the serial case, our bounds match the best
              known bounds for SDCA (both with uniform and importance
              sampling). With standard mini-batching, our bounds predict
              initial data-independent speedup as well {as \em
              additional data-driven speedup} which depends on spectral
              and sparsity properties of the data. We calculate
              theoretical speedup factors and find that they are
              excellent predictors of actual speedup in practice.
              Moreover, we illustrate that it is possible to design an
              efficient {\em mini-batch importance} sampling. The
              distributed variant of Quartz is the first distributed
              SDCA-like method with an analysis for non-separable data.
            </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> November 20, 2014</h3>
          <p> As of today, and until the end of the week, I am in Jasna,
            Slovakia, at the <a
              href="http://www.konferenciajasna.sk/category/1/"> 46th
              Conference of Slovak Mathematicians.</a> I am giving a
            plenary talk on Saturday. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> November 13, 2014</h3>
          <p> A revised version of our "simplified direct search" paper
            with <a href="http://jakubkonecny.com/">Jakub Konecny</a>
            is available <a href="papers/sds.pdf"> locally here</a> and
            on <a href="http://arxiv.org/abs/1410.0390v2">arXiv.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> November 12, 2014</h3>
          <p> Together with James Madisson, I am organizing a one day
            workshop on <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/NAIS2014/">Numerical
Algorithms














              and Intelligent Software.</a> It will take place in
            Edinburgh on December 5, 2014. The event is funded by <a
              href="http://www.nais.org.uk/">NAIS</a> (whose website
            seems to be hacked - so you migt not be able to get through
            the last link). </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> November 11, 2014</h3>
          <p> Today we have the <a
              ref="http://www.maths.ed.ac.uk/~prichtar/i_seminar.html">All














              Hands Meeting on Big Data Optimization.</a> <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zeng Qu</a>
            talked about 3 papers describing randomized coordinate
            descent methods for convex optimization problems subject to
            one or more linear constraints. The paper "A random
            coordinate descent method on large optimization problems
            with linear constraints" of Necoara, Nesterov and Glineur
            can handle a single linear constraint - by updating two
            coordinates at a time. Current best results <a
              href="http://www.optimization-online.org/DB_FILE/2012/11/3669.pdf">(of
Necoara














              and Patrascu)</a> for more constraints lead to an
            exponential dependence on the number of constraints, and
            hence are very pessimistic. The focus of the meeting was the
            paper <a href="http://arxiv.org/pdf/1409.2617v3.pdf">"Large-scale














              randomized-coordinate descent methods with non-separable
              linear constraints"</a> which claimed to have obtained an
            efficient method of handling many constraints. Based on the
            disussion we had (throug observations of Zheng Qu wo read
            the paper in soem detail), we were not convinced the
            analysis is correct. It seems some steps in the analysis are
            problematic. So, it seems, the problem of designing a
            coordinate descent method which can efficiently handle
            multiple linear constraints remains open. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> November 7, 2014</h3>
          <p> <span class="important"> NEW: 3 Postdoctoral Fellowships
              Available: Apply by December 8, 2014! </span> All three
            fellowships are for 2 years, starting date: by Sept 1, 2015.
            Optimization qualifies in all three cases; especially in the
            case of the two Whittaker Fellowships. </p>
          <ul>
            <li>(1) <a
                href="http://www.maths.ed.ac.uk/jobs/seggiebrown2014">William
Gordon














                Seggie Brown Research Fellowship.</a> The successful
              fellow will pursue world-class research as well as
              contributing to the teaching activity of the School. </li>
            <li>(2) <a href="http://www.maths.ed.ac.uk/jobs/wrfdata">Whittaker
Research














                Fellow in Mathematics for Data Science.</a> Candidates
              should have a track record of research developing new
              mathematical or statistical methods for the analysis and
              processing of large-scale datasets. </li>
            <li>(3) <a
                href="http://www.maths.ed.ac.uk/jobs/wrfindustry">Whittaker
Research














                Fellow in Mathematics for Industry or Business.</a>
              Candidates should have a track record of reseach in
              mathematics with demonstrable impact on industry or
              business (understood in their broadest sense). </li>
          </ul>
          Feel free to contact me if you are interested to apply. <img
            src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> November 4, 2014</h3>
          <p> This week, <a href="http://jakubkonecny.com/">Jakub
              Konecny</a> is on a research visit to the <a
              href="http://www.da.inf.ethz.ch/">Data Analytics Lab </a>
            led by Thomas Hofmann at ETH Zurich. Yesterday, Jakub gave a
            talk on S2GD there (and I am told, almost got lost in the
            Swiss Alps, or hurt his back, or neither of these, or some
            other such thing). I also gave a <a
              href="talks/TALK-2014-11-S2GD.ppsx">talk on S2GD </a>
            (and mentioned <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/mS2GD.pdf">mS2GD</a>
            and <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/S2CD.pdf">S2CD</a>
            as well), today, in the <a
href="http://www.anc.ed.ac.uk/events/anc-dtc-seminar-peter-richtarik-school-of-maths">machine
learning














              seminar</a> at the School of Informatics. We then had the
            <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














              Hands Meeting on Big Data Optimization </a>where Ademir
            Ribeiro described a <a
              href="http://arxiv.org/abs/1410.0390">recent paper of mine
              with Jakub on direct search</a> and outlined possible
            avenues for an extension to an adaptive setting. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> October 27, 2014</h3>
          <p> I am about to give a guest lecture in the new <a
              href="http://www.inf.ed.ac.uk/teaching/courses/irds/">Introduction














              to Research in Data Science </a> course - aimed at the
            PhD students in the first cohort of our new <a
              href="http://datascience.inf.ed.ac.uk/">Centre for
              Doctoral Training (CDT) in Data Science.</a> I will speak
            about Semi-Stochastic Gradient Descent (joint work with <a
              href="http://jakubkonecny.com/">Jakub Konecny</a>: <a
              href="http://arxiv.org/abs/1312.1666">paper</a>, <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/posters/Poster-S2GD.pdf">poster</a>).














          </p>
          <p>Recent extensions: <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/S2CD.pdf">S2CD</a>
            (one can get away with computing (random) partial
            derivatives instead of gradients) and <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/S2CD.pdf">mS2GD</a>
            (the method accelerates if mini-batches are used; i.e., if
            we compute gradients of multiple random loss functions
            instead of just a single one). </p>
          <p>The lecture will be recorded I believe and the <a
              href="http://www.inf.ed.ac.uk/teaching/courses/irds/lectures.html">slides
and














              the video will appear here</a> at some point.</p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> October 24, 2014</h3>
          <p> A very interesting read: <a
href="http://www.math.utk.edu/%7Evasili/refs/Papers/NAS13.math_in_2025.pdf">The
Mathematical














              Sciences in 2025</a>. I think this is a must-read for all
            scientists.</p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> October 22, 2014</h3>
          <p> Today we had <a href="http://www.columbia.edu/%7Ejw2966/">John
Wright














              (Columbia)</a> give a talk in the ``Mathematics and Big
            Data'' distinghished lecture series which I organize. John
            has talked about a very intriguinging phenomenon occuring in
            the modelling of high-dimensional data as the product of an
            unknown (square) dictionary matrix and a (random) sparse
            matrix: the inverse problem of finding such factors leads to
            a nonconvex problem with many local optima which can
            efficiently be solved to global optimality. The trick is in
            the observation that, surprisingly, all local minima of the
            function turn out to be global minima. Moreover, the
            function has a strcuture which allows a trust region method
            on the sphere to find a local minimum. </p>
          <p> Title: Provably Effective Representations for
            High-Dimensional Data </p>
          <p> Abstract: Finding concise, accurate representations for
            sample data is a central problem in modern data analysis. In
            this talk, we discuss several intriguing “high-dimensional”
            phenomena which arise when we try to build effective
            representations for application data. The first qualitative
            surprise involves nonconvexity. We prove that a certain
            family of nonconvex optimization problems arising in data
            analysis can actually be solved globally via efficient
            numerical algorithms, provided the data are sufficiently
            large and random. Using based on this observation, we
            describe algorithms which provably learn “dictionaries” for
            concisely representing n-dimensional signals, even when the
            representation requires O(n) non zeros for each input
            signal; the previous best results ([Spielman et. al. ’12]
            via LP relaxation) only allowed \tilde{O}(\sqrt{n}) nonzeros
            per input. The second qualitative surprise involves
            robustness. Application data are often dirty: corrupted,
            incomplete, noisy. Recovering low-dimensional models from
            corrupted data is hopelessly intractable in the worst case.
            In contrast to this worst-case picture, we show that natural
            convex programming relaxations recover low-dimensional
            objects such as sparse vectors and low-rank matrices from
            substantial fractions of “typical” errors. We illustrate the
            talk with application examples drawn from computer vision,
            audio processing, and scientific imaging. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> October 21, 2014 </h3>
          <p> I am back in Edinburgh. Today we have another <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














              Hands Meeting on Big Data Optimization</a>, led by Dominik
            Csiba. He will be speaking about a recent <a
              href="http://arxiv.org/abs/1409.2848">paper of Ohad Shamir
              on Stochastic PCA method.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> October 17, 2014 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="papers/mS2GD.pdf">mS2GD: Mini-batch semi-stochastic
              gradient descent in the proximal setting,</a> joint with <a
              href="http://jakubkonecny.com/">Jakub Konecny</a>
            (Edinburgh), <a href="http://jieliucn.weebly.com/">Jie Liu</a>
            (Lehigh) and <a href="http://www.mtakac.com/CV">Martin
              Takac</a> (Lehigh). </p>
          <p> <em>Abstract: We propose a mini-batching scheme for
              improving the theoretical complexity and practical
              performance of semi-stochastic gradient descent applied to
              the problem of minimizing a strongly convex composite
              function represented as the sum of an average of a large
              number of smooth convex functions, and simple nonsmooth
              convex function. Our method first performs a deterministic
              step (computation of the gradient of the objective
              function at the starting point), followed by a large
              number of stochastic steps. The process is repeated a few
              times with the last iterate becoming the new starting
              point. The novelty of our method is in introduction of
              mini-batching into the computation of stochastic steps. In
              each step, instead of choosing a single function, we
              sample b functions, compute their gradients, and compute
              the direction based on this. We analyze the complexity of
              the method and show that it benefits from two speedup
              effects. First, we prove that as long as b is below a
              certain threshold, we can reach predefined accuracy with
              less overall work than without mini-batching. Second, our
              mini-batching scheme admits a simple parallel
              implementation, and hence is suitable for further
              acceleration by parallelization. In the b=1 case we
              recover the complexity achieved by the Prox-SVRG method of
              Liao and Zhang. In the smooth case, our method is
              identical to the S2GD method of Konecny and Richtarik. </em>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> October 16, 2014 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="papers/S2CD.pdf">S2CD: Semi-stochastic coordinate
              descent,</a> joint with <a
              href="http://jakubkonecny.com/">Jakub Konecny</a> and <a
              href="http://www.cmapx.polytechnique.fr/%7Equ/">Zheng Qu</a>.
          </p>
          <p> <em>Abstract: We propose a novel reduced variance
              method---semi-stochastic coordinate descent (S2CD)---for
              the problem of minimizing a strongly convex function
              represented as the average of a large number of smooth
              convex functions: f(x) = (1/n)*sum_{i=1}^n f_i(x). Our
              method first performs a deterministic step (computation of
              the gradient of f at the starting point), followed by a
              large number of stochastic steps. The process is repeated
              a few times, with the last stochastic iterate becoming the
              new starting point where the deterministic step is taken.
              The novelty of our method is in how the stochastic steps
              are performed. In each such step, we pick a random
              function f_i and a random coordinate j---both using
              nonuniform distributions---and update a single coordinate
              of the decision vector only, based on the computation of
              the jth partial derivative of f_i at two different points.
              Each random step of the method constitutes an unbiased
              estimate of the gradient of f and moreover, the squared
              norm of the steps goes to zero in expectation, meaning
              that the method enjoys a reduced variance property. The
              complexity of the method is the sum of two terms: O(n
              log(1/ε)) evaluations of gradients ∇f_i and O(κ log(1/ε))
              evaluations of partial derivatives ∇j f_i, where κ is a
              novel condition number. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> October 9, 2014 </h3>
          <p> I have arrived to Hong Kong yesterday (and I think I just
            managed to get de-jetlagged). I am visiting the group of <a
              href="http://www1.se.cuhk.edu.hk/%7Esqma/">Shiqian Ma</a>
            at the Chinese University of Hong Kong and will be around
            for a couple weeks (you can find me in office #708 in the
            William M.W. Mong Building). The weather here is great, the
            campus is built on a mountain and looks and feels really
            nice. The view from the top of the university hill is
            allegedly the second best in Hong Kong. I have been there,
            the view is indeed great, although I can't confirm the local
            rank as I have not seen anything else. I am giving a <a
href="http://seminar.se.cuhk.edu.hk/content/randomized-coordinate-descent-methods-unified-theory">talk














              tomorrow</a>. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> October 3, 2014 </h3>
          <p> <a href="http://people.ufpr.br/%7Eademir.ribeiro/">Ademir
              Ribeiro</a> has joined the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_team.html">team</a>
            as a postdoc - he will stay for 6 months. </p>
          <p><em> Short bio: </em> Ademir is an Associate Professor at
            the <a href="http://www.ufpr.br/portalufpr/">Federal
              University of Parana</a>, Brazil. Among other things, he
            has worked on global and local convergence of filter and
            trust region methods for nonlinear programming and convex
            optimization. He has recently published a book entitled
            "Continuous Optimization: Theoretical and Computational
            Aspects" (in Portuguese). </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> October 3, 2014 </h3>
          <p> Today I am participating (by giving a brief talk) in an
            industrial sandpit put together by the newly established <a
              href="http://www.maxwell.ac.uk/MIGSAA.php">Maxwell
              Institute Graduate School in Analysis and its
              Applications,</a> of which I am a faculty member. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> October 1, 2014 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1410.0390">Simple complexity
              analysis of direct search,</a> joint with Jakub Konecny. </p>
          <p> <em>Abstract: We consider the problem of unconstrained
              minimization of a smooth function in the derivative-free
              setting. In particular, we study the direct search method
              (of directional type). Despite relevant research activity
              spanning several decades, until recently no complexity
              guarantees — bounds on the number of function evaluations
              needed to find a satisfying point — for methods of this
              type were established. Moreover, existing complexity
              results require long proofs and the resulting bounds have
              a complicated form. In this paper we give a very brief and
              insightful analysis of direct search for nonconvex, convex
              and strongly convex objective function, based on the
              observation that what is in the literature called an
              “unsuccessful step”, is in fact a step that can drive the
              analysis. We match the existing results in their
              dependence on the problem dimension (n) and error
              tolerance (ε), but the overall complexity bounds are much
              simpler, easier to interpret, and have better dependence
              on other problem parameters. In particular, we show that
              the number of function evaluations needed to find an
              ε-solution is O(n^2/ε) (resp. O(n^2 log(1/ε))) for the
              problem of minimizing a convex (resp. strongly convex)
              smooth function. In the nonconvex smooth case, the bound
              is O(n^2/ε^2), with the goal being the reduction of the
              norm of the gradient below ε. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> September 30, 2014</h3>
          <p> We have our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">third














              All Hands Meeting on Big Data Optimization</a> today. <a
              href="http://jakubkonecny.com/">Jakub Konecny</a> will
            tell us about what machine learning is about (i.e., quick
            and dirty intrduction to ML for optimizers) - i.e., that
            besides optimization error, there are other things a ML
            person needs to worry about, such as approximation error,
            estimation error, sample complexity and so on. Everybody is
            invited; lunch will be provided. </p>
          <p> In the afternoon, I am giving a <a
href="http://wcms.inf.ed.ac.uk/lfcs/events/lfcs-seminars-1/lfcs-seminar-by-dr.-peter-richtarik">talk
in














              the LFCS (Lab for Foundations of Computer Science)
              Seminar.</a> If you happen to be around the Informatics
            Forum in the afternoon, <a
href="http://www.ph.ed.ac.uk/news/events/2014/how-humans-and-machines-integrate-language-and-vision-inaugural-lecture-3649">this














              talk</a> looks interesting.</p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> September 16, 2014</h3>
          <p> Here are the <a href="docs/Povh_Day1_slides.pdf">slides
              from Day 1</a> of Janez Povh's course. Plenty of covered
            material is not on the slides - Janez used the whiteboard a
            lot. Tomorrow we are starting at 9:15am instead of 9:00am.
            Update (17.9.2014): <a href="docs/Povh_Day2_slides.pdf">slides













              from Day 2.</a> </p>
          <p> Today, we have had our first seminar in the "All Hands
            Meetings on Big Data Optimization" series this semester. <a
              href="http://www.maths.ed.ac.uk/%7Ekfount/">Kimon
              Fountoulakis</a> talked about <a
              href="http://arxiv.org/abs/1407.7573">Robust Block
              Coordinate Descent</a> (joint work with <a
              href="http://www.maths.ed.ac.uk/%7Ertappend/">Rachael
              Tappenden</a>) - work that arose from the discussions
            initiated at the seminar last semester. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> September 15, 2014</h3>
          <p> A few unrelated bits of news from today: It's the first
            day of the semester and I met with my 10 MSc tutees. My PhD
            student <a href="http://jakubkonecny.com/">Jakub Konecny</a>
            had his qualifying exam; he gave an impressive talk and good
            answers in the Q&amp;A session. The committee passed him and
            even uttered a few words of praise. My postdoc <a
              href="http://www.cmapx.polytechnique.fr/%7Equ/">Zheng Qu</a>
            started teaching a class and I am her TA (= tutor). Janez
            Povh (Slovenia) is visiting this week and his short course
            (6 hours) on <a href="docs/JANEZ%20POVH-sdp-co-rag.pdf">Semidefine














              Programming, Combinatorial Optimization and Real Algebraic
              Geometry </a>starts tomorrow at 9am, as earlier announced
            on this site. Also, it was unusually misty today in
            Edinburgh! I had to decline an invite for a funded visit to
            Berkeley due to a conflict with <a
              href="https://www.fing.edu.uy/eventos/focm2014/">FoCM in
              Uruguay</a>. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> September 12, 2014</h3>
          <p> Today I attended the Edinburgh Data Science Research Day:
            the official launch of our new Centre for Doctoral Training
            in Data Science. Many of our <a
              href="http://datascience.inf.ed.ac.uk/partners/">industrial














              partners</a> were present. I have thoroughly enjoyed my
            conversations with Sean Murphy (Amazon), Gary Kazantsev
            (Bloomberg), Heiga Zen (Google), Julien Cornebise (Google
            Deepmind), Leighton Pritchard (James Hutton Institute),
            Andrew Lehane (Keysight Technologies / Agilent), Igor Muttik
            (McAfee), Mike Lincoln (Quorate) and Phil Scordis (UCB
            Celltech). </p>
          <p><a href="http://www.cmapx.polytechnique.fr/%7Equ/">Zheng Qu</a>
            and I have presented a total of 4 posters at the event
            (which attracted quite a bit of attention): <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/posters/Poster-Hydra2.pdf">Hydra2</a>,
            <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/posters/Poster-S2GD.pdf">S2GD</a>,
            <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/posters/Poster-TOPSPIN.pdf">TopSpin</a>
            and <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/posters/Tappenden.pdf">ICD</a>.
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> September 7, 2014</h3>
          <p> This week I am at the <a
              href="http://www.iam.fmph.uniba.sk/mmei2014/"> 18th
              International Conference in Mathematical Methods and
              Economy and Industry </a>, in the beautiful <a
              href="http://en.wikipedia.org/wiki/Smolenice_Castle">Smolenice
Castle














            </a> (now a congress centre of the Slovak Academy of
            Sciences) in Slovakia. The conference history dates back to
            1973. I am giving a plenary talk on Wednesday. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> September 3, 2014</h3>
          <p> As of today and until the end of the week, I am at the <a
href="http://www.ima.org.uk/conferences/conferences_calendar/4th_ima_conference_on_numerical_linear_algebra_and_optimisation.cfm.html">IMA
Conference














              on Numerical Linear Algebra and Optimisation </a> in
            Birmingham. I am co-organizing two minisymposia: </p>
          <ul>
            <li>Thursday, Sept 4, 14:50-17:55, <span class="important">Optimization
and














                decomposition for image processig and related topics</span>
              (organized with C.B. Shoenlieb and T. Valkonen) </li>
            <br>
            <li> Friday, Sept 5, 9:50-14:50, <span class="important">First














                order methods and big data optimization</span>
              (organized with Z. Qu and J. Konecny) </li>
          </ul>
          I am giving a talk in the Friday minisymposium. <img
            src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> September 1, 2014 </h3>
          <p> Janez Povh (Slovenia) will deliver a short course on <span
              class="important">"Semidefinite Programming, Combinatorial
              Optimization and Real Algebraic Geometry" </span> in
            Edinburgh during September 16-17, 2014. Attendance is FREE
            -- but <a href="https://www.surveymonkey.com/s/F8GXYXG">please














              register here.</a> The course is aimed at PhD students,
            postdocs and senior researchers interested in the topic.</p>
          <p> Venue: 4325B James Clerk Maxwell Building, Kings
            Buildings, Edinburgh. The course will be delivered in two
            parts: Part I (Sept 16; 9:00-12:00) and Part II (Sept 17;
            9:00-12:00). </p>
          <em>
            <p>Abstract: In the last decade, semidefinite programming
              (loosely speaking: optimization problems with variables
              being symmetric positive semidefinite matrices) has proved
              to be a very successful and powerful tool for
              approximately solving hard problems arising in
              combinatorial optimization (e.g., MAX-CUT, Quadratic
              assignment problem, Graph colouring problem) and for
              approximately computing the optimum of a real polynomial
              over a semialgebraic set. In both cases, the objective
              function and the feasible set is simplified so that the
              new problem is an instance of the semidefinite programming
              problem. The solution of the relaxation provides lower or
              upper bound for the original problem and often also a
              starting point for ! obtaining good feasible solutions.
              This short course will cover basic definitions and
              fundamental results in the theory of semidefinite
              programming, and will demonstrate how these can be used to
              approach several well-known problems arising in
              combinatorial optimization and real algebraic geometry. </p>
          </em>
          <p> The event poster can be <a
              href="docs/JANEZ%20POVH-sdp-co-rag.pdf">downloaded here.</a>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> August 22, 2014 </h3>
          <p> This year, we are launching a new PhD programme in <a
              href="http://en.wikipedia.org/wiki/Data_science">Data
              Science. </a> Data Science is an emerging new
            interdisciplinary field, and we are quite excited to be able
            to offer this. However, the novelty also means that it makes
            sense to read about this all a bit. As a start, I recommend
            looking <a
              href="http://cilvr.cs.nyu.edu/doku.php?id=publications:cds-manifesto">here</a>
            and <a href="http://en.wikipedia.org/wiki/Data_science">here.</a>
          </p>
          <p> I felt, however, that perhaps I had a good enough excuse
            to actually pick up some book on the topic; this one caught
            my attention: <a
href="http://www.amazon.com/Doing-Data-Science-Straight-Frontline/dp/1449358659">"Doing
Data














              Science: Straight Talk from the Frontline" </a> by Rachel
            Shutt and Cathy O'Neill. At the end of the chapter on
            algorithms I've seen a spotlight column titled "Modeling and
            Algorithms at Scale". I was naturally interested. Much to my
            surprise, the text included a quote from Peter Richtarik
            from Edinburgh university... I was naturally intrigued about
            this: first, because I did not immediately recognize the
            text and, most importantly, because I did not fully agree
            with it. Funny, I know.</p>
          <p> Here is the relevant excerpt from the book: </p>
          <em>Optimization with Big Data calls for new approaches and
            theory -- this is the frontier! From a 2013 talk by Peter
            Richtarik from the University of Edinburgh: "In the big data
            domain classical approaches that rely on optimization
            methods with multiple iterations are not applicable as the
            computational cost of even a single iteration is often too
            excessive; these methods were developed in the past when
            problems of huge sizes were rare to find. We thus needs new
            methods which would be simple, gentle with data handling and
            memory requirements, and scalable. Our ability to solve
            truly huge scale problems goes hand in hand with our ability
            to utilize modern parallel computing architectures such as
            multicore processors, graphical processing units, and
            computer clusters." </em>
          <p> I was thinking: this guy apparently has some bold vision
            of some futuristic optimization algorithms which can do with
            a single iteration only! Awesome! In reality, I was
            convinced I could not have said that, as I do not know of
            any new approaches that would transcend <em>iterative</em>
            algorithmic thinking. It did not take me long to figure out
            what I actually said (turns out, at the <a
href="http://wwwf.imperial.ac.uk/%7Egmontana/bigdatamining/abstracts.html">Big
Data














              Mining workshop at Imperial College, London</a>): </p>
          <em> "Optimization with big data calls for new approaches and
            theory helping us understand what we can and cannot expect.
            In the big data domain classical approaches are not
            applicable as the computational cost of even a single
            iteration is often too excessive; these methods were
            developed in the past when problems of huge sizes were rare
            to find. We thus need new methods which would be simple,
            gentle with data handling and memory requirements, and
            scalable. Our ability to solve truly huge scale problems
            goes hand in hand with our ability to utilize modern
            parallel computing architectures such as multicore
            processors, graphical processing units, and computer
            clusters. In this talk I will describe a new approach to big
            data (convex) optimization which uses what may seem to be an
            'excessive' amount of randomization and utilizes what may
            look as a 'crazy' parallelization scheme. I will explain why
            this approach is in fact efficient and effective and well
            suited for big data optimization tasks arising in many
            fields, including machine and statistical learning, social
            media and engineering.Time permitting, I may comment on
            other optimization methods suited for big data application
            which also utilize randomization and parallelization." </em>
          <p> </p>
          <p> This is just an amusing story -- I am not really unhappy
            about the confusion caused as the statements are pretty
            vague anyway (as is often the case with abstracts for
            longish talks). I think the book is a valuable read for any
            student interested in data science. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> August 19, 2014 </h3>
          <p> <a href="http://ttic.uchicago.edu/%7Enati/">Nati Srebro
              (TTI Chicago)</a> is visiting -- he will stay until the
            end of August. Tomorrow he is giving a <a
              href="http://www.maths.ed.ac.uk/ERGO/abstracts/2014-08-srebro.html">talk
on














              Distributed Stochastic Optimization.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> August 15, 2014 </h3>
          <p> We have a new Head of School as of August 1st - a <a
              href="http://en.wikipedia.org/wiki/Representation_theory">
              representation theorist. </a> It is a funny fact that <a
href="http://genealogy.math.ndsu.nodak.edu/id.php?id=161672"> the
              advisor of my advisor's advisor was a representation
              theorist,</a> too. I wonder whether one of my descendants
            will become a representation theorist to complete the
            circle... </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="36">
          <h3> August 14, 2014 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1408.2467">Inequality-Constrained














              Matrix Completion: Adding the Obvious Helps!</a>, joint
            with Martin Takac (Edinburgh) and Jakub Marecek (IBM
            Research). It was written (and announced on my website) in
            January, but we only got around posting it to arXiv now.</p>
          <p>Abstract: <em>We propose imposing box constraints on the
              individual elements of the unknown matrix in the matrix
              completion problem and present a number of natural
              applications, ranging from collaborative filtering under
              interval uncertainty to computer vision. oreover, we
              design an alternating direction parallel coordinate
              descent method (MACO) for a smooth unconstrained
              optimization reformulation of the problem. In large scale
              numerical experiments in collaborative filtering under
              uncertainty, our method obtains solution with considerably
              smaller errors compared to classical matrix completion
              with equalities. We show that, surprisingly, seemingly
              obvious and trivial inequality constraints, when added to
              the formulation, can have a large impact. This is
              demonstrated on a number of machine learning problems.</em></p>
          <span class="rss-newpost"></span> <img
            src="imgs/fancy-line.png" align="absmiddle" height="36"> <span
            class="rss-content">
            <h3> July 29, 2014 </h3>
            <p> A revised version of the paper "Fast distributed
              coordinate descent for minimizing non-strongly convex
              losses" is now on <a
                href="http://arxiv.org/pdf/1405.5300v2.pdf">ArXiv.</a>
              The paper was accepted to <a
                href="http://mlsp2014.conwiz.dk/"> IEEE Machine Learning
                for Signal Processing (MLSP 2014).</a> </p>
            <span class="rss-newpost"></span> <img
              src="imgs/fancy-line.png" align="absmiddle" height="36"> <span
              class="rss-content">
              <h3> July 2, 2014 </h3>
              <p>As of this week, my postdoc <a
                  href="http://www.cmap.polytechnique.fr/%7Equ/">Zheng
                  Qu</a> is on a research visit at the Big Data Lab at <a
                  href="http://www.baidu.com/">Baidu</a> in Beijing
                (Baidu is China's Google). This visit is part of a joint
                research project with <a
                  href="http://stat.rutgers.edu/home/tzhang/">Tong Zhang</a>,
                who directs the Big Data Lab. The Lab conducts research
                in problems related to big data analysis, including
                large scale big data optimization. Tong Zhang
                concurrently holds a <a
                  href="http://stat.rutgers.edu/home/tzhang/">chair in
                  Statistics at Rutgers University</a>. Zheng will be
                back in the UK by the time the <a
href="http://www.ima.org.uk/conferences/conferences_calendar/4th_ima_conference_on_numerical_linear_algebra_and_optimisation.cfm.html">IMA
NLA














                  &amp; Optimisation Conference in Birmingham</a>
                starts, where she is co-organizing a minisymosium on
                gradient methods for big data problems with<a
                  href="http://jakubkonecny.com/"> Jakub Konečný</a> and
                myself.</p>
              <span class="rss-newpost"></span> <img
                src="imgs/fancy-line.png" align="absmiddle" height="36">
              <span class="rss-content">
                <h3> July 1, 2014 </h3>
                <p> I am in Lancaster, giving a keynote talk at the
                  workshop <a
                    href="http://www.lancaster.ac.uk/uclid2014/index.html">Understanding
Complex














                    and Large Industrial Data.</a> </p>
                <span class="rss-newpost"></span> <img
                  src="imgs/fancy-line.png" align="absmiddle"
                  height="36"> <span class="rss-content">
                  <h3> June 26, 2014 </h3>
                  <p> This week (June 23-27) we are running a <a
                      href="http://www.maths.ed.ac.uk/hall/NATCOR/">
                      Convex Optimization PhD course in Edinburgh.</a>
                    It is attended by students from all around the UK
                    and a few from continental Europe as well. The
                    instructors are: Stephen Boyd (Stanford), Paresh
                    Date (Brunel), Olivier Fercoq (Edinburgh), Jacek
                    Gondzio (Edinburgh), Julian Hall (Edinburgh),
                    Michael Perregaard (FICO), Sergio Garcia Quiles
                    (Edinburgh), myself, Rachael Tappenden (Edinburgh).
                    I am teaching two hours on first order methods
                    tomorrow. Here are the slides (I will only cover a
                    subset of this): <a href="docs/cdm-NATCOR.ppsx">overview</a>,
                    <a href="docs/cdm-NATCOR.pdf">theory</a>.</p>
                  <span class="rss-newpost"></span> <img
                    src="imgs/fancy-line.png" align="absmiddle"
                    height="36"> <span class="rss-content">
                    <h3> June 24, 2014 </h3>
                    <p> <a
                        href="http://acse.pub.ro/person/ion-necoara/">Ion












                        Necoara (Bucharest)</a> is visiting this week.
                      He will give a <a
                        href="http://www.maths.ed.ac.uk/ERGO/abstracts/2014-06-necoara.html">talk












                        tomorrow at 1:30pm on coordinate descent
                        methods.</a></p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> June 18, 2014 </h3>
                    <p> Congratulations to <a
                        href="http://jakubkonecny.com/">Jakub Konečný</a>,
                      1st year PhD student in the <a
                        href="http://www.maths.ed.ac.uk/">School of
                        Mathematics</a>, for being awarded the <span
                        class="important">2014 Google Europe Doctoral
                        Fellowship in Optimization Algorithms!</span>
                      The news was announced today in the <a
href="http://googleresearch.blogspot.ie/2014/06/2014-google-phd-fellowships-supporting.html">Google
Research














                        Blog.</a></p>
                    <p> This is what Google says about these
                      Fellowships: <em>Nurturing and maintaining strong
                        relations with the academic community is a top
                        priority at Google. Today, we're announcing the
                        2014 Google PhD Fellowship recipients. These
                        students, recognized for their incredible
                        creativity, knowledge and skills, represent some
                        of the most outstanding graduate researchers in
                        computer science across the globe. We're excited
                        to support them, and we extend our warmest
                        congratulations. </em></p>
                    <p> This year, Google has announced 38 Fellowships
                      to PhD students across the globe: 15 in Europe, 14
                      in North America, 4 in China, 3 in India and 2 in
                      Australia. These fellowships provide generous
                      funding for the students for up to three years to
                      help them better achieve their research
                      objectives, and open the doors to a closer
                      collaboration with Google through the
                      establishment of Google mentors and other
                      activities. Out of the 15 Europe Fellowships, 4
                      were awarded to universities in the UK: 2 in
                      Cambridge and 2 in Edinburgh. The rest went to
                      students in Switzerland (4), Germany (3), Israel
                      (2), Austria (1) and Poland (1).</p>
                    <p> Jakub has started his PhD in August 2013 at the
                      <a href="http://www.ed.ac.uk/home">University of
                        Edinburgh</a>, working under my supervision. He
                      spent his first semester of PhD studies at <a
                        href="http://www.berkeley.edu/index.html">University














                        of California Berkeley</a> as a <a
                        href="http://simons.berkeley.edu/people/jakub-konecny">visiting
graduate














                        student</a> (thanks to <a
                        href="http://www.nais.org.uk/">NAIS</a> for
                      generous support of this visit), where he
                      participated in the semester-long programme on <a
href="http://simons.berkeley.edu/programs/bigdata2013">Theoretical
                        Foundations of Big Data Analysis</a> at the
                      newly established <a
                        href="http://simons.berkeley.edu/">Simons
                        Institute for the Theory of Computing</a>. Jakub
                      also managed to take a few PhD courses, put some
                      final touches on a <a
                        href="http://arxiv.org/abs/1312.4190">JMLR paper
                        on Gesture Recognition</a> (for this work he and
                      his coauthor were awarded 2nd Prize at the
                      ChaLearn gesture challenge competition and
                      presented the work at ICPR in Tsukuba, Japan),
                      write a new paper on <a
                        href="http://arxiv.org/abs/1312.1666">Semi-Stochastic














                        Gradient Descent</a> and make progress towards
                      one more paper in collaboration with a couple of
                      Simons long term visitors. Since returning to
                      Edinburgh, besides doing research and volunteering
                      for various projects around Scotland, Jakub has
                      been co-organizing weekly <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All












                        Hands Meetings on Big Data Optimization</a>.
                      Prior to coming to Edinburgh, he studied
                      mathematics at the Comenius University in
                      Slovakia. In 2010, Jakub Konecny represented his
                      country in the International Mathematical Olympiad
                      in Kazachstan, earning a Honorable Mention.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> June 17, 2014 </h3>
                    <p> We have had our last <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">"All














                        Hands'</a>' meeting on big data optimization
                      this academic year. The speaker was Mojmír Mutný -
                      and the topic was the <a
                        href="http://jmlr.org/proceedings/papers/v28/jaggi13-supp.pdf">Frank-Wolfe














                        algorithm</a>.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> June 11, 2014 </h3>
                    <p> I've arrived to Grenoble. Having been first
                      welcome by a storm (to make me feel at home, I am
                      sure) yesterday when I arrived, today it is warm
                      and sunny. The campus is located in a beautiful
                      setting surrounded by mountains. </p>
                    <p> I will be <a
                        href="http://lear.inrialpes.fr/people/harchaoui/projects/khronos/">teaching</a>
                      (Randomized Coordinate Descent for Big Data
                      Problems) 3 hours today and 3 hours tomorrow. I
                      have two sets of slides: powerpoint for nice
                      flashy arrows, pictures, animations and, most
                      importantly, aimed at delivering insight from
                      bird's eye perspective. I also have <a
                        href="docs/cdm-talk.pdf">technical slides</a>
                      with proofs (here is a <a
                        href="docs/cdm-print.pdf">version for printing</a>).












                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> June 10, 2014 </h3>
                    <p>Today I am travelling to Grenoble, where I will
                      give a 6 hour mini-course on Randomized Coordinate
                      Descent Methods for Big Data Optimization. The
                      course is part of the <a
                        href="http://lear.inrialpes.fr/people/harchaoui/projects/khronos/">Khronos-Persyval
Days














                        on "High-Dimensional Learning and Optimization".</a>
                      Meanwhile, <a href="http://jakubkonecny.com/">Jakub













                        Konecny</a> and <a
                        href="http://www.cmap.polytechnique.fr/%7Equ/">Zheng














                        Qu</a> are attending the <a
href="http://www.kcl.ac.uk/nms/depts/mathematics/events/eventsrecords/londonoptimizationworkshop.aspx">London
Optimization














                        Workshop.</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> June 9, 2014 </h3>
                    <p><a href="http://de.linkedin.com/in/carchambeau">Cedric














                        Archambeau</a>, a manager and senior research
                      scientist at Amazon Berlin is visiting and giving
                      a <a
                        href="http://www.maths.ed.ac.uk/ERGO/abstracts/2014-06-archambeau.html">talk














                        today</a> in our seminar. It turns out Amazon is
                      currently imlementing &amp; testing the <a
                        href="http://arxiv.org/abs/1310.2059">Hydra
                        algorithm</a> developed by <a
                        href="http://www.mtakac.com/CV">Martin Takac</a>
                      and myself (here is a different variant, <a
                        href="http://arxiv.org/abs/1405.5300">Hydra^2</a>).</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> June 3, 2014 </h3>
                    <p>Today at 12:15 we have <a
                        href="http://people.maths.ox.ac.uk/%7Eszpruch/">Lukas














                        Szpruch</a> speaking in the <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














                        Hands Meetings on Big Data Optimization</a>
                      (room: JCMB 4312) about his recent work on <a
                        href="http://arxiv.org/pdf/1212.1377v1.pdf">Multilevel
Monte














                        Carlo Methods for Applications in Finance</a>.
                      Connections to optimization will be outlined. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> May 31, 2014 </h3>
                    <p><span class="important">New paper out:</span> <a
href="papers/distributed-partially-separable.pdf">Distributed Block
                        Coordinate Descent for Minimizing Partially
                        Separable Functions</a>. Joint with <a
href="http://researcher.watson.ibm.com/researcher/view.php?person=ie-jakub.marecek">Jakub
Mareček














                        (IBM)</a> and <a
                        href="http://www.mtakac.com/CV">Martin Takáč
                        (Edinburgh/Lehigh)</a>. Update (June 3): now
                      also on <a href="http://arxiv.org/abs/1406.0238">arXiv</a>.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> May 27, 2014 </h3>
                    <p> <a href="http://jakubkonecny.com/">Jakub
                        Konečný</a> was speaking today in the <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














                        Hands Meeting on Big Data Optimization</a> about
                      a recent paper of Julian Mairal on <a
                        href="http://arxiv.org/abs/1402.4419">deterministic














                        and stochastic optimization methods with
                        surrogate functions.</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> May 21, 2014 </h3>
                    <p> Two days behind us, two more to go: the SIAM
                      Conference on optimization in San Diego is its
                      middle. Likewise, we have had the first day of the
                      minisymposium on coordinate descent methods on
                      Tuesday; one more to go with further three
                      sessions on Thursday. </p>
                    <p>The first session on Tuesday was started off by
                      Yurii Nesterov (Louvain) talking about <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62749">a
                        new primal-dual subgradient algorithm</a> which
                      in the dual can be interpreted as coordinate
                      descent in the space of Lagrange multipliers. The
                      ideas are intreaguing and deserve attention. I
                      have then given a <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62750">talk</a>
                      on the <a href="http://arxiv.org/abs/1312.5799">APPROX














                        algorithm</a>, which is a coordinate descent
                      method that is at the same accelerated, parallel
                      and proximal and avoids full dimensional
                      oprations. I gave a <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/talks/Tutorial-London-2014.ppsx">3h














                        tutorial</a> on this recently at Imperial
                      College - feel free to dive into the slides if
                      interested. The session was concluded by Taiji
                      Suzuki (Tokyo) with an interesting <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62993">talk</a>
                      on combining <a
                        href="http://arxiv.org/abs/1311.0622">stochastic
                        dual coordinate ascent and the ADMM method</a>.
                      Tong Zhang will give his talk on Thursday instead
                      as he is arriving to San Diego a bit later.</p>
                    <p> In the second session, Lin Xiao (Microsoft) <a
href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62754">talked</a>
                      about a way to improve some constants in the
                      complexity analysis of coordinate descent methods
                      as analyzed by <a
href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CC4QFjAA&amp;url=http%3A%2F%2Fwww.optimization-online.org%2FDB_FILE%2F2010%2F01%2F2527.pdf&amp;ei=F6R8U9_zG8vsoASzsoDIAw&amp;usg=AFQjCNFd-NHgYy5DDdGGINuvn0B4UKGp4w&amp;bvm=bv.67229260,d.cGU">Nesterov</a>
                      and <a
                        href="http://link.springer.com/article/10.1007/s10107-012-0614-z#">Takac
and














                        myself</a>. Here is the <a
                        href="http://arxiv.org/abs/1305.4723">paper</a>.
                      I was then subbing for Olivier Fercoq (Edinburgh)
                      and delivered his talk on <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62755">univeral
coordinate













                        descent</a> - in parallel, proximal and
                      accelerated variants. Yurii Nesterov gave a
                      plenary talk on universal gradient descent the day
                      before - our work was motivated by his. Cong Dong
                      (Florida) then talked about <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62756">stochastic














                        block mirror descent</a>, joint work with
                      Guanghui Lan. As usual with papers coathored by
                      Guanghui - this was an impressive tour de force
                      march through theorems covering every conceivable
                      case and setting (convex, nonconvex, stochastic -
                      whatever you want). Tom Luo (Minnesota) was not
                      able to deliver his talk, but his coauthor Mingyi
                      Hong gave the <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62757">talk</a>
                      instead. They looked at a <a
                        href="http://arxiv.org/abs/1310.6957">wide class
                        of coordinate descent methods</a> (cyclic,
                      randomized, greedy ...) and gave O(1/k)
                      guarantees. Due to the generality of the setting,
                      however, the leading constants of these types of
                      analysis are necessarily quite pessimistic and do
                      not reflect the actual behavior of the methods
                      very well - unlike the anaysis of randomized
                      cooridnate descent, they hide big
                      dimension-dependent constants. It is an important
                      open problem to see whether it is possible to
                      prove O(n/epsilon) complexity for cyclic
                      coordinate descent.</p>
                    <p> In the <a
href="http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=18493">final
coordinate














                        descent session</a> on Tuesday we had three
                      speakers: Martin Takac (Edinburgh/Lehigh), Ambuj
                      Tewari (Michigan) and Ji Liu (Wisconsin). Martin
                      talked about the analysis and implemetation of two
                      variants of distributed coordinate descent (<a
                        href="http://arxiv.org/abs/1310.2059">Hydra</a>
                      &amp; <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/Hydra2.pdf">Hydra^2</a>)
                      and showed that the methods are indeed able to
                      solve big data problems (400GB, 3TB). Ambuj then
                      gave a very entertaining talk on his work on a
                      unifyng framework for analysing a class of
                      parallel coordinate descent methods and greedy
                      coordinate descent methods which he calls <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62760">block-greedy.</a>
                      Finally, Ji Liu talked about his joint work with
                      Steve Wright, Chris Re and Victor Bittorf on the
                      analysis of <a
                        href="http://arxiv.org/abs/1311.1873">asynchronous














                        parallel coordinate descent</a>. These methods
                      seem to work well in the non-sparse setting while
                      the Hogwild! method (asynchronous SGD) requires
                      sparsity to avoid collisions. This reminded me
                      that Martin Takac and I need to post our paper -
                      all results of which were ready in Summer 2012!
                      (ok, we have done some improvements by the end of
                      the year, but that's it) - an improved analysis of
                      <a
href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=4&amp;cad=rja&amp;uact=8&amp;ved=0CEIQFjAD&amp;url=http%3A%2F%2Fwww.eecs.berkeley.edu%2F%7Ebrecht%2Fpapers%2FhogwildTR.pdf&amp;ei=ILB8U4ubMobfoASumoGQAw&amp;usg=AFQjCNHqA532YKC_P0JgCL-C_Iu8d3Wcyg&amp;bvm=bv.67229260,d.cGU">Hogwild!</a>
                      on arXiv. Many people were asking about it - as <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/slides/Wright.pdf">Steve
is












                        advertising the analysis in his talks</a> -
                      apologies to all. This is a perfect example of the
                      situation when a minor polishing exercise that
                      should take a few days tops takes 2 years.
                      Sometimes, coming up with the results is easier
                      than writing the paper ;-)</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> May 17, 2014 </h3>
                    <p> <span class="important">New paper announcement:</span>
                      <a href="papers/Hydra2.pdf">Fast distributed
                        coordinate descent for minimizing non-strongly
                        convex losses.</a> Joint work with <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier














                        Fercoq</a>, <a
                        href="http://www.cmap.polytechnique.fr/%7Equ/">Zheng














                        Qu</a> and <a href="http://www.mtakac.com/CV">Martin














                        Takáč.</a></p>
                    <p> The method has the optimal O(1/k^2) rate. We
                      develop new stepsizes for distribited coordinate
                      descent; they apply to the <a
                        href="http://arxiv.org/abs/1310.2059">Hydra</a>
                      algorithm as well. We show that the partitioning
                      of the data among the nodes of the cluster has
                      negligible effect on the number of iterations of
                      the method, with the effect vanishing with
                      increasing levels of parallelization inside each
                      node.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> May 17, 2014 </h3>
                    <p> I am now off to San Diego, California, for the <a
                        href="http://www.siam.org/meetings/op14/">SIAM
                        Conference on Optimization</a>, where I am
                      co-organizing (and giving a talk in) a <a
                        href="Coordinate_Descent_Methods_SIOPT_San_Diego_2014.pdf">'mini'-symposium
on














                        coordinate descent methods</a> with <a
                        href="http://research.microsoft.com/en-us/people/lixiao/">
                        Lin Xiao (Microsoft Research)</a> and <a
                        href="http://people.math.sfu.ca/%7Ezhaosong/">Zhaosong














                        Lu (Simon Fraser).</a> People from the team
                      giving talks: <a href="http://jakubkonecny.com/">Jakub














                        Konečný</a>, <a href="http://www.mtakac.com/CV">Martin














                        Takáč</a>, <a
                        href="http://www.maths.ed.ac.uk/%7Ertappend/">Rachael














                        Tappenden</a> and <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier














                        Fercoq</a>.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> May 15, 2014 </h3>
                    <p> <a
                        href="http://www.maths.ed.ac.uk/%7Ertappend/">Rachael














                        Tappenden</a> will be leaving Edinburgh this
                      Summer as her contract will be over then. She has
                      accepted a postdoc position at Johns Hopkins
                      University starting in Fall 2014 where she will
                      join the group of <a
                        href="https://sites.google.com/site/danielprobinson/">Prof














                        Daniel Robinson.</a> No goodbyes yet as Rachael
                      will be around for couple more months. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> May 13, 2014 </h3>
                    <p> This week I was supposed to be in <a
                        href="http://www.math.hkbu.edu.hk/SIAM-IS14/index.html">Hong














                        Kong</a> (and give a talk 'minisymposium 50':
                      Parallel and Distributed Computation in Imaging)
                      but unfortunately could not go. </p>
                    <p>Today at 12:15 we have <a
                        href="http://www.cmap.polytechnique.fr/%7Equ/">Zheng














                        Qu</a> speaking in the <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All
Hands














                        Meetings on Big Data Optimization</a> (room
                      change: JCMB 4312) about a recent paper of <a
                        href="http://perso.uclouvain.be/olivier.devolder/">Devolder</a>,
                      <a
                        href="http://perso.uclouvain.be/francois.glineur/">Glineur</a>
                      and <a href="http://www.uclouvain.be/32349.html">Nesterov</a>
                      on <a
href="http://www.premolab.ru/sites/default/files/devolder_glineur_nesterov_first_order_methods_of_smooth_convex_optimization_with_inexact_oracle.pdf">First
order














                        methods with inexact oracle</a>. As usual,
                      refreshments are provided!</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> May 6, 2014 </h3>
                    <p> I have just arrived in Rabat, Morocco. Tomorrow
                      I am giving a keynote talk at the <a
                        href="http://3w.inpt.ac.ma/sita14/">9th
                        International Conference on Intelligent Systems.</a>
                      Needless the say, the weather is fantastic.
                      Correction (after having looked from the window:
                      seems it's going to rain...). </p>
                    <p><em>Update:</em> The conference was very nice; an
                      impressive university campus. I even got an
                      impromptu interview with the press just before my
                      talk. Also, I somehow managed to get a rooftop
                      view of the city from the medina. Climate in Rabat
                      seems to be similar to that in California.
                      Morocco: I shall be back some day!</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> May 5, 2014 </h3>
                    <p> Today I am giving a talk in the <a
                        href="http://www.ann.jussieu.fr/%7Eplc/spo.html">Seminaire














                        Parisien d'Optimisation</a> at the <a
                        href="http://www.ihp.fr/">Institut Henri
                        Poincare</a> in Paris.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> May 4, 2014 </h3>
                    <p>The <a href="http://arxiv.org/abs/1212.0873">parallel














                        coordinate descent method</a> developed by
                      Martin Takáč and myself has recently been used by
                      a team from <a href="http://www.hrl.com/">HRL
                        Labs</a> to <a
                        href="http://arxiv.org/abs/1404.7152">geotag 100
                        million public twitter accounts</a>. They have
                      used an Apache Spark implementation of the method
                      - the network they analyzed had 1 billion edges. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> April 30, 2014 </h3>
                    <p>Today, <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier














                        Fercoq</a> was leading the <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All












                        Hands Meeting on Big Data Optimization</a>. The
                      meeting was then followed by a very nice talk on <a
href="http://www.maths.ed.ac.uk/ERGO/abstracts/2014-04-drineas.html">Randomized
Algorithms














                        in Numerical Linear Algebra</a> by <a
                        href="http://www.cs.rpi.edu/%7Edrinep/">Petros
                        Drineas (RPI)</a>, who is visiting me and <a
                        href="http://www.iliasdiakonikolas.org/">Ilias
                        Diakonikolas</a> in Edinburgh this week.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> April 29, 2014 </h3>
                    <p> Martin Takáč's PhD thesis "Randomized Coordinate
                      Descent Methods for Big Data Optimization" is now
                      <a
href="http://mtakac.com/data/_uploaded/file/papers/2014/takac_phd_thesis_final.pdf">available














                        here.</a></p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> April 24, 2014 </h3>
                    <p> <a href="http://www.mtakac.com/CV">Martin Takáč</a>
                      has recently accepted a tenure-track Assistant
                      Professor position in the <a
                        href="http://www.lehigh.edu/ise/">Department of
                        Industrial and Systems Engineering</a> at <a
                        href="http://www4.lehigh.edu/">Lehigh University</a>.
                      The department is the home of a top research
                      center in computational optimization: <a
                        href="http://coral.ie.lehigh.edu/">COR@L</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> April 16, 2014 </h3>
                    <p> I've managed to submit a grant proposal today
                      but failed to locate a leak on the tube on the
                      front wheel of my bike. Maybe the tire was never
                      really flat in the first place. Or maybe I should
                      focus on applying for grants and leave mending
                      punctures to someone else...</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> April 15, 2014 </h3>
                    <p> <a
                        href="http://people.kyb.tuebingen.mpg.de/suvrit/">Suvrit














                        Sra</a> has included some of my results (joint
                      with <a href="http://mtakac.com/">Martin Takáč</a>,
                      based on <a href="http://arxiv.org/abs/1107.2848">this</a>
                      and <a href="http://arxiv.org/abs/1212.0873">this</a>
                      paper) on randomized coordinate descent methods in
                      <a
                        href="http://people.kyb.tuebingen.mpg.de/suvrit/teach/ee227a/lect21.pdf">this














                        lecture</a> of a <a
href="http://people.kyb.tuebingen.mpg.de/suvrit/teach/ee227a/lectures.html">Convex
Optimization














                        course</a> he taught at UC Berkeley last year. </p>
                    <p> Besides the obvious fact that this kind of stuff
                      makes the authors happy (thanks, Suvrit!), I am
                      also of the opinion that it is time to refresh
                      syllabuses of convex optimization courses with
                      some modern results, methods and theory. A lot of
                      exciting work has been done by the community in
                      the last 10 years or so and there is plenty of
                      material to choose from to build a modern course.
                      I am launching such a course (Modern optimization
                      methods for big data problems) in Spring 2015 (it
                      takes a year or more from start to finish to get a
                      new course approved and run over here...) here in
                      Edinburgh. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> April 11, 2014 </h3>
                    <p> Here is the <a
                        href="docs/Coordinate_Descent_Methods_SIOPT_San_Diego_2014.pdf">detailed
program














                      </a> of the <em>Coordinate Descent
                        "Mini"-Symposium</em> at the <a
                        href="http://www.siam.org/meetings/op14/">SIAM
                        Conference on Optimization</a> to be held in San
                      Diego in May 2014. The symposium consists of 6
                      sessions: 3 on May 20th and 3 on May 22nd.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> April 2, 2014 </h3>
                    <p> Today I am attending a <a
                        href="http://www.icms.org.uk/workshop.php?id=306">workshop</a>
                      in the honour of John Napier's discovery of the
                      logarithm. Napier was born and spent his life in
                      Edinburgh. Many of the talks were excellent. </p>
                    <p>Olivier Fercoq presented a <a
                        href="posters/Poster-APPROX">poster related to
                        the APPROX algorithm</a> (Accelerated, Parallel
                      and PROXimal coordinate descent), Rachael
                      Tappenden presented a poster on<a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/posters/Tappenden.pdf">Inexact














                        Coordinate Descent</a>, Martin Takáč had one on
                      the Hydra algorithm (Distributed Coordinate
                      Descent) and Jakub Konečný presented his work on <a
                        href="posters/Poster-S2GD.pdf">S2GD</a>
                      (semi-stochastic gradient descent).</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> April 1, 2014 </h3>
                    <p>Today, <a href="http://www.mtakac.com/CV">Martin
                        Takáč</a> is leading a discussion at the <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All












                        Hands Meeting on Big Data Optimization</a> about
                      a very recent (2 weeks old!) <a
                        href="http://arxiv.org/abs/1403.4699">paper</a>
                      by Xiao and Zhang.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> March 31, 2014 </h3>
                    <p> Our School was successful in obtaining funding
                      for <a
                        href="http://www.epsrc.ac.uk/newsevents/news/2014/Pages/newcdts.aspx">EPSRC
Centre














                        for Doctoral Training in Mathematical Analysis
                        and its Applications: "Maxwell Institute
                        Graduate School in Analysis and Applications"</a>.
                      I am one of the potential PhD supervisors.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> March 28, 2014 </h3>
                    <p> <a href="http://www.mtakac.com/CV">Martin Takáč</a>
                      is defending his PhD thesis today. Update: The
                      defense was successful; congratulations, Dr Takáč!</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> March 25, 2014 </h3>
                    <p> I just submitted an important grant proposal
                      (length = 26p; an effort comparable to writing a
                      paper...). Wish me luck! </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> March 12, 2014 </h3>
                    <p> Debadri Mukherjee and Mojmir Mutny have each
                      been awarded a Vacation Scholarship to work with
                      me this Summer on an undergraduate research
                      project. Debadri will work on "Applications of
                      Semi-Stochastic Gradient Descent" and Mojmir will
                      work on "Denoising and filtering of sparsely
                      sampled images and other possible applications of
                      gradient descent minimizing tools".</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> March 11, 2014 </h3>
                    <p> The video from my February talk in Moscow on the
                      APPROX algorithm is now on <a
                        href="https://www.youtube.com/watch?v=0sHOfqhCZw0">Youtube</a>.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> March 11, 2014 </h3>
                    <p> <a href="http://www.maths.ed.ac.uk/%7Ekfount/">Kimonas














                        Fountoulakis</a>, as an expert on second order
                      methods, lead the discussion today in the <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All












                        Hands Meeting on Big Data Optimization</a> about
                      Coordinate Descent Newton. In the preceding two
                      weeks we had <a
                        href="http://www.see.ed.ac.uk/%7Es0574225/">Mehrdad












                        Yaghoobi</a> and <a
                        href="http://www.cmapx.polytechnique.fr/%7Equ/">Zheng












                        Qu</a> speaking about projection onto the L1
                      ball in high dimensions, and on iterative methods
                      for finding stationary states of Markov chains,
                      respectively.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> February 25, 2014 </h3>
                    <p> Today I gave a talk at the <a
                        href="http://www.ipam.ucla.edu/programs/sgm2014/">Stochastic














                        Gradient Methods</a> workshop, <a
                        href="talks/Talk-IPAM-2014.ppsx">here are the
                        slides.</a> I primarily talked about the APPROX
                      algorithm (an efficient accelerated version of
                      parallel coordinate descent; joint work with <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier












                        Fercoq</a>), with a hint at the end at a version
                      using importance sampling (joint work with <a
                        href="http://www.cmap.polytechnique.fr/%7Equ/">Zheng














                        Qu</a>).</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> February 25, 2014 </h3>
                    At the <a
                      href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All
Hands


















                      Meeting on Big Data Optimization</a> today, <a
                      href="http://www.cmapx.polytechnique.fr/%7Equ/">Zheng


















                      Qu</a> will be leading a discussion about a recent
                    paper of Nesterov and Nemirovski: <a
href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCcQFjAA&amp;url=http%3A%2F%2Fwww.ecore.be%2FDPs%2Fdp_1359373295.pdf&amp;ei=yx_xUp-EM9Ou7AaA0IH4Cg&amp;usg=AFQjCNE9bLYSlYgyTDyyzQkFu2IkzqpWdg&amp;sig2=NzQRd7P_SUVyemFCilu8VQ&amp;bvm=bv.60444564,d.ZGU&amp;cad=rja">Finding
the
















                      stationary states of Markov chains by iterative
                      methods.</a> <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> February 23, 2014 </h3>
                    <p> Today I am heading off for California again,
                      this time to Los Angeles, to give a talk at a
                      workshop on <a
                        href="http://www.ipam.ucla.edu/programs/sgm2014/">Stochastic














                        Gradient Methods</a> at the <a
                        href="http://www.ipam.ucla.edu/default.aspx">Institute
for














                        Pure and Applied Mathematics (IPAM)</a>. I can
                      already sense <a
                        href="https://www.ipam.ucla.edu/schedule.aspx?pc=sgm2014">many














                        talks</a> will be very exciting.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> February 17, 2014 </h3>
                    <p> This week I am in London, attending the <a
                        href="http://www2.imperial.ac.uk/%7Ebm508/bigdata14.html">Big














                        Data: Challenges and Applications</a> workshop
                      at Imperial College. I have just listened to an
                      interesting general talk By David Hand, and the
                      announcement by Yike Guo of the new <a
href="http://www3.imperial.ac.uk/newsandeventspggrp/imperialcollege/newssummary/news_6-12-2013-10-46-20">Data
Science














                        Institute</a> at Imperial. </p>
                    <p>In the afternoon I am giving a 3 hour <a
                        href="talks/Tutorial-London-2014.ppsx">tutorial
                        on Big Data Convex Optimization</a> (36MB file,
                      sorry!). In the tutorial I describe 8 first-order
                      algorithms: gradient descent, projected gradient
                      descent, proximal gradient descent, fast proximal
                      gradient descent (essentially a new version of <a
href="http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CC0QFjAA&amp;url=http%3A%2F%2Fmechroom.technion.ac.il%2F%7Ebecka%2Fpapers%2F71654.pdf&amp;ei=ggECU-3tI4aphAfsi4GoBA&amp;usg=AFQjCNGuAEhwN1B11vVKlUU7SyQZIdbK8A&amp;sig2=7aFZNniH2oeJMad0Hcqi4g&amp;bvm=bv.61535280,d.ZG4">FISTA</a>),














                      <a
                        href="http://link.springer.com/article/10.1007/s10107-012-0614-z#">randomized
coordinate














                        descent</a>, <a
                        href="http://arxiv.org/abs/1212.0873">parallel
                        coordinate descent (PCDM)</a>, <a
                        href="http://arxiv.org/abs/1310.2059">distributed
coordinate














                        descent (Hydra)</a> and, finally, <a
                        href="http://arxiv.org/abs/1312.5799">fast
                        parallel coordinate descent (APPROX)</a>. </p>
                    <img src="imgs/8-in-1-chart.png"
                      longdesc="imgs/8-in-1-chart.png" width="654"
                      height="471">
                    <p> As the above chart shows, all algorithms arise
                      as special cases of the last one, which we call
                      APPROX (joint work with <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier













                        Fercoq</a>). This is the first time such a
                      synthesis is possible.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> February 11, 2014 </h3>
                    <p> I am in Moscow for the next couple days, giving
                      a <a
                        href="http://premolab.ru/content/main-premolab-seminar">talk














                        tomorrow at the Main Seminar</a> of the <a
                        href="http://premolab.ru/">Laboratory for
                        Structural Methods of Data Analysis in
                        Predictive Modelling (PreMoLab)</a> at the <a
                        href="http://phystech.edu/">Moscow Institute of
                        Physics and Technology.</a></p>
                    <p> In fact, our plane was not allowed to land, and
                      after three failed attempts he pilot decided to
                      head off to Vilnius, Lithuania. Fortunately, they
                      did not leave us there: we refueled and flew back
                      to Moscow. Happy ending. </p>
                    <p>Update (February 12): Here are the <a
                        href="talks/TALK-2014-02-Moscow.ppsx">slides</a>
                      - I talked about <a
                        href="http://arxiv.org/abs/1312.5799">Accelerated,














                        Parallel and PROXimal coordinate descent</a>
                      (joint work with <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier














                        Fercoq</a>).</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> February 4, 2014 </h3>
                    <p> <a href="http://www.mtakac.com/CV">Martin Takáč</a>
                      submitted his PhD thesis yesterday and is
                      interviewing in the US during the next couple
                      weeks. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> February 4, 2014 </h3>
                    <p> <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All
Hands














                        Meetings on Big Data Optimization:</a> <a
                        href="http://www.maths.ed.ac.uk/%7Ertappend/">Rachael














                        Tappenden</a> is speaking today about feature
                      clustering for parallel coordinate descent.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> February 3, 2014 </h3>
                    <p> <a
                        href="http://www.damtp.cam.ac.uk/people/t.j.m.valkonen/">Tuomo
Valkonen














                        (Cambridge)</a> is visiting this week. He will
                      give a talk on Wednesday Feb 5 in our ERGO
                      seminar: Extension of the Chambolle-Pock method to
                      non-linear operators. Applications to MRI </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> January 28, 2014 </h3>
                    <p> <a href="http://jakubkonecny.com/">Jakub
                        Konečný</a> has been nominated by the University
                      of Edinburgh for the <a
href="http://research.google.com/university/relations/doctoral_fellowships_europe.html">2014
Google












                        Doctoral Fellowship. </a>Good luck! </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> February 11, 2014 </h3>
                    <p> </p>
                    <span class="rss-newpost"></span><span
                      class="rss-content">
                      <h3> January 21, 2014 </h3>
                      <p> I am launching a new seminar series
                        (co-organized with <a
                          href="http://jakubkonecny.com/">Jakub Konečný</a>):













                        <span class="important">All Hands Meetings on
                          Big Data Optimization.</span></p>
                      <p>The idea is to meet for up to an hour, eat a
                        pizza (or some other food, provided) and listen
                        to someone giving an informal (perhaps
                        blackboard) talk and leading a discussion about
                        a recent paper on the topic of big data
                        optimization. Discussions are encouraged
                        throughout - and hence it would be nice (but
                        certainly not required!) if participants could
                        have (at least a brief) look at the paper
                        beforehand. </p>
                      <p> <a
                          href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">More
info














                          here</a>.</p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> January 19, 2014 </h3>
                      <p> I came back to Edinburgh last week after
                        having spent a semester at Berkeley. </p>
                      <p> A quick `man-count': <a
                          href="http://www.cmapx.polytechnique.fr/%7Equ/">Zheng












                          Qu</a> has just started as a postdoc in the
                        group. For <a href="http://jakubkonecny.com/">Jakub












                          Konečný</a> this is the first semester in
                        Edinburgh, too (since he has spent last semester
                        at Berkeley with me). <a
                          href="http://mtakac.com/">Martin Takáč</a>
                        will soon be defending his thesis and is on the
                        job market. <a
                          href="http://www.maths.ed.ac.uk/%7Ertappend/">Rachael














                          Tappenden</a> and <a
                          href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier














                          Fercoq</a> are on the job market now as well.
                      </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> December 19, 2013 </h3>
                      <p><span class="important">New paper is out:</span>
                        <a href="http://arxiv.org/abs/1312.5799">Accelerated,














                          Parallel and Proximal Coordinate Descent</a>
                        (joint with <a
                          href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier












                          Fercoq</a>).</p>
                      <p> <strong>Abstract:</strong> We propose a new
                        stochastic coordinate descent method for
                        minimizing the sum of convex functions each of
                        which depends on a small number of coordinates
                        only. <strong>Our method (APPROX) is
                          simultaneously Accelerated, Parallel and
                          PROXimal</strong>; this is the first time such
                        a method is proposed. In the special case when
                        the number of processors is equal to the number
                        of coordinates, the method converges at the rate
                        <strong>$2\bar{\omega}\bar{L} R^2/(k+2)^2$</strong>,
                        where $k$ is the iteration counter, \bar{\omega}
                        is an <em>average</em> degree of separability
                        of the loss function, $\bar{L}$ is the <em>average</em>
                        of Lipschitz constants associated with the
                        coordinates and individual functions in the sum,
                        and $R$ is the distance of the initial point
                        from the minimizer. We show that the method can
                        be implemented <strong>without the need to
                          perform full-dimensional vector operations</strong>,
                        which is considered to be the major bottleneck
                        of accelerated coordinate descent. The fact that
                        the method depends on the average degree of
                        separability, and not on the maximum degree of
                        separability, can be attributed to the use of <strong>new
safe














                          large stepsizes</strong>, leading to improved
                        expected separable overapproximation (ESO).
                        These are of independent interest and can be
                        utilized in all existing parallel stochastic
                        coordinate descent algorithms based on the
                        concept of ESO. </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> December 18, 2013 </h3>
                      <p> I am offering a PhD project on <a
href="http://www.maths.ed.ac.uk/news/2013/phd-studentships-in-mathematics-for-the-earth-and-environment">modelling
and














                          optimization in the oil and gas</a> industry.
                        <a
                          href="http://e3partnership.wordpress.com/how-to-apply/">Application













                          deadline: January 24, 2014</a>. Feel free to
                        get in touch if interested.</p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> December 18, 2013 </h3>
                      <p> <a href="http://jakubkonecny.com/">Jakub
                          Konecny</a> has a <a
                          href="http://arxiv.org/abs/1312.4190">new
                          paper</a> out, accepted to JMLR. It's on
                        one-shot learning of gestures with Microsoft
                        Kinect sensor.</p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> December 9, 2013 </h3>
                      <p> Today I am attending and giving a talk at the
                        Optimization in Machine Learning workshop at
                        NIPS. For some reason there are two versions of
                        the schedule (<a
                          href="https://sites.google.com/site/mloptstat/opt-2013/schedule">1</a>,
                        <a
                          href="https://sites.google.com/site/mloptstat/opt-2013/schedule">2</a>).
Here














                        are my <a
                          href="talks/talk-OPT2013_workshop_at_NIPS.ppsx">slides</a>.
                      </p>
                      <p> Update (Dec 11): I am back in Berkeley. I had
                        a packed room during my talk - many more than <em>n</em>
                        people showed up... </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> December 6, 2013 </h3>
                      <p> Turns out on Monday at <a
                          href="https://nips.cc/">NIPS</a> I am giving
                        my talk at the same time when Mark Zuckerberg is
                        on a discussion panel. I am buying a beer to
                        everyone who shows up during my talk (and I am
                        confident I will be able to afford it*)</p>
                      <p>*Small (illegible) script: Should more than <em>n</em>
                        people show up for my talk, I claim the right
                        not to pay anyone. Moreover, I will only reveal
                        the value of <em>n</em> after the talk. </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> December 4, 2013 </h3>
                      <p> <span class="important">A new paper is out:</span>
                        <a href="http://arxiv.org/abs/1312.1666">Semi-Stochastic












                          Gradient Descent Methods</a>, joint with <a
                          href="http://jakubkonecny.com/">Jakub Konečný</a>.
                      </p>
                      <p> We propose S2GD: a method belonging to
                        second-generation stochastic gradient descent
                        (SGD) methods, combining the stability of
                        gradient descent and computational efficiency of
                        SGD. The method runs in several epochs, in each
                        of which a full gradient is first computed, and
                        then a random number of stochastic gradients are
                        evaluated, following a geometric law. The <a
href="http://media.nips.cc/nipsbooks/nipspapers/paper_files/nips26/238.pdf">SVRG
method














                          of Johnson and Zhang</a> arises as a special
                        case. </p>
                      <p> We also propose S2GD+, which in our
                        experiments substantially outperforms all
                        methods we tested, incuding S2GD, SGD and <a
                          href="http://arxiv.org/abs/1202.6258">SAG
                          (Stochastic Average Gradient) of Le Roux,
                          Schmidt and Bach</a>. </p>
                      <img src="papers/s2gd.png" alt="cool image"
                        width="576">
                      <p> <em>Figure:</em> Comparison of SGD, SAG, S2GD
                        and S2GD+ on a badly conditioned problem with
                        million training examples. On the x-axis: #
                        evalutaions of the <em>stochastic</em>
                        gradient. </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> December 4, 2013 </h3>
                      <p> Here is a <a href="posters/Poster-NSync.pdf">new














                          poster on the `NSync algorithm</a> to be
                        presented by Martin Takáč at <a
                          href="https://nips.cc/">NIPS</a>. I am off to
                        Lake Tahoe tomorrow. Seems like the weather
                        there is <a
                          href="https://www.google.com/search?q=weather+Lake+Tahoe">a
                          bit different</a> from what I got used to in
                        Berkeley ;-)</p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> November 26, 2013 </h3>
                      <p> A revised version of the paper <em>Parallel
                          coordinate descent methods for big data
                          optimization</em> (submitted to Mathematical
                        Programming) is now <a
                          href="http://arxiv.org/abs/1212.0873v2">available














                          here</a>. Extended contributions section, new
                        experiments with real data, you can even enjoy
                        an uber-cool table summarizing the key notation
                        (thanks to the reviewer suggesting this!) in the
                        appendix. Page count: 35 -&gt; 43. I bet you are
                        wondering about the meaning of the two dates on
                        the paper...</p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> November 22, 2013 </h3>
                      <p> University of Edinburgh received cca £5
                        million in funding from <a
href="http://www.epsrc.ac.uk/newsevents/news/2013/Pages/phdnewcentres.aspx">EPSRC</a>
                        for a <a
                          href="http://datascience.inf.ed.ac.uk/">Centre
                          for Doctoral Training in Data Science</a>. I
                        am <a
                          href="http://datascience.inf.ed.ac.uk/people/">one














                          of the involved faculty</a> who will be
                        supervising PhD students in the centre. These
                        are good times for <a
                          href="http://datascience.inf.ed.ac.uk/research/">data













                          science research</a> in Edinburgh! </p>
                      <p> We have about <a
                          href="http://datascience.inf.ed.ac.uk/apply/">10
PhD














                          positions open</a> for the brightest,
                        analytically gifted students (future stars of
                        data science!), starting in September 2014. </p>
                      <p><span class="important">For full consideration,
                          apply by January 27, 2014. </span></p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> November 18, 2013 </h3>
                      <p> I have just learned (having received a request
                        for a reference letter) that <a
                          href="http://www.mtakac.com/index.php/">Martin
                          Takáč</a> was nominated (not by me) for the <a
href="http://www.claymath.org/fas/research_fellows/">Clay Research
                          Fellowship.</a> </p>
                      <p> "Clay Research Fellows are selected for their
                        research achievements and their potential to
                        become leaders in research mathematics." </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> November 18, 2013 </h3>
                      <p> This week I am attending the Simons Institute
                        workshop on <a
                          href="http://simons.berkeley.edu/workshops/bigdata2013-3">Unifying













                          Theory and Experiment for Large-Scale Networks</a>.
                        You can watch <a
                          href="http://simons.berkeley.edu/workshops/schedule/77">live














                          video of the talks.</a></p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> November 8, 2013 </h3>
                      <p> The <a href="http://itis2013.fis.unm.si/">ITIS














                          2013</a> conference is over; I met many new
                        people (virtually everybody was new to me) and
                        had a very good time. </p>
                      <p> <a href="http://www.matjazperc.com/">Matjaž
                          Perc</a> showed us all how one can have fun
                        during one's own talk; <a
                          href="http://users.ictp.it/%7Emarsili/">Matteo
                          Marsili</a> talked about an interesting
                        connection between stochastic programming,
                        sampling, entropy and nature; <a
                          href="https://sites.google.com/site/santofortunato/">Santo














                          Fortunato</a> gave a very accessible and
                        enjoyable talk about community detection in
                        social networks and <a
                          href="http://www.cse.nd.edu/%7Etmilenko/">Tijana












                          Milenkovic</a> gave an exciting talk on the
                        applications of the network alignment problem.
                        Many of the local talks were interesting.</p>
                      <p>The fact that the hotel location was a spa did
                        not hurt either. </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="36"> <br>
                      <br>
                      <h3> November 7, 2013 </h3>
                    </span>
                    <p> <span class="important">New paper announcement:</span><span
                        class="rss-content"> TOP-SPIN: <a
                          href="http://arxiv.org/abs/1311.1406">TOPic
                          discovery via Sparse Principal component
                          INterference.</a> This is Joint work with <a
                          href="http://mtakac.com/">Martin Takáč</a>, <a
                          href="http://www.sutd.edu.sg/ahipasaoglu.aspx">Selin














                          D. Ahipasaoglu</a> and <a
                          href="http://classx.sutd.edu.sg/%7Engaiman_cheung/">Ngai-Man














                          Cheung</a> (this paper alrwady was announced
                        in April, but was not posted onto arXiv until
                        now...)</span></p>
                    <table width="450" border="0">
                      <tbody>
                        <tr>
                          <td><img src="imgs/bmw_0_1.jpg" width="200"></td>
                          <td><img src="imgs/bmw_0_1_subset.jpg"
                              width="200"></td>
                        </tr>
                      </tbody>
                    </table>
                    <p> <em>Abstract:</em> We propose a novel topic
                      discovery algorithm for unlabeled images based on
                      the bag-of-words (BoW) framework. We first extract
                      a dictionary of visual words and subsequently for
                      each image compute a visual word occurrence
                      histogram. We view these histograms as rows of a
                      large matrix from which we extract sparse
                      principal components (PCs). Each PC identifies a
                      sparse combination of visual words which co-occur
                      frequently in some images but seldom appear in
                      others. Each sparse PC corresponds to a topic, and
                      images whose interference with the PC is high
                      belong to that topic, revealing the common parts
                      possessed by the images. We propose to solve the
                      associated sparse PCA problems using an
                      Alternating Maximization (AM) method, which we
                      modify for purpose of efficiently extracting
                      multiple PCs in a deflation scheme. Our approach
                      attacks the maximization problem in sparse PCA
                      directly and is scalable to high-dimensional data.
                      Experiments on automatic topic discovery and
                      category prediction demonstrate encouraging
                      performance of our approach. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> November 6, 2013 </h3>
                    <p> I am now in Paris, on my way to Zagreb and from
                      there to Dolenjske Toplice, Slovenia, to give a
                      plenary talk at the <a
                        href="http://itis2013.fis.unm.si/">ITIS
                        conference</a>. My talk is tomorrow: I'll be
                      talking about why parallelizing like crazy and
                      being lazy can be good.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> October 31, 2013 </h3>
                    <p> <a href="http://mtakac.com/">Martin Takáč </a>
                      lead a 1hr long technical discussion at <a
                        href="https://amplab.cs.berkeley.edu/">AmpLab</a>
                      on various issues related to parallelizing
                      coordinate descent (on multicore machines, GPUs
                      and supercomputers). </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> October 28, 2013 </h3>
                    <p> Tomorrow at 11:30am (actually, after everyone,
                      including me, is finished with the provided lunch
                      - kudos to the organizers!) I am giving a talk at
                      the <a href="https://amplab.cs.berkeley.edu">AmpLab</a>
                      All Hands meeting, Berkeley (Wozniak Lounge, SODA
                      Hall). I'll be speaking about <a
                        href="http://arxiv.org/abs/1310.2059">Hydra:
                        scaling coordinate descent to a cluster
                        environment.</a> Here are the slides.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> October 23, 2013 </h3>
                    <p> The slides from my today's talk at the workshop
                      <a
                        href="http://simons.berkeley.edu/workshops/schedule/76">Parallel














                        and Distributed Algorithms for Inference and
                        Optimization</a>, are <a
                        href="talks/Talk-BigData-Simons.ppsx">here</a>.
                      You can <a
                        href="http://www.youtube.com/watch?v=IQgnstB0n2E#t=538">watch














                        the talk on Youtube.</a></p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> October 21, 2013 </h3>
                    <p> This week I am attending the Simons workshop <a
href="http://simons.berkeley.edu/workshops/schedule/76">Parallel and
                        Distributed Algorithms for Inference and
                        Optimization</a>, my talk is on Wednesday. Today
                      I particularly enjoyed the talks by <a
                        href="http://simons.berkeley.edu/talks/sergei-vassilvitskii-2013-10-21">Sergei
Vassilvitskii














                        (Google)</a>, <a
                        href="http://simons.berkeley.edu/talks/sergei-vassilvitskii-2013-10-21">Joseph
Gonzalez














                        (Berkeley)</a>, <a
                        href="http://simons.berkeley.edu/talks/alekh-agarwal-2013-10-21">Alekh
Agarwal














                        (Microsoft Research)</a> and <a
                        href="http://simons.berkeley.edu/talks/tim-kraska-2013-10-21">Tim
Kraska














                        (Brown)</a>.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> October 16, 2013 </h3>
                    <p> This website got a facelift; the main change is
                      the addition of a menu leading to dedicated pages.
                      The old site with everything on a single page
                      started to look like it could one day seriously
                      rival <a
                        href="http://worlds-highest-website.com/">this
                        beauty.</a> Should you find any broken link and
                      feel like letting me know, please do. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>October 11, 2013</h3>
                    <p><span class="important">New short paper is out:</span>
                      <a href="http://arxiv.org/abs/1310.3438">On
                        optimal probabilities in stochastic coordinate
                        descent methods.</a> Joint work with <a
                        href="http://mtakac.com/index.php/">Martin Takáč</a>.
                    </p>
                    <p>We propose and analyze a new parallel coordinate
                      descent method---<span class="important">`NSync</span>---in












                      which at each iteration a random subset of
                      coordinates is updated, in parallel, allowing for
                      the subsets to be chosen <em>non-uniformly</em>.
                      Surprisingly, the strategy of updating a single
                      randomly selected coordinate per iteration---with
                      <em>optimal probabilities</em>---may require less
                      iterations, both in theory and practice, than the
                      strategy of updating all coordinates at every
                      iteration. </p>
                    <p> We believe this text is ideal as a quick point
                      of entry to the subject of parallel coordinate
                      descent. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>October 8, 2013</h3>
                    <p> <a href="http://www.ph.ed.ac.uk/higgs/">Peter
                        Higgs</a> won the Nobel Prize in Physics. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>October 8, 2013</h3>
                    <p><span class="important">New paper announcement:</span>
                      <a href="http://arxiv.org/abs/1310.2059">Distributed














                        coordinate descent method for learning with big
                        data</a>. Joint work with <a
                        href="http://mtakac.com/index.php/">Martin Takáč</a>.
                    </p>
                    <img src="img/hydra.png" height="100">
                    <p> We propose and analyze <span class="important">Hydra</span>:
                      the first distributed-memory coordinate descent
                      method. This extends methods such as PCDM, Shotgun
                      and mini-batch SDCA to big data computing. It is
                      capable of solving terabyte optimization/learning
                      problems on a cluster in minutes. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>October 7, 2013</h3>
                    <p>My university nominated me for the <a
href="http://research.microsoft.com/en-us/collaboration/awards/msrff.aspx">2014
Microsoft














                        Research Faculty Fellowship</a>. Each university
                      is only allowed to nominate a single person, and
                      every year about 7 awards are made, worldwide.
                      Wish me luck... </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>September 22, 2013</h3>
                    <p><span class="important">New paper announcement:</span>
                      <a href="http://arxiv.org/abs/1309.5885">Smooth
                        minimization of nonsmooth functions with
                        parallel coordinate descent methods</a>. This is
                      joint work with <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier












                        Fercoq</a>. </p>
                    <p> In this paper we show that parallel coordinate
                      descent methods can be applied to a fairly general
                      class of nonsmooth convex optimization problems
                      and prove that the number of iterations decreases
                      as more processors are used. The class of
                      functions includes, as special cases, L1
                      regularized L1 regression, L-infinity regression
                      and the "AdaBoost" problem (minimization of the
                      exponential loss). </p>
                    <p> The first 5 pages give a brief tutorial on
                      coordinate descent and on the issues related to
                      making the method parallel. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>September 16, 2013</h3>
                    <p>This week I am attending the first workshop of
                      the Big Data program at the <a
                        href="http://simons.berkeley.edu/">Simons
                        Institute</a>: <a
                        href="http://simons.berkeley.edu/workshops/schedule/75">Succinct














                        Data Representations and Applications.</a> All
                      talks are streamed online and also recorded. All
                      talks today were great; I particularly enjoyed
                      those by <a
                        href="http://simons.berkeley.edu/talks/michael-mahoney-2013-09-16">Michael














                        Mahoney</a>, <a
                        href="http://simons.berkeley.edu/talks/michael-mahoney-2013-09-16">Petros














                        Drineas</a> and <a
                        href="http://simons.berkeley.edu/talks/ronitt-rubinfeld-2013-09-16">Ronitt














                        Rubinfeld</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>September 10, 2013</h3>
                    <p>I am now at Google (Mountain View) to give a talk
                      on various flavors of parallel coordinate descent.
                      I have just met with Yoram Singer; the talk will
                      start at 1pm after lunch (in case any local
                      googlers are reading this). </p>
                    <p><em>Update:</em> My visit went well, there will
                      be follow-up visits. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>September 10, 2013</h3>
                    <p>University of Edinburgh has ranked <a
href="http://www.topuniversities.com/university-rankings/world-university-rankings/2013#sorting=rank+region=+country=+faculty=+stars=false+search=">17th
in














                        the 2013 QS World University Rankings</a>. I
                      doubt we could have ranked higher even if the sole
                      ranking factor was the number of UK-born
                      faculty...</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>September 7, 2013</h3>
                    <p>This week (Sept 3-6) I participated in the <a
                        href="http://simons.berkeley.edu/workshops/bigdata2013-boot-camp">Big
Data














                        Boot Camp</a>, the launching event of the <a
                        href="http://simons.berkeley.edu/programs/bigdata2013">Theoretical













                        Foundations of Big Data Analysis</a> program at
                      the <a href="http://simons.berkeley.edu/">Simons
                        Institute for the Theory of Computing</a>,
                      Berkeley. </p>
                    <p>Several colleagues blogged about it, including <a
href="https://blogs.princeton.edu/imabandit/2013/09/02/first-week-of-activity-at-the-simons-institute/">Sebastian














                        Bubeck</a>, <a
                        href="http://mrtz.org/blog/what-should-a-theory-of-big-data-do/">Moritz













                        Hardt</a>, <a
href="http://mysliceofpizza.blogspot.com/2013/09/simons-big-data-boot-camp.html">Muthu














                        Muthukrishnan</a> and Suresh Venkat (<a
                        href="http://geomblog.blogspot.com/2013/08/on-theory-of-big-data.html">1</a>,
                      <a
href="http://geomblog.blogspot.com/2013/09/statistics-geometry-and-computer-science.html">2</a>,
                      <a
href="http://geomblog.blogspot.com/2013/09/more-camping-with-high-dimensional-boots.html">3</a>),
so














                      I can stop here. Next week is more relaxed for the
                      big data folks (that is, time for research),
                      although the <a
                        href="http://simons.berkeley.edu/programs/realanalysis2013">Real













                        Analysis in Computer Science</a> people here at
                      Simons have their own boot camp then, with some
                      very nice program. I plan to attend some of the
                      lectures, for instance, <a
                        href="http://simons.berkeley.edu/workshops/schedule/317">Analytical
Methods














                        for Supervised Learning</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>August 30, 2013</h3>
                    <p><span class="important">New paper is out:</span>
                      <a href="http://arxiv.org/abs/1308.6774">Separable
                        approximations and decomposition methods for the
                        augmented Lagrangian</a>, coathored with Rachael
                      Tappenden and Burak Buke. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> August 20, 2013</h3>
                    <p>As of today (and until the end of the year) I am
                      on a sabbatical at UC Berkeley, affiliated with
                      the Simons Institute for the Theory of Computing
                      and participating in the <a
                        href="http://simons.berkeley.edu/programs/bigdata2013">Theoretical
Foundations














                        of Big Data Analysis</a> program. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>August 14, 2013</h3>
                    <p>On September 10 I will give a talk at <a
                        href="http://research.google.com/">Google</a> on
                      an invitation by <a
                        href="http://research.google.com/pubs/author28.html">Yoram














                        Singer</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>August 13, 2013</h3>
                    <p>I have accepted an invitation to become a member
                      of the <a
                        href="http://www.epsrc.ac.uk/funding/peerrev/college/Pages/intro.aspx">EPSRC
Peer














                        Review College</a>. While I've been reviewing
                      grant proposals for EPSRC for some time now, this
                      apparently makes me eligible to be asked to sit on
                      a prioritization panel. Earlier today I declined
                      to review a <a
href="http://www.epsrc.ac.uk/newsevents/news/2013/Pages/cdtoutlinecallresults.aspx">CDT
full














                        proposal</a> due to a conflict of interests (I
                      am involved in two bids) - perhaps EPSRC wanted to
                      test my honesty first and I passed the test... </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>August 9, 2013</h3>
                    <p>I have accepted an invitation to give a plenary
                      talk at the 6th <a
                        href="http://nips.cc/Conferences/2013/">NIPS</a>
                      workshop on <em>Optimization for Machine Learning</em>.
                      A link to the 2012 edition (which contains links
                      to previous editions) is <a
                        href="http://opt.kyb.tuebingen.mpg.de/index.html">here</a>.
                      The workshop will be held during December 9-10,
                      2013, at Lake Tahoe, Nevada. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>August 7, 2013</h3>
                    <p><a
                        href="http://www.maths.ed.ac.uk/people/show?person=317">Rachael














                        Tappenden</a> will stay in Edinburgh longer
                      after her current EPSRC funded appointment expires
                      at the end of January 2014. She will continue as a
                      member of the<a href="#a_team"> big data
                        optimization lab</a> as a postdoc, her work will
                      now be funded by <a
                        href="http://www.nais.org.uk/Index.php">NAIS</a>.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>August 6, 2013</h3>
                    <p><a
                        href="http://www.maths.ed.ac.uk/%7Ejfowkes/publications.html">Jaroslav
(Jari)














                        Fowkes</a> will be joining the <a
                        href="#a_team">big data optimization lab</a> as
                      a <a href="http://www.nais.org.uk/Index.php">NAIS</a>
                      postdoc, starting in October 2013. Jari has
                      recently worked on Global Optimization of <a
                        href="http://www.maths.ed.ac.uk/ERGO/pubs/ERGO-13-010.html">Lipschitz</a>
                      and <a
                        href="http://link.springer.com/article/10.1007%2Fs10898-012-9937-9">Hessian














                        Lipschitz</a> functions. He has obtained his PhD
                      in 2012 from Oxford, working under the supervision
                      of <a
                        href="http://www.maths.ox.ac.uk/people/profiles/nick.gould">Nick














                        Gould</a>; and is now working with <a
                        href="http://www.maths.ed.ac.uk/%7Eccartis/">Coralia














                        Cartis</a>, who will soon leave Edinburgh for
                      Oxford. [There seems to be a lot of movement
                      between Edinburgh and Oxford...] </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> August 6, 2013</h3>
                    <p>Zheng Qu will be joining the <a href="#a_team">big














                        data optimization lab</a> as a postdoc, starting
                      in January 2014 (her appointment is for 2 years,
                      funded by <a
                        href="http://www.epsrc.ac.uk/Pages/default.aspx">EPSRC</a>
                      and <a href="http://www.nais.org.uk/">NAIS</a>).
                    </p>
                    <p> Zheng is currently studying at École
                      Polytechnique, France, under the supervision of
                      Stéphane Gaubert. Zheng has written several
                      papers, including <a
                        href="http://arxiv.org/abs/1109.5241">Curse of
                        dimensionality reduction in max-plus based
                        approximation methods: theoretical estimates and
                        improved pruning algorithms</a>, <a
                        href="http://arxiv.org/pdf/1307.4649.pdf">Markov
                        operators on cones and non-commutative consensus</a>,
                      <a href="http://arxiv.org/abs/1206.0448">The
                        contraction rate in Thompson metric of
                        order-preserving flows on a cone</a> and <a
                        href="http://arxiv.org/abs/1302.5226">Dobrushin
                        ergodicity coefficient for Markov operators on
                        cones, and beyond</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>August 5, 2013</h3>
                    <p>I am told that <a
                        href="http://www.cs.wisc.edu/users/swright">Steve














                        Wright</a> has covered a few of my papers on
                      coordinate descent and stochastic gradient descent
                      in his <a
href="http://eventos.fct.unl.pt/iccopt2013/pages/sparse-optimization-and-applications-information-processing">Summer














                        Course</a> on <em>Sparse Optimization and
                        Applications to Information Processing</em>,
                      delivered at <a
                        href="http://eventos.fct.unl.pt/iccopt2013/">ICCOPT












                        2013 in Lisbon</a>. One of the papers is not
                      online yet (and has been 'on my desk' for quite
                      some while now) - it will be put online in August
                      or early September - apologies if you are looking
                      for it and can't find it! </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>August 2, 2013</h3>
                    <p>I am now back from the <a
                        href="http://eventos.fct.unl.pt/iccopt2013/">ICCOPT














                        conference</a>; some very inspiring talks and
                      some very blue skies. Nearly 500 participants, 412
                      session talks and 14 people from Edinburgh: <a
                        href="http://www.maths.ed.ac.uk/%7Eccartis/">Cartis</a>,
                      <a href="http://www.maths.ed.ac.uk/%7Eofercoq/">Fercoq</a>,
                      <a href="http://www.maths.ed.ac.uk/%7Ekfount/">Fountoulakis</a>,
                      <a
                        href="http://www.maths.ed.ac.uk/%7Ejfowkes/publications.html">Fowkes</a>,
                      <a href="http://www.maths.ed.ac.uk/%7Egondzio/">Gondzio</a>,
                      <a
                        href="http://www.maths.ed.ac.uk/people/show?person=195">Gonzalez-Brevis</a>,
                      <a
                        href="http://www.maths.ed.ac.uk/people/show?person=330">Gower</a>,
                      <a href="http://www.maths.ed.ac.uk/%7Eagr/">Grothey</a>,
                      <a href="http://www.maths.ed.ac.uk/hall/">Hall</a>,
                      <a
                        href="http://www.maths.ed.ac.uk/people/show?person=223">Qiang</a>,
                      Richtárik, <a href="http://mtakac.com/index.php/">Takáč</a>,
                      <a
                        href="http://www.maths.ed.ac.uk/people/show?person=317">Tappenden</a>,
                      <a href="http://www.maths.ed.ac.uk/%7Eyan/">Yan</a>
                      (that's 3.4%)! ICCOPT 2016 will be held in Tokyo.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>July 28, 2013</h3>
                    <p> Traveling to Caparica, Portugal, for the <a
                        href="http://eventos.fct.unl.pt/iccopt2013">ICCOPT














                        conference (July 27-August 1, 2013).</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>July 18, 2013</h3>
                    <p> <a
                        href="http://www.nap.edu/catalog.php?record_id=18374">Frontiers
in














                        Massive Data Analysis</a> : a 119p report
                      written by a National Academy of Sciences
                      committee chaired by <a
                        href="http://www.cs.berkeley.edu/%7Ejordan/">Michael












                        Jordan</a>. Mike asked me (and others attending
                      <a
                        href="http://simons.berkeley.edu/programs/bigdata2013">this</a>)
                      to distribute this document around - it is a good
                      read for a general reader - recommended. Free to
                      download! </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>July 3, 2013</h3>
                    <p> My baggage arrived &amp; my talk is tomorrow - I
                      am no longer forced to sport my new (lovely)
                      Chinggis Khaan T-shirt during the talk. We had a
                      nice conference dinner today, perhaps with one
                      (read: 5+) too many toast speeches. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>June 29, 2013</h3>
                    <p>Arrived. My baggage did not. I am told I may be
                      lucky tomorrow. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>June 28, 2013</h3>
                    <p> Off to Ulaanbaatar, Mongolia, to attend and give
                      a talk at <a
                        href="http://iom.num.edu.mn/iccso2013/index.html">this














                        conference.</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>June 26, 2013</h3>
                    <p> Here is a bit of news from 2012 relevant for
                      2013; apparently I forgot to post this here. I
                      will spend the Fall 2013 semester as a <span
                        class="important">visiting</span> Professor at <span
                        class="important">Berkeley</span>, participating
                      in the <a
                        href="http://simons.berkeley.edu/program_bigdata2013.html">Theoretical
Foundations














                        of Big Data Analysis</a> program run at the
                      newly established <a
                        href="http://simons.berkeley.edu/index.html">Simons












                        Institute for the Theory of Computing.</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>June 25, 2013</h3>
                    <p> Giving a talk at the <a
                        href="http://numericalanalysisconference.org.uk/">25th












                        Biennial Numerical Analysis Conference</a> in
                      Strathclyde, Glasgow. Our group has organized a
                      minisymposium on <em>Recent Advanced in Big Data
                        Problems</em>; it will be held on the first day
                      of the conference, June 25th. </p>
                    <p> <em>Speakers:</em> Rachael Tappenden
                      (Edinburgh), myself (Edinburgh), Olivier Fercoq
                      (Edinburgh), James Turner (Birmingham), Ke Wei
                      (Oxford), Martin Takáč (Edinburgh). </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>June 24, 2013</h3>
                    <p> My PhD student <span class="important"><a
                          href="http://www.mtakac.com/CV">Martin Takáč</a></span>
                      was honoured with a <span class="important">Second












                        Prize in the 16th Leslie Fox Prize Competition
                        in Numerical Analysis</span>. Here is his <a
                        href="http://arxiv.org/abs/1212.0873">winning
                        paper</a> and his <a
                        href="http://prezi.com/lnpd5w-mdj43/big-data-optimization/">talk</a>
                      (as one can expect, the slides make exponentially
                      more sense with Martin's voice-over!). </p>
                    <p> <em>The Leslie Fox Prize for Numerical Analysis
                        of the Institute of Mathematics and its
                        Applications (IMA) is a biennial prize
                        established in 1985 by the IMA in honour of
                        mathematician Leslie Fox (1918-1992). The prize
                        honours "young numerical analysts worldwide"
                        (any person who is less than 31 years old), and
                        applicants submit papers for review. A committee
                        reviews the papers, invites shortlisted
                        candidates to give lectures at the Leslie Fox
                        Prize meeting, and then awards First Prize and
                        Second Prizes based on "mathematical and
                        algorithmic brilliance in tandem with
                        presentational skills".</em> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>June 24, 2013</h3>
                    <p> Attending the <a
                        href="http://icms.org.uk/workshops/fox2013#programme">Fox














                        Prize meeting</a>.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>June 20, 2013</h3>
                    <p> Interviewing postdoc candidates. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>June 14, 2013</h3>
                    <p> A <a
                        href="posters/Poster-Minibatch-ICML2013.pdf">new
                        poster</a> to go with the <a
                        href="http://jmlr.org/proceedings/papers/v28/takac13-supp.pdf">Mini-batch
primal














                        and dual methods for SVMs</a> (ICML 2013) paper.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>June 6, 2013</h3>
                    <p>I am visiting (on invitation) the <a
                        href="https://www.dstl.gov.uk/">Defence Science
                        and Technology Lab</a> of the Ministry of
                      Defence of the United Kingdom. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>June 4, 2013</h3>
                    <p> <a href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier














                        Fercoq</a> won the<span class="important"> Best
                        PhD Thesis Prize </span>[<a
                        href="http://www.fondation-hadamard.fr/fr/prixthese">1</a>,
                      <a
href="http://www.fondation-hadamard.fr/pgmo/students/prixthese/reglement">2</a>]
                      awarded by the <a
                        href="http://www.fondation-hadamard.fr/en/PGMO">Gaspard
Monge














                        Program for Optimization and Operations Research</a><a
                        href="http://www.fondation-hadamard.fr/fr">,</a>
                      sponsored by <a
                        href="http://www.roadef.org/content/index.htm">ROADEF</a>
                      (French Operations Research Society) and <a
                        href="http://www.fondation-hadamard.fr/fr">SMAI</a>
                      (French Society for Industrial and Applied
                      Mathematics). </p>
                    <p> The prize recognizes two doctoral theses
                      defended in France in 2012, in mathematics or
                      computer science, with significant contributions
                      to optimization and operations research, both from
                      a theoretical and applied point of view. The Prize
                      attracts a 1,000 EUR check. </p>
                    <p> Olivier Fercoq wrote his thesis <em><a
                          href="http://pastel.archives-ouvertes.fr/pastel-00743187">Optimization













                          of Perron eigenvectors and applications: from
                          web ranking to chronotherapeutics</a></em>
                      under the supervision of Stéphane Gaubert and
                      Marianne Akian (CMAP + INRIA). </p>
                    <p> Prize citation (I do not dare to translate this
                      from French): <em>Cette thèse constitue une
                        "contribution majeure dans le domaine de
                        l'optimisation de fonctions d'utilité sur
                        l'ensemble des matrices positives" (selon l'un
                        des rapporteurs). Elle présente à la fois un
                        ensemble de résultats théoriques (propriétés,
                        analyse de complexité,...) et des applications
                        intéressantes.</em> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>June 2, 2013</h3>
                    <p> Travelling to Brussels for a 3-day visit to the
                      European Commission (June 3-5). </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>May 27, 2013</h3>
                    <p>Shortlisting of candidates for the <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=013502">2y
postdoc














                        position</a> is under way; interviews will take
                      place in the third week of June. I received more
                      than 50 applications. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>May 14, 2013</h3>
                    <p><a
href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDAQFjAA&amp;url=http%3A%2F%2Fwww.core.ucl.ac.be%2F%7Enesterov%2F&amp;ei=gY6SUfGGMMmN7Qbr7IHQDQ&amp;usg=AFQjCNHyLNCHVnC2cb8e0vXjkvfHox_g1w&amp;sig2=cwej73iuQ4pdu0dXARlaZA&amp;bvm=bv.46471029,d.ZGU">Yurii














                        Nesterov</a> (CORE, Louvain) is visiting me and
                      my <a href="#a_team">group</a> for a week.
                      Tomorrow at 3:30pm in 6206 JCMB he will deliver a
                      NAIS/ERGO talk titled <a
                        href="http://www.maths.ed.ac.uk/ERGO/abstracts/2013-05-nesterov.html">Dual
methods














                        for minimizing functions with bounded variation</a>.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>May 13, 2013</h3>
                    <p>I am in London, giving a <a
                        href="talks/Big_Data_Mining_London_2013.ppsx">talk</a>
                      at <a
href="http://www2.imperial.ac.uk/%7Egmontana/bigdatamining/schedule.html">Big
Data














                        Mining</a> (Imperial College) tomorrow. This
                      promises to be a very nice event, with a few
                      academia speakers (British Columbia, Edinburgh,
                      Bristol, Cambridge, UCL, Columbia) and plenty of
                      industry speakers (IBM, Financial Times, Barclays,
                      Bloomberg News, SAP, Cloudera, Last.fm, Johnson
                      Research Lab, QuBit and SAS). </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>May 8, 2013</h3>
                    <p>'Fresh' news from last week. Two new posters
                      (presented at the Optimization &amp; Big Data
                      workshop): <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/posters/Tappenden.pdf">Inexact
coordinate














                        descent</a> (joint with Rachael Tappenden and
                      Jacek Gondzio) + <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/posters/Takac.pdf">Distributed
coordinate














                        descent for big data optimization</a> (joint
                      with Martin Takáč and Jakub Mareček). </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>May 3, 2013</h3>
                    <p>The Best Poster Prize (<a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/">Optimization
&amp;














                        Big Data workshop</a>) goes to <a
                        href="http://tuomov.iki.fi/">Tuomo Valkonen
                        (Cambridge)</a>, for the poster <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/posters/Valkonen.pdf">Computational
problems














                        in magnetic resonance imaging</a>. Jury: Prof
                      Stephen Wright (Wisconsin-Madison) and Dr Imre
                      Pólik (SAS Institute). Steve's plenary/colloquium
                      talk was amazing. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>May 1, 2013 </h3>
                    <p>The <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/">Optimization
&amp;














                        Big Data</a> workshop has started! Today there
                      were 3 talks about coordinate descent methods, a
                      conditional gradient talk, an industry talk, an
                      optimization in statistics talk and a mirror
                      descent talk. I gave a talk today, too. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>April 25, 2013</h3>
                    <p>Dr Michael Grant, the co-creator of the CVX
                      matlab package for Disciplined Convex Programming,
                      has accepted my invitation to give a talk about
                      CVX and the fun behind it. He will speak on Monday
                      April 29 at 4:45pm in 6206 JCMB: <em><a
                          href="http://www.maths.ed.ac.uk/ERGO/abstracts/2013-04-grant.html">Disciplined
convex














                          programming and CVX (and thoughts on
                          academically valuable software)</a></em>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>April 22, 2013</h3>
                    <p>Our School (The School of Mathematics) has today
                      opened the Michael and Lily Atiyah Portrait
                      Gallery (3rd floor of the James Clerk Maxwell
                      Building). Here is a <a
                        href="http://www.maths.ed.ac.uk/%7Eaar/atiyahpg.pdf">pdf












                        file</a> with the portraits and some very
                      interesting comments! </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> April 19, 2013</h3>
                    <p>Fresh from the bakery, <a
                        href="http://arxiv.org/abs/1304.5530">Inexact
                        coordinate descent: complexity and
                        preconditioning</a> is a new paper, coauthored
                      with Jacek Gondzio and Rachael Tappenden. </p>
                    <p> <em>Brief blurb:</em> We prove complexity
                      bounds for a randomized block coordinate descet
                      method in which the proximal step defining an
                      iteration is performed inexactly. This is often
                      useful in the case when blocks contain more than a
                      single variable - we illustrate this on the
                      example of minimizing a quadratic function with
                      explicit block structure. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>April 19, 2013</h3>
                    <p>I am attending the <a
                        href="http://bigdata.holyrood.com/">Big Data in
                        the Public Sector</a> workshop held at Dynamic
                      Earth, Edinburgh. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> April 16, 2013</h3>
                    <p>The paper <a
                        href="http://arxiv.org/abs/1303.2314">Mini-batch
                        primal and dual methods for SVMs</a> was
                      accepted to the Proceedings of the <a
                        href="http://icml.cc/2013/">30th International
                        Conference on Machine Learning (ICML 2013)</a>.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> April 14, 2013</h3>
                    <p>New paper out: <a href="topspin2013.pdf">TOP-SPIN:














                        TOPic discovery via Sparse Principal component
                        INterference</a>, coauthored by Martin Takáč,
                      Selin Damla Ahipasaoglu and Ngai-Man Cheung. </p>
                    <p> <em>Blurb:</em> We propose an unsupervised
                      computer vision method, based on sparse PCA, for
                      discovering topics in a database of images. Our
                      approach is scalable and three or more orders of
                      magnitude faster than a competing method for
                      object recognition. It gives nearly 90% prediction
                      accuracy on a benchmark Berkeley image database. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> April 5, 2013</h3>
                    <p>I've accepted an offer to become a Field Chief
                      Editor of a new <em>open-access</em> journal: <a
href="http://www.iapress.org/index.php/soic/index">Statistics,
                        Optimization and Information Computing</a>. The
                      journal aims to publish interdisciplinary work at
                      the interface of statistics, optimization and
                      information sciences and will appear in four
                      issues annualy. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>April 3, 2013</h3>
                    <p> Several <span class="important">Chancellor's
                        Fellowships (5-year tenure-track positions)</span>
                      are available in the School of Mathematics. <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.display_form">Application</a>
                      deadline: April 18th, 2013. </p>
                    <p> <em>We welcome candidates in any area of
                        Operational Research but in particular those
                        specializing for example in nonlinear
                        programming, mixed integer programming,
                        stochastic optimization and candidates
                        interested in applying optimization to modelling
                        and solving real-life problems. </em> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>March 20, 2013</h3>
                    <p> During March 19-21 I am in Paris, giving <a
                        href="TALK-2013-03-20-Paris.pdf">a talk</a>
                      today at <a
href="http://www.ihes.fr/jsp/site/Portal.jsp?document_id=3270&amp;portlet_id=14">Fête
Parisienne














                        in Computation, Inference and Optimization</a>.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> March 18, 2013</h3>
                    <p>My EPSRC "First Grant" proposal <em>Accelerated
                        Coordinate Descent Methods for Big Data Problems</em>
                      was approved. I will be advertising a 2 year
                      postdoc position soon (most probably starting
                      sometime between June 1st 2013 and September 1st
                      2013). </p>
                    <p> It is likely the postdoc will be able to spend a
                      few weeks at UC Berkeley in the period
                      September-December 2013, participating in the <a
href="http://simons.berkeley.edu/program_bigdata2013.html">Foundations
                        of Big Data Analysis</a> programme at the <a
                        href="http://simons.berkeley.edu/">Simons
                        Institute for the Theory of Computing.</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> March 18, 2013</h3>
                    <p> <a href="http://jakubkonecny.com/">Jakub
                        Konečný</a> has been awarded the <a
href="http://www.ed.ac.uk/schools-departments/student-funding/postgraduate/uk-eu/university-scholarships/development">Principal's
Career














                        Development Scholarship</a> and will be joining
                      the group as a PhD student starting in August
                      2013. </p>
                    <p> He will spend his first semester at University
                      of California Berkeley as a visiting student
                      affiliated with the newly established <a
                        href="http://simons.berkeley.edu/">Simons
                        Institute for the Theory of Computing</a> and
                      will participate in the <a
                        href="http://simons.berkeley.edu/program_bigdata2013.html">Theoretical
Foundations














                        of Big Data Analysis</a> programme. </p>
                    <p> <em>Short bio:</em> Jakub studied mathematics
                      at Comenius University, Slovakia. In the past he
                      represented his country in the International
                      Mathematical Olympiad. Most recently, together
                      with another student teammate, Jakub won 2nd place
                      at the <a
                        href="http://gesture.chalearn.org/dissemination/icpr2012">ChaLearn













                        Gesture Challenge Competition</a> - an
                      international contest in designing a one-shot
                      video gesture recognition system. Here is a brief
                      <a
href="http://techcrunch.com/2011/12/07/chalearn-challenges-you-to-teach-a-kinect-instant-gesture-recognition/">news














                        story</a>. Jakub was invited to present the
                      results at the <a href="http://www.icpr2012.org/">21st














                        International Conference on Pattern Recognition</a>
                      in Tsukuba, Japan, and was invited to submit a
                      paper describing the system to a special issue of
                      the Journal of Machine Learning Research. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> March 14, 2013</h3>
                    <p>Poster announcement: <a
href="http://on-demand.gputechconf.com/gtc/2013/poster/pdf/P0183_ChristosDelivorias.pdf">GPU
acceleration














                        of financial models</a>. <a
                        href="http://www.gputechconf.com/page/home.html">GPU












                        Technology Conference</a>, San Jose, California.
                      Joint with Christos Delivorias, Erick Vynckier and
                      Martin Takáč. </p>
                    <p> Based on 2012 MSc thesis <a
href="http://www.hpcfinance.eu/sites/www.hpcfinance.eu/files/Christos_Delivorias_0.pdf">Case
studies














                        in acceleration of Heston's stochastic
                        volatility financial engineering model: GPU,
                        cloud and FPGA implementations</a> of Christos
                      Delivorias at the School of Mathematics,
                      University of Edinburgh. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> March 12, 2013</h3>
                    <p>New paper announcement: <a
                        href="http://arxiv.org/abs/1303.2314">Mini-batch
                        primal and dual methods for SVMs</a>, coauthored
                      with <span class="style1" style="width: 300px;">Martin













                        Takáč, Avleen Bijral and Nathan Srebro</span>. </p>
                    <p> <em>Brief blurb:</em> We parallelize Pegasos
                      (stochastic subgradient descent) and SDCA
                      (stochastic dual coordinate ascent) for support
                      vector machines and prove that the theoretical
                      parallelization speedup factor of both methods is
                      the same, and depends on the spectral norm of the
                      data. The SDCA approach is primal-dual in nature,
                      our guarantees are given in terms of the original
                      hinge loss formulation of SVMs.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> March 6, 2013</h3>
                    <p> Today I gave a <a
                        href="TALK-2013-02-SIAM-Conference.ppsx">talk</a>
                      at the Annual meeting of the <a
                        href="http://www.edsiamchapter.co.uk/esscc2013_program">Edinburgh














                        SIAM Student Chapter</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> March 5, 2013</h3>
                    <p>Martin Takáč has become a finalist in the <a
                        href="http://www.numerical.rl.ac.uk/fox/">16th
                        IMA Leslie Fox Prize competition</a>. </p>
                    <p> <em>The Leslie Fox Prize for Numerical Analysis
                        of the Institute of Mathematics and its
                        Applications (IMA) is a biennial prize
                        established in 1985 by the IMA in honour of
                        mathematician Leslie Fox (1918-1992). The prize
                        honours "young numerical analysts worldwide"
                        (any person who is less than 31 years old), and
                        applicants submit papers for review. A committee
                        reviews the papers, invites shortlisted
                        candidates to give lectures at the Leslie Fox
                        Prize meeting, and then awards First Prize and
                        Second Prizes based on "mathematical and
                        algorithmic brilliance in tandem with
                        presentational skills".</em> </p>
                    <p> The prize meeting will be held on June 24, 2013
                      at <a href="http://www.icms.org.uk/">ICMS</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> March 5, 2013</h3>
                    <p>I am attending <a
                        href="http://icms.org.uk/workshops/ergoenergy">Optimization












                        in Energy Day</a>, International Centre for
                      Mathematical Sciences, Edinburgh. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> February 26, 2013</h3>
                    <p> Today I am attending (and giving a <a
                        href="TALK-2013-02-Big-Data-and-Social-Media.ppsx">talk</a>
                      at) <a
                        href="http://www.icms.org.uk/workshops/bigdata#programme">Big














                        Data and Social Media</a>, a workshop organized
                      by Des Higham at Strathclyde university. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>February 21, 2013</h3>
                    <p>I am giving a "Research Recap" <a
                        href="ILW.ppsx">talk</a> during the Innovative
                      Learning Week at the University of Edinburgh.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> February 4-6, 2013</h3>
                    <p>Visiting <a href="http://www.uclouvain.be/">Université












                        Catholique de Louvain</a>, Belgium, and giving a
                      talk at the <a
                        href="http://www.uclouvain.be/en-44416.html">CORE














                        mathematical programming seminar</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> January 30, 2013</h3>
                    <p>Giving a <a href="TALK-2013-01-ERGO.pdf">talk</a>
                      at the <a
                        href="http://www.maths.ed.ac.uk/ERGO/abstracts/2013-01-richtarik.html">ERGO
research














                        seminar.</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> January 6-11, 2013</h3>
                    <p>I am speaking at <a
                        href="http://lear.inrialpes.fr/workshop/osl2013/index.html">Optimization













                        and Statistical Learning</a>; a workshop in Les
                      Houches, France, on the slopes of Mont Blanc. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> December 17, 2012</h3>
                    <p> <span class="important">New paper is out:</span>
                      <a href="24AM.pdf">Alternating maximization:
                        unifying framework for 8 sparse PCA formulations
                        and efficient parallel codes</a>, joint work
                      with Martin Takáč and Selin Damla Ahipasaoglu. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>December 16, 2012</h3>
                    <p> I am organizing <span class="important"><a
                          href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/">Optimization
and














                          Big Data</a></span> (workshop, trek and
                      colloquium; <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Advances_in_Large_Scale_Optimization/index.html">a
                        sequel to this 2012 event</a>). </p>
                    <p> This event will be held in Edinburgh during May
                      1-3, 2013. Headline speaker: Steve Wright
                      (Wisconsin-Madison). More info and website later.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>December 11, 2012</h3>
                    <p> I am giving a talk at the <a
                        href="http://www.strath.ac.uk/mathstat/seminars/">Numerical














                        Analysis seminar</a>, University of Strathclyde.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> December 10, 2012</h3>
                    <p><span class="important">New paper is out:</span>
                      <a href="http://arxiv.org/abs/1212.2617">Optimal
                        diagnostic tests for sporadic Creutzfeldt-Jakob
                        disease based on SVM classification of RT-QuIC
                        data</a>, joint work with William Hulme, Lynne
                      McGuire and Alison Green. In brief, we come up
                      with optimal tests for detecting the sporadic
                      Creutzfeldt-Jakob disease. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> December 7, 2012</h3>
                    <p> <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=007223">Five
3-year














                        Whittaker Postdoctoral Fellowships in the School
                        of Mathematics.</a> If you are an exceptional
                      candidate and are interested in working with me,
                      send me an email. </p>
                    <p> Closing date for applications: January 22, 2013.
                      Starting date: no later than Sept 1, 2013. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> December 4, 2012</h3>
                    <p> Our group has an opening: <a
                        href="http://www.maths.ed.ac.uk/news/2012/lectureship-id-007224">Visiting
Assistant














                        Professor position (=2.5 year Lectureship).</a>
                      Closing date of applications: January 22, 2013. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>November 23, 2012</h3>
                    <p> <span class="important">New paper is out:</span>
                      <a href="pcdm.pdf">Parallel coordinate descent
                        methods for big data optimization</a>, joint
                      work with Martin Takáč. </p>
                    <p> <em> Brief info: </em> We propose and analyze
                      a rich family of randomized parallel block
                      coordinate descent methods and show that
                      parallelization leads to acceleration on partially
                      separable problems, which naturally occur in many
                      big data application. We give simple expressions
                      for the speedup factors. We have tested one of our
                      methods on a huge-scale LASSO instance with 1
                      billion variables; while a serial coordinate
                      descent method needs 41 hours to converge, when 24
                      processors are used, the parallel method needs
                      just 2 hours. </p>
                    <p> Download the code <a
                        href="http://code.google.com/p/ac-dc/downloads/list">here.</a>
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> November 15-December 23, 2012</h3>
                    <p> Martin Takáč is on a research visit to <a
                        href="http://www.sutd.edu.sg/">SUTD (Singapore
                        University of Technology and Design)</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> October 26, 2012</h3>
                    <p> I am giving a short talk, representing the
                      School of Mathematics, at a miniworkshop organized
                      around the visit of <a
                        href="http://research.microsoft.com/en-us/people/semmott/">Stephen














                        Emmott</a> (Head of Computational Science @
                      Microsoft Research) to Edinburgh. The slides do
                      not make much sense without the voice-over, but <a
                        href="Extreme_Mountain_Climbing.pps">here they
                        are</a> anyway. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> October 21-November 11, 2012</h3>
                    <p> Martin Takáč is on a research visit to TTI
                      Chicago. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> October 11-17, 2012</h3>
                    <p> I am at the <a
                        href="http://meetings2.informs.org/phoenix2012/">INFORMS
Annual














                        Meeting</a> in Phoenix, Arizona. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> October 3, 2012</h3>
                    <p> <a href="http://www.mtakac.com/CV">Martin Takáč</a>
                      was successful in the <a
                        href="http://www.informs.org/Community/ICS/Prizes/Student-Paper-Award">INFORMS
Computing














                        Society Student Paper Award Competition</a>. As
                      the sole runner-up, he won the 2nd prize with the
                      paper <a
                        href="http://www.optimization-online.org/DB_HTML/2011/07/3089.html"><em>Iteration
Complexity&nbsp;of














                          Randomized Block-Coordinate Descent Methods
                          for Minimizing a Composite Function</em></a>,
                      coauthored with myself. </p>
                    <p> <a href="http://www.informs.org/About-INFORMS">INFORMS</a>
                      (Institute for Operations Research and the
                      Management Sciences) is the largest professional
                      society in the world for professionals in the
                      field of operations research, management science,
                      and business analytics. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> October 1, 2012</h3>
                    <p> <a
                        href="http://www.cmapx.polytechnique.fr/%7Efercoq/index.en.html">Olivier














                        Fercoq</a> joined the group as a Postdoctoral
                      Researcher. He will be working on the <em>Mathematics














                        for Vast Digital Resources</em> project funded
                      by EPSRC. </p>
                    <p> Dr Fercoq obtained his PhD in September 2012
                      from CMAP, École Polytechnique, France, under the
                      supervision of Stéphane Gaubert. His PhD
                      dissertation: <em>Optimization of Perron
                        eigenvectors and applications: from web ranking
                        to chronotherapeutics.</em> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3> September 2012</h3>
                    <p> I am now a <a
                        href="http://www.nais.org.uk/People-Lecturers.php">NAIS





                        Lecturer</a>; i.e., I am more closely affiliated
                      with the Centre for Numerical Algorithms and
                      Intelligent Software (I was a NAIS member before).
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>September 2012</h3>
                    <p> Minnan Luo (Tsinghua University) joined the
                      group as a visiting PhD student - she will stay
                      for 6 months. Minnan is the recipient of the 2012
                      Google China Anita Borg Scholarship. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>September 9-12, 2012</h3>
                    <p> I am in Birmingham at the <a
href="http://www.ima.org.uk/conferences/conferences_calendar/numerical_linear_algebra_and_optimisation.cfm">3rd
IMA





                        Conference on Numerical Linear Algebra and
                        Optimization</a>. Edinburgh has 10 people in
                      attendance + a few alumni.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>August 2012</h3>
                    <p> 16 members of <a
                        href="http://www.maths.ed.ac.uk/ERGO/">ERGO</a>
                      are attending <a href="http://ismp2012.org/">ISMP
                        Berlin</a>!</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>July 2012</h3>
                    <p>I am organizing (with F. Glineur) the <a
                        href="http://eventos.fct.unl.pt/iccopt2013">ICCOPT





                        (July 29 - Aug 1, 2013, Lisbon, Portugal)</a>
                      cluster <em>"Convex and Nonsmooth Optimization"</em>.
                      If you want to give a talk in a session in the
                      cluster and/or organize a session yourself, please
                      let me know. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="36"> <br>
                    <br>
                    <h3>July 2012</h3>
                    <p>Some of my work was covered by Steve Wright in a
                      <a
                        href="http://www.ipam.ucla.edu/programs/gss2012/">Graduate





                        Summer School on 'Deep Learning' at IPAM/UCLA</a>;
                      see <a
                        href="https://www.ipam.ucla.edu/publications/gss2012/gss2012_10763.pdf">slides





                        65-67 here (analysis of Hogwild!)</a> and <a
                        href="https://www.ipam.ucla.edu/publications/gss2012/gss2012_10766.pdf">slides
95-102





                        here (coordinate descent)</a>.</p>
                        
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

                    
<h3> June 16-23, 2012</h3>
                    <p>I am visiting the Wisconsin Institutes for
                      Discovery, University of Wisconsin-Madison. </p>
                      
                      
<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>
                    
                    
                    <h3>May 2012</h3>
                    <p> <a href="http://www.mtakac.com/CV">Martin Takáč</a>
                      won the <a
                        href="http://www.siam.org/pdf/news/1998.pdf">Best Talk Prize</a> at the <a
                        href="http://www.maths.manchester.ac.uk/%7Esiam/snscc12">SIAM National Student Chapter conference</a> in
                      Manchester. </p>
                  </span></span></span></span></span></div>
        <div style="clear: both;"> </div>
      </div>
      <div id="footer">
        <script src="x_footer.js"></script> </div>
    </div>
  </body>
</html>
