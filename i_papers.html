<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">

    <link rel="stylesheet" href="style.css">
    <title>Peter Richtarik</title>

    <link rel="alternate" type="application/rss+xml" title="Peter
      Richtarik" href="http://rsspect.com/rss/42611.xml">


      <!-- Global site tag (gtag.js) - Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-168147887-3"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-168147887-3');
      </script>




  </head>
  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <script src="table_header.js"></script> </div>
      </div>
      <ul class="menu">
        <li><a href="index.html">News</a></li>
        <li><a href="i_oldnews.html">Old News</a></li>
        <li><a class="active" href="i_papers.html">Papers</a></li>
        <li><a href="i_talks.html">Talks</a></li>
        <li><a href="i_videotalks.html">Video Talks</a></li>
        <li><a href="i_events.html">Events</a></li>
        <li><a href="i_seminar.html">Seminar</a></li>
        <li><a href="i_software.html">Code</a></li>
        <li><a href="i_team.html">Team</a></li>
        <li><a href="i_join.html">Join</a></li>
        <li><a href="i_bio.html">Bio</a></li>
        <li><a href="i_teaching.html">Teaching</a></li>
        <li><a href="i_consulting.html">Consulting</a></li>
      </ul>
      <div id="wrapper" class="main">
        <div id="content"> All papers are listed below in reverse chronological order in which they appeared online.<br>
          <br>
          <meta charset="utf-8">


          <h1> Prepared in 2022 </h1>

          [186] Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich and Peter Richtárik<br>
          <b>ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2202.09357">arXiv</a>] [code: ProxSkip, Scaffnew, SProxSkip, SplitSkip, Decentralized Scaffnew] <br>
          <br>

          [185] Dmitry Kovalev, Aleksandr Beznosikov, Abdurakhmon Sadiev, Michael Persiianov, Peter Richtárik and Alexander Gasnikov<br>
          <b>Optimal algorithms for decentralized stochastic variational inequalities</b><br>
          [<a href="https://arxiv.org/abs/2202.02771">arXiv</a>] [code: Algorithm 1, Algorithm 2] <br>
          <br>

          [184] Alexander Tyurin and Peter Richtárik<br>
          <b>Distributed nonconvex optimization with communication compression, optimal oracle complexity, and no client synchronization</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2202.01268">arXiv</a>] [code: DASHA, DASHA-PAGE, DASHA-MVR] <br>
          <br>

          [183] Peter Richtárik, Igor Sokolov, Ilyas Fatkhullin, Elnur Gasanov, Zhize Li and Eduard Gorbunov <br>
          <b>3PC: Three point compressors for communication-efficient distributed training and a better theory for lazy aggregation</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2202.00998">arXiv</a>] [code: 3PC, LAG, CLAG, EF21] <br>
          <br>

          [182] Haoyu Zhao, Boyue Li, Zhize Li, Peter Richtárik and Yuejie Chi <br>
          <b>BEER: Fast $O(1/T)$ rate for decentralized nonconvex optimization with communication compression</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2201.13320">arXiv</a>] [code: BEER] <br>
          <br>

          [181] Grigory Malinovsky, Konstantin Mishchenko and Peter Richtárik<br>
          <b>Server-side stepsizes and sampling without replacement provably help in federated optimization</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2201.11066">arXiv</a>] [code: Nastya] <br>
          <br>


          <h1> Prepared in 2021 </h1>


          [180] Dmitry Kovalev, Alexander Gasnikov and Peter Richtárik<br>
          <b>Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling</b><br>
          [<a href="https://arxiv.org/abs/2112.15199">arXiv</a>] [code: APDG] <br>
          <br>

          [179] Haoyu Zhao, Konstantin Burlachenko, Zhize Li and Peter Richtárik<br>
          <b>Faster rates for compressed federated learning with client-variance reduction</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2112.13097">arXiv</a>] [code: COFIG, FRECON] <br>
          <br>

          [178] Konstantin Burlachenko, Samuel Horváth and Peter Richtárik<br>
          <b>FL_PyTorch: optimization research simulator for federated learning</b><br>
          <a href="https://dl.acm.org/doi/pdf/10.1145/3488659.3493775">Proceedings of the 2nd ACM International Workshop on Distributed Machine Learning</a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2202.03099">arXiv</a>] [code: FL_PyTorch] <br>
          <br>

          [177] Elnur Gasanov, Ahmed Khaled, Samuel Horváth and Peter Richtárik<br>
          <b>FLIX: A simple and communication-efficient alternative to local methods in federated learning</b><br>
          24th International Conference on Artificial Intelligence and Statistics (AISTATS 2022) <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2111.11556">arXiv</a>] [code: FLIX] <br>
          <br>

          [176] Xun Qian, Rustem Islamov, Mher Safaryan and Peter Richtárik<br>
          <b>Basis matters: better communication-efficient second order methods for federated learning</b><br>
          24th International Conference on Artificial Intelligence and Statistics (AISTATS 2022) <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2111.01847">arXiv</a>] [code: BL1, BL2, BL3] <br>
          <br>

          [175] Aleksandr Beznosikov, Peter Richtárik, Michael Diskin, Max Ryabinin and Alexander Gasnikov<br>
          <b>Distributed methods with compressed communication for solving variational inequalities, with theoretical guarantees</b><br>
          [<a href="https://arxiv.org/abs/2110.03313">arXiv</a>] [code: MASHA1, MASHA2] <br>
          <br>

          [174] Rafał Szlendak, Alexander Tyurin and Peter Richtárik<br>
          <b>Permutation compressors for provably faster distributed nonconvex optimization</b><br>
          10th International Conference on Learning Representations (ICLR 2022)  <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2110.03300">arXiv</a>] [code: MARINA] [<a href="https://www.youtube.com/watch?v=aMEj2pkGrcY">video</a>] <br>
          <br>

          [173] Ilyas Fatkhullin, Igor Sokolov, Eduard Gorbunov, Zhize Li and Peter Richtárik<br>
          <b>EF21 with bells & whistles: practical algorithmic extensions of modern error feedback </b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2110.03294">arXiv</a>] [code: EF21-SGD, EF21-PAGE, EF21-PP, EF21-BC, EF21-HB, EF21-Prox] [<a href="https://github.com/IgorSokoloff/ef21_experiments_source_code">github</a>] <br>
          <br>

          [172] Xun Qian, Hanze Dong, Peter Richtárik and Tong Zhang<br>
          <b>Error compensated loopless SVRG, Quartz, and SDCA for distributed optimization</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2109.10049">arXiv</a>] [code: EC-LSVRG, EC-SDCA, EC-Quartz] <br>
          <br>

          [171] Majid Jahani, Sergey Rusakov, Zheng Shi, Peter Richtárik, Michael W. Mahoney and Martin Takáč<br>
          <b>Doubly adaptive scaled algorithm for machine learning using second-order information</b><br>
          10th International Conference on Learning Representations (ICLR 2022)  <br>
          [<a href="https://arxiv.org/abs/2109.05198">arXiv</a>] [code: OASIS] <br>
          <br>

          [170] Haoyu Zhao, Zhize Li and Peter Richtárik<br>
          <b>FedPAGE: A fast local stochastic gradient method for communication-efficient federated learning</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2108.04755">arXiv</a>] [code: FedPAGE] <br>
          <br>

          [169] Zhize Li and Peter Richtárik<br>
          <b>CANITA: Faster rates for distributed convex optimization with communication compression</b><br>
          Advances in Neural Information Processing Systems 34 (NeurIPS 2021) <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2107.09461">arXiv</a>] [code: CANITA] <br>
          <br>

          [168] 50+ authors<br>
          <b>A field guide to federated optimization</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2107.06917">arXiv</a>] <br>
          <br>

          [167] Peter Richtárik, Igor Sokolov and Ilyas Fatkhullin<br>
          <b>EF21: A new, simpler, theoretically better, and practically faster error feedback</b><br>
          Advances in Neural Information Processing Systems 34 (NeurIPS 2021) <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2106.05203">arXiv</a>] [<a href="talks/TALK-2021-EF21-FL-ICML21.pdf">slides</a>] [code: EF21, EF21+] [<a href="https://github.com/IgorSokoloff/ef21_experiments_source_code">github</a>]<br>
          <br>

          [166] Dmitry Kovalev, Elnur Gasanov, Peter Richtárik and Alexander Gasnikov<br>
          <b>Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks</b><br>
          Advances in Neural Information Processing Systems 34 (NeurIPS 2021) <br>
          [<a href="https://arxiv.org/abs/2106.04469">arXiv</a>] [code: ADOM+]<br>
          <br>

          [165] Bokun Wang, Mher Safaryan and Peter Richtárik<br>
          <b>Smoothness-aware quantization techniques</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2106.03524">arXiv</a>] [code: DCGD+, DIANA+]<br>
          <br>

          [164] Adil Salim, Lukang Sun and Peter Richtárik<br>
          <b>Complexity analysis of Stein variational gradient descent under Talagrand's inequality T1</b><br>
          [<a href="https://arxiv.org/abs/2106.03076">arXiv</a>] [code: SVGD]<br>
          <br>

          [163] Laurent Condat and Peter Richtárik<br>
          <b>MURANA: A generic framework for stochastic variance-reduced optimization</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2106.03056">arXiv</a>] [code: MURANA, ELVIRA]<br>
          <br>

          [162] Mher Safaryan, Rustem Islamov, Xun Qian and Peter Richtárik<br>
          <b>FedNL: Making Newton-type methods applicable to federated learning</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2106.02969">arXiv</a>] [code: FedNL, FedNL-PP, FedNL-CR, FedNL-LS, FedNL-BC, N0, NS]<br>
          <br>

          [161] Grigory Malinovsky, Alibek Sailanbayev and Peter Richtárik<br>
          <b>Random reshuffling with variance reduction: new analysis and better rates</b><br>
          [<a href="https://arxiv.org/abs/2104.09342">arXiv</a>] [<a href="https://www.youtube.com/watch?v=DK9CJmz6SR8&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=36">3 min video</a>] [code: RR-SVRG, SO-SVRG, Cyclic-SVRG]<br>
          <br>

          [160] Zhize Li, Slavomír Hanzely and Peter Richtárik<br>
          <b>ZeroSARAH: Efficient nonconvex finite-sum optimization with zero full gradient computation </b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2103.01447">arXiv</a>] [code: Zero-SARAH]<br>
          <br>

          [159] Adil Salim, Laurent Condat, Dmitry Kovalev and Peter Richtárik<br>
          <b>An optimal algorithm for strongly convex minimization under affine constraints</b><br>
          24th International Conference on Artificial Intelligence and Statistics (AISTATS 2022) <br>
          [<a href="https://arxiv.org/abs/2102.11079">arXiv</a>]  <br>
          <br>

          [158] Zhen Shi, Nicolas Loizou, Peter Richtárik and Martin Takáč<br>
          <b>AI-SARAH: Adaptive and implicit stochastic recursive gradient methods</b><br>
          [<a href="https://arxiv.org/abs/2102.09700">arXiv</a>] [code: AI-SARAH] <br>
          <br>

          [157] Dmitry Kovalev, Egor Shulgin, Peter Richtárik, Alexander Rogozin and Alexander Gasnikov<br>
          <b>ADOM: Accelerated decentralized optimization method for time-varying networks</b><br>
          <a href="http://proceedings.mlr.press/v139/kovalev21a.html">38th International Conference on Machine Learning (ICML 2021)</a> <br>
          <a href="https://sites.google.com/ucsd.edu/cedo/"> NSF-TRIPODS Workshop: Communication Efficient Distributed Optimization </a> <br>
          [<a href="https://arxiv.org/abs/2102.09234">arXiv</a>] [<a href="https://www.youtube.com/watch?v=jO3t4eZFdkc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=37">5 min video</a>] [<a href="posters/Poster-ADOM-ICML-2021.pdf">poster</a>] [code: ADOM] <br>
          <br>

          [156] Konstantin Mishchenko, Bokun Wang, Dmitry Kovalev and Peter Richtárik<br>
          <b>IntSGD: Floatless compression of stochastic gradients</b><br>
          10th International Conference on Learning Representations (ICLR 2022)  <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2102.08374">arXiv</a>] [<a href="https://www.youtube.com/watch?v=-YjXDdwkeqc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=39">5 min video</a>] [code: IntSGD, IntDIANA] <br>
          <br>

          [155] Eduard Gorbunov, Konstantin Burlachenko, Zhize Li and Peter Richtárik<br>
          <b>MARINA: faster non-convex distributed learning with compression </b><br>
          <a href="http://proceedings.mlr.press/v139/gorbunov21a.html">38th International Conference on Machine Learning (ICML 2021) </a> <br>
          <a href="https://sites.google.com/ucsd.edu/cedo/"> NSF-TRIPODS Workshop: Communication Efficient Distributed Optimization </a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2102.07845">arXiv</a>] [<a href="https://www.youtube.com/watch?v=o5MwC4DYbGE&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=34">5 min video</a>] [<a href="https://www.youtube.com/watch?v=bj0E94Siq74">70 min video</a>] [poster]  [code: MARINA, VR-MARINA, PP-MARINA] <br>
          <br>

          [154] Mher Safaryan, Filip Hanzely and Peter Richtárik<br>
          <b>Smoothness matrices beat smoothness constants: better communication compression techniques for distributed optimization</b><br>
          Advances in Neural Information Processing Systems 34 (NeurIPS 2021) <br>
          <a href="https://dp-ml.github.io/2021-workshop-ICLR/">ICLR Workshop: Distributed and Private Machine Learning </a><br>
          <a href="https://sites.google.com/ucsd.edu/cedo/"> NSF-TRIPODS Workshop: Communication Efficient Distributed Optimization </a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2102.07245">arXiv</a>] [<a href="https://www.youtube.com/watch?v=vSD-smU0JjE&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=46">5 min video</a>][code: DCGD+, DIANA+, ADIANA+] <br>
          <br>

          [153] Rustem Islamov, Xun Qian and Peter Richtárik<br>
          <b>Distributed second order methods with fast rates and compressed communication</b><br>
          <a href="http://proceedings.mlr.press/v139/islamov21a.html">38th International Conference on Machine Learning (ICML 2021)</a> <br>
          <a href="https://sites.google.com/ucsd.edu/cedo/"> NSF-TRIPODS Workshop: Communication Efficient Distributed Optimization </a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2102.07158">arXiv</a>] [<a href="https://www.youtube.com/watch?v=iSKBZXlaoWo&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=41">5 min video</a>] [<a href="https://www.youtube.com/watch?v=jqgyjN5kLqo">80 min video</a>] [<a href="talks/TALK-2021-Newton-Learn.pdf">slides</a>] [<a href="posters/Poster-Newton-Learn.pdf">poster</a>] [code: NS, MN, NL1, NL2, CNL]   <br>
          <br>

          [152] Konstantin Mishchenko, Ahmed Khaled and Peter Richtárik<br>
          <b>Proximal and federated random reshuffling</b><br>
          <a href="https://sites.google.com/ucsd.edu/cedo/"> NSF-TRIPODS Workshop: Communication Efficient Distributed Optimization </a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2102.06704">arXiv</a>] [<a href="https://www.youtube.com/watch?v=aHAU6OYNoKA&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=40">8 min video</a>] [code:ProxRR, FedRR] <br>
          <br>


          <h1> Prepared in 2020 </h1>


          [151] Samuel Horváth, Aaron Klein, Peter Richtárik and Cedric Archambeau<br>
          <b>Hyperparameter transfer learning with adaptive complexity</b><br>
          <a href="http://proceedings.mlr.press/v130/horvath21a.html">The 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)</a> <br>
          [<a href="https://arxiv.org/abs/2102.12810">arXiv</a>]  <br>
          <br>

          [150] Xun Qian, Hanze Dong, Peter Richtárik and Tong Zhang<br>
          <b>Error compensated loopless SVRG for distributed optimization</b><br>
          <a href="https://opt-ml.org/papers/2020/paper_61.pdf">OPT2020: 12th Annual Workshop on Optimization for Machine Learning (NeurIPS 2020 Workshop) </a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://opt-ml.org/posters/2020/poster_61.pdf">poster</a>] [code: EC-LSVRG] <br>
          <br>

          [149] Xun Qian, Hanze Dong, Peter Richtárik and Tong Zhang<br>
          <b>Error compensated proximal SGD and RDA</b><br>
          <a href="https://opt-ml.org/papers/2020/paper_59.pdf">OPT2020: 12th Annual Workshop on Optimization for Machine Learning (NeurIPS 2020 Workshop)</a> <br>
           [<a href="https://opt-ml.org/posters/2020/poster_59.pdf">poster</a>] [code: EC-SGD, EC-RDA] <br>
          <br>

          [148] Eduard Gorbunov, Filip Hanzely, and Peter Richtárik<br>
          <b>Local SGD: unified theory and new efficient methods</b><br>
          <a href="http://proceedings.mlr.press/v130/gorbunov21a.html">The 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021) </a><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2011.02828">arXiv</a>] [<a href="https://www.youtube.com/watch?v=u_KoimUuc6k&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx">5 min video</a>] [<a href="posters/Poster-Local-SGD-AISTATS-2021.pdf">poster</a>] [code: S-Local-SVRG] <br>
          <br>

          [147] Dmitry Kovalev, Anastasia Koloskova, Martin Jaggi, Peter Richtárik, and Sebastian U. Stich<br>
          <b>A linearly convergent algorithm for decentralized optimization: sending less bits for free!</b><br>
          <a href="http://proceedings.mlr.press/v130/kovalev21a.html">The 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)</a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2011.01697">arXiv</a>] [<a href="https://www.youtube.com/watch?v=0bAgav0x-8U&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=32">3 min video</a>] <br>
          <br>

          [146] Wenlin Chen, Samuel Horváth, and Peter Richtárik<br>
          <b>Optimal client sampling for federated learning</b><br>
          <a href="https://ppml-workshop.github.io/#papers">Privacy Preserving Machine Learning (NeurIPS 2020 Workshop)</a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2010.13723">arXiv</a>] [code: OCS, AOCS]  <br>
          <br>

          [145] Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richtárik<br>
          <b>Linearly converging error compensated SGD</b><br>
          <a href="https://papers.nips.cc/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html">Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2010.12292">arXiv</a>] [<a href="https://www.youtube.com/watch?v=6Hpt6hbzgjU&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=38">5 min video</a>] [code: EC-SGD-DIANA, EC-LSVRG-DIANA, EC-LSVRGstar, ...]  <br>
          <br>

          [144] Alyazeed Albasyoni, Mher Safaryan, Laurent Condat, and Peter Richtárik<br>
          <b>Optimal gradient compression for distributed and federated learning</b><br>
          <a href="http://icfl.cc/wp-content/uploads/2020/12/SpicyFL_2020_paper_31.pdf">SpicyFL 2020: NeurIPS Workshop on Scalability, Privacy, and Security in Federated Learning</a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2010.03246">arXiv</a>] [<a href="http://icfl.cc/wp-content/uploads/2020/12/Mher-Safaryan-31-poster-compression.png">poster</a>] [<a href="http://icfl.cc/wp-content/uploads/2020/12/Laurent-Condat-31-video-compression.mp4">video</a>]<br>
          <br>

          [143] Filip Hanzely, Slavomír Hanzely, Samuel Horváth, and Peter Richtárik<br>
          <b>Lower bounds and optimal algorithms for personalized federated learning</b><br>
          <a href="https://papers.nips.cc/paper/2020/hash/187acf7982f3c169b3075132380986e4-Abstract.html">Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2010.02372">arXiv</a>] [<a href="https://www.youtube.com/watch?v=YkZeROHXahc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=35">5 min video</a>] [code: APGD1, APGD2, IAPGD, AL2SGD+]  <br>
          <br>

          [142] Laurent Condat, Grigory Malinovsky, and Peter Richtárik<br>
          <b>Distributed proximal splitting algorithms with rates and acceleration</b><br>
          <a href="https://www.frontiersin.org/articles/10.3389/frsip.2021.776825/full?&utm_source=Email_to_authors_&utm_medium=Email&utm_content=T1_11.5e1_author&utm_campaign=Email_publication&field=&journalName=Frontiers_in_Signal_Processing&id=776825">Frontiers in Signal Processing, section Signal Processing for Communications, 2022</a><br>
          <a href="https://opt-ml.org/papers/2020/paper_31.pdf">OPT2020: 12th Annual Workshop on Optimization for Machine Learning (NeurIPS 2020 Workshop)</a> <br>
          <span class="important">Spotlight Talk</span><br>
          [<a href="https://arxiv.org/abs/2010.00952">arXiv</a>] [<a href="https://opt-ml.org/posters/2020/poster_31.png">poster</a>]   <br>
          <br>

          [141] Robert M. Gower, Mark Schmidt, Francis Bach and Peter Richtárik<br>
          <b>Variance-reduced methods for machine learning</b><br>
          <a href="https://ieeexplore.ieee.org/document/9226504">Proceedings of the IEEE 108 (11):1968--1983, 2020</a> <br>
          [<a href="https://arxiv.org/abs/2010.00892">arXiv</a>]   <br>
          <br>

          [140] Xun Qian, Peter Richtárik, and Tong Zhang<br>
          <b>Error compensated distributed SGD can be accelerated</b><br>
          Advances in Neural Information Processing Systems 34 (NeurIPS 2021) <br>
          <a href="https://opt-ml.org/papers/2020/paper_60.pdf">OPT2020: 12th Annual Workshop on Optimization for Machine Learning (NeurIPS 2020 Workshop)</a><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2010.00091">arXiv</a>] [<a href="https://opt-ml.org/posters/2020/poster_60.pdf">poster</a>] [code: ECLK]  <br>
          <br>

          [139] Albert S. Berahas, Majid Jahani, Peter Richtárik, and Martin Takáč<br>
          <b>Quasi-Newton methods for deep learning: forget the past, just sample</b><br>
          <a href="https://www.tandfonline.com/doi/full/10.1080/10556788.2021.1977806">Optimization Methods and Software, 2021</a> <br>
          [<a href="https://arxiv.org/abs/1901.09997v4">arXiv</a>] [code: S-LBFGS, S-LSR1]  <br>
          <br>

          [138] Zhize Li, Hongyan Bao, Xiangliang Zhang and Peter Richtárik <br>
          <b>PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization</b><br>
          <a href="http://proceedings.mlr.press/v139/li21a.html">38th International Conference on Machine Learning (ICML 2021) </a><br>
          <a href="https://opt-ml.org/papers/2020/paper_76.pdf">OPT2020: 12th Annual Workshop on Optimization for Machine Learning (NeurIPS 2020 Workshop) <span class="important">(Spotlight Talk)</span></a> <br>
          [<a href="https://arxiv.org/abs/2008.10898">arXiv</a>] [<a href="https://www.youtube.com/watch?v=_K3XPxN-vdk&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=29">5 min video</a>] [code: PAGE] <br>
          <br>

          [137] Dmitry Kovalev, Adil Salim, and Peter Richtárik <br>
          <b>Optimal and practical algorithms for smooth and strongly convex decentralized optimization</b><br>
          <a href="https://papers.nips.cc/paper/2020/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html">Advances in Neural Information Processing Systems 33 (NeurIPS 2020) </a><br>
          [<a href="https://arxiv.org/abs/2006.11773">arXiv</a>] [code: APAPC, OPAPC, Algorithm 3]  <br>
          <br>

          [136] Ahmed Khaled, Othmane Sebbouh, Nicolas Loizou, Robert M. Gower, and Peter Richtárik <br>
          <b>Unified analysis of stochastic gradient methods for composite convex and smooth optimization</b><br>
          [<a href="https://arxiv.org/abs/2006.11573">arXiv</a>] [code: SGD]  <br>
          <br>

          [135] Samuel Horváth and Peter Richtárik <br>
          <b>A better alternative to error feedback for communication-efficient distributed learning</b><br>
          <a href="https://openreview.net/forum?id=vYVI1CHPaQg">9th International Conference on Learning Representations (ICLR 2021)</a> <br>
          <a href="http://icfl.cc/wp-content/uploads/2020/12/SpicyFL_2020_paper_3-1.pdf">SpicyFL 2020: NeurIPS Workshop on Scalability, Privacy, and Security in Federated Learning</a> <br>
          <span class="important">The Best Paper Award at NeurIPS-20 Workshop on Scalability, Privacy, and Security in Federated Learning</span><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2006.11077">arXiv</a>] [<a href="http://icfl.cc/wp-content/uploads/2020/12/Samuel-Horvath-3_Better.pdf">poster</a>] [code: DCSGD] <br>
          <br>

          [134] Adil Salim and Peter Richtárik <br>
          <b>Primal dual interpretation of the proximal stochastic gradient Langevin algorithm</b><br>
          <a href="https://papers.nips.cc/paper/2020/hash/2779fda014fbadb761f67dd708c1325e-Abstract.html">Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</a> <br>
          [<a href="https://arxiv.org/abs/2006.09270">arXiv</a>] [code: PGSLA]  <br>
          <br>

          [133] Zhize Li and Peter Richtárik <br>
          <b>A unified analysis of stochastic gradient methods for nonconvex federated optimization</b> <br>
          <a href="http://icfl.cc/wp-content/uploads/2020/12/SpicyFL_2020_paper_4.pdf">SpicyFL 2020: NeurIPS Workshop on Scalability, Privacy, and Security in Federated Learning</a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2006.07013">arXiv</a>] [<a href="http://icfl.cc/wp-content/uploads/2020/12/04-video-UnifiedSGD.mp4">video</a>]  <br>
          <br>

          [132] Konstantin Mishchenko, Ahmed Khaled, and Peter Richtárik <br>
          <b>Random reshuffling: simple analysis with vast improvements</b> <br>
          <a href="https://papers.nips.cc/paper/2020/hash/c8cc6e90ccbff44c9cee23611711cdc4-Abstract.html">Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</a> <br>
          [<a href="https://arxiv.org/abs/2006.05988">arXiv</a>] [<a href="https://www.youtube.com/watch?v=0ZZY5Y_6fd4&list=PLC28kDljnOrh4XxzWpOFBIu8IHjJzmKQs">4 min video</a>] [<a href="https://github.com/konstmish/random_reshuffling">code</a>: RR, SO, IG] <br>
          <br>

          [131] Motasem Alfarra, Slavomír Hanzely, Alyazeed Albasyoni, Bernard Ghanem, and Peter Richtárik <br>
          <b>Adaptive learning of the optimal mini-batch size of SGD</b> <br>
          <a href="https://opt-ml.org/posters/2020/poster_50.pdf">OPT2020: 12th Annual Workshop on Optimization for Machine Learning (NeurIPS 2020 Workshop) </a> <br>
          [<a href="https://arxiv.org/abs/2005.01097">arXiv</a>] [<a href="https://opt-ml.org/posters/2020/poster_10.png">poster</a>] <br>
          <br>

          [130] Adil Salim, Laurent Condat, Konstantin Mishchenko, and Peter Richtárik <br>
          <b>Dualize, split, randomize: fast nonsmooth optimization algorithms</b><br>
          <a href="https://opt-ml.org/papers/2020/paper_54.pdf">OPT2020: 12th Annual Workshop on Optimization for Machine Learning (NeurIPS 2020 Workshop)</a> <br>
          [<a href="https://arxiv.org/abs/2004.02635">arXiv</a>] [<a href="https://opt-ml.org/posters/2020/poster_54.pdf">poster</a>] [code: PDDY, SPDDY, SPD3O, SPAPC] <br>
          <br>

          [129] Atal Narayan Sahu, Aritra Dutta, Aashutosh Tiwari, and Peter Richtárik <br>
          <b>On the convergence analysis of asynchronous SGD for solving consistent linear systems</b><br>
          [<a href="https://arxiv.org/abs/2004.02163">arXiv</a>] [code: DASGD] <br>
          <br>

          [128] Grigory Malinovsky, Dmitry Kovalev, Elnur Gasanov, Laurent Condat, and Peter Richtárik <br>
          <b>From local SGD to local fixed point methods for federated learning</b><br>
          <a href="http://proceedings.mlr.press/v119/malinovskiy20a.html">37th International Conference on Machine Learning (ICML 2020)</a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2004.01442">arXiv</a>] [<a href="https://www.youtube.com/watch?v=-QgQp5HnWQY&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=28">5 min video</a>] [code: LDFPM, RDFPM] <br>
          <br>

          [127] Aleksandr Beznosikov, Samuel Horváth, Peter Richtárik and Mher Safaryan<br>
          <b>On biased compression for distributed learning</b><br>
          <a href="http://icfl.cc/wp-content/uploads/2020/12/SpicyFL_2020_paper_10-1.pdf">SpicyFL 2020: NeurIPS Workshop on Scalability, Privacy, and Security in Federated Learning</a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2002.12410">arXiv</a>] [<a href="http://icfl.cc/wp-content/uploads/2020/12/Samuel-Horvath-10_Biased.pdf">poster</a>] [code: CGD, Distributed SGD with Error Feedback] <br>
          <br>

          [126] Zhize Li, Dmitry Kovalev, Xun Qian and Peter Richtárik<br>
          <b>Acceleration for compressed gradient descent in distributed and federated optimization</b><br>
          <a href="http://proceedings.mlr.press/v119/li20g.html">37th International Conference on Machine Learning (ICML 2020)</a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2002.11364">arXiv</a>] [code: ACGD, ADIANA] <br>
          <br>

          [125] Dmitry Kovalev, Robert M. Gower, Peter Richtárik and Alexander Rogozin<br>
          <b>Fast linear convergence of randomized BFGS</b><br>
          [<a href="https://arxiv.org/abs/2002.11337">arXiv</a>] [code: RBFGS] <br>
          <br>

          [124] Filip Hanzely, Nikita Doikov, Peter Richtárik and Yurii Nesterov<br>
          <b>Stochastic subspace cubic Newton method</b><br>
          <a href="http://proceedings.mlr.press/v119/hanzely20a.html">37th International Conference on Machine Learning (ICML 2020)</a> <br>
          [<a href="https://arxiv.org/abs/2002.09526">arXiv</a>] [code: SSCN] <br>
          <br>

          [123] Mher Safaryan, Egor Shulgin and Peter Richtárik<br>
          <b>Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor</b><br>
          <a href="https://academic.oup.com/imaiai/advance-article/doi/10.1093/imaiai/iaab006/6220344">Information and Inference: A Journal of the IMA, 1--24, 2021</a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2002.08958">arXiv</a>]<br>
          <br>

          [122] Filip Hanzely and Peter Richtárik<br>
          <b>Federated learning of a mixture of global and local models</b><br>
          <a href="http://icfl.cc/wp-content/uploads/2020/12/SpicyFL_2020_paper_75.pdf">SpicyFL 2020: NeurIPS Workshop on Scalability, Privacy, and Security in Federated Learning</a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/2002.05516">arXiv</a>] [<a href="https://richtarik.org/slides/Talk-2-KAUST-AI-Conference-2021.pdf">slides</a>] [<a href="http://icfl.cc/wp-content/uploads/2020/12/Filip-Hanzely-75-poster-MixtureLocalSGD.pdf">poster</a>] [<a href="http://icfl.cc/wp-content/uploads/2020/12/Filip-Hanzely-2020-12-10-21.48.47-75-video-MixtureLocalSGD.mp4">video</a>]<br>
          <br>

          [121] Samuel Horváth, Lihua Lei, Peter Richtárik and Michael I. Jordan<br>
          <b>Adaptivity of stochastic gradient methods for nonconvex optimization</b><br>
          <a href="https://opt-ml.org/papers/2020/paper_50.pdf">OPT2020: 12th Annual Workshop on Optimization for Machine Learning (NeurIPS 2020 Workshop)</a> <br>
          To appear in SIAM Journal on Mathematics of Data Science (SIMODS), 2022 <br>
          [<a href="https://arxiv.org/abs/2002.05359">arXiv</a>] [<a href="https://opt-ml.org/posters/2020/poster_50.pdf">poster</a>] <br>
          <br>

          [120] Filip Hanzely, Dmitry Kovalev and Peter Richtárik<br>
          <b>Variance reduced coordinate descent with acceleration: new method with a surprising application to finite-sum problems</b><br>
          <a href="http://proceedings.mlr.press/v119/hanzely20b.html">37th International Conference on Machine Learning (ICML 2020)</a> <br>
          [<a href="https://arxiv.org/abs/2002.04670">arXiv</a>]<br>
          <br>

          [119] Ahmed Khaled and Peter Richtárik<br>
          <b>Better theory for SGD in the nonconvex world</b><br>
          [<a href="https://arxiv.org/abs/2002.03329">arXiv</a>]<br>
          <br>


          <h1> Prepared in 2019 </h1>

          [118] Ahmed Khaled, Konstantin Mishchenko and Peter Richtárik<br>
          <b>Tighter theory for local SGD on identical and heterogeneous data</b><br>
          <a href="http://proceedings.mlr.press/v108/bayoumi20a.html">The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS 2020)</a> <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/1909.04746">arXiv</a>]<br>
          <br>

          [117] Sélim Chraibi, Ahmed Khaled, Dmitry Kovalev, Adil Salim,
          Peter Richtárik and Martin Takáč<br>
          <b>Distributed fixed point methods with compressed iterates</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/1912.09925">arXiv</a>]<br>
          <br>

          [116] Samuel Horváth, Chen-Yu Ho, Ľudovít Horváth, Atal Narayan Sahu, Marco Canini and Peter Richtárik<br>
          <b>IntML: Natural compression for distributed deep learning</b><br>
          <a href="http://learningsys.org/sosp19/index.html">Workshop on AI Systems at Symposium on Operating Systems Principles 2019 (SOSP'19)</a><br>
          [<a href="papers/intml-aisys19.pdf">pdf</a>]<br>
          <br>

          [115] Dmitry Kovalev, Konstantin Mishchenko and Peter Richtárik<br>
          <b>Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates</b><br>
          <a href="https://sites.google.com/site/optneurips19/">NeurIPS 2019 Workshop Beyond First Order Methods in ML</a><br>
          [<a href="https://arxiv.org/abs/1912.01597">arXiv</a>] [<a href="posters/Poster-SN.pdf">poster</a>] [code: SN, SCN]<br>
          <br>

          [114] Ahmed Khaled, Konstantin Mishchenko and Peter Richtárik<br>
          <b>Better communication complexity for local SGD</b><br>
          <a href="http://federated-learning.org/fl-neurips-2019/">NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality</a><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/1909.04746">arXiv</a>] [<a href="posters/Poster-LocalSGD.pdf">poster</a>] [code: local SGD]<br>
          <br>

          [113] Ahmed Khaled and Peter Richtárik<br>
          <b>Gradient descent with compressed iterates</b><br>
          <a href="http://federated-learning.org/fl-neurips-2019/">NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality</a><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/1909.04716">arXiv</a>] [<a href="posters/Poster-GDCI.pdf">poster</a>] [code: GDCI]<br>
          <br>

          [112] Ahmed Khaled, Konstantin Mishchenko and Peter Richtárik<br>
          <b>First analysis of local GD on heterogeneous data</b><br>
          <a href="http://federated-learning.org/fl-neurips-2019/">NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality</a><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/1909.04715">arXiv</a>] [poster] [code: local GD]<br>
          <br>

          [111] Jinhui Xiong, Peter Richtárik and Wolfgang Heidrich<br>
          <b>Stochastic convolutional sparse coding</b><br>
          International Symposium on Vision, Modeling and Visualization 2019<br>
          <span class="important">VMV Best Paper Award, 2019</span> [<a href="https://www.vmv2019.uni-rostock.de/program/awards/">link</a>]<br>
          [<a href="https://arxiv.org/abs/1909.00145">arXiv</a>] [code: SBCSC, SOCSC]<br>
          <br>

          [110] Xun Qian, Zheng Qu and Peter Richtárik<br>
          <b>L-SVRG and L-Katyusha with arbitrary sampling</b><br>
          <a href="https://jmlr.org/papers/v22/20-156.html">Journal of Machine Learning Research 22(112):1−47, 2021</a> <br>
          [<a href="https://arxiv.org/abs/1906.01481">arXiv</a>] [<a href="https://www.youtube.com/watch?v=QMJDOtm9wxk&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=30">5 min video</a>] [code: L-SVRG, L-Katyusha]<br>
          <br>

          [109] Xun Qian, Alibek Sailanbayev, Konstantin Mishchenko and
          Peter Richtárik<br>
          <b>MISO is making a comeback with better proofs and rates</b><br>
          [<a href="https://arxiv.org/abs/1906.01474">arXiv</a>] [code:
          MISO]<br>
          <br>

          [108] Eduard Gorbunov, Adel Bibi, Ozan Sezer, El Houcine Bergou and Peter Richtárik<br>
          <b>A stochastic derivative free optimization method with momentum</b><br>
          <a href="https://openreview.net/forum?id=HylAoJSKvH">8th International Conference on Learning Representations</a> (ICLR 2020)<br>
          [<a href="https://arxiv.org/abs/1905.13278">arXiv</a>] [<a
            href="posters/Poster-SMTP.pdf">poster</a>] [code: SMTP]<br>
          <br>

          [107] Mher Safaryan and Peter Richtárik<br>
          <b>Stochastic Sign Descent Methods: New Algorithms and Better Theory</b><br>
          <a href="http://proceedings.mlr.press/v139/safaryan21a.html">38th International Conference on Machine Learning (ICML 2021)</a><br>
          <a href="https://opt-ml.org/papers/2020/paper_14.pdf">OPT2020: 12th Annual Workshop on Optimization for Machine Learning (NeurIPS 2020 Workshop)</a> <br>
          [<a href="https://arxiv.org/abs/1905.12938">arXiv</a>] [<a href="https://opt-ml.org/posters/2020/poster_14.png">poster</a>] [code: signSGD, signSGDmaj]<br>
          <br>

          [106] Adil Salim, Dmitry Kovalev and Peter Richtárik<br>
          <b>Stochastic proximal Langevin algorithm: potential splitting and nonasymptotic rates</b><br>
          <a href="http://papers.nips.cc/paper/8891-stochastic-proximal-langevin-algorithm-potential-splitting-and-nonasymptotic-rates">33rd Conference on Neural Information Processing Systems</a>
          (NeurIPS 2019)<br>
          [<a href="https://arxiv.org/abs/1905.11768">arXiv</a>] [<a
            href="posters/Poster-SPLA.pdf">poster</a>] [code: SPLA]<br>
          <br>

          [105] Aritra Dutta, El Houcine Bergou, Yunming Xiao, Marco Canini and Peter Richtárik<br>
          <b>Direct nonlinear acceleration</b><br>
          [<a href="https://arxiv.org/abs/1905.11692">arXiv</a>] [code:
          DNA]<br>
          <br>

          [104] Konstantin Mishchenko and Peter Richtárik<br>
          <b>A stochastic decoupling method for minimizing the sum of smooth and non-smooth functions</b><br>
          [<a href="https://arxiv.org/abs/1905.11535">arXiv</a>] [code:
          SDM]<br>
          <br>

          [103] Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richtárik and Yura Malitsky<br>
          <b>Revisiting stochastic extragradient</b><br>
          <a href="http://proceedings.mlr.press/v108/mishchenko20a.html">The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS 2020)</a><br>
          NeuriPS 2019 Workshop on Smooth Games Optimization and Machine  Learning<br>
          <b> </b> [<a href="https://arxiv.org/abs/1905.11373">arXiv</a>]<br>
          <br>

          [102] Filip Hanzely and Peter Richtárik<br>
          <b>One method to rule them all: variance reduction for data, parameters and many new methods</b><b><br>
          </b> [<a href="https://arxiv.org/abs/1905.11266">arXiv</a>]
          [code: GJS + 17 algorithms]<br>
          <br>

          [101] Eduard Gorbunov, Filip Hanzely and Peter Richtárik<br>
          <b>A unified theory of SGD: variance reduction, sampling, quantization and coordinate descent</b><br>
          <a href="http://proceedings.mlr.press/v108/gorbunov20a">The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS 2020)</a><br>
          [<a href="https://arxiv.org/abs/1905.11261">arXiv</a>]<br>
          <br>

          [100] Samuel Horváth, Chen-Yu Ho, Ľudovít Horváth, Atal
          Narayan Sahu, Marco Canini and Peter Richtárik<br>
          <b>Natural compression for distributed deep learning</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/1905.10988">arXiv</a>] [<a
            href="posters/Poster-Cnat.pdf">poster</a>]<br>
          <br>

          [99] Robert M. Gower, Dmitry Kovalev, Felix Lieder and Peter
          Richtárik<br>
          <b>RSN: Randomized Subspace Newton</b><br>
          <a  href="http://papers.nips.cc/paper/8351-rsn-randomized-subspace-newton">33rd Conference on Neural Information Processing Systems</a> (NeurIPS 2019)<br>
          <b> </b> [<a href="https://arxiv.org/abs/1905.10874">arXiv</a>]
          [<a href="posters/Poster-RSN.pdf">poster</a>]<br>
          <br>

          [98] Aritra Dutta, Filip Hanzely, Jingwei Liang and Peter
          Richtárik<br>
          <b>Best pair formulation &amp; accelerated scheme for non-convex principal component pursuit</b><br>
          <a href="https://ieeexplore.ieee.org/abstract/document/9145595">IEEE Transactions on Signal Processing 68:6128-6141, 2020</a> <br>
          [<a href="https://arxiv.org/abs/1905.10598">arXiv</a>]<br>
          <br>

          [97] Nicolas Loizou and Peter Richtárik<br>
          <b>Revisiting randomized gossip algorithms: general framework, convergence rates and novel block and accelerated protocols</b><br>
          <a href="https://ieeexplore.ieee.org/document/9539193">IEEE Transactions on Information Theory 67(12):8300--8324, 2021 </a><br>
          [<a href="https://arxiv.org/abs/1905.08645">arXiv</a>]<br>
          <br>

          [96] Nicolas Loizou and Peter Richtárik<br>
          <b>Convergence analysis of inexact randomized iterative methods</b><br>
          <a href="https://epubs.siam.org/doi/abs/10.1137/19M125248X">SIAM Journal on Scientific Computing 42(6), A3979–A4016, 2020</a> <br>
          [<a href="https://arxiv.org/abs/1903.07971">arXiv</a>] [code:
          iBasic, iSDSA, iSGD, iSPM, iRBK, iRBCD]<br>
          <br>

          [95] Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson,
          Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, Masoud
          Moshref, Dan R. K. Ports and Peter Richtárik<br>
          <b>Scaling distributed machine learning with in-network aggregation</b><br>
          The 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI '21 Fall) <br>
          [<a href="https://arxiv.org/abs/1903.06701">arXiv</a>] [code:
          SwitchML]<br>
          <br>

          [94] Samuel Horváth, Dmitry Kovalev, Konstantin Mishchenko,
          Peter Richtárik and Sebastian Stich<br>
          <b>Stochastic distributed learning with gradient quantization and variance reduction</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/1904.05115">arXiv</a>] [code:
          DIANA, VR-DIANA, SVRG-DIANA]<br>
          <br>

          [93] El Houcine Bergou, Eduard Gorbunov and Peter Richtárik<br>
          <b>Stochastic three points method for unconstrained smooth minimization</b><br>
          <a href="https://epubs.siam.org/doi/abs/10.1137/19M1244378">SIAM Journal on Optimization 30(4):2726-2749, 2020</a> <br>
          [<a href="https://arxiv.org/abs/1902.03591">arXiv</a>] [code: STP]<br>
          <br>

          [92] Adel Bibi, El Houcine Bergou, Ozan Sener, Bernard Ghanem and Peter Richtárik<br>
          <b>A stochastic derivative-free optimization method with importance sampling</b><br>
          <a href="https://aaai.org/Conferences/AAAI-20/">34th AAAI Conference on Artificial Intelligence</a> (AAAI-20)<br>
          [<a href="https://arxiv.org/abs/1902.01272">arXiv</a>] [code: STP_IS]<br>
          <br>

          [91] Konstantin Mishchenko, Filip Hanzely and Peter Richtárik<br>
          <b>99% of distributed optimization is a waste of time: the issue and how to fix it</b><br>
          To appear in: Uncertainty in Artificial Intelligence, 2020 (UAI 2020) <br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/1901.09437">arXiv</a>] [code:  IBCD, ISAGA, ISGD, IASGD, ISEGA]<br>
          <br>

          [90] Konstantin Mishchenko, Eduard Gorbunov, Martin Takáč and Peter Richtárik<br>
          <b>Distributed learning with compressed gradient differences</b><br>
          <span class="important">Federated Learning Paper</span><br>
          [<a href="https://arxiv.org/abs/1901.09269">arXiv</a>] [code: DIANA]<br>
          <br>

          [89] Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin and Peter Richtárik<br>
          <b>SGD: general analysis and improved rates</b><br>
          <a href="http://proceedings.mlr.press/v97/qian19b.html">Proceedings of the 36th International Conference on Machine Learning, PMLR 97:5200-5209, 2019</a><br>
          [<a href="https://arxiv.org/abs/1901.09401">arXiv</a>] [<a
            href="posters/Poster-SGD_AS.pdf">poster</a>] [code: SGD-AS]<br>
          <br>

          [88] Dmitry Kovalev, Samuel Horváth and Peter Richtárik<br>
          <b>Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop<br>
          </b><a href="http://alt2020.algorithmiclearningtheory.org">31st International Conference on Learning Theory</a> (ALT 2020)<br>
          [<a href="https://arxiv.org/abs/1901.08689">arXiv</a>] [code: L-SVRG, L-Katyusha]<br>
          <br>


          [87] Xun Qian, Zheng Qu and Peter Richtárik<br>
          <b>SAGA with arbitrary sampling</b><br>
          <a href="http://proceedings.mlr.press/v97/qian19a.html">Proceedings of the 36th International Conference on Machine Learning, PMLR 97:5190-5199, 2019</a><br>
          [<a href="https://arxiv.org/abs/1901.08669">arXiv</a>] [<a
            href="posters/Poster-SAGA_AS.pdf">poster</a>] [code: <a
            href="https://github.com/QIANXunK/Code_SAGA_AS">SAGA-AS</a>]<br> <br>

          <h1> Prepared in 2018</h1>

          [86] Lam M. Nguyen, Phuong Ha Nguyen, P. Richtárik, Katya Scheinberg, Martin Takáč and Marten van Dijk<br>
          <b>New convergence aspects of stochastic gradient algorithms</b><br>
          <a href="http://jmlr.org/papers/v20/18-759.html">Journal of Machine Learning Research 20(176):1-49, 2019</a><br>
          [<a href="https://arxiv.org/abs/1811.12403">arXiv</a>] <br>
          <br>

          [85] Filip Hanzely, Jakub Konečný, Nicolas Loizou, Peter Richtárik and Dmitry Grishchenko<br>
          <b>A privacy preserving randomized gossip algorithm via controlled noise insertion</b> <br>
          <a href="https://ppml-workshop.github.io/ppml/papers/57.pdf">NeurIPS Privacy Preserving Machine Learning Workshop, 2018</a><br>
          [<a href="https://arxiv.org/abs/1901.09367">arXiv</a>] [<a
            href="posters/Poster-PrivateGossip.pdf">poster</a>]<br>
          <br>

          [84] Konstantin Mishchenko and Peter Richtárik<br>
          <b>A stochastic penalty model for convex and nonconvex optimization with big constraints</b> <br>
          [<a href="https://arxiv.org/abs/1810.13387">arXiv</a>]<br>
          <br>

          [83] Nicolas Loizou, Michael G. Rabbat and Peter Richtárik<br>
          <b>Provably accelerated randomized gossip algorithms</b> <br>
          <a href="https://ieeexplore.ieee.org/document/8683847">2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2019)</a><br>
          [<a href="https://arxiv.org/abs/1810.13084">arXiv</a>] [code: AccGossip]<br>
          <br>

          [82] Filip Hanzely and Peter Richtárik<br>
          <b>Accelerated coordinate descent with arbitrary sampling and best rates for minibatches<br>
          </b><a href="http://proceedings.mlr.press/v89/hanzely19a.html">Proceedings of the 22nd Int. Conf. on Artificial Intelligence and Statistics, PMLR 89:304-312, 2019</a><br>
          [<a href="https://arxiv.org/abs/1809.09354">arXiv</a>] [<a
            href="posters/Poster-ACD.pdf">poster</a>] [code: ACD]<br>
          <br>

          [81] Samuel Horváth and Peter Richtárik<br>
          <b> Nonconvex variance reduced optimization with arbitrary
            sampling</b> <br>
          <span class="important"><a
              href="http://proceedings.mlr.press/v97/horvath19a.html">Proceedings of the 36th International Conference on Machine Learning,
              PMLR 97:2781-2789, 2019</a><br>
            Horváth: Best DS<sup>3</sup> Poster Award, Paris, 2018 </span>
          (<a href="http://www.ds3-datascience-polytechnique.fr/posters/">link</a>)<br>
          [<a href="https://arxiv.org/abs/1809.04146">arXiv</a>] [<a
href="http://www.ds3-datascience-polytechnique.fr/wp-content/uploads/2018/06/DS3-342.pdf">poster</a>]
          [code: SVRG, SAGA, SARAH] <br>
          <br>

          [80] Filip Hanzely, Konstantin Mishchenko and Peter Richtárik<br>
          <b>SEGA: Variance reduction via gradient sketching</b> <br>
          <a
href="https://papers.nips.cc/paper/7478-sega-variance-reduction-via-gradient-sketching">Advances in Neural Information Processing Systems 31:2082-2093, 2018
          </a> <br>
          [<a href="https://arxiv.org/abs/1809.03054">arXiv</a>] [<a
            href="posters/Poster-SEGA.pdf">poster</a>] [<a
            href="talks/Talk-SEGA.pdf">slides</a>] [code: SEGA] [video:
          <a href="https://www.youtube.com/watch?v=n1ELUphtbgg"><img
              style="border: 0px solid;" alt="YouTube"
src="file:///Users/richtap/Documents/WORK/GitHub/My%20GitHub%20Pages%20website/imgs/youtube.png"
              width="17" height="15" border="0"></a>]<br>
          <br>

          [79] Filip Hanzely, Peter Richtárik and Lin Xiao<br>
          <b>Accelerated Bregman proximal gradient methods for relatively smooth convex optimization</b><br>
          <a href="https://link.springer.com/article/10.1007/s10589-021-00273-8">Computational Optimization and Applications 79:405–440, 2021</a> <br>
          [<a href="https://arxiv.org/abs/1808.03045">arXiv</a>] [code:
          <a href="https://github.com/Microsoft/accbpg">ABPG, ABDA</a>]<br>
          <br>

          [78] Jakub Mareček, Peter Richtárik and Martin Takáč <br>
          <b> Matrix completion under interval uncertainty: highlights</b><br>
          Lecture Notes in Computer Science, ECML-PKDD 2018<br>
          [<a href="papers/MACO-highlights.pdf">pdf</a>]<br>
          <br>

          [77] Nicolas Loizou and Peter Richtárik <br>
          <b>Accelerated gossip via stochastic heavy ball method</b><br>
          <a href="https://proceedings.allerton.csl.illinois.edu/media/files/0190.pdf#search=%22Richtarik%22">56th Annual Allerton Conference on Communication, Control, and Computing, 927-934, 2018</a><br>
          <span class="important">Press coverage</span> <a href="https://discovery.kaust.edu.sa/en/article/841/accelerating-the-grapevine-effect">[KAUST Discovery</a>] <br>
          [<a href="https://arxiv.org/abs/1809.08657">arXiv</a>] [<a href="posters/Poster-SHB_for_gossip.pdf">poster</a>]<br>
          <br>

          [76] Adel Bibi, Alibek Sailanbayev, Bernard Ghanem, Robert Mansel Gower and Peter Richtárik <br>
          <b> Improving SAGA via a probabilistic interpolation with gradient descent</b> <br>
          [<a href="https://arxiv.org/abs/1806.05633">arXiv</a>] [code: SAGD]<br>
          <br>

          [75] Aritra Dutta, Filip Hanzely and Peter Richtárik <br>
          <b>A nonconvex projection method for robust PCA</b><br>
          <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/3959">The Thirty-Third AAAI Conference on Artificial Intelligence, 2019</a> (AAAI-19)<br>
          [<a href="https://arxiv.org/abs/1805.07962">arXiv</a>]<br>
          <br>

          [74] Robert M. Gower, Peter Richtárik and Francis Bach <br>
          <b>Stochastic quasi-gradient methods: variance reduction via Jacobian sketching </b><br>
          <a href="https://link.springer.com/article/10.1007/s10107-020-01506-0"> Mathematical Programming 188:135–192, 2021 </a> <br>
          [<a href="https://arxiv.org/abs/1805.02632">arXiv</a>] [<a
            href="talks/2018%20Voronovo%20-%20TALK%201.pdf">slides</a>]
          [code: <a href="https://github.com/gowerrobert/StochOpt.jl">JacSketch</a>]
          [video: <a
href="https://youtu.be/gjgEck0zU7w?list=PLgKuh-lKre13E9dXSsif4KsGoFp4bjubd"><img
              style="border: 0px solid;" alt="YouTube"
              src="imgs/youtube.png" width="17" height="15" border="0"></a>]<br>
          <br>

          [73] Aritra Dutta, Xin Li and Peter Richtárik <br>
          <b>Weighted low-rank approximation of matrices and background modeling</b><br>
          [<a href="https://arxiv.org/abs/1804.06252">arXiv</a>]<br>
          <br>

          [72] Filip Hanzely and Peter Richtárik <br>
          <b>Fastest rates for stochastic mirror descent methods</b><br>
          <a href="https://link.springer.com/article/10.1007/s10589-021-00284-5">Computational Optimization and Applications 79:717–766, 2021</a><br>
          [<a href="https://arxiv.org/abs/1803.07374">arXiv</a>]<br>
          <br>

          [71] Lam M. Nguyen, Phuong Ha Nguyen, Marten van Dijk, Peter Richtárik, Katya Scheinberg and Martin Takáč <br>
          <b>SGD and Hogwild! convergence without the bounded gradients assumption</b> <br>
          <a href="http://proceedings.mlr.press/v80/nguyen18c.html">Proceedings of The 35th International Conference on Machine Learning, PMLR 80:3750-3758, 2018</a><br>
          [<a href="https://arxiv.org/abs/1802.03801">arXiv</a>]<br>
          <br>

          [70] Robert M. Gower, Filip Hanzely, Peter Richtárik and Sebastian Stich <b><br>
            Accelerated stochastic matrix inversion: general theory and speeding up BFGS rules for faster second-order optimization</b><br>
          <a href="https://papers.nips.cc/paper/7434-accelerated-stochastic-matrix-inversion-general-theory-and-speeding-up-bfgs-rules-for-faster-second-order-optimization">Advances in Neural Information Processing Systems 31:1619-1629, 2018</a><br>
          [<a href="https://arxiv.org/abs/1802.04079">arXiv</a>] [<a href="posters/Poster-ASBFGS.pdf">poster</a>] [code: ABFGS]<br>
          <br>

          [69] Nikita Doikov and Peter Richtárik <br>
          <b>Randomized block cubic Newton method</b><br>
          <a href="http://proceedings.mlr.press/v80/doikov18a.html">Proceedings of The 35th International Conference on Machine Learning, PMLR 80:1290-1298, 2018</a><br>
          <span class="important">Doikov: Best Talk Award, "Control, Information and Optimization", Voronovo, Russia, 2018</span><br>
          [<a href="https://arxiv.org/abs/1802.04084">arXiv</a>] [<a
            href="bib/69.bib">bib</a>] [code: RBCN]<br>
          <br>

          [68] Dmitry Kovalev, Eduard Gorbunov, Elnur Gasanov and Peter Richtárik<br>
           <b>Stochastic spectral and conjugate descent methods</b><br>
          <a href="https://papers.nips.cc/paper/7596-stochastic-spectral-and-conjugate-descent-methods">Advances in Neural Information Processing Systems 31:3358-3367, 2018</a><br>
          [<a href="https://arxiv.org/abs/1802.03703">arXiv</a>] [<a
            href="posters/Poster-SSCD.pdf">poster</a>] [code: SSD,
          SconD, SSCD, mSSCD, iSconD, iSSD] <br>
          <br>

          [67] Radoslav Harman, Lenka Filová and Peter Richtárik <b><br>
            A randomized exchange algorithm for computing optimal approximate designs of experiments</b><br>
          <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2018.1546588">Journal of the American Statistical Association</a><br>
          [<a href="https://arxiv.org/abs/1801.05661">arXiv</a>] [code: REX, <a href="https://optdesign.shinyapps.io/od_rex/">OD_REX</a>,
          <a href="https://optdesign.shinyapps.io/mvee_rex/">MVEE_REX</a>]<br>
          <br>

          [66] Ion Necoara, Andrei Patrascu and Peter Richtárik <br>
          <b>Randomized projection methods for convex feasibility
            problems: conditioning and convergence rates</b><br>
          <a href="https://epubs.siam.org/doi/abs/10.1137/18M1167061">SIAM Journal on Optimization 29(4):2814–2852, 2019</a><br>
          [<a href="https://arxiv.org/abs/1801.04873">arXiv</a>] [<a href="talks/TALK-ISMP2018-StochFeas.pdf">slides</a>]
          <br> <br>


          <h1> Prepared in 2017</h1>

          [65] Nicolas Loizou and Peter Richtárik<br>
          <b>Momentum and stochastic momentum for stochastic gradient, Newton, proximal point and subspace descent methods</b><br>
            <a href="http://link.springer.com/article/10.1007/s10589-020-00220-z">Computational Optimization and Applications 77(3):653-710, 2020</a> <br>
          [<a href="https://arxiv.org/abs/1712.09677">arXiv</a>]<br>
          <br>

          [64] Aritra Dutta and Peter Richtárik<br>
          <b>Online and batch supervised background estimation via L1
            regression</b><br>
          <a href="https://ieeexplore.ieee.org/document/8659272">IEEE
            Winter Conference on Applications in Computer Vision, 2019</a><br>
          [<a href="https://arxiv.org/abs/1712.02249">arXiv</a>]<br>
          <br>

          [63] Nicolas Loizou and Peter Richtárik<br>
          <b>Linearly convergent stochastic heavy ball method for minimizing generalization error</b><br>
          NIPS Workshop on Optimization for Machine Learning, 2017<br>
          [<a href="https://arxiv.org/abs/1710.10737">arXiv</a>] [<a href="posters/Poster-SHB.pdf">poster</a>]<br>
          <br>

          [62] Dominik Csiba and Peter Richtárik<br>
          <b>Global convergence of arbitrary-block gradient methods for generalized Polyak-Łojasiewicz functions</b><br>
          [<a href="https://arxiv.org/abs/1709.03014">arXiv</a>]<br>
          <br>

          [61] Ademir Alves Ribeiro and Peter Richtárik<br>
          <b>The complexity of primal-dual fixed point methods for ridge regression</b><br>
          <a href="https://www.sciencedirect.com/science/article/pii/S0024379518303434">Linear Algebra and its Applications 556:342-372, 2018</a><br>
          [<a href="https://arxiv.org/abs/1801.06354">arXiv</a>]<br>
          <br>

          [60] Matthias J. Ehrhardt, Pawel Markiewicz, Antonin Chambolle, Peter Richtárik, Jonathan Schott and Carola-Bibiane Schoenlieb<br>
          <b>Faster PET reconstruction with a stochastic primal-dual hybrid gradient method</b><br>
          <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10394/103941O/Faster-PET-reconstruction-with-a-stochastic-primal-dual-hybrid-gradient/10.1117/12.2272946.short?SSO=1">Proceedings of SPIE,</a><a
href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10394/103941O/Faster-PET-reconstruction-with-a-stochastic-primal-dual-hybrid-gradient/10.1117/12.2272946.short?SSO=1">
            Wavelets and Sparsity XVII, Volume 10394, pages 1039410-1 -
            1039410-11, 2017</a><br>
          [<a href="papers/PET-SPDHGM.pdf">pdf</a>] [<a href="posters/Poster-SPDHG.pdf">poster</a>] [code: <a href="https://github.com/mehrhardt/spdhg">SPDHG</a>] [video: <a href="https://www.youtube.com/watch?v=iZc2eFqS2l4"><img style="border: 0px solid;" alt="YouTube" src="imgs/youtube.png" width="17" height="15" border="0"></a>]<br>
          <br>

          [59] Aritra Dutta, Xin Li and Peter Richtárik<br>
          <b>A batch-incremental video background estimation model using weighted low-rank approximation of matrices</b><br>
          <a href="https://ieeexplore.ieee.org/document/8265427">IEEE International Conference on Computer Vision (ICCV) Workshops, 2017</a><br>
          [<a href="https://arxiv.org/abs/1707.00281">arXiv</a>] [code: inWLR]<br>
          <br>

          [58] Filip Hanzely, Jakub Konečný, Nicolas Loizou, Peter
          Richtárik and Dmitry Grishchenko<br>
          <b>Privacy preserving randomized gossip algorithms</b><br>
          [<a href="https://arxiv.org/abs/1706.07636">arXiv</a>] [<a
            href="talks/TALK-private-gossip.pdf">slides</a>]<br>
          <br>

          [57] Antonin Chambolle, Matthias J. Ehrhardt, Peter Richtárik
          and Carola-Bibiane Schoenlieb<br>
          <b>Stochastic primal-dual hybrid gradient algorithm with
            arbitrary sampling and imaging applications</b><br>
          <a href="https://epubs.siam.org/doi/abs/10.1137/17M1134834">SIAM Journal on Optimization 28(4):2783-2808, 2018</a><br>
          [<a href="https://arxiv.org/abs/1706.04957">arXiv</a>] [<a
            href="talks/TALK-SPDHG.pdf">slides</a>] [<a
            href="posters/Poster-SPDHG.pdf">poster</a>] [code: <a
            href="https://github.com/mehrhardt/spdhg">SPDHG</a>] [video:
          <a href="https://www.youtube.com/watch?v=iZc2eFqS2l4"><img
              style="border: 0px solid;" alt="YouTube"
              src="imgs/youtube.png" width="17" height="15" border="0"></a>]<br>
          <br>

          [56] Peter Richtárik and Martin Takáč<br>
          <b>Stochastic reformulations of linear systems: algorithms and convergence theory</b><br>
          <a href="https://epubs.siam.org/doi/abs/10.1137/18M1179249">SIAM Journal on Matrix Analysis and Applications 41(2):487–524, 2020</a><br>
          [<a href="https://arxiv.org/abs/1706.01108">arXiv</a>] [<a href="talks/TALK-2017-FieldsInstitute-AN70.pdf">slides</a>] [code: basic, parallel and accelerated methods] <br>
          <br>

          [55] Mojmír Mutný and Peter Richtárik<br>
          <b>Parallel stochastic Newton method</b><br>
          <a href="http://www.global-sci.org/jcm/">Journal of Computational Mathematics 36(3):404-425, 2018</a><br>
          [<a href="https://arxiv.org/abs/1705.02005">arXiv</a>] [code: PSNM]

          <br>
          <br>



<h1> Prepared in 2016</h1>


[54] Robert M. Gower and Peter Richtárik<br>
<b>Linearly convergent randomized iterative methods for computing the pseudoinverse</b><br>
[<a href="https://arxiv.org/abs/1612.06255">arXiv</a>] <br>
<br>

[53] Jakub Konečný and Peter Richtárik<br>
<b>Randomized distributed mean estimation: accuracy vs communication<br>
</b><a href="https://www.frontiersin.org/articles/10.3389/fams.2018.00062/abstract">Frontiers in Applied Mathematics and Statistics 2018</a><br>
<span class="important">Federated Learning Paper</span><br>
[<a href="https://arxiv.org/abs/1611.07555">arXiv</a>] <br>
<br>

[52] Jakub Konečný, H. Brendan McMahan, Felix Yu, Peter Richtárik, Ananda Theertha Suresh and Dave Bacon<br>
<b>Federated learning: strategies for improving communication efficiency</b><br>
<a href="https://nips.cc/Conferences/2016/Schedule?showEvent=6250">NIPS Private Multi-Party Machine Learning Workshop, 2016</a><br>
<span class="important">Federated Learning Paper <a href="https://research.googleblog.com/2017/04/federated-learning-collaborative.html">link</a></span>
[selected press coverage: <a href="https://www.theverge.com/2017/4/10/15241492/google-ai-user-data-federated-learning">The Verge</a> - <a
href="https://qz.com/952986/google-goog-may-be-using-your-phones-keyboard-to-train-its-massive-ai-brain/">Quartz</a>
- <a
href="https://motherboard.vice.com/en_us/article/google-thinks-it-can-solve-artificial-intelligences-privacy-problem">Vice</a>
<a
href="http://www.cbronline.com/news/mobility/smartphones/google-wants-make-android-smarter-ai-algorithms-phone/">CBR</a>
- <a
href="http://www.androidauthority.com/google-machine-learning-privacy-federated-learning-762978/">Android Authority</a>]<br>
[<a href="https://arxiv.org/abs/1610.05492">arXiv</a>] [bib] [<a
href="posters/Poster-FederatedLearningComm.pdf">poster</a>]<br>
<br>

[51] Jakub Konečný, H. Brendan McMahan, Daniel Ramage and
Peter Richtárik<br>
<b>Federated optimization: distributed machine learning for
on-device intelligence</b><br>
<span class="important">Federated Learning Paper </span><span
class="important"><a
href="https://research.googleblog.com/2017/04/federated-learning-collaborative.html">link</a></span>
[selected press coverage: <a
href="https://www.theverge.com/2017/4/10/15241492/google-ai-user-data-federated-learning">The Verge</a> - <a
href="https://qz.com/952986/google-goog-may-be-using-your-phones-keyboard-to-train-its-massive-ai-brain/">Quartz</a>
- <a
href="https://motherboard.vice.com/en_us/article/google-thinks-it-can-solve-artificial-intelligences-privacy-problem">Vice</a>
<a
href="http://www.cbronline.com/news/mobility/smartphones/google-wants-make-android-smarter-ai-algorithms-phone/">CBR</a>
- <a
href="http://www.androidauthority.com/google-machine-learning-privacy-federated-learning-762978/">Android Authority</a>]<br>
[<a href="https://arxiv.org/abs/1610.02527">arXiv</a>] [bib]<br>
<br>

[50] Nicolas Loizou and Peter Richtárik <br>
<b>A new perspective on randomized gossip algorithms</b><br>
<a href="http://ieeexplore.ieee.org/document/7905880/">IEEE Global Conference on Signal and Information Processing (GlobalSIP), 440-444, 2016</a><br>
[<a href="http://arxiv.org/abs/1610.04714">arXiv</a>] [bib]<br>
<br>

[49] Sashank J. Reddi, Jakub Konečný, Peter Richtárik, Barnabás Póczos, Alex Smola<br>
<b>AIDE: fast and communication efficient distributed optimization</b><br>
[<a href="https://arxiv.org/abs/1608.06879">arXiv</a>] [<a href="posters/Poster-AIDE.pdf">poster</a>]<br>
<br>

[48] Dominik Csiba and Peter Richtárik<br>
<b>Coordinate descent face-off: primal or dual?</b><br>
<a href="http://proceedings.mlr.press/v83/csiba18a">Proceedings of Algorithmic Learning Theory, PMLR 83:246-267, 2018</a><br>
[<a href="https://arxiv.org/abs/1605.08982">arXiv</a>] [<a href="bib/48.bib">bib</a>]<br>
<br>

[47] Olivier Fercoq and Peter Richtárik<br>
<b>Optimization in high dimensions via accelerated, parallel and proximal coordinate descent</b><br>
<a href="http://epubs.siam.org/doi/abs/10.1137/16M1085905">SIAM Review 58(4):739-771, 2016</a><br>
<span class="important">SIAM SIGEST Award</span> <br>
[<a href="http://arxiv.org/abs/1312.5799">arXiv</a>] [bib]<br>
<br>

[46] Robert M. Gower, Donald Goldfarb and Peter Richtárik<br>
<b>Stochastic block BFGS: squeezing more curvature out of data</b><br>
<a href="http://proceedings.mlr.press/v48/gower16.html">Proceedings   of the 33rd International Conference on Machine Learning,
PMLR 48:1869-1878, 2016</a><br>
[<a href="http://arxiv.org/abs/1603.09649">arXiv</a>] [<a
href="bib/46.bib">bib</a>] [<a
href="posters/Poster-StochBFGS.pdf">poster</a>]<br>
<br>

[45] Dominik Csiba and Peter Richtárik<br>
<b>Importance sampling for minibatches</b><br>
<a href="http://jmlr.org/papers/v19/16-241.html">Journal of
Machine Learning Research 19(27):1-21, 2018</a><br>
[<a href="http://arxiv.org/abs/1602.02283">arXiv</a>] [<a
href="bib/45.bib">bib</a>]<br>
<br>

[44] Robert M. Gower and Peter Richtárik<br>
<b>Randomized quasi-Newton updates are linearly convergent
matrix inversion algorithms</b><br>
<a href="http://epubs.siam.org/doi/pdf/10.1137/16M1062053">SIAM Journal on Matrix Analysis and Applications 38(4):1380-1409,
2017</a><br>
<span class="important">Most Downloaded SIMAX Paper <a
href="http://epubs.siam.org/action/showMostReadArticles?journalCode=sjmael&amp;">(6th place: 2018)</a></span><a
href="http://epubs.siam.org/action/showMostReadArticles?journalCode=sjmael&amp;">
</a><span class="important"></span><br>
[<a href="http://arxiv.org/abs/1602.01768">arXiv</a>]
[code: SIMI, RBFGS, AdaRBFGS, ...]
<br>
<br>




<h1>Prepared in 2015 </h1>

<p>[43] Zeyuan Allen-Zhu, Zheng Qu, Peter Richtárik and Yang Yuan<br>
<b>Even faster accelerated coordinate descent using non-uniform sampling</b><br>
<a href="http://proceedings.mlr.press/v48/allen-zhuc16.html">Proceedings of the 33rd International Conference on Machine Learning, PMLR 48:1110-1119, 2016</a><br>
[<a href="http://arxiv.org/abs/1512.09103">arXiv</a>] [<a  href="bib/43.bib">bib</a>] [code: NU_ACDM]<br>
</p>

<p>[42] Robert M. Gower and Peter Richtárik<br>
<b>Stochastic dual ascent for solving linear systems</b><br>
[<a href="http://arxiv.org/abs/1512.06890">arXiv</a>] [code:
SDA] [video: <a
href="https://www.youtube.com/watch?v=RbkhWrTbrKs"><img
style="border: 0px solid;" alt="YouTube"
src="imgs/youtube.png" width="17" height="15" border="0"></a>]
<a href="http://arxiv.org/abs/1512.04039"><br>
</a></p>

<p>[41] Chenxin Ma, Jakub Konečný, Martin Jaggi, Virginia
Smith, Michael I Jordan, P. Richtárik and Martin Takáč <b>Distributed optimization with arbitrary local solvers</b><br>
<a
href="http://www.tandfonline.com/doi/full/10.1080/10556788.2016.1278445">Optimization Methods and Software 32(4):813-848, 2017</a><br>
<span class="important">Most-Read Paper, Optimization Methods and Software, 2017</span><br>
[<a href="http://arxiv.org/abs/1512.04039">arXiv</a>] [code:
<a href="https://github.com/gingsmith/cocoa">CoCoA+</a>]<a
href="http://arxiv.org/abs/1507.08322"><br>
</a></p>

<p>[40] Martin Takáč, Peter Richtárik and Nathan Srebro <br>
<b>Distributed mini-batch SDCA</b><br>
To appear in: Journal of Machine Learning Research<br>
[<a href="http://arxiv.org/abs/1507.08322">arXiv</a>] <br>
</p>

<p>[39] Robert M. Gower and Peter Richtárik<br>
<b>Randomized iterative methods for linear systems</b> <br>
<em> </em><a
href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">SIAM
Journal on Matrix Analysis and Applications 36(4):1660-1690, 2015</a><span
class="important"><br>
Most Downloaded SIMAX Paper <a
href="http://epubs.siam.org/action/showMostReadArticles?journalCode=sjmael&amp;">(1st place: 2017-2020)</a></span>
<br><span class="important">Gower: 18th IMA Leslie Fox Prize (2nd Prize), 2017</span>
<a  href="http://people.maths.ox.ac.uk/wathen/fox/shortlist.php">link</a><br>
[<a href="http://arxiv.org/abs/1506.03296">arXiv</a>]
[<a href="talks/TALK-2015-09-Linear_Systems.pdf">slides</a>] <br>
</p>

<p>[38] Dominik Csiba and Peter Richtárik <br>
<b>Primal method for ERM with flexible mini-batching schemes and non-convex losses</b><br>
[<a href="http://arxiv.org/abs/1506.02227">arXiv</a>] [code:
dfSDCA]<br>
</p>

<p>[37] Jakub Konečný, Jie Liu, Peter Richtárik and Martin
Takáč<br>
<b>Mini-batch semi-stochastic gradient descent in the
proximal setting</b><br>
<em> </em><a
href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=7347336&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7347336">IEEE
Journal of Selected Topics in Signal Processing 10(2): 242-255,
2016</a><br>
[<a href="http://arxiv.org/abs/1504.04407">arXiv</a>] [code:
mS2GD]<br>
</p>

<p>[36] Rachael Tappenden, Martin Takáč and Peter Richtárik<br>
<b>On the complexity of parallel coordinate descent</b><br>
<a
href="http://www.tandfonline.com/doi/full/10.1080/10556788.2017.1392517">Optimization   Methods and Software 33(2):372-395, 2018</a><br>
[<a href="http://arxiv.org/abs/1503.03033">arXiv</a>]<br>
</p>

<p>[35] Dominik Csiba, Zheng Qu and Peter Richtárik<br>
<b>Stochastic dual coordinate ascent with adaptive
probabilities</b><br>
<a
href="http://jmlr.org/proceedings/papers/v37/csiba15.html">Proceedings
of the 32nd International Conference on Machine Learning,
PMLR 37:674-683, 2015</a><br>
<span class="important">Csiba: Best Contribution Award (2nd
Place), Optimization and Big Data 2015<br>
Implemented in Tensor Flow<br>
</span> [<a href="http://arxiv.org/abs/1502.08053">arXiv</a>]
[<a href="bib/35.bib">bib</a>] [<a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/posters/Csiba.pdf">poster</a>]
[code: AdaSDCA and AdaSDCA+]</p>

<p>[34] Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I.
Jordan, Peter Richtárik and Martin Takáč<br>
<b>Adding vs. averaging in distributed primal-dual
optimization</b><br>
<a href="http://jmlr.org/proceedings/papers/v37/mab15.html">Proceedings of the 32nd International Conference on Machine Learning,
PMLR 37:1973-1982, 2015</a><br>
<span class="important"> Smith: 2015 MLconf Industry Impact
Student Research Award </span> <a
href="http://mlconf.com/mlconf-industry-impact-student-research-award-winners/">link</a><br>
<span class="important"></span><span class="important">CoCoA+   is now the default linear optimizer in Tensor Flow</span><span
class="important"> </span> <a
href="https://github.com/tensorflow/tensorflow/commits/master/tensorflow/contrib/linear_optimizer">link</a><br>
[<a href="http://arxiv.org/abs/1502.03508">arXiv</a>] [<a
href="bib/34.bib">bib</a>] [<a
href="posters/Poster-CoCoA+.pdf">poster</a>] [code: <a
href="https://github.com/gingsmith/cocoa">CoCoA+</a>]<br>
</p>

<p>[33] Zheng Qu, Peter Richtárik, Martin Takáč and Olivier
Fercoq<br>
<b>SDNA: Stochastic dual Newton ascent for empirical risk
minimization</b><br>
<a href="http://jmlr.org/proceedings/papers/v48/qub16.html">Proceedings
of the 33rd International Conference on Machine Learning, </a><a
href="http://jmlr.org/proceedings/papers/v48/qub16.html">PMLR 48:1823-1832, 2016</a><br>
[<a href="http://arxiv.org/abs/1502.02268">arXiv</a>] [<a
href="bib/33.bib">bib</a>] [<a
href="talks/TALK-SDNA-ISMP2015.pdf">slides</a>] [<a
href="posters/Poster-SDNA.pdf">poster</a>] [code: SDNA]</p>






<h1>Prepared in 2014 </h1>

<p>[32] Zheng Qu and Peter Richtárik<br>
<b>Coordinate descent with arbitrary sampling II: expected
separable overapproximation</b> <br>
<a
href="http://www.tandfonline.com/doi/full/10.1080/10556788.2016.1190361">Optimization
Methods and Software 31(5):858-884, 2016</a><i><br>
</i>[<a href="http://arxiv.org/abs/1412.8063">arXiv</a>]<i><br>
</i></p>
<p>[31] Zheng Qu and Peter Richtárik<br>
<b>Coordinate descent with arbitrary sampling I: algorithms
and complexity</b><br>
<a
href="http://www.tandfonline.com/doi/full/10.1080/10556788.2016.1190360">Optimization
Methods and Software 31(5):829-857, 2016</a><br>
[<a href="http://arxiv.org/abs/1412.8060">arXiv</a>] [code:
ALPHA]<br>
</p>
<p>[30] Jakub Konečný, Zheng Qu and Peter Richtárik<br>
<b>Semi-stochastic coordinate descent</b><br>
<a
href="http://www.tandfonline.com/doi/full/10.1080/10556788.2017.1298596">Optimization
Methods and Software 32(5):993-1005, 2017</a><br>
[<a href="http://arxiv.org/abs/1412.6293">arXiv</a>] [code:
S2CD]<br>
</p>
<p>[29] Zheng Qu, Peter Richtárik and Tong Zhang<br>
<b>Quartz: Randomized dual coordinate ascent with arbitrary
sampling</b><br>
<a
href="http://papers.nips.cc/paper/5926-quartz-randomized-dual-coordinate-ascent-with-arbitrary-sampling">Advances
in Neural Information Processing Systems 28:865-873, 2015</a><br>
[<a href="http://arxiv.org/abs/1411.5873">arXiv</a>] [<a
href="talks/TALK-Quartz.ppsx">slides</a>] [code: QUARTZ]
[video: <a href="https://events.yandex.ru/lib/talks/4294/"><img
style="border: 0px solid;" alt="YouTube"
src="imgs/youtube.png" width="17" height="15" border="0"></a>]<br>
</p>
<p>[28] Jakub Konečný, Jie Liu, Peter Richtárik and Martin
Takáč<br>
<b>mS2GD: Mini-batch semi-stochastic gradient descent in the
proximal setting</b><br>
<em> </em><a href="http://opt-ml.org/papers.html">NIPS
Workshop on Optimization for Machine Learning, 2014</a><br>
[<a href="http://arxiv.org/abs/1410.4744">arXiv</a>] [<a
href="posters/Poster-mS2GD.pdf">poster</a>] [code: mS2GD]</p>
<p>[27] Jakub Konečný, Zheng Qu and Peter Richtárik<br>
<b>S2CD: Semi-stochastic coordinate descent</b><br>
<em> </em><a href="http://opt-ml.org/papers.html">NIPS
Workshop on Optimization for Machine Learning, 2014</a><br>
[<a href="papers/S2CD.pdf">pdf</a>] [<a
href="posters/Poster-S2CD.pdf">poster</a>] [code: S2CD]</p>
<p>[26] Jakub Konečný and Peter Richtárik<br>
<b>Simple complexity analysis of simplified direct search</b><br>
[<a href="http://arxiv.org/abs/1410.0390">arXiv</a>] [<a
href="talks/TALK-SDS-Slovak.ppsx">slides in Slovak</a>]
[code: SDS]</p>
<p>[25] Jakub Mareček, Peter Richtárik and Martin Takáč<br>
<b>Distributed block coordinate descent for minimizing
partially separable functions</b><br>
<a href="http://www.springer.com/us/book/9783319176888">Numerical
Analysis and Optimization, Springer Proceedings in Math. and
Statistics 134:261-288, 2015</a><br>
[<a href="http://arxiv.org/abs/1406.0238">arXiv</a>] <br>
</p>

<p>[24] Olivier Fercoq, Zheng Qu, Peter Richtárik and Martin
Takáč<br>
<b>Fast distributed coordinate descent for minimizing
non-strongly convex losses</b><br>
<em> </em><a
href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6958862&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Ftp%3D%26arnumber%3D6958862">2014
IEEE    International Workshop on Machine Learning for Signal
Processing (MLSP), 2014 </a><br>
[<a href="http://arxiv.org/abs/1405.5300v2">arXiv</a>] [<a
href="posters/Poster-Hydra2.pdf">poster</a>] [code:
Hydra^2]</p>

<p>[23] Duncan Forgan and Peter Richtárik<br>
<b>On optimal solutions to planetesimal growth models</b><br>
Technical Report ERGO 14-002, 2014<br>
[<a href="papers/planetesimals.pdf">pdf</a>]</p>

<p>[22] Jakub Mareček, Peter Richtárik and Martin Takáč <br>
<b>Matrix completion under interval uncertainty</b><br>
<a
href="http://www.sciencedirect.com/science/article/pii/S0377221716305513">European
Journal      of Operational Research 256(1):35-42, 2017</a><br>
[<a href="http://arxiv.org/abs/1408.2467">arXiv</a>] [code:
MACO]<br>
</p>

<h1>Prepared in 2013 </h1>


<p>[21] Olivier Fercoq and Peter Richtárik <br>
<b>Accelerated, Parallel and PROXimal coordinate descent</b><br>
<a href="http://epubs.siam.org/doi/abs/10.1137/130949993">SIAM
Journal      on Optimization 25(4):1997-2023, 2015</a><br>
<span class="important">Fercoq: 17th IMA Leslie Fox Prize
(Second Prize), 2015</span><br>
<span class="important">2nd Most Downloaded SIOPT Paper <a
href="http://epubs.siam.org/action/showMostReadArticles?journalCode=sjope8">(Aug
2016  - now)</a></span><br>
[<a href="http://arxiv.org/abs/1312.5799">arXiv</a>] [<a
href="posters/Poster-APPROX.pdf">poster</a>] [code:
APPROX] [video: <a
href="https://www.youtube.com/watch?v=0sHOfqhCZw0"><img
style="border: 0px solid;" alt="YouTube"
src="imgs/youtube.png" width="17" height="15" border="0"></a>]<br>
</p>

<p>[20] Jakub Konečný and Peter Richtárik<br>
<b>Semi-stochastic gradient descent methods</b><br>
<a href="https://doi.org/10.3389/fams.2017.00009">Frontiers
in Applied Mathematics and Statistics </a><a
href="https://doi.org/10.3389/fams.2017.00009">3:9, 2017</a><br>
[<a href="http://arxiv.org/abs/1312.1666">arXiv</a>] [<a
href="posters/Poster-S2GD.pdf">poster</a>] [<a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/talks/TALK-2014-11-S2GD.ppsx">slides</a>]
[code: S2GD and S2GD+]<br>
</p>

<p>[19] Peter Richtárik and Martin Takáč<br>
<b>On optimal probabilities in stochastic coordinate descent
methods</b><br>
<a style="font-style: italic;"
href="https://doi.org/10.3389/fams.2017.00009"><em> </em></a><a
href="http://link.springer.com/article/10.1007%2Fs11590-015-0916-1">Optimization  Letters</a><a
href="http://link.springer.com/article/10.1007%2Fs11590-015-0916-1">
10(6):1233-1243, 2016</a><br>
[<a href="http://arxiv.org/abs/1310.3438">arXiv</a>] [<a
href="posters/Poster-NSync.pdf">poster</a>] [code: NSync]<br>
</p>

<p>[18] Peter Richtárik and Martin Takáč<br>
<b>Distributed coordinate descent method for learning with
big data</b><br>
<a href="http://www.jmlr.org/papers/v17/15-001.html">Journal
of Machine Learning Research 17(75):1-25, 2016</a><br>
[<a href="http://arxiv.org/abs/1310.2059">arXiv</a>] [<a
href="posters/Poster-Hydra.pdf">poster</a>] [code: Hydra]<br>
</p>

<p>[17] Olivier Fercoq and Peter Richtárik<br>
<b>Smooth minimization of nonsmooth functions with parallel
coordinate descent methods</b><br>
<a
href="https://link.springer.com/chapter/10.1007/978-3-030-12119-8_4">Springer  Proceedings in Mathematics and Statistics 279:57-96, 2019</a><br>
[<a href="http://arxiv.org/abs/1309.5885">arXiv</a>] [code:
SPCDM]<br>
</p>

<p>[16] Rachael Tappenden, Peter Richtárik and Burak Buke<br>
<b>Separable approximations and decomposition methods for
the augmented Lagrangian</b><br>
<a
href="http://www.tandfonline.com/doi/abs/10.1080/10556788.2014.966824?journalCode=goms20">Optimization
Methods   and Software 30(3):643-668, 2015</a><br>
[<a href="http://arxiv.org/abs/1308.6774">arXiv</a>]<br>
</p>

<p>[15] Rachael Tappenden, Peter Richtárik and Jacek Gondzio<br>
<b>Inexact coordinate descent: complexity and
preconditioning</b><br>
<a
href="http://link.springer.com/article/10.1007/s10957-016-0867-4?wt_mc=internal.event.1.SEM.ArticleAuthorOnlineFirst">Journal
of    Optimization Theory and Applications 170(1):144-176, 2016</a><br>
[<a href="http://arxiv.org/abs/1304.5530">arXiv</a>] [<a
href="posters/ICD.pdf">poster</a>] [code: ICD]<br>
</p>

<p>[14] Martin Takáč, Selin Damla Ahipasaoglu, Ngai-Man Cheung
and Peter Richtárik<br>
<b>TOP-SPIN: TOPic discovery via Sparse Principal component
INterference</b><br>
<a
href="https://link.springer.com/chapter/10.1007/978-3-030-12119-8_8">Springer  Proceedings in Mathematics and Statistics 279:157-180,
2019</a><br>
[<a href="http://arxiv.org/abs/1311.1406">arXiv</a>] [<a
href="posters/Poster-TOPSPIN.pdf">poster</a>] [code:
TOP-SPIN]</p>

<p>[13] Martin Takáč, Avleen Bijral, Peter Richtárik and
Nathan Srebro<br>
<b>Mini-batch primal and dual methods for SVMs</b><br>
<a
href="http://jmlr.org/proceedings/papers/v28/takac13-supp.pdf">Proceedings
of  the 30th International Conference on Machine Learning,
2013</a> <br>
[<a href="https://arxiv.org/abs/1303.2314">arXiv</a>] [<a
href="posters/Poster-Minibatch-ICML2013.pdf">poster</a>]
[code: minibatch SDCA and minibatch Pegasos]</p>



<h1>Prepared in 2012 or earlier</h1>


<p> [12] Peter Richtárik, Majid Jahani, Martin Takáč and Selin Damla Ahipasaoglu<br>
<b>Alternating maximization: unifying framework for 8 sparse PCA formulations and efficient parallel codes</b><br>
<a href="https://link.springer.com/article/10.1007/s11081-020-09562-3">Optimization and Engineering 22:1493--1519, 2021</a> <br>
[<a href="http://arxiv.org/abs/1212.4137">arXiv</a>] [code: <a href="https://code.google.com/p/24am/">24am</a>]
</p>


<p>
[11] William Hulme, Peter Richtárik, Lynne McGuire and Alison Green<br>
<b>Optimal diagnostic tests for sporadic Creutzfeldt-Jakob disease based on SVM classification of RT-QuIC data</b><br>
Technical Report, 2012 <br>
[<a href="http://arxiv.org/abs/1212.2617">arXiv</a>]<br>
</p>


<p>[10] Peter Richtárik and Martin Takáč<br>
<b>Parallel coordinate descent methods for big data optimization</b><br>
<a
href="http://link.springer.com/article/10.1007/s10107-015-0901-6">Mathematical   Programming 156(1):433-484, 2016</a><br>
<span class="important">Takáč: 16th IMA Leslie Fox Prize
(2nd Prize), 2013 </span> <a
href="http://www.numerical.rl.ac.uk/people/nimg/fox/fox2013/report_2013.pdf">link</a><br>
<span class="important">#1 Top Trending Article in
Mathematical Programming Ser A and B (2017)</span> <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/imgs/2017-MAPR-trending-papers.png">link</a><br>
[<a href="http://arxiv.org/abs/1212.0873">arXiv</a>] [<a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/talks/2012-TALK-Phoenix.pdf">slides</a>]
[code: PCDM, <a
href="http://code.google.com/p/ac-dc/downloads/list">AC/DC</a>]
[video: <a
href="https://www.youtube.com/watch?v=IQgnstB0n2E"><img
style="border: 0px solid;" alt="YouTube"
src="imgs/youtube.png" width="17" height="15" border="0"></a>]
<br>
</p>


<p>
[9] Peter Richtárik and Martin Takáč<br>
<b>Efficient serial and parallel coordinate descent methods for huge-scale truss topology design</b><br>
<a href="http://www.springerlink.com/content/w6577218q5007373/">Operations Research   Proceedings 2011:27-32, Springer-Verlag, 2012</a><br>
[<a href="http://www.optimization-online.org/DB_HTML/2011/08/3118.html">Optimization Online</a>] [<a href="posters/ttd2011_poster.pdf">poster</a>]<br>
</p>


<p>
[8] Peter Richtárik and Martin Takáč<br>
<b>Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function</b> <br>
<a href="http://link.springer.com/article/10.1007/s10107-012-0614-z#">Mathematical Programming   144(2):1-38, 2014</a><br>
<span class="important">Best Student Paper (runner-up), INFORMS Computing Society, 2012</span><br>
[<a href="http://arxiv.org/abs/1107.2848">arXiv</a>] [<a href="talks/Talk-FOCM2011">slides</a>]
</p>


<p>
[7] Peter Richtárik and Martin Takáč<br>
<b>Efficiency of randomized coordinate descent methods on minimization problems with a composite objective function</b><br>
<a href="http://ecos.maths.ed.ac.uk/SPARS11/spars11.pdf">Proceedings of   </a><a href="http://ecos.maths.ed.ac.uk/SPARS11/spars11.pdf"><span class="st"> Signal Processing with Adaptive Sparse
Structured Representations</span>, 2011</a><br>
[<a href="papers/SPARS11-rcdc.pdf">pdf</a>]<br>
</p>

<p>
[6] Peter Richtárik<br>
<b>Finding sparse approximations to extreme eigenvectors: generalized power method for sparse PCA and extensions</b><br>
<a href="http://ecos.maths.ed.ac.uk/SPARS11/spars11.pdf">Proceedings of   </a><a href="http://ecos.maths.ed.ac.uk/SPARS11/spars11.pdf">
<span class="st"> Signal Processing with Adaptive Sparse Structured Representations</span>, 2011</a><br>
[<a href="papers/SPARS11-sparse.eigen.pdf">pdf</a>]<br>
</p>

<p>
[5] Peter Richtárik<br>
<b>Approximate level method for nonsmooth convex minimization</b><br>
<a href="https://springerlink3.metapress.com/content/k133309554u1qu84/resource-secured/?target=fulltext.pdf&amp;sid=aizpkkdpj0533oeywgaafldk&amp;sh=www.springerlink.com">Journal
of Optimization Theory and Applications 152(2):334–350, 2012</a><br>
[<a href="http://www.optimization-online.org/DB_HTML/2009/01/2183.html">Optimization Online</a>]<br>
</p>

<p>
[4] Michel Journée, Yurii Nesterov, Peter Richtárik and Rodolphe Sepulchre<br>
<b>Generalized power method for sparse principal component analysis</b><br>
<a href="http://jmlr.csail.mit.edu/papers/volume11/journee10a/journee10a.pdf">Journal of  Machine Learning Research 11:517–553, 2010</a><br>
[<a href="https://arxiv.org/abs/0811.4724">arXiv</a>] [<a href="talks/Talk-VOCAL2008.pdf">slides</a>] [<a href="posters/Poster-GPower.pdf">poster</a>] [code: GPower]<br>
</p>

<p>
[3] Peter Richtárik<br>
<b>Improved algorithms for convex minimization in relative scale</b><br>
<a href="http://epubs.siam.org/siopt/resource/1/sjope8/v21/i3/p1141_s1?isAuthorized=no">SIAM Journal on Optimization 21(3):1141–1167, 2011</a><br>
[<a href="papers/convex_rel_scale_revised.pdf">pdf</a>] [<a href="talks/Talk-ISMP2006.pdf">slides</a>] <br>
</p>

<p>
[2] Peter Richtárik<br>
<b>Simultaneously solving seven optimization problems in relative scale</b><br>
Technical Report, 2009 <br>
[<a href="http://www.optimization-online.org/DB_HTML/2009/01/2185.html">Optimization Online</a>]<br>
</p>

<p>
[1] Peter Richtárik<br>
<b>Some algorithms for large-scale convex and linear minimization in relative scale</b> <br>
PhD Dissertation, School of Operations Research and Information Engineering, Cornell University, 2007<br>
</p>


        </div>
        <div style="clear: both;"> </div>
      </div>
      <div id="footer">
        <script src="x_footer.js"></script> </div>
    </div>
  </body>
</html>
