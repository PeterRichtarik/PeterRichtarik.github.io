<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <link rel="stylesheet" href="style.css">
    <title>Peter Richtarik</title>
  </head>
  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <script src="table_header.js"></script> </div>
      </div>
      <ul class="menu">
        <li><a class="active" href="index.html">News</a></li>
        <li><a href="i_oldnews.html">Old News</a></li>
        <li><a href="i_papers.html">Papers</a></li>
        <li><a href="i_talks.html">Talks</a></li>
        <li><a href="i_events.html">Events</a></li>
        <li><a href="i_seminar.html">Seminar</a></li>
        <li><a href="i_software.html">Code</a></li>
        <li><a href="i_team.html">Team</a></li>
        <li><a href="i_join.html">Join</a></li>
        <li><a href="i_bio.html">Bio</a></li>
        <li><a href="i_teaching.html">Teaching</a></li>
        <li><a href="i_consulting.html">Consulting</a></li>
      </ul>
      <div id="wrapper" class="main">
        <div id="content">


<h3>April 25, 2020</h3>
<h1>Talk at the One World Optimization Seminar</h1>

<br>
I will give a talk within the <a href="https://owos.univie.ac.at">One World Optimization Seminar</a>
series on Monday, April 27, at 3pm CEST. This is a new exciting initiative, and my talk is only the
second in the series. I will speak about some new results related to second order methods and randomness.
One of the advantages of this new format is that anyone can attend - indeed, attendance is via
<a href="https://zoom.us">Zoom</a>. However, you need to register online in advance in order to get access.
Hope to "see" many of you there!
<br>

Update (April 29): The <a href="https://owos.univie.ac.at/fileadmin/user_upload/k_owos/Peter_Richtarik-Stochastic_Newton.pdf">slides</a> and <a href="https://www.youtube.com/watch?v=HGQkct3db-c">video recording</a> of my talk are now available.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 21, 2020</h3>
<h1>Filip Hanzely Accepted a Position at TTIC</h1>
<br>

<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> accepted a Research Assistant Professorship at <a href="https://www.ttic.edu">Toyota Technological Institute at Chicago</a> (TTIC).
Filip has written his thesis and will submit it soon. He is expected to graduate this Summer, and will start his new position in Chicago in the Fall. Filip has obatined multiple other offers besides this, including a Tenure-Track Assistant Professorship and a Postdoctoral Fellowship in a top machine learning group.
Congratulations!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 20, 2020</h3>
<h1>ICML, UAI and COLT Author Response Deadlines</h1>
<br>

I am busy: <a href="https://icml.cc">ICML</a> and <a href="http://www.auai.org/uai2020/call_for_papers.php">UAI</a> rebuttal deadline is today, and for
<a href="http://learningtheory.org/colt2020/">COLT</a> the deadline is on April 24.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 7, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2004.02635">"Dualize, Split, Randomize: Fast Nonsmooth Optimization Algorithms"</a> -
joint work with <a href="https://adil-salim.github.io">Adil Salim</a>, <a href="https://lcondat.github.io">Laurent Condat</a>, and <a href="https://konstmish.github.io">Konstantin Mishchenko</a>.<br>
<br>

Abstract: <i>
We introduce a new primal-dual algorithm for minimizing the sum of three convex functions, each of which has its own oracle. Namely, the first one is differentiable, smooth and possibly stochastic, the second is proximable, and the last one is a composition of a proximable function with a linear map. Our theory covers several settings that are not tackled by any existing algorithm; we illustrate their importance with real-world applications. By leveraging variance reduction, we obtain convergence with linear rates under strong convexity and fast sublinear convergence under convexity assumptions. The proposed theory is simple and unified by the umbrella of stochastic Davis-Yin splitting, which we design in this work. Finally, we illustrate the efficiency of our method through numerical experiments.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 5, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2004.02163">"On the Convergence Analysis of Asynchronous SGD for Solving Consistent Linear Systems"</a> -
joint work with <a href="https://sands.kaust.edu.sa/authors/atal-narayan-sahu/">Atal Narayan Sahu</a>, <a href="https://www.aritradutta.com">Aritra Dutta</a>, and Aashutosh Tiwari.<br>
<br>

Abstract: <i>
In the realm of big data and machine learning, data-parallel, distributed stochastic algorithms have drawn significant attention in the present days. While the synchronous versions of these algorithms are well
understood in terms of their convergence, the convergence analyses of their asynchronous counterparts are not widely studied.
In this paper, we propose and analyze a  distributed, asynchronous parallel SGD method in light of solving an arbitrary consistent linear system by reformulating the system into a stochastic optimization problem as
studied by Richtárik and Takáč in [35]. We compare the convergence rates of our asynchronous SGD algorithm with the
synchronous parallel algorithm proposed by Richtárik and Takáč in [35] under different choices of the hyperparameters---the stepsize, the damping factor, the number of processors, and the delay factor.
We show that our asynchronous parallel SGD algorithm also enjoys a global linear convergence rate, similar to the "basic method" and the synchronous parallel method in [35] for solving any arbitrary consistent
linear system via stochastic reformulation. We also show that our asynchronous parallel SGD improves upon the "basic method" with a better convergence rate when the number of processors is larger than four.
We further show that this asynchronous approach performs asymptotically better than its synchronous counterpart for certain linear systems. Moreover, for certain linear systems, we compute the minimum number of
processors required for which our asynchronous parallel SGD is better, and find that this number can be as low as two for some ill-conditioned problems.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 3, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2004.01442">"From Local SGD to Local Fixed Point Methods for Federated Learning"</a> -
joint work with Grigory Malinovsky, <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="http://elnurgasanov.me">Elnur Gasanov</a>, and <a href="https://lcondat.github.io">Laurent Condat</a>.<br>
<br>

Abstract: <i>
Most algorithms for solving optimization problems or finding saddle points of convex-concave functions are fixed point algorithms.
In this work we consider the generic problem of finding a fixed point of an average of operators, or an approximation thereof,
in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models
the computations done locally on a mobile device. We investigate two strategies to achieve such a consensus: one based on a fixed
number of local steps, and the other based on randomized computations. In both cases, the goal is to limit communication of the
locally-computed variables, which is often the bottleneck in distributed frameworks. We perform convergence analysis of both methods
and conduct a number of experiments highlighting the benefits of our approach.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 10, 2020</h3>
<h1>Area Chair for NeurIPS 2020</h1>
<br>

I will serve as an Area Chair for <a href="https://nips.cc/Conferences/2020/">NeurIPS 2020</a>, to be
held during December 6-12, 2020 in Vancouver, Canada (same location as last year). For those not in the know,
Google Scholar Metrics says that NeurIPS is the #1 conference in AI:
<br>
<br>
<a href="https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=eng_artificialintelligence"> <img alt="" src="imgs/GoogleScholarMetrics-AI.png" width="700" ></a>
<br>
<br>
 The review process has changed this year; here is a short and beautifully produced video explaining the key 5 changes:
 <br>
<br>
<a href="https://www.youtube.com/watch?v=361h6lHZGDg&feature=share"> <img alt="" src="imgs/5changes.jp2" width="700" ></a>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 9, 2020</h3>
<h1>Coronavirus at KAUST</h1>
<br>

No, Covid-19 did not catch up with anyone at KAUST yet. Still in luck. However, as in many places, its increasing omnipresence and gravitational pull is felt here as well.

<br>
<br>
For example, as of today, all KAUST lectures are moving online. And for a good reason I think: we have seen in Lombardy what the virus can do when unchecked.
I am teaching my CS 390T Federated Learning course on Sundays (yes - the work week in Saudi spans Sunday-Thursday) and Tuesdays, and hence my first online lecture will take place on Sunday March 15. I hope, at least, as I need
to decide how best to do it.
<br>
<br>

Conference travel has been limited for some time now, but the rules are even  more strict now. This seems less than necessary
as conferences drop like flies anyway. My planned travel between now and May includes a seminar talk at EPFL (Switzerland), a workshop
keynote lecture at King Faisal University (Al-Ahsa, Saudi Arabia), presentation at ICLR (Addis Ababa, Ethiopia), and SIAM Conference on Optimization (Hong Kong)
which I am helping to organize. Most of these events are cancelled, and those that survive will most probably go to sleep soon.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 27, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.12410">"On Biased Compression for Distributed Learning"</a> -
joint work with <a href="https://scholar.google.ru/citations?user=hVVJR-sAAAAJ&hl=en">Aleksandr Beznosikov</a>,
<a href="https://samuelhorvath.github.io">Samuel Horváth</a>, and <a href="https://mher-safaryan.github.io">Mher Safaryan</a>.<br>
<br>

Abstract: <i>
In the last few years, various communication  compression techniques have  emerged  as an indispensable  tool helping to alleviate the communication bottleneck in distributed learning.
However, despite the fact biased compressors often show superior performance in practice when compared to the much more studied and understood unbiased compressors,  very little
is known about them. In this work we study three classes of biased compression operators, two of which are new, and their performance when applied to  (stochastic) gradient descent
and distributed (stochastic) gradient descent.  We show for the first time that biased compressors can lead to linear convergence rates both in the single node and distributed settings.
Our distributed SGD method enjoys the ergodic rate $O \left( \frac{\delta L \exp(-K) }{\mu}  + \frac{(C + D)}{K\mu} \right)$, where $\delta$ is a compression parameter which grows when
more compression is applied, $L$ and $\mu$ are the smoothness and strong convexity constants, $C$ captures stochastic gradient noise ($C=0$ if full gradients are computed on each node)
and $D$ captures the variance of the gradients at the optimum ($D=0$ for over-parameterized models).  Further,  via a theoretical study of several synthetic and empirical distributions
of communicated gradients, we shed light on why and by how much  biased compressors outperform  their unbiased variants.  Finally, we  propose a new highly performing biased
compressor---combination of Top-$k$ and natural dithering---which in our experiments outperforms all other compression techniques.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 26, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.11364">"Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization"</a> -
joint work with <a href="https://zhizeli.github.io">Zhize Li</a>,
 <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, and <a href="https://qianxunk.github.io">Xun Qian</a>.<br>
<br>

Abstract: <i>
Due to the high communication cost in distributed and federated learning problems, methods relying on compression of communicated messages are becoming increasingly popular. While in other contexts the best performing gradient-type methods invariably rely on some form of acceleration/momentum to reduce the number of iterations, there are no methods which combine the benefits of both gradient compression and acceleration. In this paper, we remedy this situation and propose the first accelerated compressed gradient descent (ACGD) methods.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 26, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.11337">"Fast Linear Convergence of Randomized BFGS"</a> -
joint work with <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
 <a href="https://gowerrobert.github.io">Robert M. Gower</a>, and <a href="https://scholar.google.com/citations?hl=ru&user=sEjyzkgAAAAJ">Alexander Rogozin</a>.<br>
<br>

Abstract: <i>
Since the late 1950’s when quasi-Newton methods first appeared, they have become one of the most widely used and
efficient algorithmic paradigms for unconstrained optimization. Despite their immense practical success, there is
little theory that shows why these methods are so efficient. We provide a semi-local rate of convergence for the
randomized BFGS method which can be significantly better than that of gradient descent, finally giving theoretical
evidence supporting the superior empirical performance of the method.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 21, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.09526">"Stochastic Subspace Cubic Newton Method"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>,
 <a href="https://scholar.google.ru/citations?user=YNBhhjUAAAAJ&hl=en">Nikita Doikov</a> and <a href="https://en.wikipedia.org/wiki/Yurii_Nesterov">Yurii Nesterov</a>.<br>
<br>

Abstract: <i>
In this paper, we propose a new randomized second-order optimization algorithm---Stochastic Subspace Cubic Newton (SSCN)---for minimizing a high
dimensional convex function $f$. Our method can be seen both as astochastic extension of the cubically-regularized Newton method of Nesterov
and Polyak (2006), and a second-order enhancement of stochastic subspace descent of Kozak et al. (2019). We prove that as we vary the
minibatch size, the global convergence rate of SSCN interpolates between the rate of stochastic coordinate descent (CD) and the rate of cubic
regularized Newton, thus giving new insights into the connection between first and second-order methods. Remarkably, the local convergence rate
of SSCN matches the rate of stochastic subspace descent applied to the problem of minimizing the quadratic function
$\frac{1}{2} (x-x^*)^\top \nabla^2 f(x^*)(x-x^*)$, where $x^*$ is the minimizer of $f$, and hence depends on the properties of $f$ at the optimum only.
Our numerical experiments show that SSCN outperforms non-accelerated first-order CD algorithms while being competitive to their accelerated variants.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 20, 2020</h3>
<h1>New MS/PhD Student: Egor Shulgin</h1>
<br>

Egor Vladimirovich Shulgin is back at KAUST - now as an MS/PhD student. Welcome!!! <br><br>

Egor has co-authored 4 papers and a book (in Russian) entitled <a href="https://arxiv.org/abs/1907.01060">"Lecture Notes on Stochastic Processes"</a>.
Here are the papers, in reverse chronological order:<br><br>
- <a href="https://arxiv.org/abs/2002.08958">Uncertainty principle for communication compression in distributed and federated
learning and the search for an optimal compressor </a> <br>
- Adaptive catalyst for smooth convex optimization <br>
- <a href="https://arxiv.org/abs/1905.11373">Revisiting stochastic extragradient</a> (AISTATS 2020) <br>
- <a href="http://proceedings.mlr.press/v97/qian19b">SGD: general analysis and improved rates</a> (ICML 2019) <br>
<br>

Egor has a bachelor degree in Applied Mathematics from the Department of
Control and Applied Mathematics at MIPT, Dolgoprudny, Russia. He majored in
Data Analysis. His CV mentions the following as his main subjects: Probability
Theory, Random Processes, Convex Optimization, and Machine Learning.<br>
<br>


Egor’s hobbies, according to his CV, are: hiking, alpine skiing, tennis,
and judo. Notably, this list does not include table tennis. However, I
know for a fact that he is very good in it!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 20, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.08958">"Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor"</a> -
joint work with <a href="https://mher-safaryan.github.io">Mher Safaryan</a> and <a href="https://scholar.google.com/citations?user=XlmSx18AAAAJ&hl=en">Egor Shulgin</a>.<br>
<br>

Abstract: <i>
In order to mitigate the high communication cost in  distributed and federated learning, various vector compression schemes, such as
quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as
few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion
(variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of
communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more
distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators,
thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication
compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first
step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call
{\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension
independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry.
We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication
complexity improvements on previous state of the art.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 14, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.05516">"Federated Learning of a Mixture of Global and Local Models"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>.<br>
<br>

Abstract: <i>We propose a new optimization formulation for training federated learning models. The standard
  formulation has the form of an empirical risk minimization problem constructed to find a single global
  model trained from the private data stored across all participating devices. In contrast, our formulation
  seeks an explicit trade-off between this traditional global model and the local models, which can be learned
  by each device from its own private data without any communication. Further, we develop several efficient
  variants of SGD (with and without partial participation and with and without variance reduction) for solving
  the new formulation and prove communication complexity guarantees. Notably, our methods are similar but not
  identical to federated averaging / local SGD, thus shedding some light on the essence of the elusive method.
  In particular, our methods do not perform full averaging steps and instead merely take steps towards averaging.
  We argue for the benefits of this new paradigm for federated learning.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 12, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.05359">"Adaptivity of Stochastic Gradient Methods for Nonconvex Optimization"</a> -
joint work with <a href="https://samuelhorvath.github.io">Samuel Horváth</a>,
<a href="https://statistics.berkeley.edu/people/lihua-lei">Lihua Lei</a> and
<a href="https://people.eecs.berkeley.edu/~jordan/">Michael I. Jordan</a>.<br>
<br>

Abstract: <i>Adaptivity is an important yet under-studied property in modern optimization theory.
  The gap between the state-of-the-art theory and the current practice is striking in that algorithms
  with desirable theoretical guarantees typically involve drastically different settings of hyperparameters,
  such as step-size schemes and batch sizes, in different regimes. Despite the appealing theoretical results,
  such divisive strategies provide little, if any, insight to practitioners to select algorithms that work broadly
  without tweaking the hyperparameters. In this work, blending the "geometrization" technique introduced by
  Lei & Jordan 2016 and the SARAH algorithm of Nguyen et al., 2017, we propose the Geometrized SARAH
  algorithm for non-convex finite-sum and stochastic optimization. Our algorithm is proved to achieve adaptivity
  to both the magnitude of the target accuracy and the Polyak-Łojasiewicz (PL) constant if present. In addition,
  it achieves the best-available convergence rate for non-PL objectives simultaneously while outperforming
  existing algorithms for PL objectives.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 11, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.04670">"Variance Reduced Coordinate Descent with Acceleration: New Method With a
  Surprising Application to Finite-Sum Problems"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> and <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>.<br>
<br>

Abstract: <i>We propose an accelerated version of stochastic variance reduced coordinate descent -- ASVRCD. As other
variance reduced coordinate descent methods such as SEGA or SVRCD, our method can deal with problems that include a
non-separable and non-smooth regularizer, while accessing a random block of partial derivatives in each iteration only.
However, ASVRCD incorporates Nesterov's momentum, which offers favorable iteration complexity guarantees over both SEGA
and SVRCD. As a by-product of our theory, we show that a variant of Allen-Zhu (2017) is a specific case of ASVRCD,
recovering the optimal oracle complexity for the finite sum objective.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 10, 2020</h3>
<h1>Konstantin Giving a Series of Talks in the US and UK</h1>

<br>
<a href="https://konstmish.github.io/">Konstantin Mishchenko</a> is giving several talks in New York, London and Oxford. Here is his schedule:
<br>
<br>
<br>

February 10, Facebook Research, New York, "Adaptive Gradient Descent Without Descent"<br><br>
February 12, Deepmind, London, "Adaptive Gradient Descent Without Descent"<br><br>
February 12, UCL Gatsby Computational Neuroscience Unit, London, "Sinkhorn Algorithm as a Special Case of Stochastic Mirror Descent" <br><br>
February 14, Oxford University, Oxford, "Adaptive Gradient Descent Without Descent" <br><br>
February 14, Imperial College London, London, "Sinkhorn Algorithm as a Special Case of Stochastic Mirror Descent" <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 9, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.03329">"Better Theory for SGD in the Nonconvex World"</a> -
joint work with <a href="https://rka97.github.io">Ahmed Khaled</a>.<br>
<br>

Abstract: <i>Large-scale nonconvex optimization problems are ubiquitous in modern machine learning,
and among practitioners interested in solving them, Stochastic Gradient Descent (SGD)
reigns supreme. We revisit the analysis of SGD in the nonconvex setting and propose a new
variant of the recently introduced expected smoothness assumption which governs the behaviour
of the second moment of the stochastic gradient. We show that our assumption is both more general
and more reasonable than assumptions made in all prior work. Moreover, our results yield the
optimal $O(\epsilon^{-4})$ rate for finding a stationary point of nonconvex smooth functions, and
recover the optimal $O(\epsilon^{-1})$ rate for finding a global solution if the Polyak-Łojasiewicz
condition is satisfied. We compare against convergence rates under convexity and prove a theorem
on the convergence of SGD under Quadratic Functional Growth and convexity, which might be of
independent interest. Moreover, we perform our analysis in a framework which allows for a detailed
study of the effects of a wide array of sampling strategies and minibatch sizes for finite-sum
optimization problems. We corroborate our theoretical results with experiments on real and
synthetic data.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 8, 2020</h3>
<h1>Interview by Robin.ly for their Leaders in AI Platform</h1>

<br>
In December last year, while attending NeurIPS in Vancouver, I was interviewed by Robin.ly.
The video can be found here<br><br>

<a href="https://www.robinly.info/post/peter-richtarik-kaust-machine-learning-federated-learning-kaust"><img alt="" src="imgs/Leaders_in_AI_video.png" width="600"> </a><br><br>

and a podcast out of this is on soundcloud: <br><br> <br>

<a href="https://soundcloud.com/robinly/podcast-peter-richtarik"><img alt="" src="imgs/Leaders_in_AI_podcast.png" width="600"> </a><br><br>

I am in excellent company:<br><br>
<a href="https://soundcloud.com/robinly/yoshua-bengio-from-deep-learning-to-consciousness-interview-by-song-han-neurips-2019">Yoshua Bengio</a><br>
<a href="https://soundcloud.com/robinly/kai-fu-lee-the-era-of-ai-the-rise-of-china-the-future-of-work">Kai-Fu Lee</a><br>
<a href="https://soundcloud.com/robinly/the-future-of-distributed-machine-learning-max-welling-u-of-amsterdam-qualcomm-neurips-2019">Max Welling</a><br>
<a href="https://soundcloud.com/robinly/my-first-cvpr-christopher-manning-professor-director-stanford-ai-lab">Christopher Manning</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>February 8, 2020</h3>
<h1>AAAI in New York</h1>

<br> <a href="https://konstmish.github.io">Konstantin</a> is about to receive a Best Reviewer Award at AAAI 20 in New York. <a href="http://www.adelbibi.com">Adel</a> is presenting our paper
 <a href="https://arxiv.org/abs/1902.01272">"A stochastic derivative-free optimization method with importance sampling"</a>,
 joint work with El Houcine Bergou, Ozan Sener, Bernard Ghanem and myself, at the event.</br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>February 6, 2020</h3>
<h1>ICML Deadline Today!</h1>

<br>I am a walking zombie, a being without a soul, a sleepless creature of the night. Do not approach me or
you will meet your destiny. Wait three days and I shall be alive again. </br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>February 3, 2020</h3>
<h1>Samuel in San Diego</h1>

<br><a href="https://samuelhorvath.github.io">Samuel Horváth</a>
is in San Diego, and will soon be presenting the paper <a href="https://arxiv.org/abs/1901.08689">"Don’t jump through hoops and remove those loops:
SVRG and Katyusha are better without the outer loop"</a>, joint work with Dmitry Kovalev and myself,
at <a href="http://alt2020.algorithmiclearningtheory.org">ALT 2020</a> in San Diego.<br><br> Here is the
list of <a href="http://alt2020.algorithmiclearningtheory.org/accepted-papers/">all accepted papers.</a> <br><br>
Samuel will be back at KAUST in about 10 days.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 2, 2020</h3>
<h1>Eduard Gorbunov is Visting Again</h1>

<br><a href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard Gorbunov</a> came for a research visit - this is his third time at KAUST. This time, he wil stay for about 2 months.

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>January 31, 2020</h3>

<h1>Poster for AAAI-20</h1>
<br>
Our paper <a href="https://arxiv.org/abs/1902.01272">"A stochastic derivative-free optimization
  method with importance sampling"</a>, joint work with Adel Bibi, El Houcine Bergou, Ozan Sener, and Bernard Ghanem,
  will be presented at <a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a>, which will be held in New York
  during February 7-12. <br><br>

We have just prepared a poster, here it is:<br>
<br>
<a href="posters/Poster-STP_IS-AAAI20.pdf"><img src="posters/Poster-STP_IS-AAAI20_small.png" alt="STP" border="0" height="500"></a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 20, 2019</h3>
<h1>Paper Accepted to SIMAX</h1>

<br>The paper <a href="https://arxiv.org/abs/1706.01108">"Stochastic reformulations of linear systems: algorithms and convergence theory"</a>, joint work with Martin Takáč, was accepted to <a href="https://www.siam.org/publications/journals/siam-journal-on-matrix-analysis-and-applications-simax"> SIAM Journal on Matrix Analysis and Applications.</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 18, 2020</h3>
<h1>New Intern: Alexander Rogozin</h1>

<br>A new intern arrived today: Alexander Rogozin. Alexander is a final-year MSc student in the department of Control and Applied Mathematics at the <a href="https://mipt.ru/english/">Moscow Institute of Physics and Technology (MIPT).</a> <br>

<br>Some notable achievements of Alexander so far:<br>

<br>
- Co-authored <a href="https://scholar.google.com/citations?hl=en&amp;user=sEjyzkgAAAAJ">3 papers</a> in the area of decentralized optimization over time varying networks<br>
- His GPA ranks him among the top 5% students at MIPT<br>
- Tutor of Probability Theory at MIPT, 2018-now<br>
- Finished the <i>Yandex School for Data Analysis</i> (2017-2019)<br>
- Awardee of the Russian National Physics Olympiad, 2013<br>
- Certificate of Honor at Russian National Mathematics Olympiad, 2012<br>
- Winner of Russian National Physics Olympiad, 2012<br>

<br>
In 2018, Alexander participated in the Moscow Half-Marathon.
He is a holder of 4-kyu in Judo. Having studied the piano for
11 years, Alexander participated in city, regional, national
and international musical festivals and competitions. He
performed with a symphony orchestra as a piano soloist at
festivals in his hometown.
<br>
<br>
Welcome!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>January 16, 2020</h3>
<h1>Konstantin Among the Best Reviewers for AAAI-20</h1>
<br> Congratulations <a href="https://konstmish.github.io/">Kostya</a>!
(AAAI Conference on Artificial Intelligence is <a
href="https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_artificialintelligence">one of the top AI conferences</a>. The email below tells the
story.)<br> <br>
<i>
Dear Konstantin, <br>
<br>
On behalf of the Association for the Advancement of
Artificial Intelligence and the AAAI-20 Program Committee,
we are pleased to inform you that you have been selected as
one of 12 Outstanding Program Committee members for 2020 in
recognition of your outstanding service on this year's
committee. Your efforts were characterized by exceptional
care, thoroughness, and thoughtfulness in the reviews and
discussions of the papers assigned to you. <br>
<br>
In recognition of your achievement, you will be
presented with a certificate by the AAAI-20 Program
Cochairs, Vincent Conitzer and Fei Sha, during the AAAI-20
Award Ceremony on Tuesday, February 11 at 8:00am. There will
also be an announcement of this honor in the program. Please
let us know (aaai20@aaai.org) if you will be present at the
award ceremony to accept your award. <br>
<br>
Congratulations, and we look forward to seeing you in New York for AAAI-20, February 7-12. <br>
<br>
Warmest regards, <br>
<br>
Carol McKenna Hamilton<br>
Executive Director, AAAI <br>
<br>
for<br>
<br>
Vincent Conitzer and Fei Sha<br>
AAAI-20 Program Cochairs <br>
</i>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 13, 2020</h3>
<h1>Konstantin Visiting Francis Bach's Group at INRIA</h1>

<br>
<a href="https://konstmish.github.io/">Konstantin Mishchenko</a>
is visiting the <a href="https://www.di.ens.fr/sierra/">SIERRA machine learning lab</a> at INRIA, Paris,
led by <a href="https://www.di.ens.fr/%7Efbach/">Francis Bach</a>. He will give a talk on January 14
entitled "Adaptive Gradient Descent Without Descent" and based on
<a href="https://arxiv.org/abs/1910.09529">this paper.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>January 9, 2020</h3>
<h1>New Intern: Aleksandr Beznosikov</h1>
<br>
A new intern arrived today: Aleksandr Beznosikov. Aleksandr is a final-year BSc student in Applied Mathematics and Physics at <a href="https://mipt.ru/english/">Moscow Institute of Physics and Technology (MIPT).</a> <br>
<br>
Some notable achievements of Aleksandr so far:<br>
<br>
- paper <a href="https://arxiv.org/abs/1911.10645">"Derivative-Free Method For Decentralized Distributed Non-Smooth Optimization"</a>, joint work with Eduard Gorbunov and Alexander Gasnikov<br>
- Increased State Academic Scholarship for 4 year bachelor and master students at MIPT, 2018-2019 <br>
- Author of problems and organizer of the student olympiad in discrete mathematics, 2018-2019 <br>
- Abramov's Scholarship for students with the best grades at MIPT, 2017-2019<br>
- First Prize at MIPT's team mathematical tournament, 2017 <br>
- Silver Medal at International Experimental Physics Olympiad,
2015<br>
- Russian President’s Scholarship for High School Students, 2014-2015 <br>
- Prize-Winner, All-Russian School Physics Olympiad, Final Round, 2014 and 2015 <br>
- Winner, All-Russian School Programming Olympiad, Region Round, 2015-2016 <br>
- Winner, All-Russian School Physics Olympiad, Region Round, 2014-2016 <br>
- Winner, All-Russian School Maths Olympiad, Region Round, 2014-2016 <br>
<br>
Welcome!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

          <h3>January 7, 2020</h3>
          <h1>Four Papers Accepted to AISTATS 2020</h1>
          <br>
          Some of the first good news of 2020: We've had four papers
          accepted to <a href="https://www.aistats.org/">The 23rd
            International Conference on Artificial Intelligence and
            Statistics (AISTATS 2020)</a>, which will be held during
          June 3-5, 2020, in Palermo, Sicily, Italy. Here they are:<br>
          <br>
          1) <a href="https://arxiv.org/abs/1905.11261">"A unified
            theory of SGD: variance reduction, sampling, quantization
            and coordinate descent"</a> - joint work with <a
            href="https://eduardgorbunov.github.io/">Eduard Gorbunov</a>
          and <a href="https://fhanzely.github.io/index.html">Filip
            Hanzely.</a> <br>
          <br>
          Abstract: <i>In this paper we introduce a unified analysis of
            a large family of variants of proximal stochastic gradient
            descent (SGD) which so far have required different
            intuitions, convergence analyses, have different
            applications, and which have been developed separately in
            various communities. We show that our framework includes
            methods with and without the following tricks, and their
            combinations: variance reduction, importance sampling,
            mini-batch sampling, quantization, and coordinate
            sub-sampling. As a by-product, we obtain the first unified
            theory of SGD and randomized coordinate descent (RCD)
            methods, the first unified theory of variance reduced and
            non-variance-reduced SGD methods, and the first unified
            theory of quantized and non-quantized methods. A key to our
            approach is a parametric assumption on the iterates and
            stochastic gradients. In a single theorem we establish a
            linear convergence result under this assumption and
            strong-quasi convexity of the loss function. Whenever we
            recover an existing method as a special case, our theorem
            gives the best known complexity result. Our approach can be
            used to motivate the development of new useful methods, and
            offers pre-proved convergence guarantees. To illustrate the
            strength of our approach, we develop five new variants of
            SGD, and through numerical experiments demonstrate some of
            their properties. </i><br>
          <br>
          2) "Tighter theory for local SGD on identical and
          heterogeneous data" - joint work with <a
            href="https://rka97.github.io">Ahmed Khaled</a> and <a
            href="https://konstmish.github.io">Konstantin Mishchenko.</a>
          <br>
          <br>
          Abstract: <i>We provide a new analysis of local SGD, removing
            unnecessary assumptions and elaborating on the difference
            between two data regimes: identical and heterogeneous. In
            both cases, we improve the existing theory and provide
            values of the optimal stepsize and optimal number of local
            iterations. Our bounds are based on a new notion of variance
            that is specific to local SGD methods with different data.
            The tightness of our results is guaranteed by recovering
            known statements when we plug $H=1$, where $H$ is the number
            of local steps. The empirical evidence further validates the
            severe impact of data&nbsp; heterogeneity on the&nbsp;
            performance of local SGD. </i><br>
          <br>
          3) <a href="https://arxiv.org/abs/1905.11373">"Revisiting
            stochastic extragradient"</a> - joint work with <a
            href="https://konstmish.github.io">Konstantin Mishchenko</a>,
          <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
          Egor Shulgin and <a
            href="https://people.epfl.ch/yurii.malitskyi">Yura Malitsky</a>.
          <br>
          <br>
          Abstract: <i>We consider a new extension of the extragradient
            method that is motivated by approximating implicit updates.
            Since in a recent work of Chavdarova et al (2019) it was
            shown that the existing stochastic extragradient algorithm
            (called mirror-prox) of Juditsky et al (2011) diverges on a
            simple bilinear problem, we prove guarantees for solving
            variational inequality that are more general than in </i><i><i>Juditsky
et al (2011)</i>. Furthermore, we illustrate numerically
            that the proposed variant converges faster than many other
            methods on the example of</i><i><i> Chavdarova et al (2019)</i>.
            We also discuss how extragradient can be applied to training
            Generative Adversarial Networks (GANs). Our experiments on
            GANs demonstrate that the introduced approach may make the
            training faster in terms of data passes, while its higher
            iteration complexity makes the advantage smaller. To further
            accelerate method's convergence on problems such as bilinear
            minimax, we combine the extragradient step with negative
            momentum Gidel et al (2018) and discuss the optimal momentum
            value. </i><br>
          <br>
          4) <a href="https://arxiv.org/abs/1906.00506">"DAve-QN: A
            distributed averaged quasi-Newton method with local
            superlinear convergence rate"</a> - work of S. Soori, <a
            href="https://konstmish.github.io">K. Mishchenko</a>, A.
          Mokhtari, M. Dehnavi, and M. Gürbüzbalaban.<br>
          <br>
          Abstract: <i>In this paper, we consider distributed
            algorithms for solving the empirical risk minimization
            problem under the master/worker communication model. We
            develop a distributed asynchronous quasi-Newton algorithm
            that can achieve superlinear convergence. To our knowledge,
            this is the first distributed asynchronous algorithm with
            superlinear convergence guarantees. Our algorithm is
            communication-efficient in the sense that at every iteration
            the master node and workers communicate vectors of size
            O(p), where p is the dimension of the decision variable. The
            proposed method is based on a distributed asynchronous
            averaging scheme of decision vectors and gradients in a way
            to effectively capture the local Hessian information of the
            objective function. Our convergence theory supports
            asynchronous computations subject to both bounded delays and
            unbounded delays with a bounded time-average. Unlike in the
            majority of asynchronous optimization literature, we do not
            require choosing smaller stepsize when delays are huge. We
            provide numerical experiments that match our theoretical
            results and showcase significant improvement comparing to
            state-of-the-art distributed algorithms. </i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h3>January 7, 2020</h3>
          <h1>Visiting ESET in Bratislava</h1>
          <br>
          I am on a visit to <a href="https://www.eset.com/int/">ESET</a>
          - a leading internet security company headquartered in
          Bratislava, Slovakia. I have given a couple of talks on
          stochastic gradient descent and have spoken to several very
          interesting people. <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h3>January 5, 2020</h3>
          <h1>Filip Visiting Francis Bach's Group at INRIA</h1>
          <br>
          <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          is visiting the <a href="https://www.di.ens.fr/sierra/">SIERRA machine learning lab</a> at INRIA, Paris, led by Francis
          Bach. He will give a talk on January 7 entitled "One method to
          rule them all: variance reduction for data, parameters and
          many new methods", and <a
            href="https://arxiv.org/abs/1905.11266">based on a paper of
            the same title</a>. <a
href="https://fhanzely.github.io/wp-content/uploads/2020/01/jacsketch.pdf">Here are his slides.</a><br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h3>January 5, 2020</h3>
          <h1>New Intern: Grigory Malinovsky</h1>
          <br>
          A new intern arrived today: Grigory Malinovsky from <a
            href="https://mipt.ru/english/">Moscow Institute of Physics
            and Technology (MIPT).</a> Grigory wrote his BS thesis
          "Averaged Heavy Ball Method" under the supervision of Boris
          Polyak. He is now pursuing an MS degree at MIPT in Machine
          Learning.<br>
          <br>
          Among Grigory's successes belong:<br>
          <br>
          - Abramov's cholarship for tudents with the best grades at
          MIPT, 2016<br>
          - Participant in the final round of All-Russian Physics
          Olympiad, 2014<br>
          - Bronze medal at International Zhautykov Olympiad in Physics,
          2014<br>
          - Prize winner in the final round of All-Russian Physics
          Olympiad, 2013<br>
          <br>
          Welcome!<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h1> Old News</h1>
          <br>
          <a href="i_oldnews.html">Read old news</a> (2017 and earlier)<br>
          <br>
        </div>

        <div id="sidebar">
          <h6>[11/2019] A KAUST Discovery article <a
      href="https://discovery.kaust.edu.sa/en/article/892/machine-learning-models-gather-momentum">"Machine Learning Models Gather Momentum"</a>.<br>
          </h6>
          <h6><br>
          </h6>
          <h6><br>
          </h6>
          <h6>[10/2019] A KAUST Discovery article <a
      href="https://discovery.kaust.edu.sa/en/article/880/less-chat-leads-to-more-work-for-machine-learning">"Less Chat Leads to More Work for Machine Learning"</a>.<br>
          </h6>
          <h6><br>
          </h6>
          <h6><br>
          </h6>
          <h6>[9/2019] A KAUST Discovery article <a
      href="https://discovery.kaust.edu.sa/en/article/879/speeding-up-the-machine-learning-process">"Speeding up the Machine Learning Process"</a>.<br>
          </h6>
          <h6><br>
          </h6>
          <h6><br>
          </h6>
          <h6>[10/2019] I have delivered a 5hr mini-course on Stochastic
            Gradient Descent at Moscow Institute of Physics and
            Technology (MIPT), entitled <a
              href="https://youtu.be/a05S0kL5u30">"A Guided Walk Through
              the ZOO of Stochastic Gradient Descent Methods".</a> The
            lectures are now on YouTube.<br>
          </h6>
          <h6> </h6>
          <h6><br>
          </h6>
          <h6><br>
          </h6>
          <h6>[8/2019] Sign up for my <a
              href="https://piazza.com/kaust.edu.sa/fall2019/cs390ff/home">CS 390FF (Big Data Optimization) class</a> (Fall 2019). </h6>
          <br>
          <br>
          <h6>[9/2018] A <a
      href="https://www.youtube.com/watch?v=gjgEck0zU7w&amp;list=PLgKuh-lKre13E9dXSsif4KsGoFp4bjubd&amp;index=9">YouTube video</a> of my talk at the Simons Institute on the
            JacSketch algorithm. </h6>
          <br>
          <br>
          <h6> [8/2017] Video recording of my 5 hour mini-course on
            "Randomized Optimization Methods" delivered at the Data
            Science Summer School (DS3) at École Polytechnique: Parts <a
      href="https://comm.medias.polytechnique.fr/videos/richtarik_1_randonopti/">1</a>,
            <a
      href="https://comm.medias.polytechnique.fr/videos/richtarik_2_randonopti/">2</a>,
            <a
      href="https://comm.medias.polytechnique.fr/videos/richtarik_3_randonopti_08033/">3</a>,
            <a
      href="https://comm.medias.polytechnique.fr/videos/richtarik_4_randonoptimization/">4</a>,
            <a
      href="https://comm.medias.polytechnique.fr/videos/richtarik_5_randonoptimization/">5</a>
          </h6>
          <br>
          <br>
          <h6> [3/2017] I have taken up an Associate Professor position
            at <a href="https://www.kaust.edu.sa/en">KAUST</a>. I am on
            leave from Edinburgh. </h6>
          <br>
          <br>
          <h6> [12/2016] Video from my <a
              href="https://events.yandex.ru/lib/talks/4294/">talk at
              Yandex </a>entitled "Empirical Risk Minimization:
            Complexity, Duality, Sampling, Sparsity and Big Data". </h6>
          <br>
          <br>
          <h6> [10/2016] My Alan Turing Institute talk on Stochastic
            Dual Ascent for Solving Linear Systems is now on <a
              href="https://www.youtube.com/watch?v=RbkhWrTbrKs">YouTube</a>.
          </h6>
          <br>
          <br>
          <h6>[9/2015] <a
              href="talks/2015-09-Toulouse-Summer-School-Optimization.pdf">Slides</a>
            from a 6hr course on "Optimization in Machine Learning",
            Toulouse, France. </h6>
          <br>
          <br>
          <h6> [7/2015] ICML Tutorial (joint with Mark Schmidt) on <span
              class="important">Modern Convex Optimization Methods for
              Large-scale ERM:</span> <a
              href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_I.pdf">Part I</a>, <a
              href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_II.pdf">Part II</a> </h6>
          <br>
          <br>
          <h6> [2/2014] A <a
              href="https://www.youtube.com/watch?v=0sHOfqhCZw0">YouTube
              video</a> of a talk on the APPROX algorithm delivered at
            PreMoLab in Moscow. </h6>
          <br>
          <br>
          <h6> [10/2013] A <a
              href="http://www.youtube.com/watch?v=IQgnstB0n2E#t=538">YouTube video</a> of a talk I gave at the Simons Institute
            workshop on <a
              href="http://simons.berkeley.edu/workshops/bigdata2013-2">parallel
and distributed optimization and inference.</a> </h6>

              </div>

        <div style="clear: both;"> </div>
      </div>
    </div>
  </body>
</html>
