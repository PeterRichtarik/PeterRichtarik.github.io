<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="style.css">
    <title>Peter Richtarik</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-168147887-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-168147887-1');
    </script>


  </head>


  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <script src="table_header.js"></script> </div>
      </div>
      <ul class="menu">
        <li><a class="active" href="index.html">News</a></li>
        <li><a href="i_oldnews.html">Old News</a></li>
        <li><a href="i_papers.html">Papers</a></li>
        <li><a href="i_talks.html">Talks</a></li>
        <li><a href="i_videotalks.html">Video Talks</a></li>        
        <li><a href="i_events.html">Events</a></li>
        <li><a href="i_seminar.html">Seminar</a></li>
        <li><a href="i_software.html">Code</a></li>
        <li><a href="i_team.html">Team</a></li>
        <li><a href="i_join.html">Join</a></li>
        <li><a href="i_bio.html">Bio</a></li>
        <li><a href="i_teaching.html">Teaching</a></li>
        <li><a href="i_consulting.html">Consulting</a></li>
      </ul>
      <div id="wrapper" class="main">
        <div id="content">



<h3>May 10, 2021</h3>
<h1>Papers Accepted to ICML 2021</h1>
<br>
We've had several papers accepted to the <a href="https://icml.cc/Conferences/2021">International Conference on Machine Learning (ICML 2021)</a>, which will be run virtually during July 18-24, 2021.
Here they are:<br><br>


<b> 1) <a href="https://arxiv.org/abs/2102.07845">"MARINA: Faster Non-convex Distributed Learning with Compression"</a></b> - joint work with
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>,
<a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a> and <a href="https://zhizeli.github.io">Zhize Li</a>.<br>
<br>

Abstract: <i>We develop and analyze MARINA: a new communication efficient method for non-convex distributed learning over heterogeneous datasets. MARINA employs a novel communication compression strategy based on the compression of gradient differences which is reminiscent of but different from the strategy employed in the DIANA method of Mishchenko et al (2019). Unlike virtually all competing distributed first-order methods, including DIANA, ours is based on a carefully designed biased gradient estimator, which is the key to its superior theoretical and practical performance. To the best of our knowledge, the communication complexity bounds we prove for MARINA are strictly superior to those of all previous first order methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and PP-MARINA. The first method is designed for the case when the local loss functions owned by clients are either of a finite sum or of an expectation form, and the second method allows for partial participation of clients -- a feature important in federated learning. All our methods are superior to previous state-of-the-art methods in terms of the oracle/communication complexity. Finally, we provide convergence analysis of all methods for problems satisfying the Polyak-Lojasiewicz condition.
</i>
<br>
<br>

More material:
<ul>
  <li><a href="https://www.youtube.com/watch?v=o5MwC4DYbGE&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=34">Short 5 min YouTube talk by Konstantin</a></li>
  <li><a href="https://www.youtube.com/watch?v=bj0E94Siq74">Long 70 min YouTube talk by Eduard delivered at the FLOW seminar</a> </li>
  <li><a href="">poster</a></li>
</ul>
<br>



<b> 2) <a href="https://arxiv.org/abs/2008.10898">"PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization"</a> </b> - joint work with
<a href="https://zhizeli.github.io">Zhize Li</a>, <a href="https://cemse.kaust.edu.sa/people/person/hongyan-bao">Hongyan Bao</a>, and <a href="https://cemse.kaust.edu.sa/people/person/xiangliang-zhang">Xiangliang Zhang</a>.
<br>
<br>

Abstract: <i>In this paper, we propose a novel stochastic gradient estimator---ProbAbilistic Gradient Estimator (PAGE)---for nonconvex optimization. PAGE is easy to implement as it is designed via a small adjustment to vanilla SGD:
  in each iteration, PAGE uses the vanilla minibatch SGD update with probability p or reuses the previous gradient with a small adjustment, at a much lower computational cost, with probability 1−p. We give a simple formula for the
  optimal choice of p. We prove tight lower bounds for nonconvex problems, which are of independent interest. Moreover, we prove matching upper bounds both in the finite-sum and online regimes, which establish that PAGE is an optimal
  method. Besides, we show that for nonconvex functions satisfying the Polyak-Łojasiewicz (PL) condition, PAGE can automatically switch to a faster linear convergence rate. Finally, we conduct several deep learning experiments (e.g.,
  LeNet, VGG, ResNet) on real datasets in PyTorch, and the results demonstrate that PAGE not only converges much faster than SGD in training but also achieves the higher test accuracy, validating our theoretical results and confirming
  the practical superiority of PAGE. </i>
  <br>
<br>

More material:
<ul>
  <li><a href="https://www.youtube.com/watch?v=_K3XPxN-vdk&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=29">Short 5 min YouTube talk by Zhize</a></li>
</ul>
<br>


<b> 3) <a href="https://arxiv.org/abs/2102.07158">"Distributed Second Order Methods with Fast Rates and Compressed Communication"</a> </b> - work of  <a
href="https://rustem-islamov.github.io">Rustem Islamov</a> and <a href="https://qianxunk.github.io">Xun Qian</a>.<br>
<br>

Abstract: <i>
We develop several new communication-efficient second-order methods for distributed optimization. Our first method, NEWTON-STAR, is a variant of Newton's method from which it inherits its fast local quadratic rate. However, unlike Newton's method, NEWTON-STAR enjoys the same per iteration communication cost as gradient descent. While this method is impractical as it relies on the use of certain unknown parameters characterizing the Hessian of the objective function at the optimum, it serves as the starting point which enables us design practical variants thereof with strong theoretical guarantees. In particular, we design a stochastic sparsification strategy for learning the unknown parameters in an iterative fashion in a communication efficient manner. Applying this strategy to NEWTON-STAR leads to our next method, NEWTON-LEARN, for which we prove local linear and superlinear rates independent of the condition number. When applicable, this method can have dramatically superior convergence behavior when compared to state-of-the-art methods. Finally, we develop a globalization strategy using cubic regularization which leads to our next method, CUBIC-NEWTON-LEARN, for which we prove global sublinear and linear convergence rates, and a fast superlinear rate. Our results are supported with experimental results on real datasets, and show several orders of magnitude improvement on baseline and state-of-the-art methods in terms of communication complexity.
 </i> <br>
<br>

More material:
<ul>
  <li><a href="https://www.youtube.com/watch?v=iSKBZXlaoWo&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=41">Short 5 min YouTube talk by Rustem</a></li>
  <li><a href="https://www.youtube.com/watch?v=jqgyjN5kLqo">Long 80 min YouTube talk by myself delivered at the FLOW seminar</a></li>
  <li><a href="https://richtarik.org/talks/TALK-2021-Newton-Learn.pdf">my FLOW talk slides</a></li>
  <li><a href="https://richtarik.org/posters/Poster-Newton-Learn.pdf">poster</a></li>
</ul>
<br>


<b> 4) <a href="https://arxiv.org/abs/1905.12938">"Stochastic Sign Descent Methods: New Algorithms and Better Theory"</a> </b> - joint work with <a href="https://mher-safaryan.github.io">Mher Safaryan</a>. <br>
<br>

Abstract: <i>
Various gradient compression schemes have been proposed to mitigate the communication cost in distributed training of large scale machine learning models. Sign-based methods, such as signSGD, have recently been gaining popularity because of their simple compression rule and connection to adaptive gradient methods, like ADAM. In this paper, we analyze sign-based methods for non-convex optimization in three key settings: (i) standard single node, (ii) parallel with shared data and (iii) distributed with partitioned data. For single machine case, we generalize the previous analysis of signSGD relying on intuitive bounds on success probabilities and allowing even biased estimators. Furthermore, we extend the analysis to parallel setting within a parameter server framework, where exponentially fast noise reduction is guaranteed with respect to number of nodes, maintaining 1-bit compression in both directions and using small mini-batch sizes. Next, we identify a fundamental issue with signSGD to converge in distributed environment. To resolve this issue, we propose a new sign-based method, Stochastic Sign Descent with Momentum (SSDM), which converges under standard bounded variance assumption with the optimal asymptotic rate. We validate several aspects of our theoretical findings with numerical experiments.
</i> <br>
<br>

More material:
<ul>
  <li><a href="https://opt-ml.org/posters/2020/poster_14.png">poster</a></li>
</ul>
<br>


<b> 5) <a href="https://arxiv.org/abs/2102.09234">"ADOM: Accelerated Decentralized Optimization Method for Time-Varying Networks"</a> </b>
- joint work with
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://shulgin-egor.github.io">Egor Shulgin</a>,
 <a
href="https://scholar.google.com/citations?hl=ru&user=sEjyzkgAAAAJ">Alexander Rogozin</a> and <a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

Abstract: <i>We propose ADOM - an accelerated method for smooth and strongly convex decentralized optimization over time-varying networks. ADOM uses a dual oracle, i.e., we assume access to the gradient of the Fenchel conjugate of the individual loss functions. Up to a constant factor, which depends on the network structure only, its communication complexity is the same as that of accelerated Nesterov gradient method (Nesterov, 2003). To the best of our knowledge, only the algorithm of Rogozin et al. (2019) has a convergence rate with similar properties. However, their algorithm converges under the very restrictive assumption that the number of network changes can not be greater than a tiny percentage of the number of iterations. This assumption is hard to satisfy in practice, as the network topology changes usually can not be controlled. In contrast, ADOM merely requires the network to stay connected throughout time.
</i><br>

More material:
<ul>
  <li>  <a href="https://www.youtube.com/watch?v=jO3t4eZFdkc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=37">Short 5 min YouTube talk by Egor</a> </li>
  <li>  <a href="posters/Poster-ADOM-ICML-2021.pdf">poster</a> </li>
</ul>



<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>April 29, 2021</h3>
<h1>Paper Accepted to IEEE Transactions on Information Theory</h1>

<br>
Our paper <a href="https://arxiv.org/abs/1905.08645">Revisiting randomized gossip algorithms: general framework, convergence rates and novel block and accelerated protocols</a>, joint work with
<a href="https://nicolasloizou.github.io">Nicolas Loizou</a>, was accepted to IEEE Transactions on Information Theory.<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 28, 2021</h3>
<h1>KAUST Conference on Artificial Intelligence: 17 Short (up to 5 min) Talks by Members of my Team!</h1>
<br>

Today and tomorrow I am attending the <a href="https://cemse.kaust.edu.sa/ai/aii-conf-2021">KAUST Conference on Artificial Intelligence</a>. Anyone can attend for free by watching the <a href="https://kaust.zoom.us/j/96464686903">LIVE Zoom webinar stream.</a> Today I have given a short 20
  min talk today entitled "Recent Advances in Optimization for Machine Learning". Here are my slides:


  <br>

  <br>
  <a href="slides/Talk-1-KAUST-AI-Conference-2021.pdf"><img alt="" src="imgs/KAUST-AI-Conference-2021-Talk-1-Richtarik.png" width="700"></a>
  <br>


<br>


   I will deliver another 20 min talk tomorrow, entitled "On Solving a Key Challenge in Federated Learning: Local Steps, Compression
  and Personalization". Here are the slides:


    <br>

    <br>
    <a href="slides/Talk-2-KAUST-AI-Conference-2021.pdf"><img alt="" src="imgs/KAUST-AI-Conference-2021-Talk-2-Richtarik.png" width="700"></a>
    <br>


  <br>


<br>
<br>


  More importantly, 17 members (research scientists, postdocs, PhD students,  MS students and interns) of the "Optimization and Machine Learning Lab"  that I lead at KAUST have
  prepared short videos on selected recent papers they co-athored. This includes 9 papers from 2021, 7 papers from 2020 and 1 paper from 2019. Please check out their video talks!
  Here they are:

  <br>
  <br>


A talk by Konstantin Burlachenko (<a href="https://arxiv.org/abs/2102.07845">paper</a>):
<br>
<a href="https://www.youtube.com/watch?v=o5MwC4DYbGE&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=33"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Burlachenko.png" width="350"></a>
<br>

A talk by Laurent Condat (<a href="http://proceedings.mlr.press/v119/malinovskiy20a.html">paper</a>):
<br>
<a href="https://www.youtube.com/watch?v=-QgQp5HnWQY&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=27"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Condat.png" width="350"></a>
<br>

A talk by Eduard Gorbunov (<a href="https://papers.nips.cc/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html">paper</a>):
<br>
<a href="https://www.youtube.com/watch?v=6Hpt6hbzgjU&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=37"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Gorbunov.png" width="350"></a>
<br>

A talk by Filip Hanzely (<a href="http://proceedings.mlr.press/v130/gorbunov21a.html">paper</a>):
<br>
<a href="https://www.youtube.com/watch?v=u_KoimUuc6k&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Hanzely.png" width="350"></a>
<br>

A talk by Slavomir Hanzely:
<br>
<a href="https://www.youtube.com/watch?v=YkZeROHXahc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=34"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Slavomir-Hanzely.png" width="350"></a>
<br>

A talk by Samuel Horvath:
<br>
<a href="https://www.youtube.com/watch?v=M3zHT_qieB4&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=32"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Horvath.png" width="350"></a>
<br>

A talk by Rustem Islamov:
<br>
<a href="https://www.youtube.com/watch?v=iSKBZXlaoWo&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=40"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Islamov.png" width="350"></a>
<br>

A talk by Ahmed Khaled:
<br>
<a href="https://www.youtube.com/watch?v=aHAU6OYNoKA&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=39"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Khaled.png" width="350"></a>
<br>

A talk by Dmitry Kovalev:
<br>
<a href="https://www.youtube.com/watch?v=0bAgav0x-8U&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=31"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Kovalev.png" width="350"></a>
<br>

A talk by Zhize Li:
<br>
<a href="https://www.youtube.com/watch?v=_K3XPxN-vdk&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=28"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Li.png" width="350"></a>
<br>

A talk by Grigory Malinovsky:
<br>
<a href="https://www.youtube.com/watch?v=DK9CJmz6SR8&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=35"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Malinovsky.png" width="350"></a>
<br>

A talk by Konstantin Mishchenko:
<br>
<a href="https://www.youtube.com/watch?v=0ZZY5Y_6fd4&list=PLC28kDljnOrh4XxzWpOFBIu8IHjJzmKQs"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Mishchenko.png" width="350"></a>
<br>

A talk by Xun Qian:
<br>
<a href="https://www.youtube.com/watch?v=QMJDOtm9wxk&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=29"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Qian.png" width="350"></a>
<br>

A talk by Mher Safaryan:
<br>
<a href="https://www.youtube.com/watch?v=vSD-smU0JjE&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=45"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Safaryan.png" width="350"></a>
<br>

A talk by Adil Salim:
<br>
<a href="https://www.youtube.com/watch?v=jz_ylyAhkL8&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=30"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Salim.png" width="350"></a>
<br>

A talk by Egor Shulgin:
<br>
<a href="https://www.youtube.com/watch?v=jO3t4eZFdkc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=36"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Shulgin.png" width="350"></a>
<br>

A talk by Bokun Wang:
<br>
<a href="https://www.youtube.com/watch?v=-YjXDdwkeqc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=38"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Wang.png" width="350"></a>
<br>




<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




          <h3>April 21, 2021</h3>
          <h1>Area Editor for Journal of Optimization Theory and Applications</h1>
          <br>

      I have just become an <a href="https://www.springer.com/journal/10957/editors">Area Editor</a> for <a href="https://www.springer.com/journal/10957">Journal on Optimization Theory and Applications (JOTA)</a>, representing the area
      "Optimization and Machine Learning". Consider sending your best optimizaiton for machine learning papers to JOTA! We aim to provide fast and high quality reviews. <br> <br>

      Established in 1967, JOTA is one of the oldest optimization journals. For example, Mathematical Programming was established in 1972, SIAM J on Control and Optimization in 1976, and SIAM J on Optimization in 1991.

        <br><br>

       According to <a href="https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=phy_mathematicaloptimization">Google Scholar Metrics</a>, JOTA is one of the top optimization journals: <br>

       <br>
       <a href="https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=phy_mathematicaloptimization"><img alt="" src="imgs/JOTA-GoogleScholarMetrics-2021-small.png" width="750"></a>
       <br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>April 22, 2021</h3>
          <h1>Talk at AMCS/STAT Graduate Seminar at KAUST</h1>
          <br>

        Today I gave a talk entitled "Distributed second order methods with fast rates and compressed communication" at
        the AMCS/STAT Graduate Seminar at KAUST. Here is the <a href="https://cemse.kaust.edu.sa/events/event/distributed-second-order-methods-fast-rates-and-compressed-communication">official KAUST blurb.</a> I talked about
        the paper <a href="https://arxiv.org/abs/2102.07158">Distributed Second Order Methods with Fast Rates and Compressed Communication</a>.
        This is joint work with my fantastic intern <a href="https://rustem-islamov.github.io">Rustem Islamov</a> (KAUST and MIPT) and fantastic
        postdoc <a href="https://qianxunk.github.io">Xun Qian</a> (KAUST). <br>



        <br>
        <br>
        <img alt="" src="imgs/fancy-line.png" width="196" height="36">
        <br>
        <br>



        <h3>April 19, 2021</h3>
        <h1>New Paper</h1>
        <br>

        <span class="important">New paper out: </span>
        <a href="https://arxiv.org/abs/2104.09342">"Random Reshuffling with Variance Reduction: New Analysis and Better Rates"</a> -
        joint work with
        <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> and
        <a href="https://cemse.kaust.edu.sa/people/person/alibek-sailanbayev">Alibek Sailanbayev</a>.<br>
        <br>

        Abstract: <i>
         Virtually all state-of-the-art methods for training supervised machine learning models are variants of SGD enhanced with a number of additional tricks, such as minibatching, momentum, and adaptive stepsizes. One of the tricks that works so well in practice that it is used as default in virtually all widely used machine learning software is {\em random reshuffling (RR)}. However, the practical benefits of RR have until very recently been eluding attempts at being satisfactorily explained using theory. Motivated by recent development due to Mishchenko, Khaled and Richt\'{a}rik (2020), in this work we provide the first analysis of SVRG under Random Reshuffling (RR-SVRG) for general finite-sum problems. First, we show that RR-SVRG converges linearly with the rate $O(\kappa^{3/2})$ in the strongly-convex case, and can be improved further to $O(\kappa)$ in the big data regime (when $n > O(\kappa)$), where $\kappa$ is the condition number. This improves upon the previous best rate $O(\kappa^2)$ known for a variance reduced RR method in the strongly-convex case due to Ying, Yuan and Sayed (2020). Second, we obtain the first sublinear rate for general convex problems. Third, we establish similar fast rates for Cyclic-SVRG and Shuffle-Once-SVRG. Finally, we develop and analyze a more general variance reduction scheme for RR, which allows for less frequent updates of the control variate. We corroborate our theoretical results with suitably chosen experiments on synthetic and real datasets.
        </i>
        <br>

        <br>
        <br>
        <img alt="" src="imgs/fancy-line.png" width="196" height="36">
        <br>
        <br>




          <h3>April 14, 2021</h3>
          <h1>Talk at FLOW</h1>
          <br>

        Today I am giving a talk entitled "Beyond Local and Gradient Methods for Federated Learning" at
        the <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">Federated Learning One World Seminar (FLOW).</a>
        After a brief motivation spent on bashing gradient and local methods, I will talk about
        the paper <a href="https://arxiv.org/abs/2102.07158">Distributed Second Order Methods with Fast Rates and Compressed Communication</a>.
        This is joint work with my fantastic intern <a href="https://rustem-islamov.github.io">Rustem Islamov</a> (KAUST and MIPT) and fantastic
        postdoc <a href="https://qianxunk.github.io">Xun Qian</a> (KAUST).



         <br><br>
         The talk was recorded and is now available on      YouTube:


         <br>
         <a href="https://www.youtube.com/watch?v=jqgyjN5kLqo"><img alt="" src="imgs/Newton-Learn-FLOW-small.png" width="750"></a>
         <br>




          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>April 13, 2021</h3>
          <h1>Three Papers Presented to AISTATS 2021</h1>
            <br>
We've had three papers accepted to <a href="https://aistats.org/aistats2021/">The 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)</a>. The conference will be held virtually over the next few days; during April 13-15, 2021. Here are the papers:<br><br>


1. <a href="http://proceedings.mlr.press/v130/kovalev21a.html">A linearly convergent algorithm for decentralized optimization: sending less bits for free!</a>, joint work with  <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://scholar.google.com/citations?user=ldJpvE8AAAAJ&hl=en">Anastasia Koloskova</a>, <a href="https://people.epfl.ch/martin.jaggi/?lang=en">Martin Jaggi</a>, and <a href="https://www.sstich.ch">Sebastian U. Stich</a>.<br><br>

<i>Abstract:</i>  Decentralized optimization methods enable on-device training of machine learning models without a central coordinator. In many scenarios communication between devices is energy demanding and time consuming and forms the bottleneck of the entire system.
We propose a new randomized first-order method which tackles the communication bottleneck by applying randomized compression operators to the communicated messages. By combining our scheme with a new variance reduction technique that progressively throughout the iterations reduces the adverse effect of the injected quantization noise, we obtain the first scheme that converges linearly on strongly convex decentralized problems while using compressed communication only.
We prove that our method can solve the problems without any increase in the number of communications compared to the baseline which does not perform any communication compression while still allowing for a significant compression factor which depends on the conditioning of the problem and the topology of the network. Our key theoretical findings are supported by numerical experiments. <br> <br>

<a href="posters/Poster-Decentralized-DIANA.pdf"> <img alt="" src="posters/Poster-Decentralized-DIANA-small.png" width="750" ></a>
<br> <br>

2. <a href="http://proceedings.mlr.press/v130/gorbunov21a.html">Local SGD: unified theory and new efficient methods</a>, joint work with  <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> and <a href="https://fhanzely.github.io">Filip Hanzely</a>.<br><br>

<i>Abstract:</i> We present a unified framework for analyzing local SGD methods in the convex and strongly convex regimes for distributed/federated training of supervised machine learning models. We recover several known methods as a special case of our general framework, including Local-SGD/FedAvg, SCAFFOLD, and several variants of SGD not originally designed for federated learning. Our framework covers both the identical and heterogeneous data settings, supports both random and deterministic number of local steps, and can work with a wide array of local stochastic gradient estimators, including shifted estimators which are able to adjust the fixed points of local iterations for faster convergence. As an application of our framework, we develop multiple novel FL optimizers which are superior to existing methods. In particular, we develop the first linearly converging local SGD method which does not require any data homogeneity or other strong assumptions. <br> <br>


<a href="posters/Poster-Local-SGD-AISTATS-2021.pdf"> <img alt="" src="posters/Poster-Local-SGD-AISTATS-2021-small.png" width="750" ></a>
<br> <br>





3. <a href="http://proceedings.mlr.press/v130/horvath21a.html">Hyperparameter transfer learning with adaptive complexity</a>, joint work with  <a href="https://samuelhorvath.github.io">Samuel Horváth</a> and <a href="https://aaronkl.github.io">Aaron Klein</a>, and <a href="http://www0.cs.ucl.ac.uk/staff/c.archambeau/">Cedric Archambeau</a>.<br><br>

<i>Abstract:</i>  Bayesian optimization (BO) is a data-efficient approach to automatically tune the hyperparameters of machine learning models. In practice, one frequently has to solve similar hyperparameter tuning problems sequentially. For example, one might have to tune a type of neural network learned across a series of different classification problems. Recent work on multi-task BO exploits knowledge gained from previous hyperparameter tuning tasks to speed up a new tuning task. However, previous approaches do not account for the fact that BO is a sequential decision making procedure. Hence, there is in general a mismatch between the number of evaluations collected in the current tuning task compared to the number of evaluations accumulated in all previously completed tasks. In this work, we enable multi-task BO to compensate for this mismatch, such that the transfer learning procedure is able to handle different data regimes in a principled way. We propose a new multi-task BO method that learns a set of ordered, non-linear basis functions of increasing complexity via nested drop-out and automatic relevance determination. Experiments on a variety of hyperparameter tuning problems show that our method improves the sample efficiency of recently published multi-task BO methods.
<br>

<a href="posters/Poster-ABRAC-AISTATS-2021.pdf"> <img alt="" src="posters/Poster-ABRAC-AISTATS-2021-small.png" width="750" ></a>



          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>





          <h3>April 7, 2021</h3>
          <h1>Talk at All Russian Seminar in Optimization</h1>
          <br>

        Today I am giving a talk at the <a href="http://www.mathnet.ru/php/conference.phtml?&eventID=31&confid=1794&option_lang=eng">All Russian Seminar in Optimization.</a> I am talking about the paper <a href="https://arxiv.org/abs/2102.07158">Distributed Second Order Methods with Fast Rates and Compressed Communication</a>, which is joint work with
        <a href="https://rustem-islamov.github.io">Rustem Islamov (KAUST and MIPT)</a> and <a href="https://qianxunk.github.io">Xun Qian (KAUST)</a>.


         <br><br>
         The talk was recorded and uploaded to  YouTube:

         <br>
         <a href="https://www.youtube.com/watch?v=_1q-vt0nu44"><img alt="" src="imgs/Newton-Learn-All-Russian-seminar-small.png" width="750"></a>
         <br>



        <br>

        Here are the <a href="talks/TALK-2021-Newton-Learn.pdf">slides from my talk</a>, and here is a
          <a href="posters/Poster-Newton-Learn.pdf"> poster </a> that will son be presented by Rustem Islamov at the <a href="https://sites.google.com/ucsd.edu/cedo/">NSF-TRIPODS Workshop on Communication Efficient Distributed Optimization.</a>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>March 24, 2021</h3>
          <h1>Mishchenko and Gorbunov: ICLR 2021 Outstanding Reviewer Award</h1>
          <br>

          Congratulations <a href="https://konstmish.github.io/">Konstantin Mishchenko</a> and
          <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> for receiving an Outstanding
          Reviewer Award from ICLR 2021! I wish the reviews we get for our papers were as good (i.e., insighful,
          expert and thorough) as the reviews Konstantin and Eduard are writing.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>March 19, 2021</h3>
          <h1>Area Chair for NeurIPS 2021</h1>
          <br>



          I will serve as an Area Chair for NeurIPS 2021, to be held during December 6-14, 2021 virtually ( = same location as last year ;-).


          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>





          <h3>March 1, 2021</h3>
          <h1>New PhD Student: Lukang Sun</h1>
          <br>


        Lukang Sun has joined my group as a PhD student. Welcome!!!

        Lukang has an MPhil degree in mathematics form Nanjing University, China (2020), and a BA in mathematics from Jilin University, China (2017). His thesis (written in Chinese) was on the topic of
        "Harmonic functions on metric measure spaces". In this work, Lukang proposed some novel methods using optimal transport theory to generalize some results from Riemannian manifolds to metric measure
        spaces. Lukang has held visiting/exchange/temporary positions at the Hong Kong University of Science and Technology, Georgia Institute of Technology, and the Chinese University of Hong Kong.


          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>February 22, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.11079">"An Optimal Algorithm for Strongly Convex Minimization under Affine Constraints"</a> -
          joint work with
          <a href="https://adil-salim.github.io">Adil Salim</a>,
          <a href="https://lcondat.github.io">Laurent Condat</a> and
          <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>.<br>
          <br>

          Abstract: <i>
           Optimization problems under affine constraints appear in various areas of machine learning. We consider the task of
           minimizing a smooth strongly convex function $F(x)$ under the affine constraint $Kx=b$, with an oracle providing evaluations
           of the gradient of $F$ and matrix-vector multiplications by $K$ and its transpose. We provide lower bounds on the number of
           gradient computations and matrix-vector multiplications to achieve a given accuracy. Then we propose an accelerated
           primal--dual algorithm achieving these lower bounds. Our algorithm is the first optimal algorithm for this class of
           problems.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 19, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.09700">"AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods"</a> -
          joint work with
          <a href="https://coral.ise.lehigh.edu/zhs310/">Zheng Shi</a>,
          <a href="https://nicolasloizou.github.io">Nicolas Loizou</a> and
          <a href="https://engineering.lehigh.edu/faculty/martin-takac">Martin Takáč</a>.<br>
          <br>

          Abstract: <i>
           We present an adaptive stochastic variance reduced method with an implicit approach for adaptivity.
           As a variant of SARAH, our method employs the stochastic recursive gradient yet adjusts step-size
           based on local geometry. We provide convergence guarantees for finite-sum minimization problems
           and show a faster convergence than SARAH can be achieved if local geometry permits. Furthermore,
           we propose a practical, fully adaptive variant, which does not require any knowledge of local geometry
           and any effort of tuning the hyper-parameters. This algorithm implicitly computes step-size and
           efficiently estimates local Lipschitz smoothness of stochastic functions. The numerical experiments
           demonstrate the algorithm's strong performance compared to its classical counterparts and other
           state-of-the-art first-order methods.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>February 18, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.09234">"ADOM: Accelerated Decentralized Optimization Method for Time-Varying Networks"</a> -
          joint work with <a href="http://dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://shulgin-egor.github.io">Egor Shulgin</a>, <a href="https://scholar.google.com/citations?hl=ru&user=sEjyzkgAAAAJ">Alexander Rogozin</a>, and
          <a href="https://mipt.ru/education/chairs/dm/staff/gasnikov-aleksandr-vladimirovich.php">Alexander Gasnikov</a>.<br>
          <br>

          Abstract: <i>
          We propose ADOM - an accelerated method for smooth and strongly convex decentralized optimization over time-varying networks.
          ADOM uses a dual oracle, i.e., we assume access to the gradient of the Fenchel conjugate of the individual loss functions.
          Up to a constant factor, which depends on the network structure only, its communication complexity is the same as that of
          accelerated Nesterov gradient method (Nesterov, 2003). To the best of our knowledge, only the algorithm of Rogozin et al. (2019)
          has a convergence rate with similar properties. However, their algorithm converges under the very restrictive assumption that
          the number of network changes can not be greater than a tiny percentage of the number of iterations. This assumption is hard
          to satisfy in practice, as the network topology changes usually can not be controlled. In contrast, ADOM merely requires the
          network to stay connected throughout time.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 16, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.08374">"IntSGD: Floatless Compression of Stochastic Gradients"</a> -
          joint work with <a href="http://konstmish.github.io">Konstantin Mishchenko</a>, and
          <a href="https://scholar.google.com/citations?user=H9GqvAYAAAAJ&hl=en">Bokun Wang</a> and <a href="http://dmitry-kovalev.com">Dmitry Kovalev</a>.<br>
          <br>

          Abstract: <i>
          We propose a family of lossy integer compressions for Stochastic Gradient Descent (SGD) that do not communicate a single float. This is achieved by multiplying floating-point vectors with a number known to every device and then rounding to an integer number. Our theory shows that the iteration complexity of SGD does not change up to constant factors when the vectors are scaled properly. Moreover, this holds for both convex and non-convex functions, with and without overparameterization. In contrast to other compression-based algorithms, ours preserves the convergence rate of SGD even on non-smooth problems. Finally, we show that when the data is significantly heterogeneous, it may become increasingly hard to keep the integers bounded and propose an alternative algorithm, IntDIANA, to solve this type of problems.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>





          <h3>February 16, 2021</h3>
          <h1>Talk at MBZUAI</h1>
          <br>

          Today I gave a research seminar talk at <a href="https://mbzuai.ac.ae">MBZUAI.</a> I spoke about randomized second order methods.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>





          <h3>February 15, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.07845">"MARINA: Faster Non-Convex Distributed Learning with Compression"</a> -
          joint work with <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>, and
          <a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a> and <a href="https://zhizeli.github.io">Zhize Li</a>.<br>
          <br>

          Abstract: <i>
          We develop and analyze MARINA: a new communication efficient method for non-convex distributed learning over heterogeneous datasets. MARINA employs a novel communication compression strategy based on the compression of gradient differences which is reminiscent of but different from the strategy employed in the DIANA method of Mishchenko et al (2019). Unlike virtually all competing distributed first-order methods, including DIANA, ours is based on a carefully designed biased gradient estimator, which is the key to its superior theoretical and practical performance. To the best of our knowledge, the communication complexity bounds we prove for MARINA are strictly superior to those of all previous first order methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and PP-MARINA. The first method is designed for the case when the local loss functions owned by clients are either of a finite sum or of an expectation form, and the second method allows for partial participation of clients -- a feature important in federated learning. All our methods are superior to previous state-of-the-art methods in terms of the oracle/communication complexity. Finally, we provide convergence analysis of all methods for problems satisfying the Polyak-Lojasiewicz condition.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 14, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.07245">"Smoothness Matrices Beat Smoothness Constants: Better Communication Compression Techniques for Distributed Optimization"</a> -
          joint work with <a href="https://mher-safaryan.github.io">Mher Safaryan</a>, and
          <a href="https://www.ttic.edu/faculty/hanzely/">Filip Hanzely</a>.<br>
          <br>

          Abstract: <i>
           Large scale distributed optimization has become the default tool for the training of supervised machine
           learning models with a large number of parameters and training data. Recent advancements in the field
           provide several mechanisms for speeding up the training, including compressed communication, variance
           reduction and acceleration. However, none of these methods is capable of exploiting the inherently rich data-dependent smoothness
           structure of the local losses beyond standard smoothness constants. In this paper, we argue that when
           training supervised models, smoothness matrices -- information-rich generalizations of the ubiquitous
           smoothness constants -- can and should be exploited for further dramatic gains, both in theory and practice.
           In order to further alleviate the communication burden inherent in distributed optimization, we propose a
           novel communication sparsification strategy that can take full advantage of the smoothness matrices
           associated with local losses. To showcase the power of this tool, we describe how our sparsification
           technique can be adapted to three distributed optimization algorithms -- DCGD, DIANA and ADIANA -- yielding
           significant savings in terms of communication complexity. The new methods always outperform the baselines,
           often dramatically so.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 13, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.07158">"Distributed Second Order Methods with Fast Rates and Compressed Communication"</a> -
          joint work with <a href="https://rustem-islamov.github.io">Rustem Islamov</a>, and
          <a href="https://qianxunk.github.io">Xun Qian</a>.<br>
          <br>

          Abstract: <i>
           We develop several new communication-efficient second-order methods for distributed optimization. Our first method, NEWTON-STAR, is a variant of Newton's method from which it inherits its fast local quadratic rate. However, unlike Newton's method, NEWTON-STAR enjoys the same per iteration communication cost as gradient descent. While this method is impractical as it relies on the use of certain unknown parameters characterizing the Hessian of the objective function at the optimum, it serves as the starting point which enables us design practical variants thereof with strong theoretical guarantees. In particular, we design a stochastic sparsification strategy for learning the unknown parameters in an iterative fashion in a communication efficient manner. Applying this strategy to NEWTON-STAR leads to our next method, NEWTON-LEARN, for which we prove local linear and superlinear rates independent of the condition number. When applicable, this method can have dramatically superior convergence behavior when compared to state-of-the-art methods. Finally, we develop a globalization strategy using cubic regularization which leads to our next method, CUBIC-NEWTON-LEARN, for which we prove global sublinear and linear convergence rates, and a fast superlinear rate. Our results are supported with experimental results on real datasets, and show several orders of magnitude improvement on baseline and state-of-the-art methods in terms of communication complexity.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>February 12, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.06704">"Proximal and Federated Random Reshuffling"</a> -
          joint work with <a href="https://konstmish.github.io">Konstantin Mishchenko</a>, and
          <a href="https://rka97.github.io">Ahmed Khaled</a>.<br>
          <br>

          Abstract: <i>
            Random Reshuffling (RR), also known as Stochastic Gradient Descent (SGD) without replacement, is a popular and
            theoretically grounded method for finite-sum minimization. We propose two new algorithms: Proximal and Federated
            Random Reshuffing (ProxRR and FedRR). The first algorithm, ProxRR, solves composite convex finite-sum minimization
            problems in which the objective is the sum of a (potentially non-smooth) convex regularizer and an average of n smooth
            objectives. We obtain the second algorithm, FedRR, as a special case of ProxRR applied to a reformulation of distributed
            problems with either homogeneous or heterogeneous data. We study the algorithms' convergence properties with constant
            and decreasing stepsizes, and show that they have considerable advantages over Proximal and Local SGD. In particular,
            our methods have superior complexities and ProxRR evaluates the proximal operator once per epoch only. When the proximal
            operator is expensive to compute, this small difference makes ProxRR up to n times faster than algorithms that evaluate
            the proximal operator in every iteration. We give examples of practical optimization tasks where the proximal operator
            is difficult to compute and ProxRR has a clear advantage. Finally, we corroborate our results with experiments on real
            data sets.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 10, 2021</h3>
          <h1>Best Paper Award @ NeurIPS SipcyFL 2020</h1>
  <br>
    Super happy about this surprise prize; and huge congratulations to my outstanding student and collaborator <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.
    The paper was recently accepted to <a href="https://openreview.net/forum?id=vYVI1CHPaQg">ICLR 2021</a>, check it out!

<br>
<br>
<img alt="" src="imgs/NeurIPS-2020-SpicyFL-Prize.jpeg" width="600" >
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>January 24, 2021</h3>
          <h1>Spring 2021 Semester Starts at KAUST</h1>
  <br>
As of today, the Spring semester starts at KAUST. The timing of this every year conflicts with the endgame before the ICML submission deadline, and this year is no different. Except for Covid-19.
I am teaching <a href="https://piazza.com/kaust.edu.sa/spring2021/cs332">CS 332: Federated Learning</a> on Sundays and Tuesdays. The first class is today.
  <br>



          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>January 23, 2021</h3>
          <h1>Three Papers Accepted to AISTATS 2021</h1>
            <br>
We've had some papers accepted to <a href="https://aistats.org/aistats2021/">The 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)</a>. The conference will be held virtually during April 13-15, 2021. Here are the papers:<br><br>


1. <a href="http://proceedings.mlr.press/v130/kovalev21a.html">A linearly convergent algorithm for decentralized optimization: sending less bits for free!</a>, joint work with  <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://scholar.google.com/citations?user=ldJpvE8AAAAJ&hl=en">Anastasia Koloskova</a>, <a href="https://people.epfl.ch/martin.jaggi/?lang=en">Martin Jaggi</a>, and <a href="https://www.sstich.ch">Sebastian U. Stich</a>.<br><br>

<i>Abstract:</i>  Decentralized optimization methods enable on-device training of machine learning models without a central coordinator. In many scenarios communication between devices is energy demanding and time consuming and forms the bottleneck of the entire system.
We propose a new randomized first-order method which tackles the communication bottleneck by applying randomized compression operators to the communicated messages. By combining our scheme with a new variance reduction technique that progressively throughout the iterations reduces the adverse effect of the injected quantization noise, we obtain the first scheme that converges linearly on strongly convex decentralized problems while using compressed communication only.
We prove that our method can solve the problems without any increase in the number of communications compared to the baseline which does not perform any communication compression while still allowing for a significant compression factor which depends on the conditioning of the problem and the topology of the network. Our key theoretical findings are supported by numerical experiments. <br> <br>


2. <a href="http://proceedings.mlr.press/v130/gorbunov21a.html">Local SGD: unified theory and new efficient methods</a>, joint work with  <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> and <a href="https://fhanzely.github.io">Filip Hanzely</a>.<br><br>

<i>Abstract:</i> We present a unified framework for analyzing local SGD methods in the convex and strongly convex regimes for distributed/federated training of supervised machine learning models. We recover several known methods as a special case of our general framework, including Local-SGD/FedAvg, SCAFFOLD, and several variants of SGD not originally designed for federated learning. Our framework covers both the identical and heterogeneous data settings, supports both random and deterministic number of local steps, and can work with a wide array of local stochastic gradient estimators, including shifted estimators which are able to adjust the fixed points of local iterations for faster convergence. As an application of our framework, we develop multiple novel FL optimizers which are superior to existing methods. In particular, we develop the first linearly converging local SGD method which does not require any data homogeneity or other strong assumptions. <br> <br>

3. <a href="http://proceedings.mlr.press/v130/horvath21a.html">Hyperparameter transfer learning with adaptive complexity</a>, joint work with  <a href="https://samuelhorvath.github.io">Samuel Horváth</a> and <a href="https://aaronkl.github.io">Aaron Klein</a>, and <a href="http://www0.cs.ucl.ac.uk/staff/c.archambeau/">Cedric Archambeau</a>.<br><br>

<i>Abstract:</i>  Bayesian optimization (BO) is a data-efficient approach to automatically tune the hyperparameters of machine learning models. In practice, one frequently has to solve similar hyperparameter tuning problems sequentially. For example, one might have to tune a type of neural network learned across a series of different classification problems. Recent work on multi-task BO exploits knowledge gained from previous hyperparameter tuning tasks to speed up a new tuning task. However, previous approaches do not account for the fact that BO is a sequential decision making procedure. Hence, there is in general a mismatch between the number of evaluations collected in the current tuning task compared to the number of evaluations accumulated in all previously completed tasks. In this work, we enable multi-task BO to compensate for this mismatch, such that the transfer learning procedure is able to handle different data regimes in a principled way. We propose a new multi-task BO method that learns a set of ordered, non-linear basis functions of increasing complexity via nested drop-out and automatic relevance determination. Experiments on a variety of hyperparameter tuning problems show that our method improves the sample efficiency of recently published multi-task BO methods.
<br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>January 22, 2021</h3>
          <h1>Paper Accepted to Information and Inference: A Journal of the IMA</h1>
            <br>
          Our paper "Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor”, joint work with  <a href="https://mher-safaryan.github.io">Mher Safaryan</a> and <a href="https://shulgin-egor.github.io">Egor Shulgin</a>,  was accepted to
      <a href="https://academic.oup.com/imaiai">Information and Inference: A Journal of the IMA</a>.<br><br>

    <i>Abstract:</i> In order to mitigate the high communication cost in distributed and federated learning,
    various vector compression schemes, such as quantization, sparsification and dithering, have become
    very popular. In designing a compression method, one aims to communicate as few bits
    as possible, which minimizes the cost per communication round, while at the same time attempting to
    impart as little distortion (variance) to the communicated messages as possible, which minimizes
    the adverse effect of the compression on the overall number of communication rounds. However,
     intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more
     distorted the messages become. We formalize this intuition and prove an  uncertainty principle for
     randomized compression operators, thus quantifying this limitation mathematically, and
     effectively providing asymptotically tight lower bounds on what might be achievable with communication
     compression. Motivated by these developments, we call for the search for the optimal compression
      operator. In an attempt to take a first step in this direction, we consider an unbiased compression
       method inspired by the Kashin representation of vectors, which we call Kashin compression (KC).
       In contrast to all previously proposed compression mechanisms, KC enjoys a dimension independent
       variance bound for which we derive an explicit formula even in the regime when only a few bits
       need to be communicate per each vector entry.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>January 12, 2021</h3>
          <h1>Paper Accepted to ICLR 2021</h1>
  <br>
    Our paper "A Better Alternative to Error Feedback for Communication-efficient Distributed Learning'', joint work with  <a href="https://samuelhorvath.github.io">Samuel Horváth</a>,  was accepted to
      <a href="https://openreview.net/forum?id=vYVI1CHPaQg">The 9th International Conference on Learning Representations (ICLR 2021)</a>.<br><br>

    <i>Abstract:</i>  Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed compute systems. A key bottleneck of such systems is the communication overhead for exchanging information across the workers, such as stochastic gradients. Among the many techniques proposed to remedy this issue, one of the most successful is the framework of compressed communication with error feedback (EF).
    EF remains the only known technique that can deal with the error induced by contractive compressors which are not unbiased, such as Top-K.
    In this paper, we propose a new and theoretically and practically better alternative to EF for dealing with contractive compressors. In particular, we propose a construction which can transform any contractive compressor into an induced unbiased compressor. Following this transformation, existing methods able to work with unbiased compressors can be applied. We show that our approach leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. We further extend our results to federated learning with partial participation following an arbitrary
    distribution over the nodes, and demonstrate the benefits thereof. We perform several numerical experiments which validate our theoretical
    findings.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




<h3>January 11, 2021</h3>
<h1>Call for Al-Khwarizmi Doctoral Fellowships (apply by Jan 22, 2021)</h1>
<br>

If you are from Europe and want to apply for a PhD position in my Optimization and Machine Learning group at KAUST, you may wish to apply for the
<a href="https://cemse.kaust.edu.sa/alkhwarizmi">European Science Foundation Al-Khwarizmi Doctoral Fellowship</a>.

<a href="https://cemse.kaust.edu.sa/alkhwarizmi"> <img alt="" src="imgs/al-khwarizmi.png" width="200"></a>

<br>
 Here is the official blurb:
<br>
<br>
"The Al-Khwarizmi Graduate Fellowship scheme invites applications for doctoral fellowships, with the submission deadline of 22 January 2021, 17:00 CET.
The King Abdullah University of Science and Technology (KAUST) in the Kingdom of Saudi Arabia with support from the European Science Foundation (ESF)
launches a competitive doctoral fellowship scheme to welcome students from the European continent for a research journey to a top international university
in the Middle East. The applications will be evaluated via an independent peer-review process managed by the ESF. The selected applicants will be offered
generous stipends and free tuition for Ph.D. studies within one of KAUST academic programs. Strong applicants who were not awarded a Fellowship but passed
KAUST admission requirements will be offered the possibility to join the University as regular Ph.D. students with the standard benefits that include the
usual stipends and free tuition."
<br>
<br>
- Submission deadline = 22 January 2021 @ 17:00 CET <br>
- Duration of the Fellowship = 3 years (extensions may be considered in duly justified cases)<br>
- Annual living allowance/stipend = USD 38,000  (net)<br>
- Approx USD 50,000 annual benefits = free tuition, free student housing on campus, relocation support, and medical and dental coverage<br>
- Each Fellowship includes a supplementary grant of USD 6,000 at the Fellow’s disposal for research-related expenses such as conference attendance<br>
- The applications must be submitted in two steps, with the formal documents and transcripts to be submitted to KAUST Admissions in Step 1, and
the research proposal to be submitted to the ESF in Step 2. Both steps should be completed in parallel before the call deadline.<br>

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




          <h3>December 15, 2020</h3>
          <h1>Vacation</h1>
          <br>
I am on vacation until early January, 2021.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>December 12, 2020</h3>
          <h1>Paper Accepted to NSDI 2021</h1>
          <br>
Our paper ``Scaling Distributed Machine Learning with In-Network Aggregation'', joint work with  Amedeo Sapio, Marco Canini, Chen-Yu Ho,
Jacob Nelson, Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, and Dan R. K. Ports,  was accepted to
<a href="https://www.usenix.org/conference/nsdi21?ref=infosec-conferences.com">The 18th USENIX Symposium on Networked Systems Design and Implementation
(NSDI '21 Fall)</a>. <br><br>

<i>Abstract:</i>  Training machine learning models in parallel is an increasingly important workload. We accelerate distributed parallel training by designing a communication primitive that uses a programmable switch dataplane to execute a key step of the training process. Our approach, SwitchML, reduces the volume of exchanged data by aggregating the model updates from multiple workers in the network. We co-design the switch processing with the end-host protocols and ML frameworks to provide an efficient solution that speeds up training by up to 5.5× for a number of real-world benchmark models.


          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>December 8, 2020</h3>
          <h1>Fall 2020 Semester at KAUST is Over</h1>
          <br>


          The Fall 2020 semester at KAUST is now over; I've had  alot of fun teaching my CS 331 class (Stochastic Gradient Descent Methods). At the very end I run into some
          LaTeX issues after upgrading to Big Sur on Mac - should not have done that...
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>December 6, 2020</h3>
          <h1>NeurIPS 2020 Started</h1>
          <br>


          Me and the members of my group will be attending <a href="https://neurips.cc/virtual/2020/public/index.html">NeurIPS 2020</a> - the event is starting today.
          Marco Cuturi and me will co-chair the <a href="https://neurips.cc/virtual/2020/public/session_oral_21084.html">Optimization session (Track 21) on Wednesday</a>. I am particularly looking forward to the workshops: <a href="https://opt-ml.org">OPT2020</a>, <a href="https://ppml-workshop.github.io">PPML</a> and <a href="http://128.1.38.43/SpicyFL/2020/">SpicyFL.</a>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>November 24, 2020</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://opt-ml.org/papers/2020/paper_61.pdf">"Error Compensated Loopless SVRG for Distributed Optimization"</a> -
          joint work with <a href="https://qianxunk.github.io/">Xun Qian</a>, <a href="https://www.researchgate.net/profile/Hanze_Dong">Hanze Dong</a>, and
          <a href="http://tongzhang-ml.org/">Tong Zhang</a>.<br>
          <br>

          Abstract: <i>
            A key bottleneck in distributed training of large scale machine learning models is the overhead related
            to communication of gradients. In order to reduce the communicated cost, gradient compression
            (e.g., sparsification and quantization) and error compensation techniques are often used. In this
            paper, we propose and study a new efficient method in this space: error compensated loopless SVRG
            method (L-SVRG). Our method is capable of working with any contraction compressor (e.g., TopK
            compressor), and we perform analysis for strongly convex optimization problems in the composite
            case and smooth case. We prove linear convergence rates for both cases and show that in the smooth
            case the rate has a better dependence on the contraction factor associated with the compressor.
            Further, we show that in the smooth case, and under some certain conditions, error compensated
            L-SVRG has the same convergence rate as the vanilla L-SVRG method. Numerical experiments are
            presented to illustrate the efficiency of our method.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>November 24, 2020</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://opt-ml.org/papers/2020/paper_59.pdf">"Error Compensated Proximal SGD and RDA"</a> -
          joint work with <a href="https://qianxunk.github.io/">Xun Qian</a>, <a href="https://www.researchgate.net/profile/Hanze_Dong">Hanze Dong</a>, and
          <a href="http://tongzhang-ml.org/">Tong Zhang</a>.<br>
          <br>

          Abstract: <i>
            Communication cost is a key bottleneck in distributed training of large machine learning models. In
            order to reduce the amount of communicated data, quantization and error compensation techniques
            have recently been studied. While the error compensated stochastic gradient descent (SGD) with
            contraction compressor (e.g., TopK) was proved to have the same convergence rate as vanilla SGD in
            the smooth case, it is unknown in the regularized case. In this paper, we study the error compensated
            proximal SGD and error compensated regularized dual averaging (RDA) with contraction compressor
            for the composite finite-sum optimization problem. Unlike the smooth case, the leading term in the
            convergence rate of error compensated proximal SGD is dependent on the contraction compressor
            parameter in the composite case, and the dependency can be improved by introducing a reference
            point to reduce the compression noise. For error compensated RDA, we can obtain better dependency
            of compressor parameter in the convergence rate. Extensive numerical experiments are presented to
            validate the theoretical results.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



<h3>November 6, 2020</h3>
<h1>ICML 2021 Area Chair</h1>
<br>


I've accepted an invite to serve the machine learning community as an Area Chair for <a href="https://icml.cc/Conferences/2021">ICML 2021.</a>
I'll be a tough (but friendly) Area Chair:  I expect the best from the reviewers and will do all I can to make sure the reviews and reviewer discussion
are as fair and substantial as possible.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>November 3, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2011.02828">"Local SGD: Unified Theory and New Efficient Methods"</a> -
joint work with <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> and <a href="https://fhanzely.github.io/index.html">Filip Hanzely.</a><br>
<br>

Abstract: <i>
We present a unified framework for analyzing local SGD methods in the convex and strongly convex regimes for distributed/federated training of supervised machine learning models. We recover several known methods as a special case of our general framework, including Local-SGD/FedAvg, SCAFFOLD, and several variants of SGD not originally designed for federated learning. Our framework covers both the identical and heterogeneous data settings, supports both random and deterministic number of local steps, and can work with a wide array of local stochastic gradient estimators, including shifted estimators which are able to adjust the fixed points of local iterations for faster convergence. As an application of our framework, we develop multiple novel FL optimizers which are superior to existing methods. In particular, we develop the first linearly converging local SGD method which does not require any data homogeneity or other strong assumptions.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>November 3, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2011.01697">"A Linearly Convergent Algorithm for Decentralized Optimization: Sending Less Bits for Free!"</a> -
joint work with <a href="https://www.dmitry-kovalev.com ">Dmitry Kovalev</a>, <a href="https://scholar.google.com/citations?user=ldJpvE8AAAAJ&hl=en">Anastasia Koloskova</a>,
<a href="https://www.epfl.ch/labs/mlo/">Martin Jaggi</a> and <a href="https://scholar.google.com/citations?user=8l-mDfQAAAAJ&hl=en">Sebastian U. Stich</a>.<br>
<br>

Abstract: <i> Decentralized optimization methods enable on-device training of machine learning models without a central coordinator. In many scenarios communication between devices is energy demanding and time consuming and forms the bottleneck of the entire system. We propose a new randomized first-order method which tackles the communication bottleneck by applying randomized compression operators to the communicated messages. By combining our scheme with a new variance reduction technique that progressively throughout the iterations reduces the adverse effect of the injected quantization noise, we obtain the first scheme that converges linearly on strongly convex decentralized problems while using compressed communication only. We prove that our method can solve the problems without any increase in the number of communications compared to the baseline which does not perform any communication compression while still allowing for a significant compression factor which depends on the conditioning of the problem and the topology of the network. Our key theoretical findings are supported by numerical experiments.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 26, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.13723">"Optimal Client Sampling for Federated Learning"</a> -
joint work with Wenlin Chen, and <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.<br>
<br>

Abstract: <i>
It is well understood that client-master communication can be a primary bottleneck in Federated Learning. In this work, we address this issue with a novel client subsampling scheme, where we restrict the number of clients allowed to communicate their updates back to the master node. In each communication round, all participated clients compute their updates, but only the ones with "important" updates communicate back to the master. We show that importance can be measured using only the norm of the update and we give a formula for optimal client participation. This formula minimizes the distance between the full update, where all clients participate, and our limited update, where the number of participating clients is restricted. In addition, we provide a simple algorithm that approximates the optimal formula for client participation which only requires secure aggregation and thus does not compromise client privacy. We show both theoretically and empirically that our approach leads to superior performance for Distributed SGD (DSGD) and Federated Averaging (FedAvg) compared to the baseline where participating clients are sampled uniformly. Finally, our approach is orthogonal to and compatible with existing methods for reducing communication overhead, such as local methods and communication compression methods.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 23, 2020</h3>
<h1>New Paper (Spotlight @ NeurIPS 2020)</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.12292">"Linearly Converging Error Compensated SGD"</a> -
joint work with <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>, <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
and Dmitry Makarenko.<br>
<br>

Abstract: <i>
In this paper, we propose a unified analysis of variants of distributed SGD with arbitrary compressions and delayed updates. Our framework is general enough to cover different variants of quantized SGD, Error-Compensated SGD (EC-SGD) and SGD with delayed updates (D-SGD). Via a single theorem, we derive the complexity results for all the methods that fit our framework. For the existing methods, this theorem gives the best-known complexity results. Moreover, using our general scheme, we develop new variants of SGD that combine variance reduction or arbitrary sampling with error feedback and quantization and derive the convergence rates for these methods beating the state-of-the-art results. In order to illustrate the strength of our framework, we develop 16 new methods that fit this. In particular, we propose the first method called EC-SGD-DIANA that is based on error-feedback for biased compression operator and quantization of gradient differences and prove the convergence guarantees showing that EC-SGD-DIANA converges to the exact optimum asymptotically in expectation with constant learning rate for both convex and strongly convex objectives when workers compute full gradients of their loss functions. Moreover, for the case when the loss function of the worker has the form of finite sum, we modified the method and got a new one called EC-LSVRG-DIANA which is the first distributed stochastic method with error feedback and variance reduction that converges to the exact optimum asymptotically in expectation with a constant learning rate.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>October 16, 2020</h3>
<h1>Nicolas Loizou runner-up in OR Society Doctoral Award</h1>
<br>


<a href="https://nicolasloizou.github.io/">Nicolas Loizou</a>, my former PhD student (and last student to have graduated from Edinburgh after I left for KAUST), has been selected as a runner-up in the <a href="https://www.theorsociety.com/membership/awards-medals-and-scholarships/the-doctoral-award/previous-awards/">2019
OR Society Doctoral Award</a> competition. Congratuations! <br><br>

Nicolas' PhD thesis: <a href="https://arxiv.org/abs/1909.12176">Randomized Iterative Methods for Linear Systems: Momentum, Inexactness and Gossip</a>


<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 7, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.03246">"Optimal Gradient Compression for Distributed and Federated Learning"</a> -
joint work with <a href="https://alyazeedbasyoni.wixsite.com/blog">Alyazeed Albasyoni</a>, <a href="https://mher-safaryan.github.io">Mher Safaryan</a>,
and <a href="https://lcondat.github.io">Laurent Condat</a>.<br>
<br>

Abstract: <i>
Communicating information, like gradient vectors, between computing nodes in distributed and federated learning is typically an unavoidable burden, resulting in scalability issues. Indeed, communication might be slow and costly. Recent advances in communication-efficient training algorithms have reduced this bottleneck by using compression techniques, in the form of sparsification, quantization, or low-rank approximation. Since compression is a lossy, or inexact, process, the iteration complexity is typically worsened; but the total communication complexity can improve significantly, possibly leading to large computation time savings. In this paper, we investigate the fundamental trade-off between the number of bits needed to encode compressed vectors and the compression error. We perform both worst-case and average-case analysis, providing tight lower bounds. In the worst-case analysis, we introduce an efficient compression operator, Sparse Dithering, which is very close to the lower bound. In the average-case analysis, we design a simple compression operator, Spherical Compression, which naturally achieves the lower bound. Thus, our new compression schemes significantly outperform the state of the art. We conduct numerical experiments to illustrate this improvement.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 5, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.02372">"Lower Bounds and Optimal Algorithms for Personalized Federated Learning"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>, <a href="https://slavomirhanzely.wordpress.com">Slavomír Hanzely</a>,
and <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.<br>
<br>

Abstract: <i>
In this work, we consider the optimization formulation of personalized federated learning recently introduced by Hanzely and Richtárik (2020)
which was shown to give an alternative explanation to the workings of local SGD methods. Our first contribution is establishing the first lower
bounds for this formulation, for both the communication complexity and the local oracle complexity. Our second contribution is the design of
several optimal methods matching these lower bounds in almost all regimes. These are the first provably optimal methods for personalized federated
learning. Our optimal methods include an accelerated variant of FedProx, and an accelerated variance-reduced version of FedAvg / Local SGD. We
demonstrate the practical superiority of our methods through extensive numerical experiments.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>October 2, 2020</h3>
<h1>New Paper </h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.00952">"Distributed Proximal Splitting Algorithms with Rates and Acceleration"</a> -
joint work with <a href="https://lcondat.github.io">Laurent Condat</a> and Grigory Malinovsky.<br>
<br>

Abstract: <i>
We analyze several generic proximal splitting algorithms well suited for large-scale convex nonsmooth optimization. We derive sublinear and linear convergence results with new rates on the function value suboptimality or distance to the solution, as well as new accelerated versions, using varying stepsizes. In addition, we propose distributed variants of these algorithms, which can be accelerated as well. While most existing results are ergodic, our nonergodic results significantly broaden our understanding of primal-dual optimization algorithms.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 2, 2020</h3>
<h1>New Paper </h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.00892">"Variance-Reduced Methods for Machine Learning"</a> -
joint work with <a href="https://gowerrobert.github.io">Robert Mansel Gower</a>,
<a href="https://scholar.google.com/citations?user=5BtEUJcAAAAJ&hl=en">Mark Schmidt</a> and <a href="https://www.di.ens.fr/~fbach/">Francis Bach</a>.<br>
<br>


Abstract: <i>
Stochastic optimization lies at the heart of machine learning, and its cornerstone is stochastic gradient descent (SGD), a method introduced over 60 years ago. The last 8 years have seen an exciting new development: variance reduction (VR) for stochastic optimization methods. These VR methods excel in settings where more than one pass through the training data is allowed, achieving a faster convergence than SGD in theory as well as practice. These speedups underline the surge of interest in VR methods and the fast-growing body of work on this topic. This review covers the key principles and main developments behind VR methods for optimization with finite data sets and is aimed at non-expert readers. We focus mainly on the convex setting, and leave pointers to readers interested in extensions for minimizing non-convex functions.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 2, 2020</h3>
<h1>New Paper </h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.00091">"Error Compensated Distributed SGD Can Be Accelerated"</a> -
joint work with <a href="https://qianxunk.github.io">Xun Qian</a> and
<a href="https://www.cse.ust.hk/admin/people/faculty/profile/tongzhang">Tong Zhang</a>.<br>
<br>


Abstract: <i>
Gradient compression is a recent and increasingly popular technique for reducing the communication cost in distributed training of large-scale machine learning models. In this work we focus on developing efficient distributed methods that can work for any compressor satisfying a certain contraction property, which includes both unbiased (after appropriate scaling) and biased compressors such as RandK and TopK. Applied naively, gradient compression introduces errors that either slow down convergence or lead to divergence. A popular technique designed to tackle this issue is error compensation/error feedback. Due to the difficulties associated with analyzing biased compressors, it is not known whether gradient compression with error compensation can be combined with Nesterov's acceleration. In this work, we show for the first time that error compensated gradient compression methods can be accelerated. In particular, we propose and study the error compensated loopless Katyusha method, and establish an accelerated linear convergence rate under standard assumptions. We show through numerical experiments that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 30, 2020</h3>
<h1>Eduard Gorbunov Organizes All-Russian Optimization Research Seminar</h1>
<br>

My serial intern and collaborator <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> is the organizer of an <a href="http://www.mathnet.ru/php/conference.phtml?&eventID=31&confid=1794&option_lang=rus">All-Russian
Research Seminar Series on Mathematical Optimization.</a> There have been 14 speakers at this event so far, including Eduard and <a href="https://konstmish.github.io">Konstantin
  Mishchenko.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 28, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/1901.09997v4">"Quasi-Newton Methods for Deep Learning: Forget the Past, Just Sample"</a> -
joint work with <a href="https://ioe.engin.umich.edu/people/albert-s-berahas/">Albert S. Berahas</a>,
<a href="https://coral.ise.lehigh.edu/maj316/">Majid Jahani</a>,  and <a href="http://mtakac.com">Martin Takáč</a>.<br>
<br>


Abstract: <i>
We present two sampled quasi-Newton methods for deep learning: sampled LBFGS (S-LBFGS) and sampled LSR1 (S-LSR1). Contrary to the classical variants of these methods that sequentially build Hessian or inverse Hessian approximations as the optimization progresses, our proposed methods sample points randomly around the current iterate at every iteration to produce these approximations. As a result, the approximations constructed make use of more reliable (recent and local) information, and do not depend on past iterate information that could be significantly stale. Our proposed algorithms are efficient in terms of accessed data points (epochs) and have enough concurrency to take advantage of parallel/distributed computing environments. We provide convergence guarantees for our proposed methods. Numerical tests on a toy classification problem as well as on popular benchmarking neural network training tasks reveal that the methods outperform their classical variants.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>September 22, 2020</h3>
<h1>Adil Salim Giving a Virtual Talk at the Fields Institute</h1>
<br>

<a href="https://adil-salim.github.io">Adil Salim</a> is giving a virtual talk today at the <a href="http://www.fields.utoronto.ca/activities/20-21/dynamical">Second Symposium on Machine Learning and Dynamical Systems</a>, organized at the Fields Institute.
His talk "Primal Dual Interpretation of the Proximal Gradient Langevin Algorithm", based
on <a href="https://arxiv.org/abs/2006.09270">this paper</a>, is <a href="https://www.youtube.com/watch?v=4lGjecpVWzE">available
on YouTube</a>. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 30, 2020</h3>
<h1>Fall 2020 Teaching</h1>
<br>

The Fall 2020 semester started. I am teaching CS 331: Stochastic Gradient Descent Methods. <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>September 15, 2020</h3>
<h1>Four New Group Members</h1>
<br>

Four new people joined my team in August/September 2020:
<br><br>

<b>Konstantin Burlachenko</b> joins as a CS PhD student. Konstantin got a Master’s degree in CS and Control Systems from Bauman Moscow State University in 2009. 
He has since worked at a number of companies, most recently as a Senior Developer at Yandex and NVidia and a Principal Engineer at Huawei. Konstantin is
interested in software development, optimization, federated learning, graphics and vision, and forecasting models. Konstantin attended several courses at
Stanford and obtained two graduate certificates [<a href="https://online.stanford.edu/programs/data-models-and-optimization-graduate-certificate">1</a>]
[<a href="https://online.stanford.edu/programs/artificial-intelligence-graduate-certificate">2</a>].

<br>
<br>
<b>Grigory Malinovsky</b> joins as an MS/PhD student in <a href="https://cemse.kaust.edu.sa/amcs">AMCS</a>  after a successful internship at KAUST in early 2020 which led to
the paper <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/5019-Paper.pdf">“From Local SGD to Local Fixed-Point Methods for Federated Learning"</a>,
joint with <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="http://elnurgasanov.me">Elnur Gasanov</a>, Laurent Condat and myself. The paper appeared in ICML 2020.  Grisha has graduated with a BS degree from Moscow Institute of Physics and Technology (MIPT) with a thesis entitled “Averaged Heavy Ball Method” under the supervision of Boris Polyak.
Among Grigory’s successes belong:
<br>
- Abramov’s scholarship for students with the best grades at MIPT, 2016
<br>
- Participant in the final round of All-Russian Physics Olympiad, 2014
<br>
- Bronze medal at International Zhautykov Olympiad in Physics, 2014
<br>
- Prize winner in the final round of All-Russian Physics Olympiad, 2013
<br>
Grisha enjoys basketball, fitness, football and table tennis. He speaks a bit of Tatar.

<br><br>
<b>Igor Sokolov</b> joins as an MS student in <a href="https://cemse.kaust.edu.sa/amcs">AMCS.</a> Igor has a BS degree from MIPT’s Department of Control and
Applied Mathematics. Igor is the recipient of several prizes, including at the Phystech Olympiad in Physics (2014), and regional stage of the All Russian
Olympiad in Physics (2014). He won 2nd place at the Programming Conference (2012 and 2013) and was a winner of the Programming Olympiad (2011); all at the
Computer Training Center. Igor enjoys snowboarding, cycling and jogging. He coauthored a paper which will soon be posted onto arXiv.

<br><br>
<b>Bokun Wang</b> joins as a remote intern and will work in the lab for 6 months. Bokun coauthored several papers, including <a href="https://arxiv.org/abs/2005.01209">
``Riemannian Stochastic Proximal Gradient Methods for Nonsmooth Optimization over the Stiefel Manifold”.</a> He has recently interned with Tong Zhang (HKUST).
Bokun is a graduate student at UC Davis, and has a BS degree in Computer Science from University of Electronic Science and Technology of China.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 25, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2008.10898">"PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization"</a> -
joint work with <a href="https://zhizeli.github.io">Zhize Li</a>, <a href="https://mine.kaust.edu.sa/Pages/Hongyan.aspx">Hongyan Bao</a>,  and <a href="https://scholar.google.com/citations?user=BhRJe4wAAAAJ&hl=en">Xiangliang Zhang</a>.<br>
<br>


Abstract: <i>
In this paper, we propose a novel stochastic gradient estimator---ProbAbilistic Gradient Estimator (PAGE)---for nonconvex optimization. PAGE is easy to implement as it is designed via a small adjustment to vanilla SGD: in each iteration, PAGE uses the vanilla minibatch SGD update with probability p and reuses the previous gradient with a small adjustment, at a much lower computational cost, with probability 1−p. We give a simple formula for the optimal choice of p. We prove tight lower bounds for nonconvex problems, which are of independent interest. Moreover, we prove matching upper bounds both in the finite-sum and online regimes, which establish that Page is an optimal method. Besides, we show that for nonconvex functions satisfying the Polyak-Łojasiewicz (PL) condition, PAGE can automatically switch to a faster linear convergence rate. Finally, we conduct several deep learning experiments (e.g., LeNet, VGG, ResNet) on real datasets in PyTorch, and the results demonstrate that PAGE converges much faster than SGD in training and also achieves the higher test accuracy, validating our theoretical results and confirming the practical superiority of PAGE.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 25, 2020</h3>
<h1>Apple Virtual Workshop on Privacy Preserving Machine Learning</h1>
<br>

I have been invited to give a talk at the "Virtual Workshop on Privacy Preserving Machine Learning", hosted by <a href="https://www.apple.com">Apple</a>. The workshop is a two-day event starting today.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 19, 2020</h3>
<h1>Paper Accepted to SIAM Journal on Scientific Computing</h1>
<br>

The paper <a href="https://arxiv.org/abs/1903.07971">"Convergence Analysis of Inexact Randomized Iterative Methods"</a>, joint with <a href="https://www.maths.ed.ac.uk/~s1461357/">Nicolas Loizou</a>, was accepted to
<a href="https://www.siam.org/publications/journals/siam-journal-on-scientific-computing-sisc">SIAM Journal on Scientific Computing</a>.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>August 12, 2020</h3>
<h1>Paper Accepted to Computational Optimization and Applications</h1>
<br>

The paper <a href="https://arxiv.org/abs/1712.09677">"Momentum and Stochastic Momentum for Stochastic Gradient, Newton,
Proximal Point and Subspace Descent Methods"</a>, joint with <a href="https://www.maths.ed.ac.uk/~s1461357/">Nicolas Loizou</a>, was accepted to
<a href="https://www.springer.com/journal/10589">Computational Optimization and Applications</a>.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 8, 2020</h3>
<h1>New Research Intern</h1>
<br>



Wenlin Chen has joined my group as a remote intern until about the end of September 2020. During the internship, Wenlin will be working on communication efficient methods for federated learning.
Wenlin has a BS degree in mathematics from University of Manchester (ranked top 1.5% in the cohort), and is about to start an MPhil in Machine Learning at the University of Cambridge in October 2020.
Wenlin is a coauthor of an ECML 2020 paper entitled <a href="http://www.cs.man.ac.uk/%7Egbrown/publications/ecml2020webb.pdf">To Ensemble or Not Ensemble: When does End-To-End Training Fail?</a>
 where he investigated novel information-theoretic methods of training deep neural network ensembles, focusing on the resulting regularization effects and trade-offs between individual model
 capacity and ensemble diversity. He also conducted large-scale ensemble deep learning experiments using the university’s HPC Cluster CSF3.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 7, 2020</h3>
<h1>Senior PC Memeber for IJCAI 2021</h1>
<br>

I've accepted an invite to become  Senior Program Committee member for IJCAI 2021.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>







<h3>August 3, 2020</h3>
<h1>Paper Accepted to SIAM Journal on Optimization</h1>
<br>

The paper <a href="https://arxiv.org/abs/1902.03591">"Stochastic Three Points Method for Unconstrained Smooth Minimization"</a>, joint with <a href="https://ehbergou.github.io">El Houcine Bergou</a>,
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> was accepted to
<a href="https://www.siam.org/publications/journals/siam-journal-on-optimization-siopt">SIAM Journal on Optimization</a>.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 30, 2020</h3>
<h1>Filip Hanzely Defended his PhD Thesis</h1>
<br>

<a href="http://fhanzely.github.io">Filip Hanzely</a> defended his PhD thesis <a href="http://www.optimization-online.org/DB_FILE/2020/08/7967.pdf">"Optimization for
Supervised Machine Learning:  Randomized Algorithms for Data and Parameters"</a> today.
<br> <br>

Having started in Fall 2017
(I joined KAUST in March the same year), Filip is my first PhD student to graduate from KAUST. He managed to complete his
PhD in less than 3 years, and has done some trully amazing research, described by the committee (<a href="http://pages.cs.wisc.edu/~swright/">Stephen J Wright</a>, <a href="http://tongzhang-ml.org">Tong Zhang</a>,
<a href="https://www.mathematik.rwth-aachen.de/ca/cd/btbp/?allou=1&ikz=11&gguid=0x963949FB4292ED4DA761C2EE9DAD832B">Raul F Tempone</a>,
<a href="http://www.bernardghanem.com">Bernard Ghanem</a> and myself) as "Outstanding work, in all aspects. It is comprehensive as it synthesises various strands of current research, and is almost of
 an encyclopedic coverage. The work develops deep theoretical results, some of which answer long-standing open problems. Overall, highly innovative
 research and excellent thesis narrative and structure".

<br> <br>
Filip's next destination is a faculty position at <a href="https://www.ttic.edu">TTIC</a>. Congratulations, Filip!

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 17, 2020</h3>
<h1>ICML Workshop "Beyond First Order Methods in ML Systems"</h1>
<br>

Today, I have given the openinhg plenary talk at the ICML 2020 Workshop "Beyond First Order Methods in ML Systems".
The slides from my talk "Fast Linear Convergence of Randomized BFGS" are <a href="talks/TALK-2020-07-RBFGS-ICML-workshop.pdf">here.</a>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 12, 2020</h3>
<h1>Attending ICML 2020</h1>
<br>

I am attending ICML 2020 - the event is held virtually during July 12-18. My group members are presenting 5 papers, and I
will give the opening  plenary talk at the <a href="https://sites.google.com/view/optml-icml2020/home">Beyond First Order
  Methods in Machine Learning</a> workshops on Friday.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>July 9, 2020</h3>
<h1>Area Chair for ICLR 2021</h1>
<br>

I've accepted an invite to become an Area Chair for ICLR 2021.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 7, 2020</h3>
<h1>Paper Accepted to IEEE Transactions on Signal Processing</h1>
<br>

The paper ``Best Pair Formulation & Accelerated Scheme for Non-convex Principal Component Pursuit'', joint with <a href="https://aritradutta.weebly.com">Aritra Dutta</a>,
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>, and <a href="https://jliang993.github.io">Jingwei Liang</a>, was accepted to
IEEE Transactions on Signal Processing.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>July 3, 2020</h3>
<h1>Dmitry Kovalev Wins the 2020 Ilya Segalovich Scientific Prize</h1>
<br>



It is a great pleasure to announce that my PhD student <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> is one of nine recipients of the <a href="https://yandex.ru/blog/company/novye-laureaty-premii-imeni-segalovicha">2020 Ilya
Segalovich Scientific Prize for Young Researchers</a>, awarded by Yandex (a Russian equivalent of Google or Baidu) for significant advances in Computer Science. The award is focused on research of particular interest to yandex:
machine learning, computer vision, information search and data analysis, NLP and machine translation and speech synthesis/recognition.
The prize carries a cash award of 350,000 RUB ( = approx 5,000 USD).
<br>
<br>

Dmitry started MS studies at KAUST in Fall 2018 and received his MS degree in December 2019. He is a PhD student since January 2020. In this short period of time,
he has co-authored 17 papers, 15 of which are online (<a href="https://scholar.google.com/citations?user=qHFA5z4AAAAJ">Google Scholar</a>).  In my view, he is one of
the most talented young researchers coming in recent years from Russia. Dmitry's research is insighful, creative and deep.
<br>
<br>

Google translate of the announcement in Russian:
<br>
<br><i>For the second time, we selected the laureates of the Ilya Segalovich Scientific Prize. Yandex marks this award for scientists who have made significant
  advances in computer science. The prize is awarded once a year in two categories: “Young Researchers” and “Scientific Advisers”. The first nomination is for students, undergraduates and graduate students, the second - for their mentors.
Mikhail Bilenko (Head of Machine Intelligence and Research at Yandex) said: "The services and technologies of Yandex are based on science. At the same time, we are interested not only in applied developments, but also in theoretical research. They move the entire industry forward and can lead to impressive results in the future. We established the Segalovich Prize to support students and graduate students who are engaged in machine learning and other promising areas of computer science. Often, talented guys go to work in the industry while still studying. We want them to have the opportunity to continue basic research - with our financial support."
The winners are determined by the award council. It includes Yandex executives and scientists who collaborate with the company, including Ilya Muchnik, professor at Rutgers University in New Jersey, Stanislav Smirnov, professor at the University of Geneva and Fields laureate, and Alexei Efros, professor at the University of California at Berkeley. The size of the prize for young researchers is 350 thousand, and for scientific advisers - 700 thousand rubles.
This year, 12 people became laureates: three supervisors and nine young scientists. When choosing laureates among scientific scientists, we first of all took into account the contribution to community development and youth work. For young researchers, the main criterion is scientific achievements.
All laureates in the nomination “Young Researchers” have already managed to present their work at prestigious international conferences. Proceedings for such conferences are selected and reviewed by the world’s best experts in machine learning and artificial intelligence. If the work was accepted for publication at a conference, this is international recognition.</i><br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 23, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.11773">"Optimal and Practical Algorithms for Smooth and Strongly Convex Decentralized Optimization"</a> -
joint work with <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> and <a href="https://adil-salim.github.io">Adil Salim</a>.<br>
<br>

Abstract: <i>
We consider the task of decentralized minimization of the sum of smooth strongly convex functions stored across the nodes a network. For this problem, lower bounds on the number of gradient computations and the number of communication rounds required to achieve ε accuracy have recently been proven. We propose two new algorithms for this decentralized optimization problem and equip them with complexity guarantees. We show that our first method is optimal both in terms of the number of communication rounds and in terms of the number of gradient computations. Unlike existing optimal algorithms, our algorithm does not rely on the expensive evaluation of dual gradients. Our second algorithm is optimal in terms of the number of communication rounds, without a logarithmic factor. Our approach relies on viewing the two proposed algorithms as accelerated variants of the Forward Backward algorithm to solve monotone inclusions associated with the decentralized optimization problem. We also verify the efficacy of our methods against state-of-the-art algorithms through numerical experiments.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 23, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.11573">"Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization"</a> -
joint work with <a href="https://rka97.github.io">Ahmed Khaled</a>, <a href="https://othmanesebbouh.github.io">Othmane Sebbouh</a>,
<a href="https://www.maths.ed.ac.uk/~s1461357/">Nicolas Loizou</a>, and <a href="https://gowerrobert.github.io">Robert M. Gower</a>.<br>
<br>

Abstract: <i>
We present a unified theorem for the convergence analysis of stochastic gradient algorithms for minimizing a smooth and convex loss plus a convex regularizer.
We do this by extending the unified analysis of Gorbunov, Hanzely & Richtárik (2020) and dropping the requirement that the loss function be strongly convex.
Instead, we only rely on convexity of the loss function. Our unified analysis applies to a host of existing algorithms such as proximal SGD, variance reduced
methods, quantization and some coordinate descent type methods. For the variance reduced methods, we recover the best known convergence rates as special cases.
For proximal SGD, the quantization and coordinate type methods, we uncover new state-of-the-art convergence rates. Our analysis also includes any form of sampling
and minibatching. As such, we are able to determine the minibatch size that optimizes the total complexity of variance reduced methods. We showcase this by obtaining
a simple formula for the optimal minibatch size of two variance reduced methods (\textit{L-SVRG} and \textit{SAGA}). This optimal minibatch size not only improves
the theoretical total complexity of the methods but also improves their convergence in practice, as we show in several experiments.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 23, 2020</h3>
<h1>6th FLOW seminar talk tomorrow</h1>
<br>

<a href="https://www.di.ens.fr/hadrien.hendrikx/">Hadrien Hendrikx</a> will give a talk at the  <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">FLOW seminar tomorrow.</a>
Title of his talk: "Statistical Preconditioning for Federated Learning".
  <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 22, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.11077">"A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning"</a> -
joint work with <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.<br>
<br>

Abstract: <i>
Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed compute systems. A key bottleneck of such systems is the communication overhead for exchanging information across the workers, such as stochastic gradients. Among the many techniques proposed to remedy this issue, one of the most successful is the framework of compressed communication with error feedback (EF). EF remains the only known technique that can deal with the error induced by contractive compressors which are not unbiased, such as Top-K. In this paper, we propose a new and theoretically and practically better alternative to EF for dealing with contractive compressors. In particular, we propose a construction which can transform any contractive compressor into an induced unbiased compressor. Following this transformation, existing methods able to work with unbiased compressors can be applied. We show that our approach leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. We further extend our results to federated learning with partial participation following an arbitrary distribution over the nodes, and demonstrate the benefits thereof. We perform several numerical experiments which validate our theoretical findings.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 18, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.09270">"Primal Dual Interpretation of the Proximal Stochastic Gradient Langevin Algorithm"</a> -
joint work with <a href="https://adil-salim.github.io">Adil Salim</a>.<br>
<br>

Abstract: <i>
We consider the task of sampling with respect to a log concave probability distribution. The potential of the target distribution is assumed to be composite, i.e., written as the sum of a smooth convex term, and a nonsmooth convex term possibly taking infinite values. The target distribution can be seen as a minimizer of the Kullback-Leibler divergence defined on the Wasserstein space (i.e., the space of probability measures). In the first part of this paper, we establish a strong duality result for this minimization problem. In the second part of this paper, we use the duality gap arising from the first part to study the complexity of the Proximal Stochastic Gradient Langevin Algorithm (PSGLA), which can be seen as a generalization of the Projected Langevin Algorithm. Our approach relies on viewing PSGLA as a primal dual algorithm and covers many cases where the target distribution is not fully supported. In particular, we show that if the potential is strongly convex, the complexity of PSGLA is $\cO(1/\varepsilon^2)$ in terms of the 2-Wasserstein distance. In contrast, the complexity of the Projected Langevin Algorithm is $\cO(1/\varepsilon^{12})$ in terms of total variation when the potential is convex.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 17, 2020</h3>
<h1>5th FLOW seminar talk today</h1>
<br>

<a href="https://fhanzely.github.io">Filip Hanzely</a> gave a talk at the  <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">FLOW seminar today.</a>
His <a href="https://drive.google.com/file/d/1X57EMpnO8UXLXpbZrxoxuh53heUmZ2h_/view?usp=sharing">slides</a> and
<a href="https://www.youtube.com/watch?v=N10GwFwpt5I&feature=emb_title">video</a> of the talk can be found
<a href="https://sites.google.com/view/one-world-seminar-series-flow/archive#h.64km7mxogc2u">here.</a>  <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 10, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.07013">"A Unified Analysis of Stochastic Gradient Methods for Nonconvex Federated Optimization"</a> -
joint work with <a href="https://zhizeli.github.io">Zhize Li</a>.<br>
<br>

Abstract: <i>
In this paper, we study the performance of a large family of SGD variants in the smooth nonconvex regime. To this end, we propose a generic and flexible assumption capable of accurate modeling of the second moment of the stochastic gradient. Our assumption is satisfied by a large number of specific variants of SGD in the literature, including SGD with arbitrary sampling, SGD with compressed gradients, and a wide variety of variance-reduced SGD methods such as SVRG and SAGA. We provide a single convergence analysis for all methods that satisfy the proposed unified assumption, thereby offering a unified understanding of SGD variants in the nonconvex regime instead of relying on dedicated analyses of each variant. Moreover, our unified analysis is accurate enough to recover or improve upon the best-known convergence results of several classical methods, and also gives new convergence results for many new methods which arise as special cases. In the more general distributed/federated nonconvex optimization setup, we propose two new general algorithmic frameworks differing in whether direct gradient compression (DC) or compression of gradient differences (DIANA) is used. We show that all methods captured by these two frameworks also satisfy our unified assumption. Thus, our unified convergence analysis also captures a large variety of distributed methods utilizing compressed communication. Finally, we also provide a unified analysis for obtaining faster linear convergence rates in this nonconvex regime under the PL condition.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 12, 2020</h3>
<h1>Plenary Talk at Mathematics of Data Science Workshop</h1>
<br>

Today I gave a plenary talk at the <a href="https://maths-of-data.github.io/plenary-talks/">Mathematics of Data
  Science</a> workshop.  I gave the same talk as the one I gave in April at
the <a href="https://owos.univie.ac.at">One World Optimization Seminar:</a> <a href="https://www.youtube.com/watch?v=HGQkct3db-c">“On Second Order Methods and Randomness”,
which is on YouTube.</a> If you ever wondered what a 2nd order version of SGD should and should not look like, you may want to watch
the video talk. Our stochastic Newton (SN) method converges in 4/3 * n/tau * log 1/epsilon iterations when started
close enough from the solution, where n is the number of functions forming the finite sum we want to minimize, and tau is the minibatch size.
We can choose tau to be any value between 1 and n. Note that unlike all 1st order methods, the rate of SN is
independent of the condition number! 4/n The talk is based on joint work with my fantastic students <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> and <a href="https://konstmish.github.io">Konstantin Mishchenko</a>:
<a href="https://arxiv.org/abs/1912.01597">“Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates”</a>,
<a href="https://sites.google.com/site/optneurips19/">NeurIPS 2019 Workshop Beyond First Order Methods in ML</a>, 2019.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 10, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.05988">"Random Reshuffling: Simple Analysis with Vast Improvements"</a> -
joint work with <a href="https://konstmish.github.io">Konstantin Mishchenko</a> and
<a href="https://rka97.github.io">Ahmed Khaled</a>.<br>
<br>

Abstract: <i>
Random Reshuffling (RR) is an algorithm for minimizing finite-sum functions that utilizes iterative gradient
descent steps in conjunction with data reshuffling. Often contrasted with its sibling Stochastic Gradient
Descent (SGD), RR is usually faster in practice and enjoys significant popularity in convex and non-convex
optimization. The convergence rate of RR has attracted substantial attention recently and, for strongly
convex and smooth functions, it was shown to converge faster than SGD if 1) the stepsize is small, 2) the
gradients are bounded, and 3) the number of epochs is large. We remove these 3 assumptions, improve the
ependence on the condition number from $\kappa^2$ to $\kappa$ (resp.\ from $\kappa$ to $\sqrt{kappa}$) and,
in addition, show that RR has a different type of variance. We argue through theory and experiments that the
new variance type gives an additional justification of the superior performance of RR. To go beyond strong
convexity, we present several results for non-strongly convex and non-convex objectives. We show that in all
cases, our theory improves upon existing literature. Finally, we prove fast convergence of the Shuffle-Once
(SO) algorithm, which shuffles the data only once, at the beginning of the optimization process. Our theory
for strongly-convex objectives tightly matches the known lower bounds for both RR and SO and substantiates
the common practical heuristic of shuffling once or only a few times. As a byproduct of our analysis, we also
get new results for the Incremental Gradient algorithm (IG), which does not shuffle the data at all.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 5, 2020</h3>
<h1>NeurIPS Paper Deadline Today</h1>
<br>

The  <a href="https://nips.cc">NeurIPS</a> deadline has passed! Finally, I can relax a bit (= 1 day). Next deadline:
Supplementary Material for NeurIPS, on June 11... <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 1, 2020</h3>
<h1>Five Papers Accepted to ICML 2020</h1>
<br>
We've had five papers accepted to <a href="https://icml.cc/Conferences/2020">ICML 2020</a>, which will be run virtually during July 12-18, 2020.
Here they are:<br><br>


1) <a href="https://arxiv.org/abs/2002.04670">"Variance Reduced Coordinate Descent with Acceleration: New
  Method With a Surprising Application to Finite-Sum Problems"</a> - joint work with
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> and
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev.</a> <br>
<br>
Abstract: <i>We propose an accelerated version of stochastic variance reduced coordinate descent -- ASVRCD. As other variance reduced coordinate descent methods such as SEGA or SVRCD, our method can deal with problems that include a non-separable and non-smooth regularizer, while accessing a random block of partial derivatives in each iteration only. However, ASVRCD incorporates Nesterov's momentum, which offers favorable iteration complexity guarantees over both SEGA and SVRCD. As a by-product of our theory, we show that a variant of Allen-Zhu (2017) is a specific case of ASVRCD, recovering the optimal oracle complexity for the finite sum objective.</i><br>
<br>



2) <a href="https://arxiv.org/abs/2002.09526">"Stochastic Subspace Cubic Newton Method"</a> - joint work with
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>,
<a href="https://scholar.google.ru/citations?user=YNBhhjUAAAAJ&hl=en">Nikita Doikov</a> and <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>.
<br>
<br>
Abstract: <i>In this paper, we propose a new randomized second-order optimization
  algorithm---Stochastic Subspace Cubic Newton (SSCN)---for minimizing a high dimensional
  convex function f. Our method can be seen both as a stochastic extension of the
  cubically-regularized Newton method of Nesterov and Polyak (2006), and a second-order
  enhancement of stochastic subspace descent of Kozak et al. (2019). We prove that as we vary
  the minibatch size, the global convergence rate of SSCN interpolates between the rate of
  stochastic coordinate descent (CD) and the rate of cubic regularized Newton, thus giving new
  insights into the connection between first and second-order methods.
  Remarkably, the local convergence rate of SSCN matches the rate of stochastic subspace
  descent applied to the problem of minimizing the quadratic function 0.5 (x−xopt)^T f''(xopt) (x−xopt),
  where xopt is the minimizer of f, and hence depends on the properties of f at the optimum only.
  Our numerical experiments show that SSCN outperforms non-accelerated first-order CD algorithms
  while being competitive to their accelerated variants. </i><br>
<br>



3) <a href="https://arxiv.org/abs/1910.09529">"Adaptive Gradient Descent Without Descent"</a> - work of  <a
href="https://konstmish.github.io">Konstantin Mishchenko</a> and <a href="https://people.epfl.ch/yurii.malitskyi">Yura Malitsky</a>.<br>
<br>
Abstract: <i>We present a strikingly simple proof that two rules are sufficient to automate gradient descent: 1) don't increase the stepsize too fast and 2) don't overstep the local curvature. No need for functional values, no line search, no information about the function except for the gradients. By following these rules, you get a method adaptive to the local geometry, with convergence guarantees depending only on smoothness in a neighborhood of a solution. Given that the problem is convex, our method will converge even if the global smoothness constant is infinity. As an illustration, it can minimize arbitrary continuously twice-differentiable convex function. We examine its performance on a range of convex and nonconvex problems, including matrix factorization and training of ResNet-18. </i> <br>
<br>


4) <a href="https://arxiv.org/abs/2004.01442">
  "From Local SGD to Local Fixed Point Methods for Federated Learning"</a> - joint work with
Grigory Malinovsky,
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
<a href="http://elnurgasanov.me">Elnur Gasanov</a>, and
<a href="https://lcondat.github.io">Laurent Condat</a>. <br>

<br>
Abstract: <i>Most algorithms for solving optimization problems or finding saddle points of convex-concave functions are fixed point algorithms. In this work we consider the generic problem of finding a fixed point of an average of operators, or an approximation thereof, in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models the computations done locally on a mobile device. We investigate two strategies to achieve such a consensus: one based on a fixed number of local steps, and the other based on randomized computations. In both cases, the goal is to limit communication of the locally-computed variables, which is often the bottleneck in distributed frameworks. We perform convergence analysis of both methods and conduct a number of experiments highlighting the benefits of our approach.</i> <br>
<br>


5) <a href="https://arxiv.org/abs/2002.11364">"Acceleration for Compressed Gradient Descent in Distributed Optimization"</a>
- joint work with
<a href="https://zhizeli.github.io">Zhize Li</a>, <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
and <a
href="https://qianxunk.github.io">Xun Qian</a>.
<br>
<br>
Abstract: <i>The abstract contains a lot of math symbols, so <a href="https://arxiv.org/abs/2002.11364">look here instead.</a> </i><br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>May 27, 2020</h3>
<h1>NeurIPS Abstract Deadline Today</h1>
<br>

Polishing <a href="https://nips.cc">NeurIPS</a> abstracts... <br >


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>May 22, 2020</h3>
<h1>Eduard's ICLR Talk </h1>
<br>

<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> presented our paper <a href="https://openreview.net/forum?id=HylAoJSKvH">"A Stochastic Derivative Free Optimization Method with Momentum"</a> at ICLR. This paper is joint work with
Adel Bibi, Ozan Sener, and El Houcine Bergou. Eduard's 5min talk can be found here: <br >

<br>
<a href="https://iclr.cc/virtual_2020/poster_HylAoJSKvH.html"> <img alt="" src="imgs/ICLR2020-Eduard.png" width="700" ></a>
<br>



<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>May 21, 2020</h3>
<h1>"JacSketch" Paper Appeared in Mathematical Programming</h1>
<br>

The paper "Stochastic quasi-gradient methods: variance reduction via Jacobian sketching", joint work
with Robert M. Gower and Francis Bach, just appeared online on the Mathematical Programming
website: <a href="https://link.springer.com/article/10.1007/s10107-020-01506-0">Mathematical Programming, 2020</a>.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 20, 2020</h3>
<h1>Paper Appeared in SIMAX</h1>
<br>

The paper "Stochastic reformulations of linear systems: algorithms and convergence theory", joint work with
Martin Takáč, just appeared online on the SIMAX website: <a href="https://epubs.siam.org/doi/abs/10.1137/18M1179249">SIAM Journal on Matrix Analysis and Applications 41(2):487–524, 2020</a>.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>May 20, 2020</h3>
<h1>2nd FLOW Talk: Blake Woodworth (TTIC)</h1>
<br>

<a href="https://ttic.uchicago.edu/~blake/">Blake Woodworth (TTIC)</a> has given a great talk at the FLOW seminar today. His talk title was "Is Local SGD Better than Minibatch SGD?".
The slides and YouTube video <a href="https://sites.google.com/view/one-world-seminar-series-flow/archive#h.marh0ook3glo">can be found here.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 15, 2020</h3>
<h1>Paper Accepted to UAI 2020</h1>
<br>

Our paper
<a href="https://arxiv.org/abs/1901.09437">"99% of Distributed Optimization is a Waste of Time: The Issue and How to Fix it"</a> -
joint work with <a href="https://konstmish.github.io">Konstantin Mishchenko</a> and
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>,
was accepted to <a href="http://www.auai.org/uai2020/">Conference on Uncertainty in Artificial Intelligence (UAI 2020)</a>.<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 13, 2020</h3>
<h1>1st FLOW Talk: Ahmed Khaled (Cairo)</h1>
<br>

<a href="https://rka97.github.io">Ahmed Khaled (Cairo)</a> has given his first research talk ever today. Topic: "On the Convergence of Local SGD on Identical and Heterogeneous Data".
It was a great talk - I can't wait to see him give talks in the future. The abstract, link to the relevant papers, slides and YouTube video <a href="https://sites.google.com/view/one-world-seminar-series-flow/archive#h.azhfwca3oax9">are here.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 7, 2020</h3>
<h1>Three Students Attending MLSS 2020</h1>

<br>
<a href="https://samuelhorvath.github.io">Samuel Horváth</a>, <a href = "https://eduardgorbunov.github.io"> Eduard Gorbunov</a> and
<a href="https://shulgin-egor.github.io">Egor Shulgin</a> have been accepted to participate in tis year's <a href="http://mlss.tuebingen.mpg.de/2020/">Machine Learning Summer School (MLSS) in Tübingen, Germany.</a>
As most things this year, the event will be fully virtual. MLSS is highly selective; I am told this year they received more than 1300 applications for 180 spots at the event (less than 14% acceptance rate).
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 5, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2005.01097">"Adaptive Learning of the Optimal Mini-Batch Size of SGD"</a> -
joint work with <a href="https://scholar.google.com/citations?user=caAyffEAAAAJ&hl=en">Motasem Alfarra</a>,
<a href="https://slavomirhanzely.wordpress.com">Slavomír Hanzely</a>,
<a href="https://alyazeedbasyoni.wixsite.com/blog">Alyazeed Albasyoni</a>
and <a href="http://www.bernardghanem.com">Bernard Ghanem</a>.<br>
<br>

Abstract: <i>
Recent advances in the theoretical understandingof SGD (Qian et al., 2019) led to a formula for the optimal mini-batch size minimizing the number of effective data passes, i.e., the number of iterations times the mini-batch size. However, this formula is of no practical value as it depends on the knowledge of the variance of the stochastic gradients evaluated at the optimum. In this paper we design a practical SGD method capable of learning the optimal mini-batch size adaptively throughout its iterations. Our method does this provably, and in our experiments with synthetic and real data robustly exhibits nearly optimal behaviour; that is, it works as if the optimal mini-batch size was known a-priori. Further, we generalize our method to several new mini-batch strategies not considered in the literature before, including a sampling suitable for distributed implementations.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 4, 2020</h3>
<h1>FLOW: Federated Learning One World Seminar</h1>

<br>
Together with <a href = "http://researchers.lille.inria.fr/abellet/">Aurélien Bellet</a> (Inria), <a href = "https://www.cs.cmu.edu/~smithv/">Virginia Smith</a> (Carnegie Mellon) and
<a href= "https://ist.ac.at/en/research/alistarh-group/">Dan Alistarh</a> (IST Austria), we are launching <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">FLOW: Federated Learning One World Seminar.</a>
The seminar will take place on a weekly basis on Wednesdays. All talks will be delivered via Zoom. The first few talks are: <br> <br>
May 13, <a href="https://rka97.github.io">Ahmed Khaled</a> (Cairo): On the Convergence of Local SGD on Identical and Heterogeneous Data <br> <br>
May 20, <a href="http://ttic.uchicago.edu/~blake/">Blake Woodworth</a> (TTIC): Is Local SGD Better than Minibatch SGD? <br> <br>
May 27, <a href="http://papail.io">Dimitris Papailiopoulos</a> (Wisconsin Madison): Robustness in Federated Learning May be Impossible Without an All-knowing Central Authority
 <br> <br>
June 3, No talk due to NeurIPS deadline <br> <br>
June 10, <a href="https://people.epfl.ch/sai.karimireddy">Sai Praneeth Karimireddy</a> (EPFL): Stochastic Controlled Averaging for Federated Learning <br> <br>
June 17, <a href="https://fhanzely.github.io">Filip Hanzely</a> (KAUST): Federated Learning of a Mixture of Global and Local Models: Local SGD and Optimal Algorithms <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 3, 2020</h3>
<h1>Talk at the Montréal Machine Learning and Optimization Seminar</h1>

<br>
On Friday this week (May 8), I will give a talk entitled "On Second Order Methods and Randomness" at the  <a href="https://mtl-mlopt.github.io">Montréal Machine
Learning and Optimization (MTL MLOpt) Seminar</a>. This is an online seminar delivered via Google Meet. Starting time: 9am PDT.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>









<h3>April 25, 2020</h3>
<h1>Talk at the One World Optimization Seminar</h1>

<br>
I will give a talk within the <a href="https://owos.univie.ac.at">One World Optimization Seminar</a>
series on Monday, April 27, at 3pm CEST. This is a new exciting initiative, and my talk is only the
second in the series. I will speak about some new results related to second order methods and randomness.
One of the advantages of this new format is that anyone can attend - indeed, attendance is via
<a href="https://zoom.us">Zoom</a>. However, you need to register online in advance in order to get access.
Hope to "see" many of you there!
<br><br>

Update (April 29): The <a href="https://owos.univie.ac.at/fileadmin/user_upload/k_owos/Peter_Richtarik-Stochastic_Newton.pdf">slides</a> and <a href="https://www.youtube.com/watch?v=HGQkct3db-c">video recording</a> of my talk are now available.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 21, 2020</h3>
<h1>Filip Hanzely Accepted a Position at TTIC</h1>
<br>

<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> accepted a Research Assistant Professorship at <a href="https://www.ttic.edu">Toyota Technological Institute at Chicago</a> (TTIC).
Filip has written his thesis and will submit it soon. He is expected to graduate this Summer, and will start his new position in Chicago in the Fall. Filip has obatined multiple other offers besides this, including a Tenure-Track Assistant Professorship and a Postdoctoral Fellowship in a top machine learning group.

<br><br>
Congratulations!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 20, 2020</h3>
<h1>ICML, UAI and COLT Author Response Deadlines</h1>
<br>

I am busy: <a href="https://icml.cc">ICML</a> and <a href="http://www.auai.org/uai2020/call_for_papers.php">UAI</a> rebuttal deadline is today, and for
<a href="http://learningtheory.org/colt2020/">COLT</a> the deadline is on April 24.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 7, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2004.02635">"Dualize, Split, Randomize: Fast Nonsmooth Optimization Algorithms"</a> -
joint work with <a href="https://adil-salim.github.io">Adil Salim</a>, <a href="https://lcondat.github.io">Laurent Condat</a>, and <a href="https://konstmish.github.io">Konstantin Mishchenko</a>.<br>
<br>

Abstract: <i>
We introduce a new primal-dual algorithm for minimizing the sum of three convex functions, each of which has its own oracle. Namely, the first one is differentiable, smooth and possibly stochastic, the second is proximable, and the last one is a composition of a proximable function with a linear map. Our theory covers several settings that are not tackled by any existing algorithm; we illustrate their importance with real-world applications. By leveraging variance reduction, we obtain convergence with linear rates under strong convexity and fast sublinear convergence under convexity assumptions. The proposed theory is simple and unified by the umbrella of stochastic Davis-Yin splitting, which we design in this work. Finally, we illustrate the efficiency of our method through numerical experiments.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 5, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2004.02163">"On the Convergence Analysis of Asynchronous SGD for Solving Consistent Linear Systems"</a> -
joint work with <a href="https://sands.kaust.edu.sa/authors/atal-narayan-sahu/">Atal Narayan Sahu</a>, <a href="https://www.aritradutta.com">Aritra Dutta</a>, and Aashutosh Tiwari.<br>
<br>

Abstract: <i>
In the realm of big data and machine learning, data-parallel, distributed stochastic algorithms have drawn significant attention in the present days. While the synchronous versions of these algorithms are well
understood in terms of their convergence, the convergence analyses of their asynchronous counterparts are not widely studied.
In this paper, we propose and analyze a  distributed, asynchronous parallel SGD method in light of solving an arbitrary consistent linear system by reformulating the system into a stochastic optimization problem as
studied by Richtárik and Takáč in [35]. We compare the convergence rates of our asynchronous SGD algorithm with the
synchronous parallel algorithm proposed by Richtárik and Takáč in [35] under different choices of the hyperparameters---the stepsize, the damping factor, the number of processors, and the delay factor.
We show that our asynchronous parallel SGD algorithm also enjoys a global linear convergence rate, similar to the "basic method" and the synchronous parallel method in [35] for solving any arbitrary consistent
linear system via stochastic reformulation. We also show that our asynchronous parallel SGD improves upon the "basic method" with a better convergence rate when the number of processors is larger than four.
We further show that this asynchronous approach performs asymptotically better than its synchronous counterpart for certain linear systems. Moreover, for certain linear systems, we compute the minimum number of
processors required for which our asynchronous parallel SGD is better, and find that this number can be as low as two for some ill-conditioned problems.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 3, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2004.01442">"From Local SGD to Local Fixed Point Methods for Federated Learning"</a> -
joint work with Grigory Malinovsky, <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="http://elnurgasanov.me">Elnur Gasanov</a>, and <a href="https://lcondat.github.io">Laurent Condat</a>.<br>
<br>

Abstract: <i>
Most algorithms for solving optimization problems or finding saddle points of convex-concave functions are fixed point algorithms.
In this work we consider the generic problem of finding a fixed point of an average of operators, or an approximation thereof,
in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models
the computations done locally on a mobile device. We investigate two strategies to achieve such a consensus: one based on a fixed
number of local steps, and the other based on randomized computations. In both cases, the goal is to limit communication of the
locally-computed variables, which is often the bottleneck in distributed frameworks. We perform convergence analysis of both methods
and conduct a number of experiments highlighting the benefits of our approach.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 10, 2020</h3>
<h1>Area Chair for NeurIPS 2020</h1>
<br>

I will serve as an Area Chair for <a href="https://nips.cc/Conferences/2020/">NeurIPS 2020</a>, to be
held during December 6-12, 2020 in Vancouver, Canada (same location as last year). For those not in the know,
Google Scholar Metrics says that NeurIPS is the #1 conference in AI:
<br>
<br>
<a href="https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=eng_artificialintelligence"> <img alt="" src="imgs/GoogleScholarMetrics-AI.png" width="700" ></a>
<br>
<br>
 The review process has changed this year; here is a short and beautifully produced video explaining the key 5 changes:
 <br>
<br>
<a href="https://www.youtube.com/watch?v=361h6lHZGDg&feature=share"> <img alt="" src="imgs/5changes.jp2" width="700" ></a>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 9, 2020</h3>
<h1>Coronavirus at KAUST</h1>
<br>

No, Covid-19 did not catch up with anyone at KAUST yet. Still in luck. However, as in many places, its increasing omnipresence and gravitational pull is felt here as well.

<br>
<br>
For example, as of today, all KAUST lectures are moving online. And for a good reason I think: we have seen in Lombardy what the virus can do when unchecked.
I am teaching my CS 390T Federated Learning course on Sundays (yes - the work week in Saudi spans Sunday-Thursday) and Tuesdays, and hence my first online lecture will take place on Sunday March 15. I hope, at least, as I need
to decide how best to do it.
<br>
<br>

Conference travel has been limited for some time now, but the rules are even  more strict now. This seems less than necessary
as conferences drop like flies anyway. My planned travel between now and May includes a seminar talk at EPFL (Switzerland), a workshop
keynote lecture at King Faisal University (Al-Ahsa, Saudi Arabia), presentation at ICLR (Addis Ababa, Ethiopia), and SIAM Conference on Optimization (Hong Kong)
which I am helping to organize. Most of these events are cancelled, and those that survive will most probably go to sleep soon.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 27, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.12410">"On Biased Compression for Distributed Learning"</a> -
joint work with <a href="https://scholar.google.ru/citations?user=hVVJR-sAAAAJ&hl=en">Aleksandr Beznosikov</a>,
<a href="https://samuelhorvath.github.io">Samuel Horváth</a>, and <a href="https://mher-safaryan.github.io">Mher Safaryan</a>.<br>
<br>

Abstract: <i>
In the last few years, various communication  compression techniques have  emerged  as an indispensable  tool helping to alleviate the communication bottleneck in distributed learning.
However, despite the fact biased compressors often show superior performance in practice when compared to the much more studied and understood unbiased compressors,  very little
is known about them. In this work we study three classes of biased compression operators, two of which are new, and their performance when applied to  (stochastic) gradient descent
and distributed (stochastic) gradient descent.  We show for the first time that biased compressors can lead to linear convergence rates both in the single node and distributed settings.
Our distributed SGD method enjoys the ergodic rate $O \left( \frac{\delta L \exp(-K) }{\mu}  + \frac{(C + D)}{K\mu} \right)$, where $\delta$ is a compression parameter which grows when
more compression is applied, $L$ and $\mu$ are the smoothness and strong convexity constants, $C$ captures stochastic gradient noise ($C=0$ if full gradients are computed on each node)
and $D$ captures the variance of the gradients at the optimum ($D=0$ for over-parameterized models).  Further,  via a theoretical study of several synthetic and empirical distributions
of communicated gradients, we shed light on why and by how much  biased compressors outperform  their unbiased variants.  Finally, we  propose a new highly performing biased
compressor---combination of Top-$k$ and natural dithering---which in our experiments outperforms all other compression techniques.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 26, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.11364">"Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization"</a> -
joint work with <a href="https://zhizeli.github.io">Zhize Li</a>,
 <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, and <a href="https://qianxunk.github.io">Xun Qian</a>.<br>
<br>

Abstract: <i>
Due to the high communication cost in distributed and federated learning problems, methods relying on compression of communicated messages are becoming increasingly popular. While in other contexts the best performing gradient-type methods invariably rely on some form of acceleration/momentum to reduce the number of iterations, there are no methods which combine the benefits of both gradient compression and acceleration. In this paper, we remedy this situation and propose the first accelerated compressed gradient descent (ACGD) methods.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 26, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.11337">"Fast Linear Convergence of Randomized BFGS"</a> -
joint work with <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
 <a href="https://gowerrobert.github.io">Robert M. Gower</a>, and <a href="https://scholar.google.com/citations?hl=ru&user=sEjyzkgAAAAJ">Alexander Rogozin</a>.<br>
<br>

Abstract: <i>
Since the late 1950’s when quasi-Newton methods first appeared, they have become one of the most widely used and
efficient algorithmic paradigms for unconstrained optimization. Despite their immense practical success, there is
little theory that shows why these methods are so efficient. We provide a semi-local rate of convergence for the
randomized BFGS method which can be significantly better than that of gradient descent, finally giving theoretical
evidence supporting the superior empirical performance of the method.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 21, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.09526">"Stochastic Subspace Cubic Newton Method"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>,
 <a href="https://scholar.google.ru/citations?user=YNBhhjUAAAAJ&hl=en">Nikita Doikov</a> and <a href="https://en.wikipedia.org/wiki/Yurii_Nesterov">Yurii Nesterov</a>.<br>
<br>

Abstract: <i>
In this paper, we propose a new randomized second-order optimization algorithm---Stochastic Subspace Cubic Newton (SSCN)---for minimizing a high
dimensional convex function $f$. Our method can be seen both as astochastic extension of the cubically-regularized Newton method of Nesterov
and Polyak (2006), and a second-order enhancement of stochastic subspace descent of Kozak et al. (2019). We prove that as we vary the
minibatch size, the global convergence rate of SSCN interpolates between the rate of stochastic coordinate descent (CD) and the rate of cubic
regularized Newton, thus giving new insights into the connection between first and second-order methods. Remarkably, the local convergence rate
of SSCN matches the rate of stochastic subspace descent applied to the problem of minimizing the quadratic function
$\frac{1}{2} (x-x^*)^\top \nabla^2 f(x^*)(x-x^*)$, where $x^*$ is the minimizer of $f$, and hence depends on the properties of $f$ at the optimum only.
Our numerical experiments show that SSCN outperforms non-accelerated first-order CD algorithms while being competitive to their accelerated variants.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 20, 2020</h3>
<h1>New MS/PhD Student: Egor Shulgin</h1>
<br>

Egor Vladimirovich Shulgin is back at KAUST - now as an MS/PhD student. Welcome!!! <br><br>

Egor has co-authored 4 papers and a book (in Russian) entitled <a href="https://arxiv.org/abs/1907.01060">"Lecture Notes on Stochastic Processes"</a>.
Here are the papers, in reverse chronological order:<br><br>
- <a href="https://arxiv.org/abs/2002.08958">Uncertainty principle for communication compression in distributed and federated
learning and the search for an optimal compressor </a> <br>
- Adaptive catalyst for smooth convex optimization <br>
- <a href="https://arxiv.org/abs/1905.11373">Revisiting stochastic extragradient</a> (AISTATS 2020) <br>
- <a href="http://proceedings.mlr.press/v97/qian19b">SGD: general analysis and improved rates</a> (ICML 2019) <br>
<br>

Egor has a bachelor degree in Applied Mathematics from the Department of
Control and Applied Mathematics at MIPT, Dolgoprudny, Russia. He majored in
Data Analysis. His CV mentions the following as his main subjects: Probability
Theory, Random Processes, Convex Optimization, and Machine Learning.<br>
<br>


Egor’s hobbies, according to his CV, are: hiking, alpine skiing, tennis,
and judo. Notably, this list does not include table tennis. However, I
know for a fact that he is very good in it!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 20, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.08958">"Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor"</a> -
joint work with <a href="https://mher-safaryan.github.io">Mher Safaryan</a> and <a href="https://scholar.google.com/citations?user=XlmSx18AAAAJ&hl=en">Egor Shulgin</a>.<br>
<br>

Abstract: <i>
In order to mitigate the high communication cost in  distributed and federated learning, various vector compression schemes, such as
quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as
few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion
(variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of
communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more
distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators,
thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication
compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first
step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call
{\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension
independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry.
We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication
complexity improvements on previous state of the art.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 14, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.05516">"Federated Learning of a Mixture of Global and Local Models"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>.<br>
<br>

Abstract: <i>We propose a new optimization formulation for training federated learning models. The standard
  formulation has the form of an empirical risk minimization problem constructed to find a single global
  model trained from the private data stored across all participating devices. In contrast, our formulation
  seeks an explicit trade-off between this traditional global model and the local models, which can be learned
  by each device from its own private data without any communication. Further, we develop several efficient
  variants of SGD (with and without partial participation and with and without variance reduction) for solving
  the new formulation and prove communication complexity guarantees. Notably, our methods are similar but not
  identical to federated averaging / local SGD, thus shedding some light on the essence of the elusive method.
  In particular, our methods do not perform full averaging steps and instead merely take steps towards averaging.
  We argue for the benefits of this new paradigm for federated learning.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 12, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.05359">"Adaptivity of Stochastic Gradient Methods for Nonconvex Optimization"</a> -
joint work with <a href="https://samuelhorvath.github.io">Samuel Horváth</a>,
<a href="https://statistics.berkeley.edu/people/lihua-lei">Lihua Lei</a> and
<a href="https://people.eecs.berkeley.edu/~jordan/">Michael I. Jordan</a>.<br>
<br>

Abstract: <i>Adaptivity is an important yet under-studied property in modern optimization theory.
  The gap between the state-of-the-art theory and the current practice is striking in that algorithms
  with desirable theoretical guarantees typically involve drastically different settings of hyperparameters,
  such as step-size schemes and batch sizes, in different regimes. Despite the appealing theoretical results,
  such divisive strategies provide little, if any, insight to practitioners to select algorithms that work broadly
  without tweaking the hyperparameters. In this work, blending the "geometrization" technique introduced by
  Lei & Jordan 2016 and the SARAH algorithm of Nguyen et al., 2017, we propose the Geometrized SARAH
  algorithm for non-convex finite-sum and stochastic optimization. Our algorithm is proved to achieve adaptivity
  to both the magnitude of the target accuracy and the Polyak-Łojasiewicz (PL) constant if present. In addition,
  it achieves the best-available convergence rate for non-PL objectives simultaneously while outperforming
  existing algorithms for PL objectives.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 11, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.04670">"Variance Reduced Coordinate Descent with Acceleration: New Method With a
  Surprising Application to Finite-Sum Problems"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> and <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>.<br>
<br>

Abstract: <i>We propose an accelerated version of stochastic variance reduced coordinate descent -- ASVRCD. As other
variance reduced coordinate descent methods such as SEGA or SVRCD, our method can deal with problems that include a
non-separable and non-smooth regularizer, while accessing a random block of partial derivatives in each iteration only.
However, ASVRCD incorporates Nesterov's momentum, which offers favorable iteration complexity guarantees over both SEGA
and SVRCD. As a by-product of our theory, we show that a variant of Allen-Zhu (2017) is a specific case of ASVRCD,
recovering the optimal oracle complexity for the finite sum objective.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 10, 2020</h3>
<h1>Konstantin Giving a Series of Talks in the US and UK</h1>

<br>
<a href="https://konstmish.github.io/">Konstantin Mishchenko</a> is giving several talks in New York, London and Oxford. Here is his schedule:
<br>
<br>
<br>

February 10, Facebook Research, New York, "Adaptive Gradient Descent Without Descent"<br><br>
February 12, Deepmind, London, "Adaptive Gradient Descent Without Descent"<br><br>
February 12, UCL Gatsby Computational Neuroscience Unit, London, "Sinkhorn Algorithm as a Special Case of Stochastic Mirror Descent" <br><br>
February 14, Oxford University, Oxford, "Adaptive Gradient Descent Without Descent" <br><br>
February 14, Imperial College London, London, "Sinkhorn Algorithm as a Special Case of Stochastic Mirror Descent" <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 9, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.03329">"Better Theory for SGD in the Nonconvex World"</a> -
joint work with <a href="https://rka97.github.io">Ahmed Khaled</a>.<br>
<br>

Abstract: <i>Large-scale nonconvex optimization problems are ubiquitous in modern machine learning,
and among practitioners interested in solving them, Stochastic Gradient Descent (SGD)
reigns supreme. We revisit the analysis of SGD in the nonconvex setting and propose a new
variant of the recently introduced expected smoothness assumption which governs the behaviour
of the second moment of the stochastic gradient. We show that our assumption is both more general
and more reasonable than assumptions made in all prior work. Moreover, our results yield the
optimal $O(\epsilon^{-4})$ rate for finding a stationary point of nonconvex smooth functions, and
recover the optimal $O(\epsilon^{-1})$ rate for finding a global solution if the Polyak-Łojasiewicz
condition is satisfied. We compare against convergence rates under convexity and prove a theorem
on the convergence of SGD under Quadratic Functional Growth and convexity, which might be of
independent interest. Moreover, we perform our analysis in a framework which allows for a detailed
study of the effects of a wide array of sampling strategies and minibatch sizes for finite-sum
optimization problems. We corroborate our theoretical results with experiments on real and
synthetic data.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 8, 2020</h3>
<h1>Interview by Robin.ly for their Leaders in AI Platform</h1>

<br>
In December last year, while attending NeurIPS in Vancouver, I was interviewed by Robin.ly.
The video can be found here<br><br>

<a href="https://www.robinly.info/post/peter-richtarik-kaust-machine-learning-federated-learning-kaust"><img alt="" src="imgs/Leaders_in_AI_video.png" width="600"> </a><br><br>

and a podcast out of this is on soundcloud: <br><br> <br>

<a href="https://soundcloud.com/robinly/podcast-peter-richtarik"><img alt="" src="imgs/Leaders_in_AI_podcast.png" width="600"> </a><br><br>

I am in excellent company:<br><br>
<a href="https://soundcloud.com/robinly/yoshua-bengio-from-deep-learning-to-consciousness-interview-by-song-han-neurips-2019">Yoshua Bengio</a><br>
<a href="https://soundcloud.com/robinly/kai-fu-lee-the-era-of-ai-the-rise-of-china-the-future-of-work">Kai-Fu Lee</a><br>
<a href="https://soundcloud.com/robinly/the-future-of-distributed-machine-learning-max-welling-u-of-amsterdam-qualcomm-neurips-2019">Max Welling</a><br>
<a href="https://soundcloud.com/robinly/my-first-cvpr-christopher-manning-professor-director-stanford-ai-lab">Christopher Manning</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 8, 2020</h3>
<h1>AAAI in New York</h1>

<br> <a href="https://konstmish.github.io">Konstantin</a> is about to receive a Best Reviewer Award at AAAI 20 in New York. <a href="http://www.adelbibi.com">Adel</a> is presenting our paper
 <a href="https://arxiv.org/abs/1902.01272">"A stochastic derivative-free optimization method with importance sampling"</a>,
 joint work with El Houcine Bergou, Ozan Sener, Bernard Ghanem and myself, at the event.</br> <br>


 Update (May 3): Here is a <a href="https://cemse.kaust.edu.sa/vcc/news/kaust-cs-student-recognized-aaai-outstanding-program-committee-award">KAUST article about Konstantin and his achievements.</a> I am very proud.

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>February 6, 2020</h3>
<h1>ICML Deadline Today!</h1>

<br>I am a walking zombie, a being without a soul, a sleepless creature of the night. Do not approach me or
you will meet your destiny. Wait three days and I shall be alive again. </br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>February 3, 2020</h3>
<h1>Samuel in San Diego</h1>

<br><a href="https://samuelhorvath.github.io">Samuel Horváth</a>
is in San Diego, and will soon be presenting the paper <a href="https://arxiv.org/abs/1901.08689">"Don’t jump through hoops and remove those loops:
SVRG and Katyusha are better without the outer loop"</a>, joint work with Dmitry Kovalev and myself,
at <a href="http://alt2020.algorithmiclearningtheory.org">ALT 2020</a> in San Diego.<br><br> Here is the
list of <a href="http://alt2020.algorithmiclearningtheory.org/accepted-papers/">all accepted papers.</a> <br><br>
Samuel will be back at KAUST in about 10 days.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 2, 2020</h3>
<h1>Eduard Gorbunov is Visting Again</h1>

<br><a href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard Gorbunov</a> came for a research visit - this is his third time at KAUST. This time, he wil stay for about 2 months.

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>January 31, 2020</h3>

<h1>Poster for AAAI-20</h1>
<br>
Our paper <a href="https://arxiv.org/abs/1902.01272">"A stochastic derivative-free optimization
  method with importance sampling"</a>, joint work with Adel Bibi, El Houcine Bergou, Ozan Sener, and Bernard Ghanem,
  will be presented at <a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a>, which will be held in New York
  during February 7-12. <br><br>

We have just prepared a poster, here it is:<br>
<br>
<a href="posters/Poster-STP_IS-AAAI20.pdf"><img src="posters/Poster-STP_IS-AAAI20_small.png" alt="STP" border="0" height="500"></a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 20, 2019</h3>
<h1>Paper Accepted to SIMAX</h1>

<br>The paper <a href="https://arxiv.org/abs/1706.01108">"Stochastic reformulations of linear systems: algorithms and convergence theory"</a>, joint work with Martin Takáč, was accepted to <a href="https://www.siam.org/publications/journals/siam-journal-on-matrix-analysis-and-applications-simax"> SIAM Journal on Matrix Analysis and Applications.</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 18, 2020</h3>
<h1>New Intern: Alexander Rogozin</h1>

<br>A new intern arrived today: Alexander Rogozin. Alexander is a final-year MSc student in the department of Control and Applied Mathematics at the <a href="https://mipt.ru/english/">Moscow Institute of Physics and Technology (MIPT).</a> <br>

<br>Some notable achievements of Alexander so far:<br>

<br>
- Co-authored <a href="https://scholar.google.com/citations?hl=en&amp;user=sEjyzkgAAAAJ">3 papers</a> in the area of decentralized optimization over time varying networks<br>
- His GPA ranks him among the top 5% students at MIPT<br>
- Tutor of Probability Theory at MIPT, 2018-now<br>
- Finished the <i>Yandex School for Data Analysis</i> (2017-2019)<br>
- Awardee of the Russian National Physics Olympiad, 2013<br>
- Certificate of Honor at Russian National Mathematics Olympiad, 2012<br>
- Winner of Russian National Physics Olympiad, 2012<br>

<br>
In 2018, Alexander participated in the Moscow Half-Marathon.
He is a holder of 4-kyu in Judo. Having studied the piano for
11 years, Alexander participated in city, regional, national
and international musical festivals and competitions. He
performed with a symphony orchestra as a piano soloist at
festivals in his hometown.
<br>
<br>
Welcome!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>January 16, 2020</h3>
<h1>Konstantin Mishchenko Among the Best Reviewers for AAAI-20</h1>
<br> Congratulations <a href="https://konstmish.github.io/">Kostya</a>!
(AAAI Conference on Artificial Intelligence is <a
href="https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_artificialintelligence">one of the top AI conferences</a>. The email below tells the
story.)<br> <br>
<i>
Dear Konstantin, <br>
<br>
On behalf of the Association for the Advancement of
Artificial Intelligence and the AAAI-20 Program Committee,
we are pleased to inform you that you have been selected as
one of 12 Outstanding Program Committee members for 2020 in
recognition of your outstanding service on this year's
committee. Your efforts were characterized by exceptional
care, thoroughness, and thoughtfulness in the reviews and
discussions of the papers assigned to you. <br>
<br>
In recognition of your achievement, you will be
presented with a certificate by the AAAI-20 Program
Cochairs, Vincent Conitzer and Fei Sha, during the AAAI-20
Award Ceremony on Tuesday, February 11 at 8:00am. There will
also be an announcement of this honor in the program. Please
let us know (aaai20@aaai.org) if you will be present at the
award ceremony to accept your award. <br>
<br>
Congratulations, and we look forward to seeing you in New York for AAAI-20, February 7-12. <br>
<br>
Warmest regards, <br>
<br>
Carol McKenna Hamilton<br>
Executive Director, AAAI <br>
<br>
for<br>
<br>
Vincent Conitzer and Fei Sha<br>
AAAI-20 Program Cochairs <br>
</i>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 13, 2020</h3>
<h1>Konstantin Visiting Francis Bach's Group at INRIA</h1>

<br>
<a href="https://konstmish.github.io/">Konstantin Mishchenko</a>
is visiting the <a href="https://www.di.ens.fr/sierra/">SIERRA machine learning lab</a> at INRIA, Paris,
led by <a href="https://www.di.ens.fr/%7Efbach/">Francis Bach</a>. He will give a talk on January 14
entitled "Adaptive Gradient Descent Without Descent" and based on
<a href="https://arxiv.org/abs/1910.09529">this paper.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>January 9, 2020</h3>
<h1>New Intern: Aleksandr Beznosikov</h1>
<br>
A new intern arrived today: Aleksandr Beznosikov. Aleksandr is a final-year BSc student in Applied Mathematics and Physics at <a href="https://mipt.ru/english/">Moscow Institute of Physics and Technology (MIPT).</a> <br>
<br>
Some notable achievements of Aleksandr so far:<br>
<br>
- paper <a href="https://arxiv.org/abs/1911.10645">"Derivative-Free Method For Decentralized Distributed Non-Smooth Optimization"</a>, joint work with Eduard Gorbunov and Alexander Gasnikov<br>
- Increased State Academic Scholarship for 4 year bachelor and master students at MIPT, 2018-2019 <br>
- Author of problems and organizer of the student olympiad in discrete mathematics, 2018-2019 <br>
- Abramov's Scholarship for students with the best grades at MIPT, 2017-2019<br>
- First Prize at MIPT's team mathematical tournament, 2017 <br>
- Silver Medal at International Experimental Physics Olympiad,
2015<br>
- Russian President’s Scholarship for High School Students, 2014-2015 <br>
- Prize-Winner, All-Russian School Physics Olympiad, Final Round, 2014 and 2015 <br>
- Winner, All-Russian School Programming Olympiad, Region Round, 2015-2016 <br>
- Winner, All-Russian School Physics Olympiad, Region Round, 2014-2016 <br>
- Winner, All-Russian School Maths Olympiad, Region Round, 2014-2016 <br>
<br>
Welcome!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

          <h3>January 7, 2020</h3>
          <h1>Four Papers Accepted to AISTATS 2020</h1>
          <br>
          Some of the first good news of 2020: We've had four papers
          accepted to <a href="https://www.aistats.org/">The 23rd
            International Conference on Artificial Intelligence and
            Statistics (AISTATS 2020)</a>, which will be held during
          June 3-5, 2020, in Palermo, Sicily, Italy. Here they are:<br>
          <br>
          1) <a href="https://arxiv.org/abs/1905.11261">"A unified
            theory of SGD: variance reduction, sampling, quantization
            and coordinate descent"</a> - joint work with <a
            href="https://eduardgorbunov.github.io/">Eduard Gorbunov</a>
          and <a href="https://fhanzely.github.io/index.html">Filip
            Hanzely.</a> <br>
          <br>
          Abstract: <i>In this paper we introduce a unified analysis of
            a large family of variants of proximal stochastic gradient
            descent (SGD) which so far have required different
            intuitions, convergence analyses, have different
            applications, and which have been developed separately in
            various communities. We show that our framework includes
            methods with and without the following tricks, and their
            combinations: variance reduction, importance sampling,
            mini-batch sampling, quantization, and coordinate
            sub-sampling. As a by-product, we obtain the first unified
            theory of SGD and randomized coordinate descent (RCD)
            methods, the first unified theory of variance reduced and
            non-variance-reduced SGD methods, and the first unified
            theory of quantized and non-quantized methods. A key to our
            approach is a parametric assumption on the iterates and
            stochastic gradients. In a single theorem we establish a
            linear convergence result under this assumption and
            strong-quasi convexity of the loss function. Whenever we
            recover an existing method as a special case, our theorem
            gives the best known complexity result. Our approach can be
            used to motivate the development of new useful methods, and
            offers pre-proved convergence guarantees. To illustrate the
            strength of our approach, we develop five new variants of
            SGD, and through numerical experiments demonstrate some of
            their properties. </i><br>
          <br>
          2) "Tighter theory for local SGD on identical and
          heterogeneous data" - joint work with <a
            href="https://rka97.github.io">Ahmed Khaled</a> and <a
            href="https://konstmish.github.io">Konstantin Mishchenko.</a>
          <br>
          <br>
          Abstract: <i>We provide a new analysis of local SGD, removing
            unnecessary assumptions and elaborating on the difference
            between two data regimes: identical and heterogeneous. In
            both cases, we improve the existing theory and provide
            values of the optimal stepsize and optimal number of local
            iterations. Our bounds are based on a new notion of variance
            that is specific to local SGD methods with different data.
            The tightness of our results is guaranteed by recovering
            known statements when we plug $H=1$, where $H$ is the number
            of local steps. The empirical evidence further validates the
            severe impact of data&nbsp; heterogeneity on the&nbsp;
            performance of local SGD. </i><br>
          <br>
          3) <a href="https://arxiv.org/abs/1905.11373">"Revisiting
            stochastic extragradient"</a> - joint work with <a
            href="https://konstmish.github.io">Konstantin Mishchenko</a>,
          <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
          Egor Shulgin and <a
            href="https://people.epfl.ch/yurii.malitskyi">Yura Malitsky</a>.
          <br>
          <br>
          Abstract: <i>We consider a new extension of the extragradient
            method that is motivated by approximating implicit updates.
            Since in a recent work of Chavdarova et al (2019) it was
            shown that the existing stochastic extragradient algorithm
            (called mirror-prox) of Juditsky et al (2011) diverges on a
            simple bilinear problem, we prove guarantees for solving
            variational inequality that are more general than in </i><i><i>Juditsky
et al (2011)</i>. Furthermore, we illustrate numerically
            that the proposed variant converges faster than many other
            methods on the example of</i><i><i> Chavdarova et al (2019)</i>.
            We also discuss how extragradient can be applied to training
            Generative Adversarial Networks (GANs). Our experiments on
            GANs demonstrate that the introduced approach may make the
            training faster in terms of data passes, while its higher
            iteration complexity makes the advantage smaller. To further
            accelerate method's convergence on problems such as bilinear
            minimax, we combine the extragradient step with negative
            momentum Gidel et al (2018) and discuss the optimal momentum
            value. </i><br>
          <br>
          4) <a href="https://arxiv.org/abs/1906.00506">"DAve-QN: A
            distributed averaged quasi-Newton method with local
            superlinear convergence rate"</a> - work of S. Soori, <a
            href="https://konstmish.github.io">K. Mishchenko</a>, A.
          Mokhtari, M. Dehnavi, and M. Gürbüzbalaban.<br>
          <br>
          Abstract: <i>In this paper, we consider distributed
            algorithms for solving the empirical risk minimization
            problem under the master/worker communication model. We
            develop a distributed asynchronous quasi-Newton algorithm
            that can achieve superlinear convergence. To our knowledge,
            this is the first distributed asynchronous algorithm with
            superlinear convergence guarantees. Our algorithm is
            communication-efficient in the sense that at every iteration
            the master node and workers communicate vectors of size
            O(p), where p is the dimension of the decision variable. The
            proposed method is based on a distributed asynchronous
            averaging scheme of decision vectors and gradients in a way
            to effectively capture the local Hessian information of the
            objective function. Our convergence theory supports
            asynchronous computations subject to both bounded delays and
            unbounded delays with a bounded time-average. Unlike in the
            majority of asynchronous optimization literature, we do not
            require choosing smaller stepsize when delays are huge. We
            provide numerical experiments that match our theoretical
            results and showcase significant improvement comparing to
            state-of-the-art distributed algorithms. </i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h3>January 7, 2020</h3>
          <h1>Visiting ESET in Bratislava</h1>
          <br>
          I am on a visit to <a href="https://www.eset.com/int/">ESET</a>
          - a leading internet security company headquartered in
          Bratislava, Slovakia. I have given a couple of talks on
          stochastic gradient descent and have spoken to several very
          interesting people. <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h3>January 5, 2020</h3>
          <h1>Filip Visiting Francis Bach's Group at INRIA</h1>
          <br>
          <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          is visiting the <a href="https://www.di.ens.fr/sierra/">SIERRA machine learning lab</a> at INRIA, Paris, led by Francis
          Bach. He will give a talk on January 7 entitled "One method to
          rule them all: variance reduction for data, parameters and
          many new methods", and <a
            href="https://arxiv.org/abs/1905.11266">based on a paper of
            the same title</a>. <a
href="https://fhanzely.github.io/wp-content/uploads/2020/01/jacsketch.pdf">Here are his slides.</a><br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h3>January 5, 2020</h3>
          <h1>New Intern: Grigory Malinovsky</h1>
          <br>
          A new intern arrived today: Grigory Malinovsky from <a
            href="https://mipt.ru/english/">Moscow Institute of Physics
            and Technology (MIPT).</a> Grigory wrote his BS thesis
          "Averaged Heavy Ball Method" under the supervision of Boris
          Polyak. He is now pursuing an MS degree at MIPT in Machine
          Learning.<br>
          <br>
          Among Grigory's successes belong:<br>
          <br>
          - Abramov's cholarship for tudents with the best grades at
          MIPT, 2016<br>
          - Participant in the final round of All-Russian Physics
          Olympiad, 2014<br>
          - Bronze medal at International Zhautykov Olympiad in Physics,
          2014<br>
          - Prize winner in the final round of All-Russian Physics
          Olympiad, 2013<br>
          <br>
          Welcome!<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h1> Old News</h1>
          <br>
          <a href="i_oldnews.html">Read old news</a> (2017 and earlier)<br>
          <br>
        </div>


<div id="sidebar">

<h6>[5/2020]  We have launched <a
href="https://sites.google.com/view/one-world-seminar-series-flow/home">FLOW: Federated Learning One World Seminar</a>.
</h6>
<br>
<br>


<h6>[11/2019] A KAUST Discovery article <a
href="https://discovery.kaust.edu.sa/en/article/892/machine-learning-models-gather-momentum">"Machine Learning Models Gather Momentum"</a>.
</h6>
<br>
<br>


<h6>[10/2019] A KAUST Discovery article <a
href="https://discovery.kaust.edu.sa/en/article/880/less-chat-leads-to-more-work-for-machine-learning">"Less Chat Leads to More Work for Machine Learning"</a>.
</h6>
<br>
<br>


<h6>[9/2019] A KAUST Discovery article <a
href="https://discovery.kaust.edu.sa/en/article/879/speeding-up-the-machine-learning-process">"Speeding up the Machine Learning Process"</a>.
</h6>
<br>
<br>


<h6>[10/2019] I have delivered a 5hr mini-course on Stochastic
Gradient Descent at Moscow Institute of Physics and
Technology (MIPT), entitled <a
href="https://youtu.be/a05S0kL5u30">"A Guided Walk Through
the ZOO of Stochastic Gradient Descent Methods".</a> The
lectures are now on YouTube.
</h6>
<br>
<br>


<h6>[8/2019] Sign up for my <a
href="https://piazza.com/kaust.edu.sa/fall2019/cs390ff/home">CS 390FF (Big Data Optimization) class</a> (Fall 2019). </h6>
<br>
<br>

<h6>[9/2018] A <a
href="https://www.youtube.com/watch?v=gjgEck0zU7w&amp;list=PLgKuh-lKre13E9dXSsif4KsGoFp4bjubd&amp;index=9">YouTube video</a> of my talk at the Simons Institute on the
JacSketch algorithm. </h6>
<br>
<br>

<h6> [8/2017] Video recording of my 5 hour mini-course on
"Randomized Optimization Methods" delivered at the Data
Science Summer School (DS3) at École Polytechnique: Parts <a
href="https://comm.medias.polytechnique.fr/videos/richtarik_1_randonopti/">1</a>,
<a
href="https://comm.medias.polytechnique.fr/videos/richtarik_2_randonopti/">2</a>,
<a
href="https://comm.medias.polytechnique.fr/videos/richtarik_3_randonopti_08033/">3</a>,
<a
href="https://comm.medias.polytechnique.fr/videos/richtarik_4_randonoptimization/">4</a>,
<a
href="https://comm.medias.polytechnique.fr/videos/richtarik_5_randonoptimization/">5</a>
</h6>
<br>
<br>


<h6> [3/2017] I have taken up an Associate Professor position
at <a href="https://www.kaust.edu.sa/en">KAUST</a>. I am on
leave from Edinburgh. </h6>
<br>
<br>


<h6> [12/2016] Video from my <a
href="https://events.yandex.ru/lib/talks/4294/">talk at
Yandex </a>entitled "Empirical Risk Minimization:
Complexity, Duality, Sampling, Sparsity and Big Data". </h6>
<br>
<br>


<h6> [10/2016] My Alan Turing Institute talk on Stochastic
Dual Ascent for Solving Linear Systems is now on <a
href="https://www.youtube.com/watch?v=RbkhWrTbrKs">YouTube</a>.
</h6>
<br>
<br>


<h6>[9/2015] <a
href="talks/2015-09-Toulouse-Summer-School-Optimization.pdf">Slides</a>
from a 6hr course on "Optimization in Machine Learning",
Toulouse, France. </h6>
<br>
<br>


<h6> [7/2015] ICML Tutorial (joint with Mark Schmidt) on <span
class="important">Modern Convex Optimization Methods for
Large-scale ERM:</span> <a
href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_I.pdf">Part I</a>, <a
href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_II.pdf">Part II</a> </h6>
<br>
<br>


<h6> [2/2014] A <a
href="https://www.youtube.com/watch?v=0sHOfqhCZw0">YouTube
video</a> of a talk on the APPROX algorithm delivered at
PreMoLab in Moscow. </h6>
<br>
<br>


<h6> [10/2013] A <a
href="http://www.youtube.com/watch?v=IQgnstB0n2E#t=538">YouTube video</a> of a talk I gave at the Simons Institute
workshop on <a
href="http://simons.berkeley.edu/workshops/bigdata2013-2">parallel
and distributed optimization and inference.</a> </h6>



              </div>

        <div style="clear: both;"> </div>
      </div>
    </div>
  </body>
</html>
