<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <link rel="stylesheet" href="style.css">
    <title>Peter Richtarik</title>
  </head>
  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <script src="table_header.js"></script> </div>
      </div>
      <ul class="menu">
        <li><a class="active" href="index.html">News</a></li>
        <li><a href="i_oldnews.html">Old News</a></li>
        <li><a href="i_papers.html">Papers</a></li>
        <li><a href="i_talks.html">Talks</a></li>
        <li><a href="i_events.html">Events</a></li>
        <li><a href="i_seminar.html">Seminar</a></li>
        <li><a href="i_software.html">Code</a></li>
        <li><a href="i_team.html">Team</a></li>
        <li><a href="i_join.html">Join</a></li>
        <li><a href="i_bio.html">Bio</a></li>
        <li><a href="i_teaching.html">Teaching</a></li>
        <li><a href="i_consulting.html">Consulting</a></li>
      </ul>
      <div id="wrapper" class="main">
        <div id="content">
          <h3>October 4, 2019</h3>
          <h1> NeurIPS 2019 Workshop Papers</h1>
          <br>
          We have had several papers accepted to various NeurIPS 2019
          workshops. Here they are:<br>
          <br>
          <h2><a href="http://federated-learning.org/fl-neurips-2019/">Workshop


              on Federated Learning for Data Privacy and Confidentiality</a></h2>
          1. <a href="https://arxiv.org/abs/1909.04716">Gradient
            Descent with Compressed Iterates</a>, by <a
            href="https://rka97.github.io">Ahmed Khaled</a> and me<br>
          <br>
          2. <a href="https://arxiv.org/abs/1909.04746">Better
            Communication Complexity for Local SGD</a>, by <a
            href="https://rka97.github.io">Ahmed Khaled</a>, <a
            href="https://konstmish.github.io">Konstantin Mishchenko</a>
          and me<br>
          <br>
          3. <a href="https://arxiv.org/abs/1909.04715">First Analysis
            of Local GD on Heterogeneous Data</a>, by <a
            href="https://rka97.github.io">Ahmed Khaled</a>, <a
            href="https://konstmish.github.io">Konstantin Mishchenko</a>
          and me<br>
          <br>
          <h2><a href="https://sites.google.com/site/optneurips19/">Beyond


              First Order Methods in ML</a></h2>
          4. Stochastic Newton and Cubic Newton Methods with Simple
          Local Linear-Quadratic Rates, by <a
            href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a
            href="https://konstmish.github.io">Konstantin Mishchenko</a>
          and me<br>
          <br>
          5. <a href="https://arxiv.org/abs/1802.09022">An Accelerated
            Method for Derivative-Free Smooth Stochastic Convex
            Optimization</a>, by <a
            href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>,
          <a href="https://www.wias-berlin.de/people/dvureche/?lang=1">Pavel


            Dvurechensky</a> and <a
            href="http://www.mathnet.ru/eng/person27590">Alexander
            Gasnikov</a><br>
          <meta name="qrichtext" content="1">
          <style type="text/css">
p, li { white-space: pre-wrap; }
</style><br>
          <h2> <a
              href="https://optrl2019.github.io/accepted_papers.html">Optimization


              Foundations for Reinforcement Learning</a></h2>
          6. <a href="https://arxiv.org/abs/1905.13278">A Stochastic
            Derivative Free Optimization Method with Momentum</a>, by <a
            href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>,
          <a href="http://www.adelbibi.com">Adel Bibi</a>, <a
            href="http://ozansener.net">Ozan Sener</a>, <a
            href="https://ehbergou.github.io">El Houcine Bergou</a> and
          me<br>
          <br>
          <h2><a href="https://sites.google.com/view/otml2019/">Optimal
              Transport &amp; Machine Learning</a></h2>
          7. <a href="https://arxiv.org/abs/1909.06918">Sinkhorn
            Algorithm as a Special Case of Stochastic Mirror Descent</a>,
          by <a href="https://konstmish.github.io">Konstantin
            Mishchenko</a><br>
          <br>
          <h2><a href="https://sgo-workshop.github.io">Smooth Games
              Optimization and Machine Learning Workshop: Bridging Game
              Theory and Deep Learning</a></h2>
          8. <a href="https://arxiv.org/abs/1905.11373">Revisiting
            Stochastic Extragradient</a>, by <a
            href="https://konstmish.github.io">Konstantin Mishchenko</a>,
          <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a
href="https://scholar.google.ru/citations?user=XlmSx18AAAAJ&amp;hl=en">Egor
            Shulgin</a>, me, and <a
            href="https://www.researchgate.net/profile/Yura_Malitsky">Yura
            Malitsky</a><br>
          <br>
          &nbsp;<br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>October 3, 2019</h3>
          <h1> Best Paper Award at VMV 2019</h1>
          <br>
          The paper <a href="https://arxiv.org/abs/1909.00145">"Stochastic


            convolutional sparse coding"</a>, joint work with <a
            href="https://vccimaging.org/People/xiongj/">Jinhui Xiong</a>
          and <a href="https://vccimaging.org/People/heidriw/">Wolfgang
            Heidrich</a>, has won the 2019 Vision, Modeling and
          Visualization (VMV) Best Paper Award.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>October 1, 2019</h3>
          <h1> New Postdoc: Mher Safaryan</h1>
          <br>
          <a href="https://mher-safaryan.github.io">Mher Safaryan</a>
          joined my group today as a postdoc. He got his PhD in 2018 in
          Mathematics at Yerevan State University, Armenia, under the
          supervision of <a
            href="http://math.sci.am/user/grigori-karagulyan">Grigori
            Karagulyan</a>. During his PhD, Mher worked on several
          problems in harmonic analysis and algebra. Mher and me have
          recently written a paper <a
            href="http://arxiv.org/abs/1905.12938">on stochastic sign
            descent methods.</a><br>
          <br>
          Welcome! <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 29, 2019</h3>
          <h1> Nizhny Novgorod</h1>
          <br>
          I just took a 1hr flight from Moscow to Nizhny Novgorod. I
          will stay here until October 3 and deliver four lectures:
          three lectures on Oct 1 at <a
            href="https://nnov.hse.ru/en/latna/conferences/ada2019">"Approximation









            and Data Analysis"</a> (an event organized by Moscow State
          University, Higher School of Economics and Russian Academy of
          Sciences), and one lecture on Oct 2 at <a
            href="http://www.huawei.com/ru">Huawei.</a> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 26, 2019</h3>
          <h1> MIPT </h1>
          <br>
          After a couple days in Germany, I am now traveling to Moscow
          to visit Moscow Institute of Physics and Technology (MIPT).
          Indeed, I am writing this onboard a flight from Frankfurt to
          Domodedovo. <a
            href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard</a>
          will pick me up at the airport. Thanks Eduard!<br>
          <br>
          Alexander Gasnikov put together a nice workshop around my
          visit, with excellent speakers: V. Spokoiny, E. Tyrtyshnikov,
          A. Nazin, P. Dvurechensky, and A. Tremba. The workshop will
          start at 2pm on the 27th of September and will take place
          somewhere at MIPT. I do not know where yet - as there is no
          website for the event and I was not yet informed - but I am
          sure I will learn about this before the workshop starts ;-)<br>
          <br>
          The day after, on September 28th, I will deliver a series of
          lectures for MIPT students entitled "A Guided Walk Through the
          ZOO of Stochastic Gradient Descent Methods". This mini-course
          is aimed to serve as the best introduction to the topic of
          SGD, and is largely based on research originating from my
          group at KAUST. We will start at 10:45am and finish at 6pm.
          And yes, there will be breaks.<br>
          <br>
          <br>
          <i>Update (Sept 29):</i> My visit of MIPT is over, today I
          will fly to Nizhny Novgorod. My mini-course was recorded and
          should appear on YouTube at some point. There might have been
          an issue with voice recording towards the end though...<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 24, 2019</h3>
          <h1> Bielefeld </h1>
          <br>
          I am on my way to Bielefeld to give a talk at a <a
            href="https://www.uni-bielefeld.de/mathematik/numerik/workshop50.html">numerical














            analysis workshop</a> associated with the celebrations of 50
          years of mathematics at Bielefeld. I normally do not have a
          chance to hang out with numerical PDE people; but I am glad I
          did. It was a fun event. Moreover, my rather esoteric talk
          (relative to the workshop theme) on stochastic Newton and
          gradient methods was met with surprising enthusiasm. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 15, 2019</h3>
          <h1> KAUST Professor Wins Distinguished Speaker Award </h1>
          <br>
          KAUST wrote a <a
href="https://www.kaust.edu.sa/en/news/kaust-professor-peter-richt%C3%A1rik-wins-distinguished-speaker-award">short


















            article about me</a>... <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 15, 2019</h3>
          <h1>Heading to DIMACS </h1>
          <br>
          I am on my way to Rutgers, to attend the <a
            href="http://dimacs.rutgers.edu/events/details?eID=316">DIMACS























            Workshop on Randomized Numerical Linear Algebra, Statistics,
            and Optimization,</a> which is to take place at the <a
            href="http://dimacs.rutgers.edu">Center for Discrete
            Mathematics and Computer Science (DIMACS)</a> during
          September 16-18, 2019. <br>
          <br>
          My talk, entitled "Variance Reduction for Gradient
          Compression", is on Monday afternoon. The talk will be
          recorded and put on <a
            href="https://www.youtube.com/watch?v=n1ELUphtbgg">YouTube.</a>
          <br>
          <br>
          Abstract: <i>Over the past few years, various randomized
            gradient compression (e.g., quantization, sparsification,
            sketching) techniques have been proposed for reducing
            communication in distributed training of very large machine
            learning models. However, despite high level of research
            activity in this area, surprisingly little is known about
            how such compression techniques should properly interact
            with first order optimization algorithms. For instance,
            randomized compression increases the variance of the
            stochastic gradient estimator, and this has an adverse
            effect on convergence speed. While a number of
            variance-reduction techniques exists for taming the variance
            of stochastic gradients arising from sub-sampling in
            finite-sum optimization problems, no variance reduction
            techniques exist for taming the variance introduced by
            gradient compression. Further, gradient compression
            techniques are invariably applied to unconstrained problems,
            and it is not known whether and how they could be applied to
            solve constrained or proximal problems. In this talk I will
            give positive resolutions to both of these problems. In
            particular, I will show how one can design fast
            variance-reduced proximal stochastic gradient descent
            methods in settings where stochasticity comes from gradient
            compression. </i><br>
          <br>
          This talk is based on:<br>
          <br>
          <i><a
href="https://papers.nips.cc/paper/7478-sega-variance-reduction-via-gradient-sketching">[1]</a>
            Filip Hanzely, Konstantin Mishchenko and Peter Richtárik.
            SEGA: Variance reduction via gradient sketching, NeurIPS
            2018</i><i><br>
          </i><i><br>
          </i><i><a href="https://arxiv.org/abs/1901.09269">[2]</a>
            Konstantin Mishchenko, Eduard Gorbunov, Martin Takáč and
            Peter Richtárik. Distributed learning with compressed
            gradient differences, arXiv:1901.09269</i><i>, 2019<br>
          </i><i><br>
          </i><i><a href="https://arxiv.org/abs/1901.09437">[3]</a>
            Konstantin Mishchenko, Filip Hanzely and Peter Richtárik.
            99% of distributed optimization is a waste of time: the
            issue and how to fix it, arXiv:1901.09437</i><i>, 2019<br>
          </i><i><br>
          </i><i><a href="https://arxiv.org/abs/1904.05115">[4]</a>
            Samuel Horváth, Dmitry Kovalev, Konstantin Mishchenko, Peter
            Richtárik and Sebastian Stich. Stochastic distributed
            learning with gradient quantization and variance reduction,
            arXiv:1904.05115, 2019</i><br>
          <br>
          <br>
          Update (September 16, 2019): I have given my talk today, <a
            href="talks/TALK-Variance-Reduction-for-Gradient-Compression.pdf">here

















            are the slides.</a><br>
          <br>
          Update (October 4, 2019): <a
            href="https://www.youtube.com/watch?v=n1ELUphtbgg">My talk
            is on YouTube now.</a>&nbsp; Here is a <a
href="https://www.youtube.com/watch?v=dvw-LnqfhLg&amp;list=PLKVCRT3MRed4yWkXj3CLKadDdClCvhQu1">playlist




            of all the other talks</a> from the event.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 12, 2019</h3>
          <h1>Sciencetown Podcast<br>
          </h1>
          <br>
          <img src="imgs/Sciencetown.png" alt="Sciencetown" width="300"
            height="300"><br>
          <br>
          Today, I spent about an hour in <a
            href="https://www.linkedin.com/in/nicholasdemille/">Nicholas
            Demille's</a> podcast studio. We have chatted about machine
          learning, life and and my work for about an hour. The material
          will be used for the next episode of the <a
href="https://browse.entale.co/show/b1481445-70e4-47aa-b9d4-2f9d1d8fe439">Sciencetown
















            podcast</a> Nicholas is preparing.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 12, 2019</h3>
          <h1>Nicolas Loizou's PhD Thesis </h1>
          <br>
          Here is a <a href="papers/2019_PhD_Thesis_Nicolas_Loizou.pdf">copy

























            of Nicolas' PhD thesis entitled "Randomized Iterative
            Methods for Linear Systems: Momentum,<br>
            Inexactness and Gossip"</a>. <a
            href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas</a>
          defended in June, and has just arrived to Montréal to take up
          a postdoctoral position at <a href="https://mila.quebec/en/">MILA</a>.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 10, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1909.04746">"Better
            communication complexity for local SGD"</a> - joint work
          with Ahmed Khaled and <a href="https://konstmish.github.io">Konstantin




























            Mishchenko.</a><br>
          <br>
          Abstract: <i>We revisit the local Stochastic Gradient Descent
            (local SGD) method and prove new convergence rates. We close
            the gap in the theory by showing that it works under
            unbounded gradients and extend its convergence to weakly
            convex functions. Furthermore, by changing the assumptions,
            we manage to get new bounds that explain in what regimes
            local SGD is faster that its non-local version. For
            instance, if the objective is strongly convex, we show that,
            up to constants, it is sufficient to synchronize $M$ times
            in total, where $M$ is the number of nodes. This improves
            upon the known requirement of Stich (2018) of $\sqrt{TM}$
            synchronization times in total, where $T$ is the total
            number of iterations, which helps to explain the empirical
            success of local SGD.</i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 10, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1909.04716">"Gradient descent
            with compressed iterates"</a> - joint work with Ahmed
          Khaled.<br>
          <br>
          Abstract: <i>We propose and analyze a new type of stochastic
            first order method: gradient descent with compressed
            iterates (GDCI). GDCI in each iteration first compresses the
            current iterate using a lossy randomized compression
            technique, and subsequently takes a gradient step. This
            method is a distillation of a key ingredient in the current
            practice of federated learning, where a model needs to be
            compressed by a mobile device before it is sent back to a
            server for aggregation. Our analysis provides a step towards
            closing the gap between the theory and practice of federated
            learning, and opens the possibility for many extensions.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 10, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1909.04715">"First analysis of
            local GD on heterogeneous data"</a> - joint work with Ahmed
          Khaled and <a href="https://konstmish.github.io">Konstantin
            Mishchenko.</a><br>
          <br>
          Abstract: <i>We provide the first convergence analysis of
            local gradient descent for minimizing the average of smooth
            and convex but otherwise arbitrary functions. Problems of
            this form and local gradient descent as a solution method
            are of importance in federated learning, where each function
            is based on private data stored by a user on a mobile
            device, and the data of different users can be arbitrarily
            heterogeneous. We show that in a low accuracy regime, the
            method has the same communication complexity as gradient
            descent.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 7, 2019</h3>
          <h1>New Visitor: Xiuxian Li</h1>
          <br>
          <a
href="https://scholar.google.com.hk/citations?user=dQSGQSAAAAAJ&amp;hl=zh-CN">Xiuxian

































            Li</a> (Nanyang Technological University, Singapore) is
          visiting me at KAUST for a week. He is giving a <a
href="https://cemse.kaust.edu.sa/events/event/distributed-algorithms-computing-common-fixed-point-group-nonexpansive-operators">CS

































            seminar talk on Monday at noon</a> entitled "Distributed
          Algorithms for Computing a Common Fixed Point of a Group of
          Nonexpansive Operators".<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 4, 2019</h3>
          <h1>Three Papers Accepted to NeurIPS 2019</h1>
          <br>
          The long-awaited decisions just came. We've had three papers
          accepted; I was involved with the first two of them. The third
          is a collaboration of <a href="https://adil-salim.github.io">Adil





































            Salim</a> with people from Gatsby:<br>
          <br>
          <a href="https://arxiv.org/abs/1905.10874">"RSN: Randomized
            Subspace Newton"</a> - joint work with <a
            href="https://gowerrobert.github.io/">Robert Mansel Gower</a>,
          <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>
          and <a
            href="http://www.opt.uni-duesseldorf.de/%7Elieder/de/inhalt.php">Felix








































            Lieder.</a><br>
          <br>
          Abstract:<i> We develop a randomized Newton method capable of
            solving learning problems with huge dimensional feature
            spaces, which is a common setting in applications such as
            medical imaging, genomics and seismology. Our method
            leverages randomized sketching in a new way, by finding the
            Newton direction constrained to the space spanned by a
            random sketch. We develop a simple global linear convergence
            theory that holds for practically all sketching techniques,
            which gives the practitioners the freedom to design custom
            sketching approaches suitable for particular applications.
            We perform numerical experiments which demonstrate the
            efficiency of our method as compared to accelerated gradient
            descent and the full Newton method. Our method can be seen
            as a refinement and randomized extension of the results of
            Karimireddy, Stich, and Jaggi (2019).</i><br>
          <br>
          <a href="https://arxiv.org/abs/1905.11768">"Stochastic
            proximal Langevin algorithm: potential splitting and
            nonasymptotic rates"</a> - joint work with <a
            href="https://adil-salim.github.io">Adil Salim</a> and <a
            href="https://www.dmitry-kovalev.com">Dmitry Kovalev.</a><br>
          <br>
          Abstract:<i> We propose a new algorithm---Stochastic Proximal
            Langevin Algorithm (SPLA)---for sampling from a log concave
            distribution. Our method is a generalization of the Langevin
            algorithm to potentials expressed as the sum of one
            stochastic smooth term and multiple stochastic nonsmooth
            terms. In each iteration, our splitting technique only
            requires access to a stochastic gradient of the smooth term
            and a stochastic proximal operator for each of the nonsmooth
            terms. We establish nonasymptotic sublinear and linear
            convergence rates under convexity and strong convexity of
            the smooth term, respectively, expressed in terms of the KL
            divergence and Wasserstein distance. We illustrate the
            efficiency of our sampling technique through numerical
            simulations on a Bayesian learning task.</i> <br>
          <br>
          <a href="https://adil-salim.github.io/Research/mmd19.pdf">"Maximum





































            mean discrepancy gradient flow"</a> - work of <a
            href="https://michaelarbel.github.io">Michael Arbel</a>, <a
            href="https://akorba.github.io">Anna Korba</a>, <a
            href="https://adil-salim.github.io">Adil Salim</a> and <a
            href="http://www.gatsby.ucl.ac.uk/%7Egretton/">Arthur
            Gretton.</a><br>
          <br>
          Abstract:<i> We construct a Wasserstein gradient flow of the
            maximum mean discrepancy (MMD) and study its convergence
            properties. The MMD is an integral probability metric
            defined for a reproducing kernel Hilbert space (RKHS), and
            serves as a metric on probability measures for a
            sufficiently rich RKHS. We obtain conditions for convergence
            of the gradient flow towards a global optimum, that can be
            related to particle transport when optimizing neural
            networks. We also propose a way to regularize this MMD flow,
            based on an injection of noise in the gradient. This
            algorithmic fix comes with theoretical and empirical
            evidence. The practical implementation of the flow is
            straightforward, since both the MMD and its gradient have
            simple closed-form expressions, which can be easily
            estimated with samples.</i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 4, 2019</h3>
          <h1>Best NeurIPS 2019 Reviewer Award </h1>
          <br>
          I have received the following email: "Thank you for all your
          hard work reviewing for NeurIPS 2019! We are delighted to
          inform you that you were one of the 400 highest-scoring
          reviewers this year! You will therefore be given access (for a
          limited period of time) to one free registration to this
          year’s conference; you will later receive additional
          information by email explaining how to access your
          registration." Thanks NeurIPS!<br>
          <br>
          <i>Update:</i> <a href="https://konstmish.github.io">Konstantin




































            Mishchenko</a> also got this award. Congrats!<br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 3, 2019</h3>
          <h1>New Postdoc: Zhize Li<br>
          </h1>
          <br>
          <a href="https://zhizeli.github.io">Zhize Li</a> joined my
          group today as a postdoc. He got his PhD in Computer Science
          from Tsinghua University in July 2019, and is interested in
          "theoretical computer science and machine learning, in
          particular (non-)convex optimization algorithms, machine
          learning, algorithms and data structures". His PhD thesis
          "Simple and Fast Optimization Methods for Machine Learning"
          won the <span class="important">2019 Tsinghua Outstanding
            Doctoral Dissertation Award.</span> <br>
          Zhize has written 12 papers, including publications in venues
          such as NeurIPS, ICLR, COLT, IJCAI, SAGT, DCC and SPIRE. <br>
          <br>
          Welcome!<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 29, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="papers/SCSC-VMV2019.pdf">"Stochastic convolutional
            sparse coding"</a> - joint work with <a
            href="https://jhxiong.github.io">Jinhui Xiong</a> and <a
            href="http://vccimaging.org/People/heidriw">Wolfgang
            Heidrich</a>. <br>
          <br>
          Abstract: <i>State-of-the-art methods for Convolutional
            Sparse Coding usually employ Fourier-domain solvers in order
            to speed up the convolution operators. However, this
            approach is not without shortcomings. For example,
            Fourier-domain representations implicitly assume circular
            boundary conditions and make it hard to fully exploit the
            sparsity of the problem as well as the small spatial support
            of the filters.</i><i> In this work, we propose a novel
            stochastic spatial-domain solver, in which a randomized
            subsampling strategy is introduced during the learning of
            sparse codes. Afterwards, we extend the proposed strategy in
            conjunction with online learning, scaling the CSC model up
            to very large sample sizes. In both cases, we show
            experimentally that the proposed subsampling strategy, with
            a reasonable selection of the subsampling rate, outperforms
            the state-of-the-art frequency-domain solvers in terms of
            execution time without losing in learning quality. Finally,
            we evaluate the effectiveness of the over-complete
            dictionary learned from large-scale datasets, which
            demonstrates an improved sparse representation of the
            natural images on account of more abundant learned image
            features.</i><br>
          <br>
          The paper was accepted to and will appear in the <a
            href="https://www.vmv2019.uni-rostock.de">International
            Symposium on Vision, Modeling, and Visualization 2019 (VMV
            2019).</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 25, 2019</h3>
          <h1>Fall 2019 Semester Started</h1>
          <br>
          The Fall 2019 semester has just started and I am again
          teaching CS 390FF: Special Topics in Data Sciences (Big Data
          Optimization). I have redesigned some key portions of the
          course based on some fresh and hot research from 2018 and
          2019. You can sign up for the course via <a
            href="https://piazza.com/kaust.edu.sa/fall2019/cs390ff/home">Piazza.</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 22, 2019</h3>
          <h1>New MS/PhD Student: Alyazeed Basyoni</h1>
          <br>
          <a href="https://alyazeedbasyoni.wixsite.com/blog">Alyazeed
            Basyoni</a> just arrived at KAUST to start his MS/PhD
          studies under my supervision. Welcome!!!<br>
          <br>
          In 2019, Alyazeed obtained his BS in Computer Science from
          Carnegie Mellon University. Desiting to learn more, Alyazeed
          ended up taking many graduate level courses, inlcuding courses
          in Probability Theory, Deep Reinforcement Learning, Convex
          Optimization, Machine Learning, Randomized Algorithms,
          Probabilistic Combinatorics, and Measure and Integration.<br>
          <br>
          Alyazeed already has varied industrial experience: <br>
          - At Ansatz, he implemented a fast, low cost, futures
          execution engine (it was deployed)<br>
          - At Dropbox, he implemented a tool that allows clients to
          search, preview, select and embed content from third-party
          providers into Paper. <br>
          - At Petuum, he contributed to the open source Dynamic Neural
          Network package, DyNet.<br>
          <br>
          When Alyazeed is bored, he writes OS kernels (in C, from
          scratch), helps the USA mathematics olympiad team by grading
          mock exams and delivering short lectures, programs <a
            href="https://www.youtube.com/watch?v=sQ9w7H2rY2Y">games</a>,
          and fools around with C, Python, SML, OCaml, and Go.<br>
          <br>
          Alyazeed has a <a
            href="https://www.imo-official.org/team_r.aspx?code=SAU&amp;year=2012">Silver



















































            Medal from the 53rd International Mathematics Olympiad</a>
          (held in Mar del Plata, Argentina in 2012), where he
          represented Saudi Arabia. By the way, at the same Olympiad, my
          student <a
            href="https://www.imo-official.org/team_r.aspx?code=KAZ&amp;year=2012">Alibek



















































            Sailanbayev got a Bronze Medal</a>. What a coincidence!
          Alyazeed was the first Saudi to win a Silver medal at IMO.<br>
          <br>
          At KAUST, you will find Alyazeed in Building 1, Level 2.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 22, 2019</h3>
          <h1>New MS/PhD Student: Slavomír Hanzely</h1>
          <br>
          <a href="https://slavomirhanzely.wordpress.com">Slavomír
            Hanzely</a> just arrived at KAUST to start his MS/PhD
          studies under my supervision. Welcome!!!<br>
          <br>
          In 2019, Slavomír ("Slavo") obtained his BS degree in Computer
          Science from Comenius University, Slovakia. This, by the way,
          is also where I studied back in the day. Slavo was eager to
          learn faster than the study program required, and ended up
          taking many more courses than necessary - all without
          sacrificing his grades. <br>
          <br>
          Throughout his high schools and university studies, Slavo has
          been active in various mathematical and computer science
          olympiads and competitions, at regional, national and
          international level. Here are some highlights from his
          achievements:<br>
          - 2017, 8-10th Place in Vojtech Jarník International
          Mathematical Competition (1st place among Czech and Slovak
          contestants)<br>
          - 2016, represented Slovakia at the 57th International
          Mathematical Olympiad (held in Hong Kong)<br>
          - 2016, 3rd Place at the Slovak National Mathematical Olympiad<br>
          - 2016, 1st Place at Slovak Mathematical Olympiad, Regional
          Round <br>
          - 2016, 1st Place at Slovak Informatics Olympiad, Regional
          Round<br>
          - 2015, Bronze Medal, Middle European Mathematical Olympiad<br>
          - 2015, 2nd Place at Slovak Informatics Olympiad, Regional
          Round<br>
          - 2014, 1st Place at Slovak Mathematical Olympiad, Regional
          Round <br>
          - 2013, 1st Place at Slovak Mathematical Olympiad, Regional
          Round <br>
          <br>
          Slavo has been active with marking solutions for the Slovak
          National Mathematical Olympiad, preparing the Slovak team for
          the International Mathematical Olympiad, marking solutions of
          various correspondence contests in mathematics and computer
          science, and organizing summer camps for highly talented
          Slovak pupils in mathematics and computer science.<br>
          <br>
          At KAUST, you will find Slavo in Building 1, Level 2.<br>
          <br>
          Disambiguation: Slavo's older brother <a
            href="https://fhanzely.github.io/index.html">Filip</a> is
          also at KAUST, studying towards his PhD in my group.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 21, 2019</h3>
          <h1>2 Interviews in 1 Day</h1>
          <br>
          I have been interviewed twice today. First by <a
            href="https://dmurphyfreelancer.wixsite.com/dmurphyfreelancer">David




















































            Murphy</a> for a KAUST article related to the "Distinguished
          Speaker Award" I received at <a
            href="https://iccopt2019.berlin">ICCOPT</a> earlier this
          month, and then by <a
href="https://www.slovenskezahranicie.sk/sk/udalost/2955/rozhlasova-publicistka-lubica-hargasova-ocenena-zlatym-biatecom-hospodarskeho-klubu-za-projekt--nasi-a-svetovi-v-rtvs">Ľubica




















































            Hargašová</a> (who was kind enough to travel to meet me) for
          her <a href="https://www.rtvs.sk">RTVS</a> (Slovak Radio and
          Television) radio show <a
            href="https://slovensko.rtvs.sk/relacie/nasi-a-svetovi">"Naši




















































            a Svetoví"</a> ("Ours and of the World") about Slovaks who
          found success abroad. The former interview will lead to a
          written piece (in English), while the latter interview was
          recorded and should air at some point in September (in
          Slovak).<br>
          <br>
          [By the way - I was officially on vacation today...]<br>
          <br>
          <i>Update (September 7, 2019):</i> A (short compilation from)
          the interview aired today at Radio Slovensko. The recording
          can be listened to <a
href="https://slovensko.rtvs.sk/relacie/nasi-a-svetovi/204797/nasi-a-svetovi-peter-richtarik">online.



























          </a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 11, 2019</h3>
          <h1>2 Postdoc Positions </h1>
          <br>
          I have two postdoc positions open in the area of optimization
          and/or machine learning, to be filled by January 2020. If
          interested, send me an email! Include your CV and explain why
          you are interested. <br>
          <br>
          Application deadline: no deadline; positions will be open
          until filled<br>
          <br>
          Position start: By January 2020<br>
          <br>
          Duration: 1 to 3 years (based on agreement)<br>
          <br>
          Conditions: Very competitive salary and benefits; Travel
          funding and access to state-of-the-art facilities; On-campus
          accommodation. The KAUST campus is home of around 7,000
          people, and comprises a land area of 36 km2. Includes
          restaurants, schools, shops, cinema, two private beaches,
          recreation centers, supermarket, medical center, etc.<br>
          <br>
          Application process: Send an email to me (peter dot richtarik
          at kaust dot edu dot sa), explain why you are interested in
          the position, and enclose your CV. If your CV catches my
          attention, I may ask for reference letters and extra
          materials. Alternatively, you may instruct your letter writers
          to send letters to me (by email) right away. Shortlisted
          candidates will progress to a Skype interview. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 8, 2019</h3>
          <h1>My Group @ ICCOPT</h1>
          <br>
          Many members of my (combined KAUST-Edinburgh-MIPT) group
          attended ICCOPT. Here is info on their talks plus links to the
          underlying papers and slides (if available):<br>
          <br>
          <ul>
            <li><a href="https://adil-salim.github.io">Adil Salim</a></li>
            <li>paper <a href="https://arxiv.org/abs/1905.11768">"Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates"</a></li>
          </ul>
          <ul>
            <li><a href="https://qianxunk.github.io">Xun Qian</a></li>
            <li>paper <a href="http://proceedings.mlr.press/v97/qian19b.html">"SGD: General Analysis and Improved Rates"</a>, ICML 2019</li>
            <li>[<a href="talks/TALK-SGD-AS.pdf">slides</a>]</li>
          </ul>
          <ul>
            <li><a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas Loizou</a><br></li>
            <li>paper <a href="http://proceedings.mlr.press/v97/assran19a.html">"Stochastic Gradient Push for Distributed Deep Learning"</a>, ICML 2019</li>
            <li>[<a href="talks/TALK-SGPush-ICCOPT2019-Nicolas.pdf">slides</a>]<br></li>
          </ul>
          <ul>
            <li><a href="https://konstmish.github.io">Konstantin Mishchenko</a><br></li>
            <li>paper <a href="https://arxiv.org/abs/1905.11535">"A Stochastic Decoupling Method for Minimizing the Sum of Smooth and Non-smooth Functions"</a></li>
            <li>[<a href="talks/TALK-SDM-ICCOPT2019-Konstantin.pdf">slides</a>]<br></li>
            <li><br></li>
            <li><a href="https://samuelhorvath.github.io">Samuel Horváth</a></li>
            <li>paper <a href="https://arxiv.org/abs/1904.05115">"Stochastic Distributed Learning with Gradient Quantization and Variance Reduction"</a></li>
          </ul>
          <ul>
            <li><a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a><br></li>
            <li>paper <a href="https://arxiv.org/abs/1905.11373">"Revisiting Stochastic Extragradient"</a></li>
            <li>[<a href="https://www.dmitry-kovalev.com/posts/iccopt_2019/iccopt.pdf">slides</a>]<br></li>
            <li><br></li>
            <li><a href="https://elnurgasanov.tilda.ws">Elnur Gasanov</a></li>
            <li><br></li>
            <li>Dmitry Kamzolov</li>
            <li>talk "Composite High-Order Method for Convex Optimization"</li>
            <li><br></li>
            <li>Egor Shulgin (attended the Summer School and the conference)</li>
            <li><br></li>
            <li>Igor Sokolov (attended the Summer School and the conference)</li>
          </ul>
          <ul>
            <li><a href="https://richtarik.org">Peter Richtárik</a></li>
            <li>paper <a href="https://papers.nips.cc/paper/7478-sega-variance-reduction-via-gradient-sketching">"SEGA: Variance Reduction via Gradient Sketching"</a>, NeurIPS 2018</li>
            <li>[<a href="talks/TALK-SEGA-ICCOPT2019.pdf">slides</a>]</li>
            <li><br></li>
          </ul>
          <br>
          Several former members of my KAUST and Edinburgh groups
          attended as well:<br>
          <br>
          <ul>
            <li><a href="https://ehbergou.github.io">El Houcine Bergou</a> <br></li>
            <li>paper <a href="https://arxiv.org/abs/1902.03591">"Stochastic Three Points Method for Unconstrained Smooth Minimization"</a></li>
          </ul>
          <ul>
            <li><a href="https://www.aritradutta.com">Aritra Dutta</a><br></li>
            <li>paper <a href="https://arxiv.org/abs/1707.00133">"Weighted Singular Value Thresholding and its Application to Background Estimation"</a></li>
          </ul>
          <ul>
            <li><a href="http://mtakac.com">Martin Takáč</a><br></li>
            <li>paper <a href="https://arxiv.org/abs/1901.09997">"Quasi-Newton Methods for Deep Learning: Forget the Past, Just Sample"</a></li>
            <li><br></li>
            <li><a href="https://gowerrobert.github.io">Robert Gower</a> <br></li>
            <li>talk "Expected Smoothness is the Key to Understanding the Mini-batch Complexity of Stochastic Gradient Methods"</li>
            <li>[<a href="https://arxiv.org/abs/1805.02632">paper 1</a> (JacSketch)] [<a href="http://proceedings.mlr.press/v97/qian19b.html">paper 2</a> (SGD)]  [<a href="https://gowerrobert.github.io/pdf/papers/free_SVRG.pdf">paper 3</a> (SVRG)]</li>
            <li>[<a href="https://gowerrobert.github.io/pdf/talks/expected_smoothness.pdf">slides</a>]</li>
          </ul>
          <ul>
            <li><a href="https://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a></li>
            <li>talk: "Adaptive Primal-Dual Coordinate Descent Methods for Non-smooth Composite Minimization with Linear Operator"<br></li>
            <li><br></li>
            <li><a href="http://www.math.canterbury.ac.nz/%7Er.tappenden/">Rachael Tappenden</a></li>
            <li>talk <a href="https://arxiv.org/abs/1710.03695">"Underestimate Sequences via Quadratic Averaging"</a><br></li>
            <li><br></li>
            <li><a href="https://researcher.watson.ibm.com/person/ie-jakub.marecek">Jakub Mareček</a></li>
            <li>talk: "Time-varying Non-convex Optimisation: Three Case Studies"<br></li>
          </ul>
          <br>
          It's 18 people in total (and I am not counting
          students/postdocs of my former students)! We had a distinct
          presence, and most importantly, had fun at the event!<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 3, 2019</h3>
          <h1>ICCOPT Summer School Slides </h1>
          <br>
          My ICCOPT summer school course slides are here: <br>
          <br>
          <a href="talks/ZOO_course_ICCOPT2019.pdf"><img
              src="imgs/ZOO_course_cover.png" alt="slides" width="700"
              border="0" height="494"></a><br>
          <br>
          Here are supplementary (flashy Powerpoint) slides about <a
            href="talks/ZOO_course_ICCOPT2019-SGD-SR.pdf">SGD-SR</a><i>
          </i>and <a href="talks/ZOO_course_ICCOPT2019-SEGA.pdf">SEGA</a><i>.
          </i><br>
          <br>
          I was pleasantly surprised to have received a "distinguished
          speaker" award:<br>
          <br>
          <img src="imgs/golden_bear.png" alt="Bear" width="700"
            height="474"><br>
          <br>
          The bear probably represents the speed with which I delivered
          the lectures... ;-)<br>
          <br>
          Update (September 15, 2019): KAUST wrote a <a
href="https://www.kaust.edu.sa/en/news/kaust-professor-peter-richt%C3%A1rik-wins-distinguished-speaker-award">short


















            article about this, and other things...<br>
          </a><br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 30, 2019</h3>
          <h1>On my way to Berlin for ICCOPT </h1>
          <br>
          I am on my way to Berlin to first teach in the <a
            href="https://iccopt2019.berlin/index.php?page=summerschool">ICCOPT










































































            Summer School</a>, and then to attend the <a
            href="https://iccopt2019.berlin/index.php">ICCOPT conference</a>.
          On August 3rd I will deliver a 1 day (4 x 1.5 hours) short
          course entitled "A Guided Walk Through the ZOO of Stochastic
          Gradient Descent Methods". Here is what the course is going to
          be about:<br>
          <br>
          <i>Stochastic gradient descent (SGD) in one of its many
            variants is the workhorse method for training modern
            supervised machine learning models. However, the world of
            SGD methods is vast and expanding, which makes it hard to
            understand its landscape and inhabitants. In this tutorial I
            will offer a guided walk through the ZOO of SGD methods. I
            will chart the landscape of this beautiful world, and make
            it easier to understand its inhabitants and their
            properties. In particular, I will introduce a unified
            analysis of a large family of variants of proximal
            stochastic gradient descent (SGD) which so far have required
            different intuitions, convergence analyses, have different
            applications, and which have been developed separately in
            various communities. This framework includes methods with
            and without the following tricks, and their combinations:
            variance reduction, data sampling, coordinate sampling,
            importance sampling, mini-batching and quantization. As a
            by-product, the presented framework offers the first unified
            theory of SGD and randomized coordinate descent (RCD)
            methods, the first unified theory of variance reduced and
            non-variance-reduced SGD methods, and the first unified
            theory of quantized and non-quantized methods.</i><i><br>
          </i> <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 25, 2019</h3>
          <h1>NeurIPS Reviews Arrived<br>
          </h1>
          <br>
          NeurIPS reviews came in. As usual, most reviewers assigned to
          evaluate my papers are not quite at home in my area, or simply
          provide an educated guess only. This leads to many rather
          meaningless and noisy reviews (this is in sharp contrast with
          journal submissions in top journals where more often than not
          the reviewers are knowledgeable). This is something that took
          me some time to get used to back in the day... The reason for
          this? A trade-off between the quality of the reviews and the
          speed of the accept/reject decision. Thanks to the few
          reviewers who actually understood our results and were able to
          provide useful feedback! Now we have until July 31 to prepare
          author response, aka a "rebuttal". <br>
          <br>
          An interesting innovation this year: a system was put in place
          to automatically flag some papers with a common subset of
          authors as potentially being a "dual submission". A dual
          submission is essentially a single set of results presented as
          two (usually slightly) different papers, which is a trick
          aimed to increase chances of acceptance. When incentives are
          high, people are inventive... Some of my work got flagged this
          way, and incorrectly so. The problem I can see right away is
          that some reviewers, already busy with many reviews and other
          tasks, apparently consider this as a convenient excuse to
          spend less time reviewing and simply taking the flag at face
          value, which allows them to simply <i>claim</i> dual
          submission without providing any supporting evidence. Do we
          really want AI to do reviews for us as well? No, we do not!
          This is a big danger to the serious researchers in the
          community; and it is not at all clear to me whether this issue
          was considered before the system was launched. Do the benefits
          outweigh the costs? People like me who would never think of a
          dual submission will be on the losing side. This would not
          have to happen if the reviewers took their job seriously and
          evaluated the papers properly. But perhaps this new system
          will eliminate some of the genuine dual submissions - and I
          have seen some in the past. What's worse, we are now forced to
          compare the two papers flagged as potentially dual submission
          in the rebuttal. This on its own is a great idea - but not
          delivered correctly because no extra space is given to write
          the author response. We already have just a single page to
          respond, which I never found to be enough. Now, there is even
          less space to respond to the actual review comments - which
          almost by definition will lead to such papers to be rejected.
          After all, the reviewer will not get a response to all
          criticism, and will interpret this in the obvious way. To sum
          this up: I am not happy with this new system, and the
          community should not be either.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 19, 2019</h3>
          <h1>Konstantin @ Stanford<br>
          </h1>
          <br>
          <a href="https://konstmish.github.io">Konstantin</a> is
          visiting <a href="https://web.stanford.edu/%7Eboyd/">Stephen
            Boyd</a> at Stanford.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 15, 2019</h3>
          <h1>Konstantin @ Frontiers of Deep Learning<br>
          </h1>
          <br>
          <a href="https://konstmish.github.io">Konstantin</a> is
          attending the Simons Institute (Berkeley) workshop <a
            href="https://simons.berkeley.edu/workshops/dl2019-1">Frontiers
















































































            of Deep Learning.</a> The schedule and videos of the talks
          will become <a
            href="https://simons.berkeley.edu/workshops/schedule/10627">available
















































































            here.</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 14, 2019</h3>
          <h1>ICIAM 2019 - Valencia, Spain<br>
          </h1>
          <br>
          I am attending <a href="https://iciam2019.org">ICIAM 2019</a>,
          the largest scientific meeting of industrial and applied
          mathematicians taking place once every four years. I am giving
          a 30 min talk on Wednesday in an invited session on
          optimization (11am-1pm). I will be leaving Valencia on
          Saturday.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 13, 2019</h3>
          <h1>Accelerating the Grapevine Effect<br>
          </h1>
          <br>
          My recent work with <a
            href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a> on randomized gossip algorithms is featured in
          the KAUST Discovery magazine. You can <a
href="https://discovery.kaust.edu.sa/en/article/841/accelerating-the-grapevine-effect">read


the
















































































            article online here.</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 11, 2019</h3>
          <h1>Martin Takáč Giving a Talk in Bratislava<br>
          </h1>
          <br>
          Today, my former PhD student <a href="http://www.mtakac.com">Martin





















































































            Takáč</a> (and now an Assistant Professor at Lehigh
          University, USA) is giving a popular science talk in
          Bratislava, Slovakia. The talk is entitled: "Current trends in
          big data and artificial intelligence". I understand the talk
          will be delivered in Slovak language.<br>
          <br>
          <img src="imgs/Plagat_VvC_jul.png" alt="Plagat" width="700"
            height="989"><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 9, 2019</h3>
          <h1>Filip @ Google<br>
          </h1>
          <br>
          <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          started a research internship at Google, New York. He will be
          back at KAUST in early October.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 6, 2019</h3>
          <h1>Nature index: KAUST #52 Globally and #4 in Western Asia<br>
          </h1>
          <br>
          The <a
            href="https://www.nature.com/articles/d41586-019-01919-8">2019

























































































            Nature index rankings</a> were published. Here is what
          Nature says about its new "fractional count" rankings, "Our
          measure, fractional count (FC), is based on the share of
          articles published in 82 prestigious scientific journals,
          selected by an independent panel of scientists and tracked by
          the Nature Index database." <a
            href="https://www.nature.com/articles/d41586-019-01919-8">The

























































































            full story can be found here.</a><br>
          <br>
          In the western Asia region, among academic institutions, and
          in the "nature &amp; science" area, KAUST was ranked #4. Here
          is a <a
href="https://www.natureindex.com/institution-outputs/generate/Nature%20&amp;%20Science/regions-Western%20Asia/academic/score">list

























































































            of the top 20 institutions</a>:<br>
          <br>
          01.&nbsp;&nbsp;&nbsp; Weizmann Institute of Science (WIS)<br>
          02.&nbsp;&nbsp;&nbsp; Technion-Israel Institute of Technology
          (IIT)<br>
          03.&nbsp;&nbsp;&nbsp; Tel Aviv University (TAU)<br>
          04.&nbsp;&nbsp;&nbsp; <font color="#ff9900">King Abdullah
            University of Science and Technology (KAUST)</font><br>
          05.&nbsp;&nbsp;&nbsp; Hebrew University of Jerusalem (HUJI)<br>
          06.&nbsp;&nbsp;&nbsp; New York University Abu Dhabi (NYUAD)<br>
          07.&nbsp;&nbsp;&nbsp; Sharif University of Technology (SUT)<br>
          08.&nbsp;&nbsp;&nbsp; Ben-Gurion University of the Negev (BGU)<br>
          09.&nbsp;&nbsp;&nbsp; Bar-Ilan University (BIU)<br>
          10.&nbsp;&nbsp;&nbsp; King Saud University (KSU)<br>
          11.&nbsp;&nbsp;&nbsp; Istanbul University<br>
          12.&nbsp;&nbsp;&nbsp; The University of Jordan<br>
          13.&nbsp;&nbsp;&nbsp; E. A. Buketov Karaganda State University
          (KSU)<br>
          14.&nbsp;&nbsp;&nbsp; University of Haifa (HU)<br>
          15.&nbsp;&nbsp;&nbsp; Nazarbayev University (NU)<br>
          16.&nbsp;&nbsp;&nbsp; S. Toraighyrov Pavlodar State University
          (PSU)<br>
          17.&nbsp;&nbsp;&nbsp; University of Tehran (UT)<br>
          18.&nbsp;&nbsp;&nbsp; Middle East Technical University (METU)<br>
          19.&nbsp;&nbsp;&nbsp; A. A. Baitursynov Kostanay State
          University<br>
          20.&nbsp;&nbsp;&nbsp; Koç University (KU)<br>
          <br>
          Globally, also among academic institutions, KAUST ranked #52
          in the area <a
href="https://www.natureindex.com/institution-outputs/generate/Nature%20&amp;%20Science/global/academic/n_article">"nature


























































































            &amp; science" (article count)</a><br>
          and #79 in the area <a
href="https://www.natureindex.com/institution-outputs/generate/Physical%20Sciences/global/academic/score">"physical

























































































            sciences" (fractional count).</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 3, 2019</h3>
          <h1> 2019 Shanghai Rankings </h1>
          <br>
          In the 2019 Shanghai rankings, KAUST was <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/computer-science-engineering.html">ranked


























































































            101-150 in Computer Science and Engineering</a>. This is
          quite some achievement for a university that did not yet exist
          10 years ago, and one that currently has about 150 faculty
          only! We are still growing, and plan to reach full capacity in
          about 5 years. <br>
          <br>
          Here are notable rankings in some other fields: <br>
          <br>
          25. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/energy-science-engineering.html">Energy


























































































            Science &amp; Engineering</a><br>
          32. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/nanoscience-nanotechnology.html">Nanoscience


























































































            &amp; Nanotechnology </a><br>
          33. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/materials-science-engineering.html">Materials

































































































            Science &amp; Engineering</a><br>
          33. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/mechanical-engineering.html">Mechanical

































































































            Engineering</a><br>
          38. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/chemical-engineering.html">Chemical

































































































            Engineering</a><br>
          50. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/telecommunication-engineering.html">Telecommunication

































































































            Engineering</a><br>
          51-75. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/chemistry.html">Chemistry</a><br>
          51-75. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/water-resources.html">Water

































































































            Resources</a><br>
          101-150. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/computer-science-engineering.html">Computer

































































































            Science &amp; Engineering</a><br>
          101-150. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/environmental-science-engineering.html">Environmental

































































































            Science &amp; Engineering</a><br>
          201-300. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/earth-sciences.html">Earth

































































































            Sciences</a><br>
          301-400. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/mathematics.html">Mathematics</a><br>
          301-400. <a
href="http://www.shanghairanking.com/Shanghairanking-Subject-Rankings/electrical-electronic-engineering.html">Electrical

































































































            &amp; Electronic Engineering</a><br>
          <br>
          Overall, <a
            href="http://www.shanghairanking.com/ARWU2018.html">KAUST is
            ranked 201-300 globally.</a> Four years ago, when KAUST was
          6 years old, our ranking was 301-400. Five years ago, KAUST
          was ranked 401-500. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 1, 2019</h3>
          <h1>Promotion to Full Professor<br>
          </h1>
          <br>
          I have been promoted to full professor. <br>
          <br>
          What does this mean? Some people thought about this quite a
          bit [<a href="http://rtalbert.org/full-professor/">1</a>, <a
href="https://kardiagroup.com/changes-at-promotion-to-full-professor/">2</a>,
          <a
href="https://www.chronicle.com/article/You-re-a-Full-Professor-Now/245403">3</a>].

In


























































































          my case, the most immediate and obvious changes are: <br>
          <br>
          i) I now have a 5 year rolling contract at KAUST. That means
          that each year my contract gets automatically extended by one
          year (until it does not - which I do not expect will happen -
          at which point I will have 5 years to find another job).<br>
          <br>
          ii) My KAUST baseline research funding will increase (I do not
          yet know by how much; but I expect a roughly 40-50% increase).
          This means I can either <a
            href="https://richtarik.org/i_join.html">grow</a> the <a
            href="https://richtarik.org/i_team.html">group</a>, or do
          more with the current group. In any case, this is an excellent
          boost which will have a positive effect one way or another.<br>
          <br>
          iii) My salary will increase.<br>
          <br>
          I will reflect on this in more depth at some point in the
          future. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 30, 2019</h3>
          <h1>Amazon Internship</h1>
          <br>
          <a href="https://samuelhorvath.github.io/">Samuel</a> has
          started his research internship in Machine Learning Science
          group at Amazon, Berlin, Germany.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 28, 2019</h3>
          <h1> Nicolas Loizou: Thesis Defense</h1>
          <br>
          <a
href="https://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Nicolas





































































































            Loizou</a> successfully defended his PhD thesis "Randomized
          iterative methods for linear systems: momentum, inexactness
          and gossip" today. Congratulations!!! Nicolas is the last
          student graduating from my Edinburgh group. He will join <a
            href="https://mila.quebec/">MILA</a>, Montréal, in the Fall.
          <br>
          <br>
          <i>Update (September 12, 2019):</i> Here is his <a
            href="papers/2019_PhD_Thesis_Nicolas_Loizou.pdf">PhD thesis.</a>
          <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 24, 2019</h3>
          <h1>Dmitry, Adil and Elnur @ DS3 2019</h1>
          <br>
          <a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>,
          <a href="https://adil-salim.github.io/">Adil Salim</a> and <a
            href="https://elnurgasanov.tilda.ws/">Elnur Gasanov</a> are
          attending the <a
            href="https://www.ds3-datascience-polytechnique.fr/">Data
            Science Summer School (DS3) at École Polytechnique, Paris,
            France</a>. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 23, 2019</h3>
          <h1>Paper Accepted</h1>
          <br>
          The paper <a href="https://arxiv.org/abs/1811.12403">"New
            convergence aspects of stochastic gradient algorithms"</a>,
          joint work with <a
href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-lamnguyen.mltd">Lam











































































































            M. Nguyen</a>, <a
            href="https://scl.uconn.edu/people/ha/info.php">Phuong Ha
            Nguyen</a>, <a href="https://coral.ise.lehigh.edu/katyas/">Katya












































































































            Scheinberg</a>, <a href="http://mtakac.com">Martin Takáč</a>
          and <a href="http://www.ee.uconn.edu/marten-van-dijk/">Marten
            van Dijk</a>, was accepted to <a
            href="http://www.jmlr.org/">JMLR</a>. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 20, 2019</h3>
          <h1>Paper Accepted</h1>
          <br>
          The paper <a href="https://arxiv.org/abs/1801.04873">"Randomized

















































































































            projection methods for convex feasibility problems:
            conditioning and convergence rates"</a>, joint work with <a
            href="http://acse.pub.ro/person/ion-necoara/">Ion Necoara</a>
          and <a href="https://dblp.org/pers/hd/p/Patrascu:Andrei">Andrei











































































































            Patrascu</a>, was accepted to <a
href="https://www.siam.org/Publications/Journals/SIAM-Journal-on-Optimization-SIOPT">SIAM

















































































































            Journal on Optimization</a>. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 17, 2019</h3>
          <h1>Dmitry @ Summer School in Voronovo</h1>
          <br>
          <a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>
          is attending <a href="https://ssopt.org/">"Control,
            Information and Optimization"</a> Summer School in Voronovo,
          Moscow region, Russia.<br>
          <br>
          <i>Update:</i> Dmitry won the Best Poster Award for his poster
          describing the paper <a
            href="https://arxiv.org/abs/1904.05115">"Stochastic
            distributed learning with gradient quantization and variance
            reduction"</a>. Congratulations!!! The paper was co-autored
          by <a href="https://samuelhorvath.github.io/">Samuel Horváth</a>,
          <a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>,
          <a href="https://konstmish.github.io/">Konstantin Mishchenko</a>,
          myself and <a href="https://sstich.ch/">Sebastian Stich</a>.
          <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 17, 2019</h3>
          <h1>Workshop at the Isaac Newton Institute, Cambridge</h1>
          <br>
          I am at the <a href="https://www.newton.ac.uk/">Isaac Newton
            Institute for Mathematical Sciences</a> at the <a
            href="https://www.cam.ac.uk/">University of Cambridge</a>,
          attending the workshop <a
            href="https://www.newton.ac.uk/event/ascw03">"Approximation,
            Sampling, and Compression in High Dimensional Problems"</a>.
          My talk is on Thursday June 20; I will speak about <a
            href="https://arxiv.org/abs/1805.02632">JacSketch.</a> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 16, 2019</h3>
          <h1>Konstantin @ Bath</h1>
          <br>
          <a href="https://konstmish.github.io/">Konstantin Mishchenko</a>
          is visiting <a href="https://mehrhardt.github.io/">Matthias
            J. Ehrhardt</a> at <a href="https://www.bath.ac.uk/">University
















































































































            of Bath</a>, United Kingdom.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 14, 2019</h3>
          <h1>ICML Workshops Started<br>
          </h1>
          <br>
          The main ICML conference is over; the workshops start today
          and continue tomorrow.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 13, 2019</h3>
          <h1>KAUST President @ ICML 2019</h1>
          <br>
          KAUST president, <a
            href="https://en.wikipedia.org/wiki/Tony_F._Chan">Tony Chan</a>,
          attended ICML yesterday. I have shown him around and we have
          jointly attended a number of interesting talks and sessions.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 11, 2019</h3>
          <h1>ICML 2019 Talks</h1>
          <br>
          We have given three talks today; one by Samuel and two by me.
          Here are the slides:<br>
          <br>
          <a href="talks/TALK-nonconvex-AS.pdf">Slides for "Nonconvex
            Variance Reduced Optimization with Arbitrary Sampling"</a>
          (5 min oral)<br>
          <a href="talks/TALK-SGD-AS.pdf">Slides for "SGD: General
            Analysis and Improved Rates"</a> (20 min oral)<br>
          <a href="talks/TALK-SAGA-AS.pdf">Slides for "SAGA with
            Arbitrary Sampling"</a> (5 min oral)<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 9, 2019</h3>
          <h1>ICML 2019<br>
          </h1>
          <br>
          I am in Los Angeles, attending <a href="https://icml.cc/">ICML






























































































































            2019</a>. I am here until June 16; and will attend the
          workshops as well. <a
            href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas</a>,
          <a href="https://konstmish.github.io/">Konstantin</a>, <a
            href="http://www.ali-sa.org/">Alibek</a>, <a
            href="https://samuelhorvath.github.io/">Samuel</a>, <a
            href="https://adil-salim.github.io/">Adil</a>, <a
            href="https://www.aritradutta.com/">Aritra</a>, and <a
            href="https://ehbergou.github.io/">El Houcine</a> are here,
          too. <br>
          <br>
          KAUST has a booth at ICML - check out booth #212! We are
          hiring! We have openings for MS/PhD positions, postdocs,
          research scientists, assistant professors, associate professor
          and full professors.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 8, 2019</h3>
          <h1>New Intern Arrived: Ahmed Khaled Ragab from Cairo</h1>
          <br>
          Ahmed Khaled Ragab (Cairo University) just arrived to KAUST
          for a research internship. Welcome!<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 6, 2019</h3>
          <h1>ICML 2019 Posters</h1>
          <br>
          We have prepared posters for our ICML 2019 papers:<br>
          <br>
          <a href="http://proceedings.mlr.press/v97/horvath19a.html">"Nonconvex




































































































































            Variance Reduced Optimization with Arbitrary Sampling"</a><br>
          oral talk, Tuesday June 11 @ 11:35-11:40am in Room 104 (<a
            href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4756">schedule</a>)<br>
          poster, Tuesday June 11 @ 6:30pm-9:00pm in Pacific Ballroom
          #95 (<a
            href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=3831">schedule</a>)<br>
          <br>
          <a href="http://proceedings.mlr.press/v97/qian19b.html">"SGD:
            General Analysis and Improved Rates"</a><br>
          20 min oral talk, Tuesday June 11 @ 2:40-3:00pm in Room 103 (<a
href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4669">schedule</a>)<br>
          poster, Tuesday June 11 @ 6:30pm-9:00pm in Pacific Ballroom
          #195 (<a
            href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4155">schedule</a>)<br>
          <br>
          <a href="http://proceedings.mlr.press/v97/qian19a.html">"SAGA
            with Arbitrary Sampling"</a><br>
          oral talk, Tuesday June 11 @ 3:15-3:20pm in Room 103 (<a
            href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4673">schedule</a>)<br>
          poster, Tuesday June 11 @ 6:30pm-9:00pm in Pacific Ballroom
          #199 (<a
            href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=3969">schedule</a>)<br>
          <br>
          <br>
          Here are the posters:<br>
          <br>
          <a href="posters/Poster-nonconvex_AS.pdf"><img
              src="posters/Poster-nonconvex_AS_small.png" alt="Poster
              nonconvex VR opt with arbitrary sampling" width="700"
              border="0" height="526"></a><br>
          <br>
          <a href="posters/Poster-SGD_AS.pdf"><img
              src="posters/Poster-SGD_AS_small.png" alt="Poster SGD with
              arbitrary sampling" width="700" border="0" height="524"></a><br>
          <br>
          <a href="posters/Poster-SAGA_AS.pdf"><img
              src="posters/Poster-SAGA_AS_small.png" alt="Poster SAGA
              with Arbitrary Sampling" width="700" border="0"
              height="527"></a><br>
          <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 4, 2019</h3>
          <h1>New Paper<br>
          </h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1906.01481">"L-SVRG and
            L-Katyusha with arbitrary sampling"</a> - joint work with <a
            href="https://qianxunk.github.io/">Xun Qian</a> and <a
            href="https://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu.</a><br>
          <br>
          Abstract: <i>We develop and analyze a new family of
            nonaccelerated and accelerated loopless variance-reduced
            methods for finite sum optimization problems. Our
            convergence analysis relies on a novel expected smoothness
            condition which upper bounds the variance of the stochastic
            gradient estimation by a constant times a distance-like
            function. This allows us to handle with ease arbitrary
            sampling schemes as well as the nonconvex case. We perform
            an in-depth estimation of these expected smoothness
            parameters and propose new importance samplings which allow
            linear speedup when the expected minibatch size is in a
            certain range. Furthermore, a connection between these
            expected smoothness parameters and expected separable
            overapproximation (ESO) is established, which allows us to
            exploit data sparsity as well. Our results recover as
            special cases the recently proposed loopless SVRG and
            loopless Katyusha methods.</i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 4, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1906.01474">"MISO is making a
            comeback with better proofs and rates"</a> - joint work with
          <a href="https://qianxunk.github.io/">Xun Qian</a>, <a
            href="http://www.ali-sa.org/">Alibek Sailanbayev</a> and <a
            href="https://konstmish.github.io/">Konstantin Mishchenko.</a>
          <br>
          <br>
          Abstract: <i>MISO, also known as Finito, was one of the first
            stochastic variance reduced methods discovered, yet its
            popularity is fairly low. Its initial analysis was
            significantly limited by the so-called Big Data assumption.
            Although the assumption was lifted in subsequent work using
            negative momentum, this introduced a new parameter and
            required knowledge of strong convexity and smoothness
            constants, which is rarely possible in practice. We
            rehabilitate the method by introducing a new variant that
            needs only smoothness constant and does not have any extra
            parameters. Furthermore, when removing the strong convexity
            constant from the stepsize, we present a new analysis of the
            method, which no longer uses the assumption that every
            component is strongly convex. This allows us to also obtain
            so far unknown nonconvex convergence of MISO. To make the
            proposed method efficient in practice, we derive
            minibatching bounds with arbitrary uniform sampling that
            lead to linear speedup when the expected minibatch size is
            in a certain range. Our numerical experiments show that MISO
            is a serious competitor to SAGA and SVRG and sometimes
            outperforms them on real datasets.</i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 3, 2019</h3>
          <h1>Elnur Visiting Grenoble</h1>
          <br>
          <a href="https://elnurgasanov.tilda.ws/">Elnur Gasanov</a> is
          visiting <a href="https://ljk.imag.fr/membres/Jerome.Malick/">Jérôme















































































































            Malick</a> and his group in Grenoble. He will stat there
          until the end of June.<br>
          <br>
          Update (June 29): Elnur's visit was extended until until July
          19.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 30, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1905.13278">"A stochastic
            derivative free optimization method with momentum"</a> -
          joint work with <a href="https://eduardgorbunov.github.io/">Eduard








































































































































            Gorbunov</a>, <a href="http://www.adelbibi.com/">Adel Bibi</a>,
          Ozan Sezer and <a href="https://ehbergou.github.io/">El
            Houcine Bergou.</a><br>
          <br>
          Abstract: <i>We consider the problem of unconstrained
            minimization of a smooth objective function in R^d in
            setting where only function evaluations are possible. We
            propose and analyze stochastic zeroth-order method with
            heavy ball momentum. In particular, we propose SMTP - a
            momentum version of the stochastic three-point method (STP)
            of Bergou et al (2018). We show new complexity results for
            non-convex, convex and strongly convex functions. We test
            our method on a collection of learning to continuous control
            tasks on several MuJoCo environments with varying difficulty
            and compare against STP, other state-of-the-art
            derivative-free optimization algorithms and against policy
            gradient methods. SMTP significantly outperforms STP and all
            other methods that we considered in our numerical
            experiments. Our second contribution is SMTP with importance
            sampling which we call SMTP_IS. We provide convergence
            analysis of this method for non-convex, convex and strongly
            convex objectives.</i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 30, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1905.12938">"On stochastic sign
            descent methods"</a> - joint work with <a
            href="https://mher-safaryan.github.io">Mher Safaryan.</a><br>
          <br>
          Abstract: <i>Various gradient compression schemes have been
            proposed to mitigate the communication cost in distributed
            training of large scale machine learning models. Sign-based
            methods, such as signSGD, have recently been gaining
            popularity because of their simple compression rule and
            connection to adaptive gradient methods, like ADAM. In this
            paper, we perform a general analysis of sign-based methods
            for non-convex optimization. Our analysis is built on
            intuitive bounds on success probabilities and does not rely
            on special noise distributions nor on the boundedness of the
            variance of stochastic gradients. Extending the theory to
            distributed setting within a parameter server framework, we
            assure variance reduction with respect to number of nodes,
            maintaining 1-bit compression in both directions and using
            small mini-batch sizes. We validate our theoretical findings
            experimentally. </i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 29, 2019</h3>
          <h1>Tong Zhang @ KAUST</h1>
          <br>
          <a href="http://tongzhang-ml.org/">Tong Zhang</a> is visiting
          me at KAUST. He is giving a talk at noon today in the <a
            href="https://ml.kaust.edu.sa/seminar.html">ML Hub Seminar
            Series.</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 28, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1905.11768">"Stochastic proximal
            Langevin algorithm: potential splitting and nonasymptotic
            rates"</a> - joint work with <a
            href="https://adil-salim.github.io/">Adil Salim</a> and <a
            href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>.<br>
          <br>
          Abstract: <i>We propose a new algorithm---Stochastic Proximal
            Langevin Algorithm (SPLA)---for sampling from a log concave
            distribution. Our method is a generalization of the Langevin
            algorithm to potentials expressed as the sum of one
            stochastic smooth term and multiple stochastic nonsmooth
            terms. In each iteration, our splitting technique only
            requires access to a stochastic gradient of the smooth term
            and a stochastic proximal operator for each of the nonsmooth
            terms. We establish nonasymptotic sublinear and linear
            convergence rates under convexity and strong convexity of
            the smooth term, respectively, expressed in terms of the KL
            divergence and Wasserstein distance. We illustrate the
            efficiency of our sampling technique through numerical
            simulations on a Bayesian learning task. </i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 28, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1905.11692">"Direct nonlinear
            acceleration"</a> - joint work with <a
            href="https://www.aritradutta.com/">Aritra Dutta</a>, <a
            href="https://ehbergou.github.io/">El Houcine Bergou</a>,
          Yunming Xiao and <a href="https://mcanini.github.io/">Marco
            Canini.</a><br>
          <br>
          Abstract: <i>Optimization acceleration techniques such as
            momentum play a key role in state-of-the-art machine
            learning algorithms. Recently, generic vector sequence
            extrapolation techniques, such as regularized nonlinear
            acceleration (RNA) of Scieur et al., were proposed and shown
            to accelerate fixed point iterations. In contrast to RNA
            which computes extrapolation coefficients by (approximately)
            setting the gradient of the objective function to zero at
            the extrapolated point, we propose a more direct approach,
            which we call direct nonlinear acceleration (DNA). In DNA,
            we aim to minimize (an approximation of) the function value
            at the extrapolated point instead. We adopt a regularized
            approach with regularizers designed to prevent the model
            from entering a region in which the functional approximation
            is less precise. While the computational cost of DNA is
            comparable to that of RNA, our direct approach significantly
            outperforms RNA on both synthetic and real-world datasets.
            While the focus of this paper is on convex problems, we
            obtain very encouraging results in accelerating the training
            of neural networks. </i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 27, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1905.11535">"A stochastic
            decoupling method for minimizing the sum of smooth and
            non-smooth functions"</a> - joint work with <a
            href="https://konstmish.github.io/">Konstantin Mishchenko.</a><br>
          <br>
          Abstract: <i>We consider the problem of minimizing the sum of
            three convex functions: i) a smooth function $f$ in the form
            of an expectation or a finite average, ii) a non-smooth
            function $g$ in the form of a finite average of proximable
            functions $g_j$, and iii) a proximable regularizer $R$. We
            design a variance reduced method which is able
            progressively&nbsp; learn the proximal operator of $g$ via
            the computation of the proximal operator of a single
            randomly selected function $g_j$ in each iteration only. Our
            method can provably and efficiently accommodate many
            strategies for the estimation of the gradient of $f$,
            including via standard and variance-reduced stochastic
            estimation, effectively decoupling the smooth part of the
            problem from the non-smooth part. We prove a number of
            iteration complexity results, including a general $O(1/t)$
            rate, $O(1/t^2)$ rate in the case of strongly convex $f$,
            and several linear rates in special cases, including
            accelerated linear rate. For example, our method achieves a
            linear rate for the problem of minimizing a strongly convex
            function $f$ under linear constraints under no assumption on
            the constraints beyond consistency. When combined with SGD
            or SAGA estimators for the gradient of $f$, this&nbsp; leads
            to&nbsp; a very efficient method for empirical risk
            minimization with large linear constraints.&nbsp; Our method
            generalizes several existing algorithms, including
            forward-backward splitting, Douglas-Rachford splitting,
            proximal SGD, proximal SAGA, SDCA, randomized Kaczmarz and
            Point-SAGA. However, our method leads to many new specific
            methods in special cases; for instance,&nbsp; we obtain the
            first randomized variant of the Dykstra's method for
            projection onto the intersection of closed convex sets. </i>
          <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 27, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1905.11373">"Revisiting
            stochastic extragradient"</a> - joint work with <a
            href="https://konstmish.github.io/">Konstantin Mishchenko</a>,
          <a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>,
          <a href="https://github.com/shulgin-egor">Egor Shulgin</a> and
          <a
            href="https://scholar.google.com/citations?user=GI_-KjoAAAAJ&amp;hl=en">Yura




















































































































































            Malitsky.</a><br>
          <br>
          Abstract: <i>We consider a new extension of the extragradient
            method that is motivated by approximating implicit updates.
            Since in the recent work of Chavdarova et al (2019) it was
            shown that the existing stochastic extragradient algorithm
            (called mirror-prox) of Juditsky et al (2011) diverges on a
            simple bilinear problem, we prove guarantees for solving
            variational inequality that are more general. Furthermore,
            we illustrate numerically that the proposed variant
            converges faster than many other methods on the example of </i><i><i>Chavdarova




















































































































































              et al (2019)</i>. We also discuss how extragradient can be
            applied to training Generative Adversarial Networks (GANs).
            Our experiments on GANs demonstrate that the introduced
            approach may make the training faster in terms of data
            passes, while its higher iteration complexity makes the
            advantage smaller. To further accelerate method's
            convergence on problems such as bilinear minimax, we combine
            the extragradient step with the negative momentum of Gidel
            et al (2018) and discuss the optimal momentum value. </i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 27, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1905.11266">"One method to rule
            them all: variance reduction for data, parameters and many
            new methods"</a> - joint work with <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely.</a><br>
          <br>
          Abstract: <i>We propose a remarkably general variance-reduced
            method suitable for solving regularized empirical risk
            minimization problems with either a large number of training
            examples, or a large model dimension, or both. In special
            cases, our method reduces to several known and previously
            thought to be unrelated methods, such as SAGA, LSVRG,
            JacSketch, SEGA and ISEGA, and their arbitrary sampling and
            proximal generalizations. However, we also highlight a large
            number of new specific algorithms with interesting
            properties. We provide a single theorem establishing linear
            convergence of the method under smoothness and quasi strong
            convexity assumptions. With this theorem we recover
            best-known and sometimes improved rates for known methods
            arising in special cases. As a by-product, we provide the
            first unified method and theory for stochastic gradient and
            stochastic coordinate descent type methods. </i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 27, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1905.11261">"A unified theory of
            SGD: variance reduction, sampling, quantization and
            coordinate descent"</a> - joint work with <a
            href="https://eduardgorbunov.github.io/">Eduard Gorbunov</a>
          and <a href="https://fhanzely.github.io/index.html">Filip
            Hanzely.</a><br>
          <br>
          Abstract: <i>In this paper we introduce a unified analysis of
            a large family of variants of proximal stochastic gradient
            descent (SGD) which so far have required different
            intuitions, convergence analyses, have different
            applications, and which have been developed separately in
            various communities. We show that our framework includes
            methods with and without the following tricks, and their
            combinations: variance reduction, importance sampling,
            mini-batch sampling, quantization, and coordinate
            sub-sampling.&nbsp; As a by-product, we obtain the first
            unified theory of SGD and randomized coordinate descent
            (RCD) methods,&nbsp; the first unified theory of variance
            reduced and non-variance-reduced SGD methods, and the first
            unified theory of quantized and non-quantized methods. A key
            to our approach is a parametric assumption on the iterates
            and stochastic gradients. In a single theorem we establish a
            linear convergence result under this assumption and
            strong-quasi convexity of the loss function. Whenever we
            recover an existing method as a special case, our theorem
            gives the best known complexity result. Our approach can be
            used to motivate the development of new useful methods, and
            offers pre-proved convergence guarantees. To illustrate the
            strength of our approach, we develop five new variants of
            SGD, and through numerical experiments demonstrate some of
            their properties. </i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 27, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1905.10988">"Natural compression
            for distributed deep learning"</a> - joint work with <a
            href="https://samuelhorvath.github.io/">Samuel Horváth</a>,
          <a href="https://www.chenyuho.com/">Chen-Yu Ho</a>, Ľudovít
          Horváth, Atal Narayan Sahu and <a
            href="https://mcanini.github.io/">Marco Canini.</a><br>
          <br>
          Abstract: <i>Due to their hunger for big data, modern deep
            learning models are trained in parallel, often in
            distributed environments, where communication of model
            updates is the bottleneck. Various update compression (e.g.,
            quantization, sparsification, dithering) techniques have
            been proposed in recent years as a successful tool to
            alleviate this problem. In this work, we introduce a new,
            remarkably simple and theoretically and practically
            effective compression technique, which we call natural
            compression (NC). Our technique is applied individually to
            all entries of the to-be-compressed update vector and works
            by randomized rounding to the nearest (negative or positive)
            power of two. NC is "natural" since the nearest power of two
            of a real expressed as a float can be obtained without any
            computation, simply by ignoring the mantissa. We show that
            compared to no compression, NC increases the second moment
            of the compressed vector by the tiny factor 9/8 only, which
            means that the effect of NC on the convergence speed of
            popular training algorithms, such as distributed SGD, is
            negligible. However, the communications savings enabled by
            NC are substantial, leading to 3-4x improvement in overall
            theoretical running time. For applications requiring more
            aggressive compression, we generalize NC to natural
            dithering, which we prove is exponentially better than the
            immensely popular random dithering technique. Our
            compression operators can be used on their own or in
            combination with existing operators for a more aggressive
            combined effect. Finally, we show that NC is particularly
            effective for the in-network aggregation (INA) framework for
            distributed training, where the update aggregation is done
            on a switch, which can only perform integer computations. </i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 26, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1905.10874">"Randomized Subspace
            Newton"</a> - joint work with <a
            href="https://gowerrobert.github.io/">Robert Mansel Gower</a>,
          <a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>
          and <a
            href="http://www.opt.uni-duesseldorf.de/%7Elieder/de/inhalt.php">Felix


















































































































































            Lieder. </a><br>
          <br>
          Abstract: <i>We develop a randomized Newton method capable of
            solving learning problems with huge dimensional feature
            spaces, which is a common setting in applications such as
            medical imaging, genomics and seismology. Our method
            leverages randomized sketching in a new way, by finding the
            Newton direction constrained to the space spanned by a
            random sketch. We develop a simple global linear convergence
            theory that holds for practically all sketching techniques,
            which gives the practitioners the freedom to design custom
            sketching approaches suitable for particular applications.
            We perform numerical experiments which demonstrate the
            efficiency of our method as compared to accelerated gradient
            descent and the full Newton method. Our method can be seen
            as a refinement and randomized extension of the results of
            Karimireddy, Stich, and Jaggi (2019). </i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 25, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1905.10598">"Best pair
            formulation &amp; accelerated scheme for non-convex
            principal component pursuit"</a> - joint work with <a
            href="https://richtarik.org/i_team.html">Aritra Dutta</a>, <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          and <a href="https://jliang993.github.io/">Jingwei Liang</a>.<br>
          <br>
          Abstract: <i>The best pair problem aims to find a pair of
            points that minimize the distance between two disjoint sets.
            In this paper, we formulate the classical robust principal
            component analysis (RPCA) as the best pair; which was not
            considered before. We design an accelerated proximal
            gradient scheme to solve it, for which we show global
            convergence, as well as the local linear rate. Our extensive
            numerical experiments on both real and synthetic data
            suggest that the algorithm outperforms relevant baseline
            algorithms in the literature.</i><i> </i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 26, 2019</h3>
          <h1>Filip @ Berkeley</h1>
          <br>
          As of today, <a href="https://fhanzely.github.io/index.html">Filip

































































































































































            Hanzely</a> is visiting <a
            href="https://www.stat.berkeley.edu/%7Emmahoney/">Michael
            Mahoney</a> at UC Berkeley. He will stay there until June
          18. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 22, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1905.08645">"Revisiting
            randomized gossip algorithms: general framework, convergence
            rates and novel block and accelerated protocols"</a> - joint
          work with <a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas

































































































































































            Loizou.</a><br>
          <br>
          Abstract: <i>In this work we present a new framework for the
            analysis and design of randomized gossip algorithms for
            solving the average consensus problem. We show how classical
            randomized iterative methods for solving linear systems can
            be interpreted as gossip algorithms when applied to special
            systems encoding the underlying network and explain in
            detail their decentralized nature. Our general framework
            recovers a comprehensive array of well-known gossip
            algorithms as special cases, including the pairwise
            randomized gossip algorithm and path averaging gossip, and
            allows for the development of provably faster variants. The
            flexibility of the new approach enables the design of a
            number of new specific gossip methods. For instance, we
            propose and analyze novel block and the first provably
            accelerated randomized gossip protocols, and dual randomized
            gossip algorithms. From a numerical analysis viewpoint, our
            work is the first that explores in depth the decentralized
            nature of randomized iterative methods for linear systems
            and proposes them as methods for solving the average
            consensus problem. We evaluate the performance of the
            proposed gossip protocols by performing extensive
            experimental testing on typical wireless network topologies.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 12, 2019</h3>
          <h1>Nicolas @ ICASSP 2019</h1>
          <br>
          Nicolas Loizou is attending <a
            href="https://2019.ieeeicassp.org/">ICASSP 2019</a> (2019
          IEEE International Conference on Acoustics, Speech and Signal
          Processing) in Brighton, UK, where is presenting the paper <a
            href="https://ieeexplore.ieee.org/document/8683847">"Provably































































































































































            accelerated randomized gossip algorithms"</a>, joint work
          with Michael Rabbat and me.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 9, 2019</h3>
          <h1>Samuel Visiting Michael Jordan @ Berkeley<br>
          </h1>
          <br>
          Starting today, <a href="https://samuelhorvath.github.io/">Samuel





































































































































































            Horváth</a> is visiting <a
            href="https://people.eecs.berkeley.edu/%7Ejordan/">Michael
            I. Jordan</a> at UC Berkeley. He will stay there for a
          month.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>May 2, 2019</h3>
          <h1>PhD Proposal Defense</h1>
          <br>
          <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          defended his PhD proposal and progressed to PhD candidacy.
          Congratulations! <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>April 29, 2019</h3>
          <h1>PhD Proposal Defense</h1>
          <br>
          <a href="https://konstmish.github.io/">Konstantin Mishchenko</a>
          defended his PhD proposal and progressed to PhD candidacy.
          Congratulations! <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>April 23, 2019</h3>
          <h1>Xavier Bresson @ KAUST</h1>
          <br>
          I invited <a href="http://www.ntu.edu.sg/home/xbresson/">Xavier









































































































































































            Bresson</a> to KAUST; he arrived yesterday. Today he is
          giving an <a href="https://ml.kaust.edu.sa/seminar.html">ML
            Hub seminar talk</a> on "Convolutional Neural Networks on
          Graphs". On April 24 &amp; 25 he will be teaching his <a
            href="https://ml.kaust.edu.sa/tutorial.html">Industrial
            Short Course on Deep Learning and Latest AI Algorithms</a>.
          <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>April 22, 2019</h3>
          <h1>Four Papers Accepted to ICML 2019</h1>
          <br>
          The long-awaited decisions just came! We've had four papers
          accepted:<br>
          <br>
          <a href="https://arxiv.org/abs/1809.04146">"Nonconvex variance
            reduced optimization with arbitrary sampling"</a> - joint
          work with <a href="https://samuelhorvath.github.io/">Samuel
            Horváth</a>.<br>
          <br>
          Abstract:<i> We provide the first importance sampling variants
            of variance reduced algorithms for empirical risk
            minimization with non-convex loss functions. In particular,
            we analyze non-convex versions of SVRG, SAGA and SARAH. Our
            methods have the capacity to speed up the training process
            by an order of magnitude compared to the state of the art on
            real datasets. Moreover, we also improve upon current
            mini-batch analysis of these methods by proposing importance
            sampling for minibatches in this setting. Surprisingly, our
            approach can in some regimes lead to superlinear speedup
            with respect to the minibatch size, which is not usually
            present in stochastic optimization. All the above results
            follow from a general analysis of the methods which works
            with arbitrary sampling, i.e., fully general randomized
            strategy for the selection of subsets of examples to be
            sampled in each iteration. Finally, we also perform a novel
            importance sampling analysis of SARAH in the convex setting.</i><br>
          <br>
          <a href="https://arxiv.org/abs/1901.09401">"SGD: General
            analysis and improved rates"</a> - joint work with <a
            href="https://gowerrobert.github.io/">Robert Mansel Gower</a>,
          <a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a>, <a href="https://qianxunk.github.io/">Xun Qian</a>,
          <a href="http://www.ali-sa.org/">Alibek Sailanbayev</a> and
          Egor Shulgin.<br>
          <br>
          Abstract:<i> We propose a general yet simple theorem
            describing the convergence of SGD under the arbitrary
            sampling paradigm. Our theorem describes the convergence of
            an infinite array of variants of SGD, each of which is
            associated with a specific probability law governing the
            data selection rule used to form mini-batches. This is the
            first time such an analysis is performed, and most of our
            variants of SGD were never explicitly considered in the
            literature before. Our analysis relies on the recently
            introduced notion of expected smoothness and does not rely
            on a uniform bound on the variance of the stochastic
            gradients. By specializing our theorem to different
            mini-batching strategies, such as sampling with replacement
            and independent sampling, we derive exact expressions for
            the stepsize as a function of the mini-batch size. With this
            we can also determine the mini-batch size that optimizes the
            total complexity, and show explicitly that as the variance
            of the stochastic gradient evaluated at the minimum grows,
            so does the optimal mini-batch size. For zero variance, the
            optimal mini-batch size is one. Moreover, we prove
            insightful stepsize-switching rules which describe when one
            should switch from a constant to a decreasing stepsize
            regime.</i><br>
          <br>
          <a href="https://arxiv.org/abs/1901.08669">"SAGA with
            arbitrary sampling"</a> - joint work with <a
            href="https://qianxunk.github.io/">Xun Qian</a> and <a
            href="https://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a>.<br>
          <br>
          Abstract:<i> We study the problem of minimizing the average of
            a very large number of smooth functions, which is of key
            importance in training supervised learning models. One of
            the most celebrated methods in this context is the SAGA
            algorithm. Despite years of research on the topic, a
            general-purpose version of SAGA---one that would include
            arbitrary importance sampling and minibatching
            schemes---does not exist. We remedy this situation and
            propose a general and flexible variant of SAGA following the
            arbitrary sampling paradigm. We perform an iteration
            complexity analysis of the method, largely possible due to
            the construction of new stochastic Lyapunov functions. We
            establish linear convergence rates in the smooth and
            strongly convex regime, and under a quadratic functional
            growth condition (i.e., in a regime not assuming strong
            convexity). Our rates match those of the primal-dual method
            Quartz for which an arbitrary sampling analysis is
            available, which makes a significant step towards closing
            the gap in our understanding of complexity of primal and
            dual methods for finite sum problems.</i><br>
          <br>
          <a href="https://arxiv.org/abs/1811.10792">"Stochastic
            gradient push for distributed deep learning"</a> - this is
          the work of my student <a
            href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a>, joint with his Facebook coauthors Mahmoud
          Assran, Nicolas Ballas and Michael Rabbat.<br>
          <br>
          Abstract:<i> Distributed data-parallel algorithms aim to
            accelerate the training of deep neural networks by
            parallelizing the computation of large mini-batch gradient
            updates across multiple nodes. Approaches that synchronize
            nodes using exact distributed averaging (e.g., via
            AllReduce) are sensitive to stragglers and communication
            delays. The PushSum gossip algorithm is robust to these
            issues, but only performs approximate distributed averaging.
            This paper studies Stochastic Gradient Push (SGP), which
            combines PushSum with stochastic gradient updates. We prove
            that SGP converges to a stationary point of smooth,
            non-convex objectives at the same sub-linear rate as SGD,
            that all nodes achieve consensus, and that SGP achieves a
            linear speedup with respect to the number of compute nodes.
            Furthermore, we empirically validate the performance of SGP
            on image classification (ResNet-50, ImageNet) and machine
            translation (Transformer, WMT'16 En-De) workloads. Our code
            will be made publicly available.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>April 14, 2019</h3>
          <h1>Filip @ AISTATS 2019</h1>
          <br>
          Today, <a href="https://fhanzely.github.io/index.html">Filip
            Hanzely</a> is travelling to Naha, Okinawa, Japan, to attend
          <a href="https://www.aistats.org">AISTATS 2019.</a> He will
          present our paper <a href="https://arxiv.org/abs/1809.09354">"Accelerated













































































































































































            coordinate descent with arbitrary sampling and best rates
            for minibatches"</a>. Here is the poster for the paper:<br>
          <br>
          <a href="posters/Poster-ACD.pdf"><img
              src="posters/Poster-ACD-small.png" alt="Poster - ACD"
              align="middle" width="700" border="0" height="1070"></a><br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>April 10, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1904.05115">"Stochastic
            distributed learning with gradient quantization and variance
            reduction"</a> - joint work with <a
            href="https://samuelhorvath.github.io">Samuel Horváth,</a> <a
            href="https://www.dmitry-kovalev.com">Dmitry Kovalev,</a> <a
            href="https://konstmish.github.io">Konstantin Mishchenko,</a>
          and <a href="https://sstich.ch">Sebastian Stich.</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>April 9, 2019</h3>
          <h1>Alexey Kroshnin @ KAUST</h1>
          <br>
          <a href="https://www.hse.ru/en/org/persons/219293044">Alexey
            Kroshnin</a> arrived at KAUST today and will stay here until
          the end of April. Alexey's research interests include
          fundamental theory of optimal transport, geometry of
          Wasserstein spaces, Wasserstein barycenters, dynamical systems
          on Wasserstein spaces, probability theory, measure theory,
          functional analysis and computational complexity theory.<br>
          <br>
          Alexey will work with <a href="https://konstmish.github.io">Konstantin


































































































































































































































            Mishchenko</a> and me on randomized methods for feasibility
          problems. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>April 8, 2019</h3>
          <h1>Nicolas Loizou @ KAUST</h1>
          <br>
          <a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a> arrived at KAUST today and will stay here until
          mid-May. He is finishing writing up his PhD thesis, and plans
          to defend in the Summer. Once he is done with the thesis, we
          will work do some work towards NeurIPS 2019. Nicolas got
          several job offers and chose to join <a
            href="https://mila.quebec">MILA</a> as a postdoc in
          September 2019. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>March 19, 2019</h3>
          <h1>New Paper<br>
          </h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1903.07971">"Convergence
            analysis of inexact randomized iterative methods"</a> -
          joint work with <a
            href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou.</a><br>
          <br>
          Abstract: <i>In this paper we present a convergence rate
            analysis of inexact variants of several randomized iterative
            methods. Among the methods studied are: stochastic gradient
            descent, stochastic Newton, stochastic proximal point and
            stochastic subspace ascent. A common feature of these
            methods is that in their update rule a certain sub-problem
            needs to be solved exactly. We relax this requirement by
            allowing for the sub-problem to be solved inexactly. In
            particular, we propose and analyze inexact randomized
            iterative methods for solving three closely related
            problems: a convex stochastic quadratic optimization
            problem, a best approximation problem and its dual, a
            concave quadratic maximization problem. We provide iteration
            complexity results under several assumptions on the
            inexactness error. Inexact variants of many popular and some
            more exotic methods, including randomized block Kaczmarz,
            randomized Gaussian Kaczmarz and randomized block coordinate
            descent, can be cast as special cases. Numerical experiments
            demonstrate the benefits of allowing inexactness.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>March 18, 2019</h3>
          <h1>Dmitry in Moscow</h1>
          <br>
          As of today, <a
            href="https://vcc.kaust.edu.sa/Pages/Kovalev.aspx">Dmitry
            Kovalev</a> is visiting Moscow - he will stay there for two
          weeks and will give two research talks while there (one in
          Boris Polyak's group and another at MIPT).<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>March 17, 2019</h3>
          <h1>Zheng Qu @ KAUST</h1>
          <br>
          <a href="https://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu (The
            University of Hong Kong)</a> is visiting me at KAUST this
          week. She will stay for a week, and will give the Machine
          Learning Hub seminar on Thursday.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>March 9, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1903.06701">"Scaling distributed
            machine learning with in-network aggregation"</a> - joint
          work with <a
            href="https://ecrc.kaust.edu.sa/Pages/Sapio.aspx">Amedeo
            Sapio</a>, <a href="https://mcanini.github.io">Marco Canini</a>,
          <a href="https://www.chenyuho.com">Chen-Yu Ho</a>, <a
            href="https://www.microsoft.com/en-us/research/people/jacnels/">Jacob










































































































































































































































            Nelson</a>, <a
            href="https://www.kaust.edu.sa/en/study/faculty/panagiotis-kalnis">Panos










































































































































































































































            Kalnis</a>, <a
            href="https://www.linkedin.com/in/changhoon-chang-kim-b3394317/">Changhoon










































































































































































































































            Kim</a>, <a
            href="https://www.cs.washington.edu/people/faculty/arvind">Arvind










































































































































































































































            Krishnamurthy</a>, <a
            href="https://scholar.google.com/citations?user=okIQd4oAAAAJ&amp;hl=en">Masoud










































































































































































































































            Moshref</a>, and <a
            href="https://www.microsoft.com/en-us/research/people/dports/">Dan









































































































































































































































            R. K. Ports.</a><br>
          <br>
          Abstract: <i>Training complex machine learning models in
            parallel is an increasingly important workload. We
            accelerate distributed parallel training by designing a
            communication primitive that uses a programmable switch
            dataplane to execute a key step of the training process. Our
            approach, SwitchML, reduces the volume of exchanged data by
            aggregating the model updates from multiple workers in the
            network. We co-design the switch processing with the
            end-host protocols and ML frameworks to provide a robust,
            efficient solution that speeds up training by up to 300%,
            and at least by 20% for a number of real-world benchmark
            models.</i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>March 9, 2019</h3>
          <h1>Ľubomír Baňas @ KAUST</h1>
          <br>
          Ľubomír Baňas (Bielefeld) is arriving today at KAUST for a
          research visit; he will stay for a week. He will give an <a
href="https://cemse.kaust.edu.sa/events/Pages/AMCS-seminar--Numerical-approximation-of-the-stochastic-Cahn-Hilliard-equation-and-the-%28stochastic%29-Hele-Shaw-probl-125059.aspx">AMCS











































































































































































































































            seminar talk on Wednesday. </a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>March 4, 2019</h3>
          <h1>Atal Joining KAUST as a PhD Student</h1>
          <br>
          My former intern, Atal Sahu (IIT Kanpur), joined KAUST as an
          MS student in the group of <a
            href="https://mcanini.github.io">Marco Canini</a>. <br>
          <br>
          Atal: Welcome back!<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 23, 2019</h3>
          <h1>Senior PC Member for IJCAI 2019</h1>
          <br>
          I have accepted an invite to serve as a Senior Program
          Committee Member at the 28th International Joint Conference on
          Artificial Intelligence (<a href="https://ijcai19.org">IJCAI
            2019</a>). The conference will take place in Macao, China,
          during August 10-16, 2019. The first IJCAI conference was held
          in 1969.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 20, 2019</h3>
          <h1>I am in Vienna</h1>
          <br>
          I am in Vienna, visiting the <span style="caret-color:
            rgb(51, 51, 51); color: rgb(51, 51, 51); font-family:
            Verdana, Arial, &quot;Trebuchet MS&quot;, sans-serif,
            Georgia, Courier, &quot;Times New Roman&quot;, serif;
            font-size: 14px; font-style: normal; font-variant-caps:
            normal; font-weight: normal; letter-spacing: normal;
            orphans: auto; text-align: start; text-indent: 0px;
            text-transform: none; white-space: normal; widows: auto;
            word-spacing: 0px; -webkit-text-size-adjust: auto;
            -webkit-text-stroke-width: 0px; background-color: rgb(255,
            255, 255); text-decoration: none; display: inline
            !important; float: none;"><a href="https://www.esi.ac.at">Erwin



















































































































































































































































              Schrödinger International Institute for Mathematics and
              Physics (ESI)</a> which is the hosting a program on <a
href="https://www.esi.ac.at/activities/events/2019/modern-maximal-monotone-operator-theory">Modern



















































































































































































































































              Maximal Monotone Operator Theory: From Nonsmooth
              Optimization to Differential Inclusions</a></span>.<br>
          <br>
          On February 22 I am teaching a one-day (5 hrs) doctoral course
          on randomized methods in convex optimization. I offered two
          possible courses to the students, and they picked (almost
          unanimously) <a
            href="https://www.esi.ac.at/activities/events/2019/files/richtarik">this



















































































































































































































































            one</a>.<br>
          <br>
          During February 25-March 1, I am attending the workshop <a
href="https://www.univie.ac.at/projektservice-mathematik/e/?event=nualnoop">Numerical



















































































































































































































































            Algorithms in Nonsmooth Optimization</a>. My talk is on
          February 26; I am speaking about the <a
href="https://papers.nips.cc/paper/7478-sega-variance-reduction-via-gradient-sketching">"SEGA"



















































































































































































































































            paper (NeurIPS 2018)</a> - joint work with <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          and <a href="https://konstmish.github.io">Konstantin
            Mishchenko</a>. My SEGA slides are here (click on the image
          to get the pdf file):<br>
          <br>
          <a href="talks/Talk-SEGA.pdf"><img
              src="imgs/SEGA-cover-slide.png" alt="Click to download the
              slides" width="600" border="2" height="447"></a><br>
          <br>
          <div align="center"><i>&nbsp;</i><br>
          </div>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 18, 2019</h3>
          <h1>Konstantin @ EPFL</h1>
          <br>
          As of today, Konstantin Mishchenko is visiting Martin Jaggi's
          <a href="https://mlo.epfl.ch">Machine Learning and
            Optimization Laboratory at EPFL</a>. He will stay there for
          a month.<i><br>
            <br>
            Update (March 17): Konstantin is back at KAUST now.<br>
          </i> <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 12, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1902.03591">"Stochastic three
            points method for unconstrained smooth minimization"</a> -
          joint work with <a
            href="https://vcc.kaust.edu.sa/Pages/Bergou.aspx">El Houcine
            Bergou</a> and <a
            href="https://eduardgorbunov.github.io/index.html">Eduard
            Gorbunov</a>.<br>
          <br>
          Abstract: <i>In this paper we consider the unconstrained
            minimization problem of a smooth function in R^n in a
            setting where only function evaluations are possible. We
            design a novel randomized direct search method based on
            stochastic three points (STP) and analyze its complexity. At
            each iteration, STP generates a random search direction
            according to a certain fixed probability law. Our
            assumptions on this law are very mild: roughly speaking, all
            laws which do not concentrate all measure on any halfspace
            passing through the origin will work. For instance, we allow
            for the uniform distribution on the sphere and also
            distributions that concentrate all measure on a positive
            spanning set. Given a current iterate x, STP compares the
            objective function at three points: x, x+αs and x−αs, where
            α&gt;0 is a stepsize parameter and s is the random search
            direction. The best of these three points is the next
            iterate. We analyze the method STP under several stepsize
            selection schemes (fixed, decreasing, estimated through
            finite differences, etc). We study non-convex, convex and
            strongly convex cases.&nbsp; We also propose a parallel
            version for STP, with iteration complexity bounds which do
            not depend on the dimension n.<br>
            <br>
          </i>Comment:<i> The paper was finalized in March 2018; but we
            only put it online now.<br>
          </i> <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 11, 2019</h3>
          <h1>Internships Available in my Group</h1>
          <br>
          I always have research internships available in my group @ <a
            href="https://www.kaust.edu.sa/en">KAUST</a> throughout the
          year for outstanding and highly motivated students. If you are
          from Europe, USA, Canada, Australia or New Zealand, you are
          eligible for the <a
href="https://vsrp.kaust.edu.sa/Pages/Topics%20in%20Machine%20Learning%20and%20Optimization.aspx">Visiting

























































































































































































































































            Student Research Program (VSRP).</a> These internships are a
          minimum 3 months and a maximum 6 months in duration. We have a
          different internship program dedicated to applicants from
          elsewhere. Shorter internships are possible with this program.
          Drop me an email if you are interested in working with me,
          explaining why you are interested, attaching your CV and
          complete transcript of grades. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 8, 2019</h3>
          <h1>Group Photo</h1>
          <br>
          This is my research group: <br>
          <br>
          <a href="imgs/Richtarik-group-2019-02-medium.jpeg"><img
              src="imgs/Richtarik-group-2019-02-small.png" alt="My
              research group (February 2019)" width="700" border="0"
              height="353"></a><br>
          <br>
          People on the photo: <br>
          <br>
          <i>Postdocs:</i> Aritra Dutta, El-Houcine Bergou, Xun Qian <br>
          <br>
          <i>PhD students:</i> Filip Hanzely, Konstantin Mishchenko,
          Alibek Sailanbayev, Samuel Horváth <br>
          <br>
          <i>MS/PhD students:</i> Elnur Gasanov, Dmitry Kovalev <br>
          <br>
          <i>interns:</i> Eduard Gorbunov, Dmitry Kamzolov, Igor
          Sokolov, Egor Shulgin, Vladislav Elsukov (all belong to my
          group at MIPT where I am a visiting professor), Ľudovít
          Horváth (from Comenius University)<br>
          <br>
          Comment: Nicolas Loizou (Edinburgh) is not on the photo; we
          will photoshop him in once he comes for a visit in April... <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 4, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1902.01272">"A stochastic
            derivative-free optimization method with importance
            sampling"</a> - joint work with <a
            href="http://www.adelbibi.com">Adel Bibi</a>, <a
            href="https://vcc.kaust.edu.sa/Pages/Bergou.aspx">El Houcine
            Bergou</a>, <a href="http://ozansener.net">Ozan Sener</a>
          and <a
            href="https://www.kaust.edu.sa/en/study/faculty/bernard-ghanem">Bernard























































































































































































































            Ghanem</a>.<br>
          <br>
          Abstract: <i>We consider the problem of unconstrained
            minimization of a smooth objective function in R^n in a
            setting where only function evaluations are possible. While
            importance sampling is one of the most popular techniques
            used by machine learning practitioners to accelerate the
            convergence of their models when applicable, there is not
            much existing theory for this acceleration in the
            derivative-free setting. In this paper, we propose an
            importance sampling version of the stochastic three points
            (STP) method proposed by Bergou et al. and derive new
            improved complexity results on non-convex, convex and
            λ-strongly convex functions. We conduct extensive
            experiments on various synthetic and real LIBSVM datasets
            confirming our theoretical results. We further test our
            method on a collection of continuous control tasks on
            several MuJoCo environments with varying difficulty. Our
            results suggest that STP is practical for high dimensional
            continuous control problems. Moreover, the proposed
            importance sampling version results in a significant sample
            complexity improvement.</i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 27, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1901.09437">"99% of parallel
            optimization is inevitably a waste of time"</a> - joint work
          with <a href="https://konstmish.github.io">Konstantin
            Mishchenko</a> and <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely</a>.<br>
          <br>
          Abstract: <i>It is well known that many optimization methods,
            including SGD, SAGA, and Accelerated SGD for
            over-parameterized models, do not scale linearly in the
            parallel setting. In this paper, we present a new version of
            block coordinate descent that solves this issue for a number
            of methods. The core idea is to make the sampling of
            coordinate blocks on each parallel unit independent of the
            others. Surprisingly, we prove that the optimal number of
            blocks to be updated by each of $n$ units in every iteration
            is equal to $m/n$, where $m$ is the total number of blocks.
            As an illustration, this means that when $n=100$ parallel
            units are used, 99% of work is a waste of time. We
            demonstrate that with $m/n$ blocks used by each unit the
            iteration complexity often remains the same. Among other
            applications which we mention, this fact can be exploited in
            the setting of distributed optimization to break the
            communication bottleneck. Our claims are justified by
            numerical experiments which demonstrate almost a perfect
            match with our theory on a number of datasets.</i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 26, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1901.09269">"Distributed
            learning with compressed gradient differences"</a> - joint
          work with <a href="https://konstmish.github.io">Konstantin
            Mishchenko</a>, <a
            href="https://eduardgorbunov.github.io/index.html">Eduard
            Gorbunov</a> and <a href="http://mtakac.com">Martin Takáč</a>.<br>
          <br>
          Abstract: <i>Training very large machine learning models
            requires a distributed computing approach, with
            communication of the model updates often being the
            bottleneck. For this reason, several methods based on the
            compression (e.g., sparsification and/or quantization) of
            the updates were recently proposed, including QSGD (Alistarh
            et al., 2017), TernGrad (Wen et al., 2017), SignSGD
            (Bernstein et al., 2018), and DQGD (Khirirat et al., 2018).
            However, none of these methods are able to learn the
            gradients, which means that they necessarily suffer from
            several issues, such as the inability to converge to the
            true optimum in the batch mode, inability to work with a
            nonsmooth regularizer, and slow convergence rates. In this
            work we propose a new distributed learning
            method---DIANA---which resolves these issues via compression
            of gradient differences. We perform a theoretical analysis
            in the strongly convex and nonconvex settings and show that
            our rates are vastly superior to existing rates. Our
            analysis of block quantization and differences between l2
            and l∞ quantization closes the gaps in theory and practice.
            Finally, by applying our analysis technique to TernGrad, we
            establish the first convergence rate for this method.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 26, 2019</h3>
          <h1>Filip and Aritra @ AAAI 2019 in Hawaii</h1>
          <br>
          <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          and <a href="https://www.aritradutta.com">Aritra Dutta</a>
          are on their way to <a
            href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a>,
          to be held during Jan 27-Feb 1, 2019 in Honolulu, Hawaii.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 25, 2019</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1901.09401">"SGD: general
            analysis and improved rates"</a> - joint work with <a
            href="https://perso.telecom-paristech.fr/rgower/">Robert
            Mansel Gower</a>, <a
            href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a>, <a href="https://qianxunk.github.io">Xun Qian</a>,
          <a href="http://www.ali-sa.org">Alibek Sailanbayev</a> and <a
            href="https://www.linkedin.com/in/egor-shulgin-a34373127">Egor























































































































































































































            Shulgin</a>.<br>
          <br>
          Abstract: <i>We propose a general yet simple theorem
            describing the convergence of SGD under the arbitrary
            sampling paradigm. Our theorem describes the convergence of
            an infinite array of variants of SGD, each of which is
            associated with a specific probability law governing the
            data selection rule used to form minibatches. This is the
            first time such an analysis is performed, and most of our
            variants of SGD were never explicitly considered in the
            literature before. Our analysis relies on the recently
            introduced notion of expected smoothness and does not rely
            on a uniform bound on the variance of the stochastic
            gradients. By specializing our theorem to different
            mini-batching strategies, such as sampling with replacement
            and independent sampling, we derive exact expressions for
            the stepsize as a function of the mini-batch size. With this
            we can also determine the mini-batch size that optimizes the
            total complexity, and show explicitly that as the variance
            of the stochastic gradient evaluated at the minimum grows,
            so does the optimal mini-batch size. For zero variance, the
            optimal mini-batch size is one. Moreover, we prove
            insightful stepsize-switching rules which describe when one
            should switch from a constant to a decreasing stepsize
            regime.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 24, 2019</h3>
          <h1>Two New Papers</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1901.08689">"Don’t jump through
            hoops and remove those loops: SVRG and Katyusha are better
            without the outer loop"</a> - joint work with Dmitry Kovalev
          and <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.<br>
          <br>
          Abstract: <i>The stochastic variance-reduced gradient method
            (SVRG) and its accelerated variant (Katyusha) have attracted
            enormous attention in the machine learning community in the
            last few years due to their superior theoretical properties
            and empirical behaviour on training supervised machine
            learning models via the empirical risk minimization
            paradigm. A key structural element in both of these methods
            is the inclusion of an outer loop at the beginning of which
            a full pass over the training data is made in order to
            compute the exact gradient, which is then used to construct
            a variance-reduced estimator of the gradient. In this work
            we design loopless variants of both of these methods. In
            particular, we remove the outer loop and replace its
            function by a coin flip performed in each iteration designed
            to trigger, with a small probability, the computation of the
            gradient. We prove that the new methods enjoy the same
            superior theoretical convergence properties as the original
            methods. However, we demonstrate through numerical
            experiments that our methods have substantially superior
            practical behavior.<br>
            <br>
          </i> <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1901.08669">"SAGA with arbitrary
            sampling"</a> - joint work with <a
            href="https://qianxunk.github.io">Xun Qian</a> and <a
            href="https://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a>.<br>
          <br>
          Abstract: <i>We study the problem of minimizing the average
            of a very large number of smooth functions, which is of key
            importance in training supervised learn- ing models. One of
            the most celebrated methods in this context is the SAGA
            algorithm of Defazio et al. (2014). Despite years of
            research on the topic, a general-purpose version of SAGA—one
            that would include arbitrary importance sampling and
            minibatching schemes—does not exist. We remedy this
            situation and propose a general and flexible variant of SAGA
            following the arbitrary sampling paradigm. We perform an
            iteration complexity analysis of the method, largely
            possible due to the construction of new stochastic Lyapunov
            functions. We establish linear convergence rates in the
            smooth and strongly convex regime, and under a quadratic
            functional growth condition (i.e., in a regime not assuming
            strong convexity). Our rates match those of the primal-dual
            method Quartz (Qu et al., 2015) for which an arbitrary
            sampling analysis is available, which makes a significant
            step towards closing the gap in our understanding of
            complexity of primal and dual methods for finite sum
            problems.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 15, 2019</h3>
          <h1>El Houcine Moving on to a New Position<br>
          </h1>
          <br>
          <a href="https://vcc.kaust.edu.sa/Pages/Bergou.aspx">El
            Houcine Bergou's</a> 1 year postdoc contract in my group
          ended; he now a postdoc in <a
            href="https://www.kaust.edu.sa/en/study/faculty/panagiotis-kalnis">Panos
















































































































































































































































            Kalnis</a>' group here at KAUST. I am looking forward to
          further collaboration with El Houcine and Panos.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 14, 2019</h3>
          <h1>ICML 2019 Deadline Approaching</h1>
          <br>
          <a href="https://icml.cc/Conferences/2019">ICML</a> deadline
          is upon us (on Jan 23)... Everyone in my group is working hard
          towards the deadline. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 10, 2019</h3>
          <h1>AI Committee Lead</h1>
          <br>
          I've been asked to lead an Aritificial Intelligence Committee
          at KAUST whose role is to prepare a strategic plan for growing
          AI research and activities at KAUST over the next 5 years.
          This will be a substantial investment, and will involve a
          large number of new faculty, research scientist, postdoc and
          PhD and MS/PhD positions; investment into computing
          infrastructure and more. (The committee started its work in
          2018; I am positing the news with some delay...)<br>
          <br>
          Independently to this, Bernard Ghanem, Marco Canini, Panos
          Kalnis and me have established the <a
            href="https://ml.kaust.edu.sa">Machine Learning Hub at KAUST</a>,
          with the aim to advance ML research and training activities
          for the benefit of the entire KAUST community. The website is
          only visible from within the KAUST network at the moment. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 6, 2019</h3>
          <h1>Back @ KAUST: People Counting<br>
          </h1>
          <br>
          I am back at KAUST. <a
            href="http://maiage.jouy.inra.fr/?q=fr/bergou">El Houcine</a>,
          <a href="https://konstmish.github.io">Konstantin</a> and <a
            href="https://qianxunk.github.io">Xun</a> are here. <a
            href="https://www.aritradutta.com">Aritra</a> is on his way
          to <a href="http://wacv19.wacv.net">WACV 2019, Hawaii</a>. <a
            href="https://samuelhorvath.github.io">Samuel</a> and <a
            href="https://fhanzely.github.io/index.html">Filip</a> will
          come back tomorrow. <a
            href="https://vcc.kaust.edu.sa/Pages/Sailanbayev.aspx">Alibek</a>
          and <a href="https://vcc.kaust.edu.sa/Pages/Gasanov.aspx">Elnur</a>
          are arriving soon, too.<br>
          <br>
          I will have several interns/research visitors from <a
href="https://mipt.ru/science/visiting_prof/piter-rikhtarik-peter-richtarik.php">my













































































































































































































































































            group at MIPT</a> visiting me at KAUST during
          January-February: <br>
          <br>
          - <a href="https://www.researchgate.net/profile/Egor_Shulgin">Egor














































































































































































































            Shulgin</a> (Jan 6 - Feb 21) <br>
          - Dmitry Kamzolov (Jan 10 - Feb 18)<br>
          - Vladislav Elsukov (Jan 11 - Feb 15) <br>
          - <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>
          (Jan 13 - Feb 24) <br>
          - Igor Sokolov (Jan 18 - Feb 25)<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 3, 2019</h3>
          <h1>Visiting Rado Harman</h1>
          <br>
          I am visiting <a
            href="http://www.iam.fmph.uniba.sk/ospm/Harman/">Radoslav
            Harman</a> @ Comenius University, Slovakia.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>December 22, 2018</h3>
          <h1>Vacation</h1>
          <br>
          I am on vacation until the end of the year.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>December 22, 2018</h3>
          <h1>Paper Accepted to AISTATS 2019<br>
          </h1>
          <br>
          The paper <a href="https://arxiv.org/abs/1809.09354">"Accelerated
coordinate

descent

























































































































































































            with arbitrary sampling and best rates for minibatches"</a>,
          coauthored with <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely</a>,
          was accepted to <a href="https://www.aistats.org">the 22nd
            International Conference on Artificial Intelligence and
            Statistics (AISTATS 2019)</a>. The conference will take
          place in Naha, Okinawa, Japan, during April 16-18, 2019. The
          acceptance email said: <i>"There were 1,111 submissions for
            AISTATS this year, of which the program committee accepted
            360 for presentation at the conference; among these, 28
            papers were accepted for oral presentation, and 332 for
            poster presentation."</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>December 22, 2018</h3>
          <h1>I will Deliver Summer School Lectures @ ICCOPT 2019</h1>
          <br>
          I have accepted an invite to deliver half-a-day worth of
          summer school lectures on optimization in machine learning at
          the <a href="https://iccopt2019.berlin">International
            Conference on Continuous Optimization (ICCOPT 2019)</a>. The
          Summer School and the main conference take place in Berlin in
          August 2019. The Summer School precedes the main event, and
          spans two days: August 3-4. The main conference runs from
          August 5 until August 8.<br>
          <br>
          ICCOPT is the flagship conference series of the <a
            href="http://www.mathopt.org">Mathematical Optimization
            Society (MOS)</a> on continuous optimization, covering a
          wide range of topics in the field. The individual conferences
          are typically held once every three years. The last three
          editions of the conference took place in <a
            href="http://www.iccopt2016.tokyo">Tokyo, Japan (2016)</a>,
          <a href="https://eventos.fct.unl.pt/iccopt2013/">Lisbon,
            Portugal (2013)</a>, and <a
            href="http://iccopt2010.cmm.uchile.cl">Santiago, Chile
            (2010)</a>. I attended all three.<br>
          <br>
          There are two more key conferences in optimization that take
          place once in three years; each runs in a differenty year, so
          that one takes place every year. They are: <a
            href="https://ismp2018.sciencesconf.org">ISMP (International
            Symposium on Mathematical Programming)</a> and <a
            href="https://www.siam.org/Conferences/CM/Main/op20">OP
            (SIAM Conference on Optimization)</a>. The last ISMP took
          place in Bordeaux in Summer 2018. The <a
            href="https://www.siam.org/Conferences/CM/Main/op20">next OP
            conference will be in Hong Kong during May 26-29, 2020.</a>
          I am a member of the organizing committee for OP2020 which is
          collectively responsible for the selection of invited plenary
          and tutorial speakers, summer school lecturers, and the
          organization of mini-symposia. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>December 14, 2018</h3>
          <h1>Alibek and Samuel Graduated with MS Degrees<br>
          </h1>
          <br>
          <a href="https://vcc.kaust.edu.sa/Pages/Sailanbayev.aspx">Alibek</a>
          and <a href="https://samuelhorvath.github.io">Samuel</a>
          received their MS degrees today. Congratulations! Both will
          continue as PhD students in my group as of January 2019. <br>
          <br>
          Earlier today, I had the great pleasure and honor to meet with
          <a href="https://en.wikipedia.org/wiki/Kai-Fu_Lee">Kai-Fu Lee</a>
          (CEO of <a
href="http://www.sinovationventures.com/index.php/home/aboutus/teams.html">Sinovation

























































































































































































            Ventures</a>; former president of Google China; founder
          &amp; former managing director of Microsoft Research Asia) for
          a 2hr discussion about AI. I recommend that you watch some of
          his videos <br>
          <br>
          <a
            href="https://www.ted.com/talks/kai_fu_lee_how_ai_can_save_our_humanity">TED

























































Talk
2018:

























































































































































































            How AI Can Save Humanity</a><br>
          <a href="https://www.youtube.com/watch?v=oNAFI3Lh97Y">'AI
            Superpowers': A Conversation With Kai-Fu Lee</a><br>
          <a href="https://www.youtube.com/watch?v=0u2qLxTK4mw">The
            Future of AI with Kai-Fu Lee: Udacity Talks</a><br>
          <a href="https://www.youtube.com/watch?v=9VlhMfo7mjY">The Race
            for AI: Book Talk with Dr. Kai-Fu Lee</a><br>
          <br>
          and read his most recent book:<br>
          <br>
          <a
href="https://www.amazon.com/AI-Superpowers-China-Silicon-Valley/dp/132854639X">AI

























































Superpowers:




China,

























































































































































































            Silicon Valley and the New World Order</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>December 11, 2018</h3>
          <h1>Konstantin and Filip Back From Their Internships</h1>
          <br>
          <a href="https://konstmish.github.io">Konstantin</a> and <a
            href="https://fhanzely.github.io/index.html">Filip</a> are
          back (from Amazon internship / Microsoft Research visit,
          respectively). They stopped by NeurIPS on their way back. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>December 10, 2018</h3>
          <h1>Robert Gower @ KAUST</h1>
          <br>
          The final exam for CS 390FF course is today. <a
            href="https://perso.telecom-paristech.fr/rgower/">Robert
            Gower</a> arrived at KAUST for a research visit; he will
          stay until December 20. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>December 8, 2018</h3>
          <h1>Back @ KAUST</h1>
          <br>
          I am back at KAUST now. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>December 2, 2018</h3>
          <h1>Attending NeurIPS 2018</h1>
          <br>
          I have arrived in Montréal to attend the <a
            href="https://nips.cc">NeurIPS</a> (formerly known as NIPS)
          conference. I was welcome with rain, which this is a good
          thing as far as I am concerned!. Tutorials are starting
          tomorrow; after that we have three days of the main conference
          and then two days of workshops. My group is presening three
          papers accepted to the main conference (paper <a
            href="https://nips.cc/Conferences/2018/Schedule?showEvent=11220">SEGA</a>,
          <a
            href="https://nips.cc/Conferences/2018/Schedule?showEvent=11176">ASBFGS</a>
          and <a
            href="https://nips.cc/Conferences/2018/Schedule?showEvent=11338">SSCD</a>)
          and <a href="https://arxiv.org/abs/1706.07636">one paper
            accepted to a workshop</a>. <br>
          <br>
          I am using the conference Whova app; feel free to get in
          touch! I am leaving on Thursday evening, so catch me before
          then... I've posted a few job openings we have at KAUST
          through the app: internships in my lab (apply by sending me
          your cv and transcript of university grades), postdoc and
          research scientist positions (apply by sending a cv +
          motivation letter), and <a
            href="https://apply.interfolio.com/58109">machine learning
            faculty positions</a> at all ranks (women and junior
          applicants are particularly encouraged to apply). <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>November 30, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1811.12403">"New convergence
            aspects of stochastic gradient algorithms"</a> - joint work
          with <a
href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-lamnguyen.mltd">Lam

























































































































































































            M. Nguyen</a>, <a
            href="https://scl.uconn.edu/people/ha/info.php">Phuong Ha
            Nguyen</a>, <a href="https://coral.ise.lehigh.edu/katyas/">Katya

























































































































































































            Scheinberg</a>, <a
            href="https://www.lehigh.edu/engineering/faculty/profiles/takac.html">Martin

























































































































































































            Takáč</a>, and <a
            href="http://www.ee.uconn.edu/marten-van-dijk/">Marten van
            Dijk</a>.<br>
          <br>
          Abstract: <i><span style="caret-color: rgb(0, 0, 0); color:
              rgb(0, 0, 0); font-family: &quot;Lucida Grande&quot;,
              helvetica, arial, verdana, sans-serif; font-size:
              14.399999618530273px; font-style: normal;
              font-variant-caps: normal; font-weight: normal;
              letter-spacing: normal; orphans: auto; text-align: start;
              text-indent: 0px; text-transform: none; white-space:
              normal; widows: auto; word-spacing: 0px;
              -webkit-text-size-adjust: auto; -webkit-text-stroke-width:
              0px; background-color: rgb(255, 255, 255);
              text-decoration: none; display: inline !important; float:
              none;">The classical convergence analysis of SGD is
              carried out under the assumption that the norm of the
              stochastic gradient is uniformly bounded. While this might
              hold for some loss functions, it is violated for cases
              where the objective function is strongly convex. In Bottou
              et al. (2016), a new analysis of convergence of SGD is
              performed under the assumption that stochastic gradients
              are bounded with respect to the true gradient norm. We
              show that for stochastic problems arising in machine
              learning such bound always holds; and we also propose an
              alternative convergence analysis of SGD with diminishing
              learning rate regime, which results in more relaxed
              conditions than those in Bottou et al. (2016). We then
              move on the asynchronous parallel setting, and prove
              convergence of Hogwild! algorithm in the same regime in
              the case of diminished learning rate. It is well-known
              that SGD converges if a sequence of learning rates<span
                class="Apple-converted-space">&nbsp;</span></span></i><i><span
              class="MathJax" id="MathJax-Element-1-Frame" tabindex="0"
              style="display: inline; font-style: normal; font-weight:
              normal; line-height: normal; font-size:
              14.399999618530273px; text-indent: 0px; text-align: left;
              text-transform: none; letter-spacing: normal;
              word-spacing: 0px; word-wrap: normal; white-space: nowrap;
              float: none; direction: ltr; max-width: none; max-height:
              none; min-width: 0px; min-height: 0px; border: 0px;
              padding: 0px; margin: 0px; caret-color: rgb(0, 0, 0);
              color: rgb(0, 0, 0); font-family: &quot;Lucida
              Grande&quot;, helvetica, arial, verdana, sans-serif;
              font-variant-caps: normal; orphans: auto; widows: auto;
              -webkit-text-size-adjust: auto; -webkit-text-stroke-width:
              0px; text-decoration: none;"><nobr style="transition:
                none; border: 0px; padding: 0px; margin: 0px; max-width:
                5000em; max-height: 5000em; min-width: 0px; min-height:
                0px; vertical-align: 0px; line-height: normal;
                text-decoration: none; white-space: nowrap !important;"><span
                  class="math" id="MathJax-Span-1" style="transition:
                  none; display: inline-block; position: static; border:
                  0px; padding: 0px; margin: 0px; vertical-align: 0px;
                  line-height: normal; text-decoration: none; width:
                  2.162em;"><span style="transition: none; display:
                    inline-block; position: relative; border: 0px;
                    padding: 0px; margin: 0px; vertical-align: 0px;
                    line-height: normal; text-decoration: none; width:
                    1.812em; height: 0px; font-size:
                    17.13599967956543px;"><span style="transition: none;
                      display: inline; position: absolute; border: 0px;
                      padding: 0px; margin: 0px; vertical-align: 0px;
                      line-height: normal; text-decoration: none; clip:
                      rect(1.637em, 1001.695em, 2.921em, -999.997em);
                      top: -2.506em; left: 0em;"><span class="mrow"
                        id="MathJax-Span-2" style="transition: none;
                        display: inline; position: static; border: 0px;
                        padding: 0px; margin: 0px; vertical-align: 0px;
                        line-height: normal; text-decoration: none;"><span
                          class="mo" id="MathJax-Span-3"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px;
                          margin: 0px; vertical-align: 0px; line-height:
                          normal; text-decoration: none; font-family:
                          STIXGeneral-Regular;">{</span><span
                          class="msubsup" id="MathJax-Span-4"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px;
                          margin: 0px; vertical-align: 0px; line-height:
                          normal; text-decoration: none;"><span
                            style="transition: none; display:
                            inline-block; position: relative; border:
                            0px; padding: 0px; margin: 0px;
                            vertical-align: 0px; line-height: normal;
                            text-decoration: none; width: 0.82em;
                            height: 0px;"><span style="transition: none;
                              display: inline; position: absolute;
                              border: 0px; padding: 0px; margin: 0px;
                              vertical-align: 0px; line-height: normal;
                              text-decoration: none; clip: rect(3.388em,
                              1000.47em, 4.438em, -999.997em); top:
                              -4.024em; left: 0em;"><span
                                class="texatom" id="MathJax-Span-5"
                                style="transition: none; display:
                                inline; position: static; border: 0px;
                                padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;"><span
                                  class="mrow" id="MathJax-Span-6"
                                  style="transition: none; display:
                                  inline; position: static; border: 0px;
                                  padding: 0px; margin: 0px;
                                  vertical-align: 0px; line-height:
                                  normal; text-decoration: none;"><span
                                    class="mo" id="MathJax-Span-7"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-family: STIXGeneral-Regular;">η</span></span></span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span><span
                              style="transition: none; display: inline;
                              position: absolute; border: 0px; padding:
                              0px; margin: 0px; vertical-align: 0px;
                              line-height: normal; text-decoration:
                              none; top: -3.79em; left: 0.528em;"><span
                                class="mi" id="MathJax-Span-8"
                                style="transition: none; display:
                                inline; position: static; border: 0px;
                                padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;
                                font-size: 12.115151405334473px;
                                font-family: STIXGeneral-Italic;">t<span
                                  style="transition: none; display:
                                  inline-block; position: static;
                                  border: 0px; padding: 0px; margin:
                                  0px; vertical-align: 0px; line-height:
                                  normal; text-decoration: none;
                                  overflow: hidden; height: 1px; width:
                                  0.003em;"></span></span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span></span></span><span
                          class="mo" id="MathJax-Span-9"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px;
                          margin: 0px; vertical-align: 0px; line-height:
                          normal; text-decoration: none; font-family:
                          STIXGeneral-Regular;">}</span></span><span
                        style="transition: none; display: inline-block;
                        position: static; border: 0px; padding: 0px;
                        margin: 0px; vertical-align: 0px; line-height:
                        normal; text-decoration: none; width: 0px;
                        height: 2.512em;"></span></span></span><span
                    style="transition: none; display: inline-block;
                    position: static; border-width: 0px;
                    border-left-style: solid; padding: 0px; margin: 0px;
                    vertical-align: -0.344em; line-height: normal;
                    text-decoration: none; overflow: hidden; width: 0px;
                    height: 1.253em;"></span></span></nobr></span></i><i><span
              style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);
              font-family: &quot;Lucida Grande&quot;, helvetica, arial,
              verdana, sans-serif; font-size: 14.399999618530273px;
              font-style: normal; font-variant-caps: normal;
              font-weight: normal; letter-spacing: normal; orphans:
              auto; text-align: start; text-indent: 0px; text-transform:
              none; white-space: normal; widows: auto; word-spacing:
              0px; -webkit-text-size-adjust: auto;
              -webkit-text-stroke-width: 0px; background-color: rgb(255,
              255, 255); text-decoration: none; display: inline
              !important; float: none;"><span
                class="Apple-converted-space">&nbsp;</span>satisfies<span
                class="Apple-converted-space">&nbsp;</span></span></i><i><span
              class="MathJax" id="MathJax-Element-2-Frame" tabindex="0"
              style="display: inline; font-style: normal; font-weight:
              normal; line-height: normal; font-size:
              14.399999618530273px; text-indent: 0px; text-align: left;
              text-transform: none; letter-spacing: normal;
              word-spacing: 0px; word-wrap: normal; white-space: nowrap;
              float: none; direction: ltr; max-width: none; max-height:
              none; min-width: 0px; min-height: 0px; border: 0px;
              padding: 0px; margin: 0px; caret-color: rgb(0, 0, 0);
              color: rgb(0, 0, 0); font-family: &quot;Lucida
              Grande&quot;, helvetica, arial, verdana, sans-serif;
              font-variant-caps: normal; orphans: auto; widows: auto;
              -webkit-text-size-adjust: auto; -webkit-text-stroke-width:
              0px; text-decoration: none;"><nobr style="transition:
                none; border: 0px; padding: 0px; margin: 0px; max-width:
                5000em; max-height: 5000em; min-width: 0px; min-height:
                0px; vertical-align: 0px; line-height: normal;
                text-decoration: none; white-space: nowrap !important;"><span
                  class="math" id="MathJax-Span-10" style="transition:
                  none; display: inline-block; position: static; border:
                  0px; padding: 0px; margin: 0px; vertical-align: 0px;
                  line-height: normal; text-decoration: none; width:
                  6.539em;"><span style="transition: none; display:
                    inline-block; position: relative; border: 0px;
                    padding: 0px; margin: 0px; vertical-align: 0px;
                    line-height: normal; text-decoration: none; width:
                    5.488em; height: 0px; font-size:
                    17.13599967956543px;"><span style="transition: none;
                      display: inline; position: absolute; border: 0px;
                      padding: 0px; margin: 0px; vertical-align: 0px;
                      line-height: normal; text-decoration: none; clip:
                      rect(1.52em, 1005.43em, 2.979em, -999.997em); top:
                      -2.506em; left: 0em;"><span class="mrow"
                        id="MathJax-Span-11" style="transition: none;
                        display: inline; position: static; border: 0px;
                        padding: 0px; margin: 0px; vertical-align: 0px;
                        line-height: normal; text-decoration: none;"><span
                          class="munderover" id="MathJax-Span-12"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px;
                          margin: 0px; vertical-align: 0px; line-height:
                          normal; text-decoration: none;"><span
                            style="transition: none; display:
                            inline-block; position: relative; border:
                            0px; padding: 0px; margin: 0px;
                            vertical-align: 0px; line-height: normal;
                            text-decoration: none; width: 2.045em;
                            height: 0px;"><span style="transition: none;
                              display: inline; position: absolute;
                              border: 0px; padding: 0px; margin: 0px;
                              vertical-align: 0px; line-height: normal;
                              text-decoration: none; clip: rect(3.096em,
                              1000.878em, 4.438em, -999.997em); top:
                              -4.024em; left: 0em;"><span class="mo"
                                id="MathJax-Span-13" style="transition:
                                none; display: inline; position: static;
                                border: 0px; padding: 0px; margin: 0px;
                                vertical-align: 0.003em; line-height:
                                normal; text-decoration: none;
                                font-family: STIXGeneral-Regular;">∑</span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span><span
                              style="transition: none; display: inline;
                              position: absolute; border: 0px; padding:
                              0px; margin: 0px; vertical-align: 0px;
                              line-height: normal; text-decoration:
                              none; clip: rect(3.563em, 1000.762em,
                              4.205em, -999.997em); top: -4.491em; left:
                              0.937em;"><span class="mi"
                                id="MathJax-Span-14" style="transition:
                                none; display: inline; position: static;
                                border: 0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;
                                font-size: 12.115151405334473px;
                                font-family: STIXGeneral-Regular;">∞</span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span><span
                              style="transition: none; display: inline;
                              position: absolute; border: 0px; padding:
                              0px; margin: 0px; vertical-align: 0px;
                              line-height: normal; text-decoration:
                              none; clip: rect(3.388em, 1001.112em,
                              4.205em, -999.997em); top: -3.732em; left:
                              0.937em;"><span class="texatom"
                                id="MathJax-Span-15" style="transition:
                                none; display: inline; position: static;
                                border: 0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;"><span
                                  class="mrow" id="MathJax-Span-16"
                                  style="transition: none; display:
                                  inline; position: static; border: 0px;
                                  padding: 0px; margin: 0px;
                                  vertical-align: 0px; line-height:
                                  normal; text-decoration: none;"><span
                                    class="mi" id="MathJax-Span-17"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-size: 12.115151405334473px;
                                    font-family: STIXGeneral-Italic;">t<span
                                      style="transition: none; display:
                                      inline-block; position: static;
                                      border: 0px; padding: 0px; margin:
                                      0px; vertical-align: 0px;
                                      line-height: normal;
                                      text-decoration: none; overflow:
                                      hidden; height: 1px; width:
                                      0.003em;"></span></span><span
                                    class="mo" id="MathJax-Span-18"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-size: 12.115151405334473px;
                                    font-family: STIXGeneral-Regular;">=</span><span
                                    class="mn" id="MathJax-Span-19"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-size: 12.115151405334473px;
                                    font-family: STIXGeneral-Regular;">0</span></span></span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span></span></span><span
                          class="msubsup" id="MathJax-Span-20"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px
                          0px 0px 0.178em; margin: 0px; vertical-align:
                          0px; line-height: normal; text-decoration:
                          none;"><span style="transition: none; display:
                            inline-block; position: relative; border:
                            0px; padding: 0px; margin: 0px;
                            vertical-align: 0px; line-height: normal;
                            text-decoration: none; width: 0.82em;
                            height: 0px;"><span style="transition: none;
                              display: inline; position: absolute;
                              border: 0px; padding: 0px; margin: 0px;
                              vertical-align: 0px; line-height: normal;
                              text-decoration: none; clip: rect(3.388em,
                              1000.47em, 4.438em, -999.997em); top:
                              -4.024em; left: 0em;"><span
                                class="texatom" id="MathJax-Span-21"
                                style="transition: none; display:
                                inline; position: static; border: 0px;
                                padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;"><span
                                  class="mrow" id="MathJax-Span-22"
                                  style="transition: none; display:
                                  inline; position: static; border: 0px;
                                  padding: 0px; margin: 0px;
                                  vertical-align: 0px; line-height:
                                  normal; text-decoration: none;"><span
                                    class="mo" id="MathJax-Span-23"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-family: STIXGeneral-Regular;">η</span></span></span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span><span
                              style="transition: none; display: inline;
                              position: absolute; border: 0px; padding:
                              0px; margin: 0px; vertical-align: 0px;
                              line-height: normal; text-decoration:
                              none; top: -3.79em; left: 0.528em;"><span
                                class="mi" id="MathJax-Span-24"
                                style="transition: none; display:
                                inline; position: static; border: 0px;
                                padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;
                                font-size: 12.115151405334473px;
                                font-family: STIXGeneral-Italic;">t<span
                                  style="transition: none; display:
                                  inline-block; position: static;
                                  border: 0px; padding: 0px; margin:
                                  0px; vertical-align: 0px; line-height:
                                  normal; text-decoration: none;
                                  overflow: hidden; height: 1px; width:
                                  0.003em;"></span></span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span></span></span><span
                          class="mo" id="MathJax-Span-25"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px
                          0px 0px 0.295em; margin: 0px; vertical-align:
                          0px; line-height: normal; text-decoration:
                          none; font-family: STIXGeneral-Regular;">→</span><span
                          class="mi" id="MathJax-Span-26"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px
                          0px 0px 0.295em; margin: 0px; vertical-align:
                          0px; line-height: normal; text-decoration:
                          none; font-family: STIXGeneral-Regular;">∞</span></span><span
                        style="transition: none; display: inline-block;
                        position: static; border: 0px; padding: 0px;
                        margin: 0px; vertical-align: 0px; line-height:
                        normal; text-decoration: none; width: 0px;
                        height: 2.512em;"></span></span></span><span
                    style="transition: none; display: inline-block;
                    position: static; border-width: 0px;
                    border-left-style: solid; padding: 0px; margin: 0px;
                    vertical-align: -0.413em; line-height: normal;
                    text-decoration: none; overflow: hidden; width: 0px;
                    height: 1.462em;"></span></span></nobr></span></i><i><span
              style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);
              font-family: &quot;Lucida Grande&quot;, helvetica, arial,
              verdana, sans-serif; font-size: 14.399999618530273px;
              font-style: normal; font-variant-caps: normal;
              font-weight: normal; letter-spacing: normal; orphans:
              auto; text-align: start; text-indent: 0px; text-transform:
              none; white-space: normal; widows: auto; word-spacing:
              0px; -webkit-text-size-adjust: auto;
              -webkit-text-stroke-width: 0px; background-color: rgb(255,
              255, 255); text-decoration: none; display: inline
              !important; float: none;"><span
                class="Apple-converted-space">&nbsp;</span>and<span
                class="Apple-converted-space">&nbsp;</span></span></i><i><span
              class="MathJax" id="MathJax-Element-3-Frame" tabindex="0"
              style="display: inline; font-style: normal; font-weight:
              normal; line-height: normal; font-size:
              14.399999618530273px; text-indent: 0px; text-align: left;
              text-transform: none; letter-spacing: normal;
              word-spacing: 0px; word-wrap: normal; white-space: nowrap;
              float: none; direction: ltr; max-width: none; max-height:
              none; min-width: 0px; min-height: 0px; border: 0px;
              padding: 0px; margin: 0px; caret-color: rgb(0, 0, 0);
              color: rgb(0, 0, 0); font-family: &quot;Lucida
              Grande&quot;, helvetica, arial, verdana, sans-serif;
              font-variant-caps: normal; orphans: auto; widows: auto;
              -webkit-text-size-adjust: auto; -webkit-text-stroke-width:
              0px; text-decoration: none;"><nobr style="transition:
                none; border: 0px; padding: 0px; margin: 0px; max-width:
                5000em; max-height: 5000em; min-width: 0px; min-height:
                0px; vertical-align: 0px; line-height: normal;
                text-decoration: none; white-space: nowrap !important;"><span
                  class="math" id="MathJax-Span-27" style="transition:
                  none; display: inline-block; position: static; border:
                  0px; padding: 0px; margin: 0px; vertical-align: 0px;
                  line-height: normal; text-decoration: none; width:
                  6.422em;"><span style="transition: none; display:
                    inline-block; position: relative; border: 0px;
                    padding: 0px; margin: 0px; vertical-align: 0px;
                    line-height: normal; text-decoration: none; width:
                    5.372em; height: 0px; font-size:
                    17.13599967956543px;"><span style="transition: none;
                      display: inline; position: absolute; border: 0px;
                      padding: 0px; margin: 0px; vertical-align: 0px;
                      line-height: normal; text-decoration: none; clip:
                      rect(1.52em, 1005.313em, 2.979em, -999.997em);
                      top: -2.506em; left: 0em;"><span class="mrow"
                        id="MathJax-Span-28" style="transition: none;
                        display: inline; position: static; border: 0px;
                        padding: 0px; margin: 0px; vertical-align: 0px;
                        line-height: normal; text-decoration: none;"><span
                          class="munderover" id="MathJax-Span-29"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px;
                          margin: 0px; vertical-align: 0px; line-height:
                          normal; text-decoration: none;"><span
                            style="transition: none; display:
                            inline-block; position: relative; border:
                            0px; padding: 0px; margin: 0px;
                            vertical-align: 0px; line-height: normal;
                            text-decoration: none; width: 2.045em;
                            height: 0px;"><span style="transition: none;
                              display: inline; position: absolute;
                              border: 0px; padding: 0px; margin: 0px;
                              vertical-align: 0px; line-height: normal;
                              text-decoration: none; clip: rect(3.096em,
                              1000.878em, 4.438em, -999.997em); top:
                              -4.024em; left: 0em;"><span class="mo"
                                id="MathJax-Span-30" style="transition:
                                none; display: inline; position: static;
                                border: 0px; padding: 0px; margin: 0px;
                                vertical-align: 0.003em; line-height:
                                normal; text-decoration: none;
                                font-family: STIXGeneral-Regular;">∑</span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span><span
                              style="transition: none; display: inline;
                              position: absolute; border: 0px; padding:
                              0px; margin: 0px; vertical-align: 0px;
                              line-height: normal; text-decoration:
                              none; clip: rect(3.563em, 1000.762em,
                              4.205em, -999.997em); top: -4.491em; left:
                              0.937em;"><span class="mi"
                                id="MathJax-Span-31" style="transition:
                                none; display: inline; position: static;
                                border: 0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;
                                font-size: 12.115151405334473px;
                                font-family: STIXGeneral-Regular;">∞</span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span><span
                              style="transition: none; display: inline;
                              position: absolute; border: 0px; padding:
                              0px; margin: 0px; vertical-align: 0px;
                              line-height: normal; text-decoration:
                              none; clip: rect(3.388em, 1001.112em,
                              4.205em, -999.997em); top: -3.732em; left:
                              0.937em;"><span class="texatom"
                                id="MathJax-Span-32" style="transition:
                                none; display: inline; position: static;
                                border: 0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;"><span
                                  class="mrow" id="MathJax-Span-33"
                                  style="transition: none; display:
                                  inline; position: static; border: 0px;
                                  padding: 0px; margin: 0px;
                                  vertical-align: 0px; line-height:
                                  normal; text-decoration: none;"><span
                                    class="mi" id="MathJax-Span-34"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-size: 12.115151405334473px;
                                    font-family: STIXGeneral-Italic;">t<span
                                      style="transition: none; display:
                                      inline-block; position: static;
                                      border: 0px; padding: 0px; margin:
                                      0px; vertical-align: 0px;
                                      line-height: normal;
                                      text-decoration: none; overflow:
                                      hidden; height: 1px; width:
                                      0.003em;"></span></span><span
                                    class="mo" id="MathJax-Span-35"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-size: 12.115151405334473px;
                                    font-family: STIXGeneral-Regular;">=</span><span
                                    class="mn" id="MathJax-Span-36"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-size: 12.115151405334473px;
                                    font-family: STIXGeneral-Regular;">0</span></span></span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span></span></span><span
                          class="msubsup" id="MathJax-Span-37"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px
                          0px 0px 0.178em; margin: 0px; vertical-align:
                          0px; line-height: normal; text-decoration:
                          none;"><span style="transition: none; display:
                            inline-block; position: relative; border:
                            0px; padding: 0px; margin: 0px;
                            vertical-align: 0px; line-height: normal;
                            text-decoration: none; width: 0.937em;
                            height: 0px;"><span style="transition: none;
                              display: inline; position: absolute;
                              border: 0px; padding: 0px; margin: 0px;
                              vertical-align: 0px; line-height: normal;
                              text-decoration: none; clip: rect(3.388em,
                              1000.47em, 4.438em, -999.997em); top:
                              -4.024em; left: 0em;"><span
                                class="texatom" id="MathJax-Span-38"
                                style="transition: none; display:
                                inline; position: static; border: 0px;
                                padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;"><span
                                  class="mrow" id="MathJax-Span-39"
                                  style="transition: none; display:
                                  inline; position: static; border: 0px;
                                  padding: 0px; margin: 0px;
                                  vertical-align: 0px; line-height:
                                  normal; text-decoration: none;"><span
                                    class="mo" id="MathJax-Span-40"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-family: STIXGeneral-Regular;">η</span></span></span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span><span
                              style="transition: none; display: inline;
                              position: absolute; border: 0px; padding:
                              0px; margin: 0px; vertical-align: 0px;
                              line-height: normal; text-decoration:
                              none; clip: rect(3.388em, 1000.411em,
                              4.205em, -999.997em); top: -4.374em; left:
                              0.528em;"><span class="mn"
                                id="MathJax-Span-41" style="transition:
                                none; display: inline; position: static;
                                border: 0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;
                                font-size: 12.115151405334473px;
                                font-family: STIXGeneral-Regular;">2</span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span><span
                              style="transition: none; display: inline;
                              position: absolute; border: 0px; padding:
                              0px; margin: 0px; vertical-align: 0px;
                              line-height: normal; text-decoration:
                              none; clip: rect(3.446em, 1000.295em,
                              4.205em, -999.997em); top: -3.79em; left:
                              0.528em;"><span class="mi"
                                id="MathJax-Span-42" style="transition:
                                none; display: inline; position: static;
                                border: 0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;
                                font-size: 12.115151405334473px;
                                font-family: STIXGeneral-Italic;">t<span
                                  style="transition: none; display:
                                  inline-block; position: static;
                                  border: 0px; padding: 0px; margin:
                                  0px; vertical-align: 0px; line-height:
                                  normal; text-decoration: none;
                                  overflow: hidden; height: 1px; width:
                                  0.003em;"></span></span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span></span></span><span
                          class="mo" id="MathJax-Span-43"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px
                          0px 0px 0.295em; margin: 0px; vertical-align:
                          0px; line-height: normal; text-decoration:
                          none; font-family: STIXGeneral-Regular;">&lt;</span><span
                          class="mi" id="MathJax-Span-44"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px
                          0px 0px 0.295em; margin: 0px; vertical-align:
                          0px; line-height: normal; text-decoration:
                          none; font-family: STIXGeneral-Regular;">∞</span></span><span
                        style="transition: none; display: inline-block;
                        position: static; border: 0px; padding: 0px;
                        margin: 0px; vertical-align: 0px; line-height:
                        normal; text-decoration: none; width: 0px;
                        height: 2.512em;"></span></span></span><span
                    style="transition: none; display: inline-block;
                    position: static; border-width: 0px;
                    border-left-style: solid; padding: 0px; margin: 0px;
                    vertical-align: -0.413em; line-height: normal;
                    text-decoration: none; overflow: hidden; width: 0px;
                    height: 1.462em;"></span></span></nobr></span></i><i><span
              style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);
              font-family: &quot;Lucida Grande&quot;, helvetica, arial,
              verdana, sans-serif; font-size: 14.399999618530273px;
              font-style: normal; font-variant-caps: normal;
              font-weight: normal; letter-spacing: normal; orphans:
              auto; text-align: start; text-indent: 0px; text-transform:
              none; white-space: normal; widows: auto; word-spacing:
              0px; -webkit-text-size-adjust: auto;
              -webkit-text-stroke-width: 0px; background-color: rgb(255,
              255, 255); text-decoration: none; display: inline
              !important; float: none;">. We show the convergence of SGD
              for strongly convex objective function without using
              bounded gradient assumption when<span
                class="Apple-converted-space">&nbsp;</span></span></i><i><span
              class="MathJax" id="MathJax-Element-4-Frame" tabindex="0"
              style="display: inline; font-style: normal; font-weight:
              normal; line-height: normal; font-size:
              14.399999618530273px; text-indent: 0px; text-align: left;
              text-transform: none; letter-spacing: normal;
              word-spacing: 0px; word-wrap: normal; white-space: nowrap;
              float: none; direction: ltr; max-width: none; max-height:
              none; min-width: 0px; min-height: 0px; border: 0px;
              padding: 0px; margin: 0px; caret-color: rgb(0, 0, 0);
              color: rgb(0, 0, 0); font-family: &quot;Lucida
              Grande&quot;, helvetica, arial, verdana, sans-serif;
              font-variant-caps: normal; orphans: auto; widows: auto;
              -webkit-text-size-adjust: auto; -webkit-text-stroke-width:
              0px; text-decoration: none;"><nobr style="transition:
                none; border: 0px; padding: 0px; margin: 0px; max-width:
                5000em; max-height: 5000em; min-width: 0px; min-height:
                0px; vertical-align: 0px; line-height: normal;
                text-decoration: none; white-space: nowrap !important;"><span
                  class="math" id="MathJax-Span-45" style="transition:
                  none; display: inline-block; position: static; border:
                  0px; padding: 0px; margin: 0px; vertical-align: 0px;
                  line-height: normal; text-decoration: none; width:
                  2.162em;"><span style="transition: none; display:
                    inline-block; position: relative; border: 0px;
                    padding: 0px; margin: 0px; vertical-align: 0px;
                    line-height: normal; text-decoration: none; width:
                    1.812em; height: 0px; font-size:
                    17.13599967956543px;"><span style="transition: none;
                      display: inline; position: absolute; border: 0px;
                      padding: 0px; margin: 0px; vertical-align: 0px;
                      line-height: normal; text-decoration: none; clip:
                      rect(1.637em, 1001.695em, 2.921em, -999.997em);
                      top: -2.506em; left: 0em;"><span class="mrow"
                        id="MathJax-Span-46" style="transition: none;
                        display: inline; position: static; border: 0px;
                        padding: 0px; margin: 0px; vertical-align: 0px;
                        line-height: normal; text-decoration: none;"><span
                          class="mo" id="MathJax-Span-47"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px;
                          margin: 0px; vertical-align: 0px; line-height:
                          normal; text-decoration: none; font-family:
                          STIXGeneral-Regular;">{</span><span
                          class="msubsup" id="MathJax-Span-48"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px;
                          margin: 0px; vertical-align: 0px; line-height:
                          normal; text-decoration: none;"><span
                            style="transition: none; display:
                            inline-block; position: relative; border:
                            0px; padding: 0px; margin: 0px;
                            vertical-align: 0px; line-height: normal;
                            text-decoration: none; width: 0.82em;
                            height: 0px;"><span style="transition: none;
                              display: inline; position: absolute;
                              border: 0px; padding: 0px; margin: 0px;
                              vertical-align: 0px; line-height: normal;
                              text-decoration: none; clip: rect(3.388em,
                              1000.47em, 4.438em, -999.997em); top:
                              -4.024em; left: 0em;"><span
                                class="texatom" id="MathJax-Span-49"
                                style="transition: none; display:
                                inline; position: static; border: 0px;
                                padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;"><span
                                  class="mrow" id="MathJax-Span-50"
                                  style="transition: none; display:
                                  inline; position: static; border: 0px;
                                  padding: 0px; margin: 0px;
                                  vertical-align: 0px; line-height:
                                  normal; text-decoration: none;"><span
                                    class="mo" id="MathJax-Span-51"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-family: STIXGeneral-Regular;">η</span></span></span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span><span
                              style="transition: none; display: inline;
                              position: absolute; border: 0px; padding:
                              0px; margin: 0px; vertical-align: 0px;
                              line-height: normal; text-decoration:
                              none; top: -3.79em; left: 0.528em;"><span
                                class="mi" id="MathJax-Span-52"
                                style="transition: none; display:
                                inline; position: static; border: 0px;
                                padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;
                                font-size: 12.115151405334473px;
                                font-family: STIXGeneral-Italic;">t<span
                                  style="transition: none; display:
                                  inline-block; position: static;
                                  border: 0px; padding: 0px; margin:
                                  0px; vertical-align: 0px; line-height:
                                  normal; text-decoration: none;
                                  overflow: hidden; height: 1px; width:
                                  0.003em;"></span></span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span></span></span><span
                          class="mo" id="MathJax-Span-53"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px;
                          margin: 0px; vertical-align: 0px; line-height:
                          normal; text-decoration: none; font-family:
                          STIXGeneral-Regular;">}</span></span><span
                        style="transition: none; display: inline-block;
                        position: static; border: 0px; padding: 0px;
                        margin: 0px; vertical-align: 0px; line-height:
                        normal; text-decoration: none; width: 0px;
                        height: 2.512em;"></span></span></span><span
                    style="transition: none; display: inline-block;
                    position: static; border-width: 0px;
                    border-left-style: solid; padding: 0px; margin: 0px;
                    vertical-align: -0.344em; line-height: normal;
                    text-decoration: none; overflow: hidden; width: 0px;
                    height: 1.253em;"></span></span></nobr></span></i><i><span
              style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);
              font-family: &quot;Lucida Grande&quot;, helvetica, arial,
              verdana, sans-serif; font-size: 14.399999618530273px;
              font-style: normal; font-variant-caps: normal;
              font-weight: normal; letter-spacing: normal; orphans:
              auto; text-align: start; text-indent: 0px; text-transform:
              none; white-space: normal; widows: auto; word-spacing:
              0px; -webkit-text-size-adjust: auto;
              -webkit-text-stroke-width: 0px; background-color: rgb(255,
              255, 255); text-decoration: none; display: inline
              !important; float: none;"><span
                class="Apple-converted-space">&nbsp;</span>is a
              diminishing sequence and<span
                class="Apple-converted-space">&nbsp;</span></span></i><i><span
              class="MathJax" id="MathJax-Element-5-Frame" tabindex="0"
              style="display: inline; font-style: normal; font-weight:
              normal; line-height: normal; font-size:
              14.399999618530273px; text-indent: 0px; text-align: left;
              text-transform: none; letter-spacing: normal;
              word-spacing: 0px; word-wrap: normal; white-space: nowrap;
              float: none; direction: ltr; max-width: none; max-height:
              none; min-width: 0px; min-height: 0px; border: 0px;
              padding: 0px; margin: 0px; caret-color: rgb(0, 0, 0);
              color: rgb(0, 0, 0); font-family: &quot;Lucida
              Grande&quot;, helvetica, arial, verdana, sans-serif;
              font-variant-caps: normal; orphans: auto; widows: auto;
              -webkit-text-size-adjust: auto; -webkit-text-stroke-width:
              0px; text-decoration: none;"><nobr style="transition:
                none; border: 0px; padding: 0px; margin: 0px; max-width:
                5000em; max-height: 5000em; min-width: 0px; min-height:
                0px; vertical-align: 0px; line-height: normal;
                text-decoration: none; white-space: nowrap !important;"><span
                  class="math" id="MathJax-Span-54" style="transition:
                  none; display: inline-block; position: static; border:
                  0px; padding: 0px; margin: 0px; vertical-align: 0px;
                  line-height: normal; text-decoration: none; width:
                  6.539em;"><span style="transition: none; display:
                    inline-block; position: relative; border: 0px;
                    padding: 0px; margin: 0px; vertical-align: 0px;
                    line-height: normal; text-decoration: none; width:
                    5.488em; height: 0px; font-size:
                    17.13599967956543px;"><span style="transition: none;
                      display: inline; position: absolute; border: 0px;
                      padding: 0px; margin: 0px; vertical-align: 0px;
                      line-height: normal; text-decoration: none; clip:
                      rect(1.52em, 1005.43em, 2.979em, -999.997em); top:
                      -2.506em; left: 0em;"><span class="mrow"
                        id="MathJax-Span-55" style="transition: none;
                        display: inline; position: static; border: 0px;
                        padding: 0px; margin: 0px; vertical-align: 0px;
                        line-height: normal; text-decoration: none;"><span
                          class="munderover" id="MathJax-Span-56"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px;
                          margin: 0px; vertical-align: 0px; line-height:
                          normal; text-decoration: none;"><span
                            style="transition: none; display:
                            inline-block; position: relative; border:
                            0px; padding: 0px; margin: 0px;
                            vertical-align: 0px; line-height: normal;
                            text-decoration: none; width: 2.045em;
                            height: 0px;"><span style="transition: none;
                              display: inline; position: absolute;
                              border: 0px; padding: 0px; margin: 0px;
                              vertical-align: 0px; line-height: normal;
                              text-decoration: none; clip: rect(3.096em,
                              1000.878em, 4.438em, -999.997em); top:
                              -4.024em; left: 0em;"><span class="mo"
                                id="MathJax-Span-57" style="transition:
                                none; display: inline; position: static;
                                border: 0px; padding: 0px; margin: 0px;
                                vertical-align: 0.003em; line-height:
                                normal; text-decoration: none;
                                font-family: STIXGeneral-Regular;">∑</span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span><span
                              style="transition: none; display: inline;
                              position: absolute; border: 0px; padding:
                              0px; margin: 0px; vertical-align: 0px;
                              line-height: normal; text-decoration:
                              none; clip: rect(3.563em, 1000.762em,
                              4.205em, -999.997em); top: -4.491em; left:
                              0.937em;"><span class="mi"
                                id="MathJax-Span-58" style="transition:
                                none; display: inline; position: static;
                                border: 0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;
                                font-size: 12.115151405334473px;
                                font-family: STIXGeneral-Regular;">∞</span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span><span
                              style="transition: none; display: inline;
                              position: absolute; border: 0px; padding:
                              0px; margin: 0px; vertical-align: 0px;
                              line-height: normal; text-decoration:
                              none; clip: rect(3.388em, 1001.112em,
                              4.205em, -999.997em); top: -3.732em; left:
                              0.937em;"><span class="texatom"
                                id="MathJax-Span-59" style="transition:
                                none; display: inline; position: static;
                                border: 0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;"><span
                                  class="mrow" id="MathJax-Span-60"
                                  style="transition: none; display:
                                  inline; position: static; border: 0px;
                                  padding: 0px; margin: 0px;
                                  vertical-align: 0px; line-height:
                                  normal; text-decoration: none;"><span
                                    class="mi" id="MathJax-Span-61"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-size: 12.115151405334473px;
                                    font-family: STIXGeneral-Italic;">t<span
                                      style="transition: none; display:
                                      inline-block; position: static;
                                      border: 0px; padding: 0px; margin:
                                      0px; vertical-align: 0px;
                                      line-height: normal;
                                      text-decoration: none; overflow:
                                      hidden; height: 1px; width:
                                      0.003em;"></span></span><span
                                    class="mo" id="MathJax-Span-62"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-size: 12.115151405334473px;
                                    font-family: STIXGeneral-Regular;">=</span><span
                                    class="mn" id="MathJax-Span-63"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-size: 12.115151405334473px;
                                    font-family: STIXGeneral-Regular;">0</span></span></span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span></span></span><span
                          class="msubsup" id="MathJax-Span-64"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px
                          0px 0px 0.178em; margin: 0px; vertical-align:
                          0px; line-height: normal; text-decoration:
                          none;"><span style="transition: none; display:
                            inline-block; position: relative; border:
                            0px; padding: 0px; margin: 0px;
                            vertical-align: 0px; line-height: normal;
                            text-decoration: none; width: 0.82em;
                            height: 0px;"><span style="transition: none;
                              display: inline; position: absolute;
                              border: 0px; padding: 0px; margin: 0px;
                              vertical-align: 0px; line-height: normal;
                              text-decoration: none; clip: rect(3.388em,
                              1000.47em, 4.438em, -999.997em); top:
                              -4.024em; left: 0em;"><span
                                class="texatom" id="MathJax-Span-65"
                                style="transition: none; display:
                                inline; position: static; border: 0px;
                                padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;"><span
                                  class="mrow" id="MathJax-Span-66"
                                  style="transition: none; display:
                                  inline; position: static; border: 0px;
                                  padding: 0px; margin: 0px;
                                  vertical-align: 0px; line-height:
                                  normal; text-decoration: none;"><span
                                    class="mo" id="MathJax-Span-67"
                                    style="transition: none; display:
                                    inline; position: static; border:
                                    0px; padding: 0px; margin: 0px;
                                    vertical-align: 0px; line-height:
                                    normal; text-decoration: none;
                                    font-family: STIXGeneral-Regular;">η</span></span></span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span><span
                              style="transition: none; display: inline;
                              position: absolute; border: 0px; padding:
                              0px; margin: 0px; vertical-align: 0px;
                              line-height: normal; text-decoration:
                              none; top: -3.79em; left: 0.528em;"><span
                                class="mi" id="MathJax-Span-68"
                                style="transition: none; display:
                                inline; position: static; border: 0px;
                                padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none;
                                font-size: 12.115151405334473px;
                                font-family: STIXGeneral-Italic;">t<span
                                  style="transition: none; display:
                                  inline-block; position: static;
                                  border: 0px; padding: 0px; margin:
                                  0px; vertical-align: 0px; line-height:
                                  normal; text-decoration: none;
                                  overflow: hidden; height: 1px; width:
                                  0.003em;"></span></span><span
                                style="transition: none; display:
                                inline-block; position: static; border:
                                0px; padding: 0px; margin: 0px;
                                vertical-align: 0px; line-height:
                                normal; text-decoration: none; width:
                                0px; height: 4.03em;"></span></span></span></span><span
                          class="mo" id="MathJax-Span-69"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px
                          0px 0px 0.295em; margin: 0px; vertical-align:
                          0px; line-height: normal; text-decoration:
                          none; font-family: STIXGeneral-Regular;">→</span><span
                          class="mi" id="MathJax-Span-70"
                          style="transition: none; display: inline;
                          position: static; border: 0px; padding: 0px
                          0px 0px 0.295em; margin: 0px; vertical-align:
                          0px; line-height: normal; text-decoration:
                          none; font-family: STIXGeneral-Regular;">∞</span></span><span
                        style="transition: none; display: inline-block;
                        position: static; border: 0px; padding: 0px;
                        margin: 0px; vertical-align: 0px; line-height:
                        normal; text-decoration: none; width: 0px;
                        height: 2.512em;"></span></span></span><span
                    style="transition: none; display: inline-block;
                    position: static; border-width: 0px;
                    border-left-style: solid; padding: 0px; margin: 0px;
                    vertical-align: -0.413em; line-height: normal;
                    text-decoration: none; overflow: hidden; width: 0px;
                    height: 1.462em;"></span></span></nobr></span></i><i><span
              style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);
              font-family: &quot;Lucida Grande&quot;, helvetica, arial,
              verdana, sans-serif; font-size: 14.399999618530273px;
              font-style: normal; font-variant-caps: normal;
              font-weight: normal; letter-spacing: normal; orphans:
              auto; text-align: start; text-indent: 0px; text-transform:
              none; white-space: normal; widows: auto; word-spacing:
              0px; -webkit-text-size-adjust: auto;
              -webkit-text-stroke-width: 0px; background-color: rgb(255,
              255, 255); text-decoration: none; display: inline
              !important; float: none;">. In other words, we extend the
              current state-of-the-art class of learning rates
              satisfying the convergence of SGD.</span></i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <br>
          <h3>November 28, 2018</h3>
          <h1>Nicolas Loizou Looking for Jobs</h1>
          <br>
          <a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a> is on the job market; he will get is PhD in 2019.
          He is looking for research positions in academia (Assistant
          Prof / postdoc) and industry (Research Scientist). Nicolas
          will be at <a href="https://nips.cc">NeurIPS</a> next week,
          presenting his work on privacy-preserving randomized gossip
          algorithms in the <a
            href="https://ppml-workshop.github.io/ppml/">PPML workshop</a>.
          At the moment, Nicolas is interning at <a
            href="https://research.fb.com/category/facebook-ai-research/">Facebook































































AI



Research

























































































































































































            (FAIR)</a>, where he has done some great work on
          decentralized training of deep learning models, and on
          accelerated decentralized gossip communication protocols. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>November 22, 2018</h3>
          <h1>NeurIPS 2018 Posters</h1>
          <br>
          Here are the posters of our papers accepted to this year's <a
            href="https://nips.cc">NeurIPS</a>: <br>
          <br>
          <a href="posters/Poster-ASBFGS.pdf"><img
              src="imgs/thmb-ASBFGS.png" alt="ASBFGS poster" width="750"
              border="0" height="563"></a><br>
          [<a href="https://arxiv.org/abs/1802.04079">paper on arXiv</a>]<br>
          <br>
          <a href="posters/Poster-SEGA.pdf"><img
              src="imgs/thmb-SEGA.png" alt="SEGA poster" width="750"
              border="0" height="564"></a><br>
          <a href="https://arxiv.org/abs/1809.03054">[paper on arXiv]</a><br>
          <br>
          <a href="posters/Poster-SSCD.pdf"><img
              src="imgs/thmb-SSCD.png" alt="SSCD poster" width="600"
              border="0" height="812"></a><br>
          <a href="https://arxiv.org/abs/1802.03703">[paper on arXiv]</a><br>
          <br>
          <br>
          <br>
          The poster for our <a
            href="https://ppml-workshop.github.io/ppml/">Privacy
            Preserving Machine Learning</a> NeurIPS workshop paper was
          not finalized yet. I will include a link here once it is
          ready. Update (November 28): The poster is now ready:<br>
          <br>
          <a href="posters/Poster-PrivateGossip.pdf"><img
              src="imgs/thmb-PrivateGossip.png" alt="Privacy Preserving
              Randomized Gossip" width="600" border="0" height="812"></a><br>
          <br>
          [<a href="https://arxiv.org/abs/1706.07636">full-length paper
            on arXiv</a>]<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>November 18, 2018</h3>
          <h1>New Postdoc: Xun Qian</h1>
          <br>
          Xun QIAN just joined my <a
            href="https://www.maths.ed.ac.uk/%7Eprichtar/i_team.html">group

























































































































































































            at KAUST</a> as a postdoc. He has a <a
            href="https://repository.hkbu.edu.hk/etd_oa/422/">PhD in
            Mathematics (August 2017) from Hong Kong Baptist University.</a>
          His PhD thesis is on "Continuous methods for convex
          programming and convex semidefinite programming" (<a
href="https://repository.hkbu.edu.hk/cgi/viewcontent.cgi?article=1422&amp;context=etd_oa">pdf</a>),






































































supervised



by

























































































































































































          <a href="http://www.math.hkbu.edu.hk/%7Eliliao/">Li-Zhi Liao</a>.<br>
          <br>
          Some of Xun's papers:<br>
          <br>
          H. W. Yue, Li-Zhi Liao, and Xun Qian. Two interior point
          continuous trajectory models for convex quadratic programming
          with bound constraints, to appear in <i>Pacific Journal on
            Optimization</i><br>
          <br>
          Xun Qian, Li-Zhi Liao, Jie Sun and Hong Zhu. The convergent
          generalized central paths for linearly constrained convex
          programming, <a
            href="https://epubs.siam.org/doi/10.1137/16M1104172"><i>SIAM
              Journal on Optimization</i> 28(2):1183-1204, 2018</a><br>
          <br>
          Xun Qian and Li-Zhi Liao. Analysis of the primal affine
          scaling continuous trajectory for convex programming, <a
            href="http://www.ybook.co.jp/online2/oppjo/vol14/p261.html"><i>Pacific






































































Journal



on

























































































































































































              Optimization</i> 14(2):261-272, 2018</a><br>
          <br>
          Xun Qian and Li-Zhi Liao and Jie Sun. Analysis of some
          interior point continuous trajectories for convex programming,
          <a
href="https://www.tandfonline.com/doi/full/10.1080/02331934.2017.1279160"><i>Optimization</i>
            66(4):589-608, 2017</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>November 16, 2018</h3>
          <h1>Nicolas Visiting MILA</h1>
          <br>
          <a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a> is giving a talk today at <a
            href="https://mila.quebec/en/">Mila</a>, University of
          Montréal. He is speaking about <a
            href="https://arxiv.org/abs/1712.09677">"Momentum and
            Stochastic Momentum for Stochastic Gradient, Newton,
            Proximal Point and Subspace Descent Methods".</a> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>November 13, 2018</h3>
          <h1>Nicolas Visiting McGill</h1>
          <br>
          <a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a> is giving a talk today in the Mathematics in
          Machine Learning Seminar at McGill University. He is speaking
          about <a href="https://arxiv.org/abs/1712.09677">"Momentum
            and Stochastic Momentum for Stochastic Gradient, Newton,
            Proximal Point and Subspace Descent Methods"</a>, joint work
          with me. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>November 12, 2018</h3>
          <h1>Statistics and Data Science Workshop @ KAUST</h1>
          <br>
          Today I am giving a talk at the <a
href="https://stat.kaust.edu.sa/calender/Pages/Page-2018-03-14_11-12-20-AM.aspx">Statistics













































































and



Data

























































































































































































            Science Workshop</a> held here at KAUST. I am speaking about
          the <a href="https://arxiv.org/abs/1805.02632">JacSketch
            paper</a>. Here is a <a
            href="https://www.youtube.com/watch?v=gjgEck0zU7w&amp;feature=youtu.be">YouTube

























































































































































































            video</a> of the same talk, one I gave in September at the
          Simons Institute.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>November 4, 2018</h3>
          <h1>Paper Accepted to WACV 2019</h1>
          <br>
          The paper <a href="https://arxiv.org/abs/1712.02249">"Online
            and batch incremental video background estimation"</a>,
          joint work with <a href="https://www.aritradutta.com">Aritra
            Dutta</a>, has just been accepted to IEEE Winter Conference
          on Applications of Computer Vision (<a
            href="http://wacv19.wacv.net">WACV 2019</a>). The conference
          will take place during January 7-January 11, 2019 in Honolulu,
          Hawaii. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>November 4, 2018</h3>
          <h1>Back @ KAUST</h1>
          <br>
          I am back from annual leave. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>November 3, 2018</h3>
          <h1>Paper Accepted to PPML 2018</h1>
          <br>
          The paper "A Privacy Preserving Randomized Gossip Algorithm
          via Controlled Noise Insertion", joint work with Nicolas
          Loizou, Filip Hanzely, Jakub Konečný and Dmitry Grishchenko,
          has been accepted to the NIPS Workshop on Privacy-Preserving
          Machine Learning (PPML 2018). The full-length paper, which
          includes a number of additional algorithms and results, <a
            href="https://arxiv.org/abs/1706.07636">can be found on
            arXiv here</a>.<br>
          <br>
          The acceptance email said: "We received an astonishing number
          of high quality submissions to the Privacy Preserving Machine
          Learning workshop and we are delighted to inform you that your
          submission <i>A Privacy Preserving Randomized Gossip
            Algorithm via Controlled Noise Insertion</i> (57) was
          accepted to be presented at the workshop."<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>November 1, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1810.13387">"A stochastic
            penalty model for convex and nonconvex optimization with big
            constraints"</a> - joint work with <a
            href="https://konstmish.github.io">Konstantin Mishchenko</a>.<br>
          <br>
          Abstract: <i>The last decade witnessed a rise in the
            importance of supervised learning applications involving <u>big

























































































































































































              data</u> and <u>big models</u>. Big data refers to
            situations where the amounts of training data available and
            needed causes difficulties in the training phase of the
            pipeline. Big model refers to situations where large
            dimensional and over-parameterized models are needed for the
            application at hand. Both of these phenomena lead to a
            dramatic increase in research activity aimed at taming the
            issues via the design of new sophisticated optimization
            algorithms. In this paper we turn attention to the <u>big
              constraints</u> scenario and argue that elaborate machine
            learning systems of the future will necessarily need to
            account for a large number of real-world constraints, which
            will need to be incorporated in the training process. This
            line of work is largely unexplored, and provides ample
            opportunities for future work and applications. To handle
            the big constraints regime, we propose a stochastic penalty
            formulation which reduces the problem to the well understood
            big data regime. Our formulation has many interesting
            properties which relate it to the original problem in
            various ways, with mathematical guarantees. We give a number
            of results specialized to nonconvex loss functions, smooth
            convex functions, strongly convex functions and convex
            constraints. We show through experiments that our approach
            can beat competing approaches by several orders of magnitude
            when a medium accuracy solution is required.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>November 1, 2018</h3>
          <h1>Aritra and El Houcine @ 2018 INFORMS Annual Meeting</h1>
          <br>
          <a href="https://www.aritradutta.com">Aritra Dutta</a> and <a
            href="https://vcc.kaust.edu.sa/Pages/Bergou.aspx">El Houcine
            Bergou</a> are on their way to Phoenix, Arizona, to give
          talks at the <a
href="https://www.informs.org/Meetings-Conferences/INFORMS-Conference-Calendar/2018-INFORMS-Annual-Meeting-Phoenix">2018






















































































INFORMS



Annual

























































































































































































            Meeting</a>. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>October 31, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1810.13084">"Provably
            accelerated randomized gossip algorithms"</a> - joint work
          with <a href="https://arxiv.org/abs/1810.13084">Nicolas
            Loizou</a> and <br>
          <a href="https://research.fb.com/people/rabbat-mike/">Michael
            G. Rabbat</a>.<br>
          <br>
          Abstract: <i>In this work we present novel provably
            accelerated gossip algorithms for solving the average
            consensus problem. The proposed protocols are inspired from
            the recently developed accelerated variants of the
            randomized Kaczmarz method - a popular method for solving
            linear systems. In each gossip iteration all nodes of the
            network update their values but only a pair of them exchange
            their private information. Numerical experiments on popular
            wireless sensor networks showing the benefits of our
            protocols are also presented.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>October 31, 2018</h3>
          <h1>Paper Accepted to AAAI 2019</h1>
          <br>
          The paper <a href="https://arxiv.org/abs/1805.07962">"A
            nonconvex projection method for robust PCA"</a>, joint work
          with Aritra Dutta and Filip Hanzely, has been accepted to the
          Thirty-Third AAAI Conference on Artificial Intelligence (<a
            href="https://aaai.org/Conferences/AAAI-19/">AAAI-19)</a>.
          The conference will take place during January 27-February 1,
          2019, in Honolulu, Hawaii, USA.<br>
          <br>
          The acceptance email said: "We had a record number of over
          7,700 submissions this year. Of those, 7,095 were reviewed,
          and due to space limitations we were only able to accept 1,150
          papers, yielding an acceptance rate of 16.2%. There was
          especially stiff competition this year because of the number
          of submissions, and you should be proud of your success."<br>
          <br>
          Abstract:<i> Robust principal component analysis (RPCA) is a
            well-studied problem with the goal of decomposing a matrix
            into the sum of low-rank and sparse components. In this
            paper, we propose a nonconvex feasibility reformulation of
            RPCA problem and apply an alternating projection method to
            solve it. To the best of our knowledge, we are the first to
            propose a method that solves RPCA problem without
            considering any objective function, convex relaxation, or
            surrogate convex constraints. We demonstrate through
            extensive numerical experiments on a variety of
            applications, including shadow removal, background
            estimation, face detection, and galaxy evolution, that our
            approach matches and often significantly outperforms current
            state-of-the-art in various ways.</i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>October 30, 2018</h3>
          <h1>Paper Accepted to JASA</h1>
          <br>
          The paper <a href="https://arxiv.org/abs/1801.05661">"A
            randomized exchange algorithm for computing optimal
            approximate designs of experiments"</a>, joint work with <a
            href="http://www.iam.fmph.uniba.sk/ospm/Harman/">Radoslav
            Harman</a> and <a
            href="http://www.iam.fmph.uniba.sk/ospm/Filova/">Lenka
            Filová</a>, has been accepted to
          <meta charset="utf-8">
          Journal of the American Statistical Association (JASA).<br>
          <br>
          Abstract: <i>We propose a class of subspace ascent methods
            for computing optimal approximate designs that covers both
            existing as well as new and more efficient algorithms.
            Within this class of methods, we construct a simple,
            randomized exchange algorithm (REX). Numerical comparisons
            suggest that the performance of REX is comparable or
            superior to the performance of state-of-the-art methods
            across a broad range of problem structures and sizes. We
            focus on the most commonly used criterion of D-optimality
            that also has applications beyond experimental design, such
            as the construction of the minimum volume ellipsoid
            containing a given set of data-points. For D-optimality, we
            prove that the proposed algorithm converges to the optimum.
            We also provide formulas for the optimal exchange of weights
            in the case of the criterion of A-optimality. These formulas
            enable one to use REX for computing A-optimal and I-optimal
            designs.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>October 25, 2018</h3>
          <h1>Annual Leave</h1>
          <br>
          I am about to go on an annual leave to an island in the Indian
          ocean. I will likely have no functioning internet, and will
          not be reading my emails (maybe I'll read one or two *if* I
          get internet over there, but do not expect me to respond as
          the purpose of annual leave is to relax and recharge). I will
          be back at KAUST and operational on November 4, teaching at
          9am. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>October 22, 2018</h3>
          <h1>Sebastian Stich @ KAUST</h1>
          <br>
          <a href="https://sstich.ch">Sebastian Stich</a> is visiting me
          at KAUST. He will stay here for three weeks, and will give a
          CS seminar talk on November 12. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>October 20, 2018</h3>
          <h1>Filip @ MSR</h1>
          <br>
          <a href="https://vcc.kaust.edu.sa/Pages/Hanzely.aspx">Filip
            Hanzely</a> is visiting <a
            href="https://www.microsoft.com/en-us/research/people/lixiao/">Lin

























































































































































































            Xiao</a> at Microsoft Research in Redmond, Washington. He
          will be back roughly in a month. While in the US, he will also
          drop by Phoenix to give a talk at the <a
href="https://www.informs.org/Meetings-Conferences/INFORMS-Conference-Calendar/2018-INFORMS-Annual-Meeting-Phoenix">2018

































































































INFORMS



Annual

























































































































































































            Meeting</a>.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>October 15, 2018</h3>
          <h1>Filip Received NIPS Travel award</h1>
          <br>
          Congratulations to <a
            href="https://vcc.kaust.edu.sa/Pages/Hanzely.aspx">Filip
            Hanzely</a> for receiving a <a
            href="https://nips.cc/Conferences/2018/Dates">NIPS</a>
          Travel Award ($1,500). Filip is the coauthor of 2 papers
          accepted to NIPS this year: <br>
          <br>
          <a href="https://arxiv.org/abs/1802.04079">"Accelerated
            stochastic matrix inversion: general theory and speeding up
            BFGS rules for faster second-order optimization"</a> - joint
          work with Robert M. Gower, me, and Sebastian Stich.<br>
          <br>
          <a href="https://arxiv.org/abs/1809.03054">"SEGA: Variance
            reduction via gradient sketching"</a> - joint work with
          Konstantin Mishchenko and me.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>October 8, 2018</h3>
          <h1>Paper published in SIAM Journal on Optimization</h1>
          <br>
          The paper <i>"Stochastic Primal-Dual Hybrid Gradient
            Algorithm with Arbitrary Sampling and Imaging Applications"</i>
          (<a href="https://arxiv.org/abs/1706.04957">arXiv preprint
            here</a>), coauthored with Antonin Chambolle , Matthias J.
          Ehrhardt, and Carola-Bibiane Schönlieb, was <a
            href="https://epubs.siam.org/doi/abs/10.1137/17M1134834">just

































































































published



by

























































































































































































            the SIAM Journal on Optimization</a>.<br>
          <br>
          Here are related <a href="talks/TALK-SPDHG.pdf">slides</a>, <a
href="https://www.maths.ed.ac.uk/%7Eprichtar/posters/Poster-SPDHG.pdf">poster</a>,
          <a href="https://github.com/mehrhardt/spdhg">GitHub code</a>
          and a <a href="https://www.youtube.com/watch?v=iZc2eFqS2l4">YouTube

























































































































































































            talk</a>.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36"><br>
          <br>
          <br>
          <h3>October 3, 2018</h3>
          <h1>Filip back from Amazon internship </h1>
          <br>
          <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          is now back from an internship at the Amazon Scalable Machine
          Learning group in Berlin. While there, he was working on
          Bayesian optimization for deep learning. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 25, 2018</h3>
          <h1>New paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1809.09354">"Accelerated
            coordinate descent with arbitrary sampling and best rates
            for minibatches"</a> - joint work with <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely.</a>
          <br>
          <span style="font-style: italic;"></span> <br>
          Abstract: <i>Accelerated coordinate descent is a widely
            popular optimization algorithm due to its efficiency on
            large-dimensional problems. It achieves state-of-the-art
            complexity on an important class of empirical risk
            minimization problems. In this paper we design and analyze
            an accelerated coordinate descent (ACD) method which in each
            iteration updates a random subset of coordinates according
            to an arbitrary but fixed probability law, which is a
            parameter of the method. If all coordinates are updated in
            each iteration, our method reduces to the classical
            accelerated gradient descent method AGD of Nesterov. If a
            single coordinate is updated in each iteration, and we pick
            probabilities proportional to the square roots of the
            coordinate-wise Lipschitz constants, our method reduces to
            the currently fastest coordinate descent method NUACDM of
            Allen-Zhu, Qu, Richt\'{a}rik and Yuan. While mini-batch
            variants of ACD are more popular and relevant in practice,
            there is no importance sampling for ACD that outperforms the
            standard uniform mini-batch sampling. Through insights
            enabled by our general analysis, we design new importance
            sampling for mini-batch ACD which significantly outperforms
            previous state-of-the-art minibatch ACD in practice. We
            prove a rate that is at most O(√</i><i><i>τ</i>) times worse
            than the rate of minibatch ACD with uniform sampling, but
            can be O(n/τ) times better, where τ is the minibatch size.
            Since in modern supervised learning training systems it is
            standard practice to choose τ ≪ n, and often τ=O(1), our
            method can lead to dramatic speedups. Lastly, we obtain
            similar results for minibatch nonaccelerated CD as well,
            achieving improvements on previous best rates.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3><br>
          </h3>
          <h3>September 23, 2018</h3>
          <h1>Visiting Simons Institute, UC Berkeley</h1>
          <br>
          I am at the Simons Institute, UC Berkeley, attending the
          workshop <a
            href="https://simons.berkeley.edu/data-science-2018-1">"Randomized

































































































Numerical



Linear

























































































































































































            Algebra and Applications"</a>. This workshop is a part of
          the semester-long program <a
            href="https://simons.berkeley.edu/programs/datascience2018">"Foundations

































































































of



Data

























































































































































































            Science"</a>. <br>
          <br>
          My talk is on Tuesday, Sept 25, at 9:30am, PST. I will be
          talking about <a href="https://arxiv.org/abs/1805.02632">"Stochastic

































































































Quasi-Gradient



Methods:

























































































































































































            Variance Reduction via Jacobian Sketching"</a> (joint work
          with R.M. Gower and F. Bach). All <a
            href="https://simons.berkeley.edu/workshops/schedule/6681">talks

























































































































































































            are live-streamed</a> and recorded, and will be uploaded
          onto YouTube. <br>
          <br>
          Update (Sept 25): <a
href="https://www.youtube.com/watch?v=gjgEck0zU7w&amp;list=PLgKuh-lKre13E9dXSsif4KsGoFp4bjubd&amp;index=9">My

































































































talk



is

























































































































































































            on YouTube.</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 17, 2018</h3>
          <h1>Area chair for ICML 2019</h1>
          <br>
          I have accepted an invite to serve as an Area Chair for <a
            href="https://icml.cc/Conferences/2019">The 36th
            International Conference on Machine Learning (ICML 2019)</a>.
          The event will be held in Long Beach, California, June 10-15,
          2019.<br>
          <br>
          Submission deadline: January 23, 2019<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 14, 2018</h3>
          <h1>Paper published in JMLR</h1>
          <br>
          The paper <a href="http://jmlr.org/papers/v19/16-241.html">"Importance

































































































sampling



for

























































































































































































            minibatches"</a>, coauthored with Dominik Csiba, was just
          published by the Journal of Machine Learning Research.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>August 13, 2018</h3>
          <h1>New paper &amp; best poster award</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1809.04146">"Nonconvex variance
            reduced optimization with arbitrary sampling"</a> - joint
          work with <a href="https://samuelhorvath.github.io">Samuel
            Horváth.</a> <br>
          <span style="font-style: italic;"></span> <br>
          Abstract: <i>We provide the first importance sampling
            variants of variance reduced algorithms for empirical risk
            minimization with non-convex loss functions. In particular,
            we analyze non-convex versions of SVRG, SAGA and SARAH. Our
            methods have the capacity to speed up the training process
            by an order of magnitude compared to the state of the art on
            real datasets. Moreover, we also improve upon current
            mini-batch analysis of these methods by proposing importance
            sampling for minibatches in this setting. Surprisingly, our
            approach can in some regimes lead to superlinear speedup
            with respect to the minibatch size, which is not usually
            present in stochastic optimization. All the above results
            follow from a general analysis of the methods which works
            with arbitrary sampling, i.e., fully general randomized
            strategy for the selection of subsets of examples to be
            sampled in each iteration. Finally, we also perform a novel
            importance sampling analysis of SARAH in the convex setting.</i><br>
          <br>
          A poster based on the results of this paper won a Best Poster
          Prize. Below I am recycling an earlier blog post I made about
          this in June (the paper was not available online at that
          time):<br>
          <br>
          <font color="#ff9966"><span class="important">Best DS</span><span
              class="important"><span style="box-sizing: border-box;
                margin: 0px; padding: 0px; border: 0px none; outline:
                0px none; font-size: 14px; vertical-align: baseline;
                font-family: &quot;Open
                Sans&quot;,Helvetica,Arial,Lucida,sans-serif;
                font-style: normal; font-variant-caps: normal;
                letter-spacing: normal; text-align: center; text-indent:
                0px; text-transform: none; white-space: normal;
                word-spacing: 0px; -moz-text-size-adjust: auto;
                -webkit-text-stroke-width: 0px; text-decoration: none;
                font-weight: 400; background-position: 0px 0px;"></span><span
                style="box-sizing: border-box; margin: 0px; padding:
                0px; border: 0px none; outline: 0px none; font-size:
                14px; vertical-align: baseline; font-family: &quot;Open
                Sans&quot;,Helvetica,Arial,Lucida,sans-serif;
                font-style: normal; font-variant-caps: normal;
                letter-spacing: normal; text-align: center; text-indent:
                0px; text-transform: none; white-space: normal;
                word-spacing: 0px; -moz-text-size-adjust: auto;
                -webkit-text-stroke-width: 0px; text-decoration: none;
                font-weight: 400; background-position: 0px 0px;"><sup
                  style="box-sizing: border-box; margin: 0px; padding:
                  0px; border: 0px; outline: 0px; font-size: smaller;
                  vertical-align: super; position: relative; height:
                  0px; line-height: 1; bottom: 0.3em !important;
                  background-position: 0px 0px; background-repeat:
                  initial initial;">3</sup></span> Poster Award</span>
            for Samuel Horváth</font><br>
          <br>
          <span class="important"></span><a
            href="https://vcc.kaust.edu.sa/Pages/Horvath.aspx">Samuel
            Horváth</a> attended the <a
            href="http://www.ds3-datascience-polytechnique.fr">Data
            Science Summer School</a> (<span style="box-sizing:
            border-box; margin: 0px; padding: 0px; border: 0px; outline:
            0px; font-size: 14px; vertical-align: baseline; caret-color:
            rgb(102, 102, 102); color: rgb(102, 102, 102); font-family:
            &quot;Open Sans&quot;, Helvetica, Arial, Lucida, sans-serif;
            font-style: normal; font-variant-caps: normal;
            letter-spacing: normal; orphans: auto; text-align: center;
            text-indent: 0px; text-transform: none; white-space: normal;
            widows: auto; word-spacing: 0px; -webkit-text-size-adjust:
            auto; -webkit-text-stroke-width: 0px; text-decoration: none;
            font-weight: 400; background-position: 0px 0px;
            background-repeat: initial initial;">DS</span><span
            style="box-sizing: border-box; margin: 0px; padding: 0px;
            border: 0px; outline: 0px; font-size: 14px; vertical-align:
            baseline; caret-color: rgb(102, 102, 102); color: rgb(102,
            102, 102); font-family: &quot;Open Sans&quot;, Helvetica,
            Arial, Lucida, sans-serif; font-style: normal;
            font-variant-caps: normal; letter-spacing: normal; orphans:
            auto; text-align: center; text-indent: 0px; text-transform:
            none; white-space: normal; widows: auto; word-spacing: 0px;
            -webkit-text-size-adjust: auto; -webkit-text-stroke-width:
            0px; text-decoration: none; font-weight: 400;
            background-position: 0px 0px; background-repeat: initial
            initial;"><sup style="box-sizing: border-box; margin: 0px;
              padding: 0px; border: 0px; outline: 0px; font-size:
              smaller; vertical-align: super; position: relative;
              height: 0px; line-height: 1; bottom: 0.3em !important;
              background-position: 0px 0px; background-repeat: initial
              initial;">3</sup></span>), which took place during June
          25-29, 2018, at École Polytechnique in Paris, France. Based on
          the <a href="http://www.ds3-datascience-polytechnique.fr">event

























































































































































































            website</a>, the event gathered 500 participants from 34
          countries and 6 continents, out of which 290 were MS and PhD
          students and postdocs, and 110 professionals. Selected
          guest/speaker names (out of 41): Cédric Villani, Nicoló
          Cesa-Bianchi, Mark Girolami, Yann Lecun, Suvrit Sra,
          Jean-Philippe Vert, Adrian Weller, Marco Cuturi, Arthur
          Gretton, and Andreas Krause.<br>
          <br>
          <span style="box-sizing: border-box; margin: 0px; padding:
            0px; border: 0px; outline: 0px; font-size: 14px;
            vertical-align: baseline; caret-color: rgb(102, 102, 102);
            color: rgb(102, 102, 102); font-family: &quot;Open
            Sans&quot;, Helvetica, Arial, Lucida, sans-serif;
            font-style: normal; font-variant-caps: normal;
            letter-spacing: normal; orphans: auto; text-align: center;
            text-indent: 0px; text-transform: none; white-space: normal;
            widows: auto; word-spacing: 0px; -webkit-text-size-adjust:
            auto; -webkit-text-stroke-width: 0px; text-decoration: none;
            font-weight: 400; background-position: 0px 0px;
            background-repeat: initial initial;"></span>The event also
          included a best-poster competition, with an impressive total
          of <a
            href="http://www.ds3-datascience-polytechnique.fr/posters/">170

























































































































































































            posters</a>. Samuel's poster won the <span
            class="important">Best DS</span><span class="important"><font
              color="#ff9966"><span style="box-sizing: border-box;
                margin: 0px; padding: 0px; border: 0px none; outline:
                0px none; font-size: 14px; vertical-align: baseline;
                font-family: &quot;Open
                Sans&quot;,Helvetica,Arial,Lucida,sans-serif;
                font-style: normal; font-variant-caps: normal;
                letter-spacing: normal; text-align: center; text-indent:
                0px; text-transform: none; white-space: normal;
                word-spacing: 0px; -moz-text-size-adjust: auto;
                -webkit-text-stroke-width: 0px; text-decoration: none;
                font-weight: 400; background-position: 0px 0px;"></span><span
                style="box-sizing: border-box; margin: 0px; padding:
                0px; border: 0px none; outline: 0px none; font-size:
                14px; vertical-align: baseline; font-family: &quot;Open
                Sans&quot;,Helvetica,Arial,Lucida,sans-serif;
                font-style: normal; font-variant-caps: normal;
                letter-spacing: normal; text-align: center; text-indent:
                0px; text-transform: none; white-space: normal;
                word-spacing: 0px; -moz-text-size-adjust: auto;
                -webkit-text-stroke-width: 0px; text-decoration: none;
                font-weight: 400; background-position: 0px 0px;"><sup
                  style="box-sizing: border-box; margin: 0px; padding:
                  0px; border: 0px; outline: 0px; font-size: smaller;
                  vertical-align: super; position: relative; height:
                  0px; line-height: 1; bottom: 0.3em !important;
                  background-position: 0px 0px; background-repeat:
                  initial initial;">3</sup></span></font> Poster Award.</span><font
            color="#ff9966"> </font>The poster, entitled <i>Nonconvex
            variance reduced optimization with arbitrary sampling</i>,
          is based on a paper of the same title, joint work with me, and
          currently under review.<br>
          <br>
          Here is the poster:<br>
          <br>
          <a
href="http://www.ds3-datascience-polytechnique.fr/wp-content/uploads/2018/06/DS3-342.pdf"><img
              src="posters/Poster-DS3-small.png" alt="Poster"
              width="700" border="2" height="526"></a><br>
          <br>
          And here is the award:<br>
          <br>
          <img src="docs/best_poster_DS3.jpg" alt="" width="600"
            height="348"><br>
          <br>
          This first prize carries a 500 EUR cash award.<br>
          <br>
          Samuel: Congratulations!!! <br>
          <br>
          <br>
          Update (October 11, 2018): Here is a <a
href="https://www.kaust.edu.sa/en/news/Pages/best-poster-award-at-Data-Science-Summer-School.aspx">KAUST



News

























































































































































































            article</a> about this. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>September 5, 2018</h3>
          <h1>Three papers accepted to NIPS 2018</h1>
          <br>
          The long-awaited decisions arrived today! We've had three
          papers accepted to the Thirty-second Annual Conference on
          Neural Information Processing Systems (NIPS 2018):<br>
          &nbsp;<br>
          <a href="https://arxiv.org/abs/1802.03703">"Stochastic
            spectral and conjugate descent methods"</a> - joint work
          with Dmitry Kovalev, <a
            href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard

























































































































































































            Gorbunov</a> and Elnur Gasanov.<br>
          <br>
          Abstract: <i>The state-of-the-art methods for solving
            optimization problems in big dimensions are variants of
            randomized coordinate descent (RCD). In this paper we
            introduce a fundamentally new type of acceleration strategy
            for RCD based on the augmentation of the set of coordinate
            directions by a few spectral or conjugate directions. As we
            increase the number of extra directions to be sampled from,
            the rate of the method improves, and interpolates between
            the linear rate of RCD and a linear rate independent of the
            condition number. We develop and analyze also inexact
            variants of these methods where the spectral and conjugate
            directions are allowed to be approximate only. We motivate
            the above development by proving several negative results
            which highlight the limitations of RCD with importance
            sampling.</i><br>
          <br>
          <br>
          <a href="https://arxiv.org/abs/1802.04079"> "Accelerated
            stochastic matrix inversion: general theory and speeding up
            BFGS rules for faster second-order optimization"</a> - joint
          work with <a
            href="https://perso.telecom-paristech.fr/rgower/">Robert M.
            Gower</a>, <a href="https://fhanzely.github.io/index.html">Filip

























































































































































































            Hanzely</a> and <a href="https://sstich.ch">Sebastian Stich</a>.
          <br>
          <br>
          Abstract: <i>We present the first accelerated randomized
            algorithm for solving linear systems in Euclidean spaces.
            One essential problem of this type is the matrix inversion
            problem. In particular, our algorithm can be specialized to
            invert positive definite matrices in such a way that all
            iterates (approximate solutions) generated by the algorithm
            are positive definite matrices themselves. This opens the
            way for many applications in the field of optimization and
            machine learning. As an application of our general theory,
            we develop the first accelerated (deterministic and
            stochastic) quasi-Newton updates. Our updates lead to
            provably more aggressive approximations of the inverse
            Hessian, and lead to speed-ups over classical
            non-accelerated rules in numerical experiments. Experiments
            with empirical risk minimization show that our rules can
            accelerate training of machine learning models.</i><i><br>
          </i><i> </i><br>
          <br>
          <a href="https://arxiv.org/abs/1809.03054">"SEGA: Variance
            reduction via gradient sketching"</a> - joint work with <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          and <a href="https://konstmish.github.io">Konstantin
            Mishchenko</a>.<br>
          <br>
          Abstract: <i>We propose a novel randomized first order
            optimization method—SEGA (SkEtched GrAdient method)—which
            progressively throughout its iterations builds a
            variance-reduced estimate of the gradient from random linear
            measurements (sketches) of the gradient provided at each
            iteration by an oracle. In each iteration, SEGA updates the
            current estimate of the gradient through a
            sketch-and-project operation using the information provided
            by the latest sketch, and this is subsequently used to
            compute an unbiased estimate of the true gradient through a
            random relaxation procedure. This unbiased estimate is then
            used to perform a gradient step. Unlike standard subspace
            descent methods, such as coordinate descent, SEGA can be
            used for optimization problems with a non-separable proximal
            term. We provide a general convergence analysis and prove
            linear convergence for strongly convex objectives. In the
            special case of coordinate sketches, SEGA can be enhanced
            with various techniques such as importance sampling,
            mini-batching and acceleration, and its rate is up to a
            small constant factor identical to the best-known rate of
            coordinate descent.</i><br>
          <br>
          <br>
          As an added bonus, I got a free NIPS registration as one of
          the top-ranked reviewers this year. Thanks NIPS!<br>
          <br>
          The conference will take place during December 3-8, 2018 in
          Montreal, Canada.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3><br>
          </h3>
          <h3>September 3, 2018</h3>
          <h1>Two new MS/PhD students</h1>
          <br>
          Two new students just joined my group at KAUST:<br>
          <br>
          - Dmitry Kovalev (from Moscow Institute of Physics and
          Technology)<br>
          <br>
          - Elnur Gasanov (from Moscow Institute of Physics and
          Technology)<br>
          <br>
          Welcome!<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3><br>
          </h3>
          <h3>August 29, 2018</h3>
          <h1>People away on internships</h1>
          <br>
          Several people from my group are away on internships. <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          has been with Amazon Scalable Machine Learning group in
          Berlin, Germany, since June and will stay until the end of
          September. <a href="https://konstmish.github.io">Konstantin
            Mishchenko</a> is with Amazon, Seattle, USA since August,
          and will stay there until the end of November. <a
            href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a> is with <a
            href="https://research.fb.com/category/facebook-ai-research/">FAIR</a>
          at Facebook in Montreal, Canada since August and will stay
          there for four months.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3><br>
          </h3>
          <h3>August 26, 2018</h3>
          <h1>Fall semester started</h1>
          <br>
          The Fall semester is starting at KAUST today. I am teaching
          CS390FF: "Selected Topics in Data Sciences" (Sundays and
          Tuesdays, 9-10:30am in Bldg 9: 4125). <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3><br>
          </h3>
          <h3>August 12, 2018</h3>
          <h1>Attending a workshop on Optimization in Machine Learning @
            Lehigh</h1>
          <br>
          I am on my way to Bethlehem, Pennsylvania, to give a talk at
          the <a href="http://coral.ie.lehigh.edu/%7Emopta/">DIMACS
            Workshop on Optimization in Machine Learning</a>, taking
          place at <a
            href="https://en.wikipedia.org/wiki/Lehigh_University">Lehigh

























































































































































































            University</a> during August 13-15, 2018. The workshop is
          part of a larger event which also includes the MOPTA
          conference (Aug 15-17) and the TRIPODS Summer School for PhD
          students (Aug 10-12). <br>
          <br>
          I am giving a talk on Tuesday entitled "Stochastic
          quasi-gradient methods: variance reduction via Jacobian
          sketching", joint work with Robert M. Gower and Francis Bach.
          <a href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a> is attending as well; he is presenting a poster
          on Tuesday and giving a talk on Thursday, both on the same
          topic: "Revisiting randomized gossip algorithms", and based on
          these two papers: [<a href="http://arxiv.org/abs/1610.04714">GlobalSIP2016</a>],

























































































































































































          [<a href="papers/acc_gossip.pdf">Allerton2018</a>]. <br>
          <br>
          The <a href="http://coral.ie.lehigh.edu/%7Emopta/program">speaker

























































































































































































            line-up</a> is excellent. On the other hand, the weather in
          Bethlehem does not seem to be particularly welcoming:<br>
          <br>
          <img src="imgs/weather-Lehigh.png" alt="Weather forecast"
            width="680" height="519"><br>
          <br>
          Meanwhile, this is what we are supposed to have at KAUST
          during the same week:<br>
          <br>
          <img src="imgs/weather-KAUST.png" alt="weather forecast"
            width="680" height="520"><br>
          <br>
          I'd welcome a convex combination of the two instead ;-)<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3><br>
          </h3>
          <h3>August 10, 2018</h3>
          <h1>New paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1808.03045">"Accelerated Bregman
            proximal gradient methods for relatively smooth convex
            optimization"</a> - joint work with <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          and <a
            href="https://www.microsoft.com/en-us/research/people/lixiao/">Lin

























































































































































































            Xiao</a>. <br>
          <span style="font-style: italic;"></span> <br>
          Abstract: <i>We consider the problem of minimizing the sum of
            two convex functions: one is differentiable and relatively
            smooth with respect to a reference convex function, and the
            other can be nondifferentiable but simple to optimize. The
            relatively smooth condition is much weaker than the standard
            assumption of uniform Lipschitz continuity of the gradients,
            thus significantly increases the scope of potential
            applications. We present accelerated Bregman proximal
            gradient (ABPG) methods that employ the Bregman distance of
            the reference function as the proximity measure. These
            methods attain an O(1/k^</i><i><i>γ</i>) convergence rate in
            the relatively smooth setting, where γ ∈ [1,2] is determined
            by a triangle scaling property of the Bregman distance. We
            develop adaptive variants of the ABPG method that
            automatically ensure the best possible rate of convergence
            and argue that the O(1/k^2) rate is attainable in most
            cases. We present numerical experiments with three
            applications: D-optimal experiment design, Poisson linear
            inverse problem, and relative-entropy nonnegative
            regression. In all experiments, we obtain numerical
            certificates showing that these methods do converge with the
            O(1/k^2) rate.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3><br>
          </h3>
          <h3>August 7, 2018</h3>
          <h1>Paper accepted to SIAM Journal on Optimization</h1>
          <br>
          The paper <a href="https://arxiv.org/abs/1706.04957">"Stochastic




























































































































primal-dual



hybrid

























































































































































































            gradient algorithm with arbitrary sampling and imaging
            applications"</a>, joint work with Antonin Chambolle,
          Matthias J. Ehrhardt, and Carola-Bibiane Schönlieb, was
          accepted to <i>SIAM Journal on Optimization</i>.<br>
          <br>
          Here is a <a
            href="https://www.youtube.com/watch?v=iZc2eFqS2l4">YouTube
            video</a> of a talk I gave on this topic. Here is the <a
            href="https://github.com/mehrhardt/spdhg">SPDHG GitHub code.</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3><br>
          </h3>
          <h3>August 4, 2018</h3>
          <h1>Paper accepted to Allerton</h1>
          <br>
          The paper <a href="papers/acc_gossip.pdf">"Accelerated gossip
            via stochastic heavy ball method"</a>, joint work with <a
            href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a>, was accepted to Allerton (<i>56th Annual
            Allerton Conference on Communication, Control, and Computing</i>,
          2018).
          <meta name="qrichtext" content="1">
          <style type="text/css">
p, li { white-space: pre-wrap; }
</style><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3><br>
          </h3>
          <h3>July 27, 2018</h3>
          <h1>NIPS rebuttals</h1>
          <br>
          Working on NIPS author feedback... The deadline is on August
          1st.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3><br>
          </h3>
          <h3>July 23, 2018</h3>
          <h1>Paper published by Linear Algebra and its Applications</h1>
          <br>
          The paper "The complexity of primal-dual fixed point methods
          for ridge regression", coauthored with <a
            href="https://docs.ufpr.br/%7Eademir.ribeiro/">Ademir Alves
            Ribeiro</a>, just appeared online in <a
href="https://www.sciencedirect.com/science/article/pii/S0024379518303434"><i>Linear

































































































































Algebra



and

























































































































































































              its Applications</i></a>.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3><br>
          </h3>
          <h3>July 22, 2018</h3>
          <h1>Plenary talk in Brazil</h1>
          <br>
          I am in Foz do Iguacu, Brazil, attending the conference <a
            href="http://www.foz2018.com/default">Mathematics and it
            Applications</a> and <a
            href="http://www.foz2018.com/continuous-optimization">XII
            Brazilian Workshop on Continuous Optimization</a>. I will
          give a plenary talk on Thursday.&nbsp;&nbsp;&nbsp; <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <h3><br>
          </h3>
          <h3>July 16, 2018</h3>
          <h1>New paper</h1>
          <br>
          <span class="important"></span><span class="important">New
            paper out: </span><a href="papers/MACO-highlights.pdf">"Matrix





































































































































completion



under

























































































































































































            interval uncertainty: highlights"</a><a> </a>
          <meta http-equiv="Content-Type" content="text/html;
            charset=UTF-8">
          - joint work with <a
            href="https://researcher.watson.ibm.com/person/ie-jakub.marecek">Jakub

























































































































































































            Mareček</a> and <a href="http://mtakac.com">Martin Takáč.</a>
          To appear in <a href="http://www.ecmlpkdd2018.org"><i>ECML-PKDD</i>
            2018</a>.<br>
          <span style="font-style: italic;"></span>
          <p> </p>
          <p> <img alt="" src="imgs/fancy-line.png" width="196" height="36"><br> </p>
          <h3>July 10, 2018</h3>
          <h1>Most-read paper in Optimization Methods and Software in
            2017</h1>
          <br>
          The paper <a
href="https://www.tandfonline.com/doi/full/10.1080/10556788.2016.1278445">Distributed






































































































































Optimization



with

























































































































































































            Arbitrary Local Solvers</a>, joint work with Chenxin Ma,
          Jakub Konečný, Martin Jaggi, Virginia Smith, Michael I Jordan,
          and Martin Takáč, was the most-read paper in the <a
            href="https://www.tandfonline.com/toc/goms20/current">OMS
            journal</a> in year 2017. <br>
          <br>
          This is how I know: I clicked on the "Read our most-read
          article of 2017 for free here" link available <a
            href="https://www.tandfonline.com/toc/goms20/current">on
            this website</a>, and got a nice surprise.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36"><br>
          <br>
          <h3>July 9, 2018</h3>
          <h1>New paper</h1>
          <br>
          <span class="important"></span><span class="important">New
            paper out: </span><a href="papers/acc_gossip.pdf">"Accelerated





































































































































gossip



via

























































































































































































            stochastic heavy ball method"</a><a> </a>- joint work with
          <a href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou.</a> <br>
          <span style="font-style: italic;"></span>
          <p><br>Abstract: <i>In this paper we show how the stochastic heavy ball method (SHB)—a popular method for solving stochastic convex and non-convex optimization problems—operates as a randomized gossip algorithm. In particular, we focus on two special cases of SHB: the Randomized Kaczmarz method with momentum and its block variant. Building upon a recent framework for the design and analysis of randomized gossip algorithms [19] we interpret the distributed nature of the proposed methods. We present novel protocols for solving the average consensus problem where in each step all nodes of the network update their values but only a subset of them exchange their private values. Numerical experiments on popular wireless sensor networks showing the benefits of our protocols are also presented.<br><br></i></p>
          <p> <img alt="" src="imgs/fancy-line.png" width="196" height="36"></p>
          <br>
          <h3>July 3, 2018</h3>
          <h1>Editor @ OMS</h1>
          <br>
          <span class="important"></span>I have joined the Editorial
          Board of <a
            href="https://www.tandfonline.com/toc/goms20/current">Optimization






































































































































Methods



and

























































































































































































            Software</a>. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>July 1, 2018</h3>
          <h1>23rd International Symposium on Mathematical Programming</h1>
          <br>
          I am on my way to Bordeaux, to attend <a
            href="https://ismp2018.sciencesconf.org">ISMP (23rd
            International Symposium on Mathematical Programming)</a>.
          With Alexandre d’Aspremont, Olivier Beaumont, and Suvrit Sra,
          we have organized stream 4a: "Learning: Machine Learning, Big
          Data, Cloud Computing, and Huge-Scale Optimization". Here is
          the schedule of talks which are based on papers I am co-author
          of (highlighted in red):<br>
          <br>
          <meta http-equiv="Content-Type" content="text/html;
            charset=UTF-8">
          <p class="MsoNormal"><b style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt">Coordinate Descent and Randomized Direct Search Methods (Continuous Optimization</span></b><b style="mso-bidi-font-weight:
normal"><span style="font-size:10.0pt;mso-ansi-language:EN-GB" lang="EN-GB">)<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"><br>RandomM - Mo 3:15pm-4:45pm, Format: 3x30 min <br>Room: Salle KC6 Building: K, Intermediate 1, Zone: 10 <br>Invited Session 211<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"><br>Organizer: Martin Takáč, Lehigh University, US<o:p></o:p></span></b></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">1 - When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent<o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Asu Ozdaglar, MIT, US, talk 1486<o:p></o:p></span><span style="font-size:10.0pt"><br>Co-Authors: Mert Gurbuzbalaban, Nuri Vanli, Pablo Parrilo <o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">2 - Random direct search method for unconstrained smooth minimization<o:p></o:p></span><span style="font-size:10.0pt;color:red">
Speaker: El houcine Bergou, KAUST-INRA, SA, talk 421<o:p></o:p></span><span style="font-size:10.0pt;color:red">
Co-Authors: Peter Richtárik, Eduard Gorbunov<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">3 - Active Metric Learning for Supervised Classification<o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Dimitri Papageorgiou, ExxonMobil, US, talk 1275 <o:p></o:p></span><span style="font-size:10.0pt">
Co-Authors: Krishnan Kumaran, Martin Takáč <o:p></o:p></span>
</p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p><b>Machine learning and sparse optimisation (Continuous Optimization)
NLP - Tu 8:30am-10:30am, Format: 4x30 min <br>Room: Salle 05 Building: Q, 1st floor, Zone: 11<br>Invited Session 109<br>Organizer: Coralia Cartis, University of Oxford, GB </b> </o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">1 - Condition numbers and weak average-case complexity in optimization<o:p></o:p></span><span style="font-size:10.0pt"><br>Speaker: Martin Lotz, The University of Manchester, GB, talk 957 <o:p></o:p></span><span style="font-size:10.0pt"><br>Co-Authors: Dennis Amelunxen, Jake Walvin<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">2 - A Long (Random) Walk Solves All Your (Linear) Problems <o:p></o:p></span><span style="font-size:10.0pt;color:red"><br>Speaker: Armin Eftekhari, Alan Turing Institute, GB, talk 1199<o:p></o:p></span><span style="font-size:10.0pt;color:red">
Co-Authors: Martin Lotz, Peter Richtárik<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">3 - Manifold lifting: problems and methods <o:p></o:p></span><span style="font-size:10.0pt"><br>Speaker: Florentin Goyens, Oxford University, GB, talk 1182 <o:p></o:p></span><span style="font-size:10.0pt">
Co-Authors: Armin Eftekhari, Coralia Cartis, Greg Ongie<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">4 - Sparse non-negative super-resolution: simplified and stabilized<o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Jared Tanner, University of Oxford, GB, talk 1462
Co-Authors: Armin Eftekhari, Andrew Thompson, Bogdan Toader, Hemant Tyagi <o:p></o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><b style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt"><o:p></o:p></span></b><b style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt">Recent advances in first-order algorithms for non-smooth optimization (Continuous Optimization)<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
NonSmooth - We 8:30am-10:30am, Format: 4x30 min<br>Room: Salle LC4 Building: L, Intermediate 1, Zone: 9 <o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
Invited Session 198<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
Organizer: Thomas Pock, Graz University of Technology, AT <o:p></o:p></span></b> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">1 - Non-smooth Non-convex Bregman Minimization: Unification and New Algorithms<o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Peter Ochs, Saarland University, DE, talk 134 <o:p></o:p></span><span style="font-size:10.0pt">
Co-Authors: Jalal Fadili <o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">2 - Primal-dual algorithm for linearly constrained optimization problem<o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Yura Malitsky, Univeristy of Göttingen, DE, talk 155 <o:p></o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">3 - Stochastic PDHG with Arbitrary Sampling and Applications to Medical Imaging<o:p></o:p></span><span style="font-size:10.0pt;color:red">
Speaker: Matthias Ehrhardt, University of Cambridge, GB, talk 127 <o:p></o:p></span><span style="font-size:10.0pt;color:red">
Co-Authors: Carola Schoenlieb, Peter Richtárik, Antonin Chambolle<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">4 - Acceleration and global convergence of the NL- PDHGM<o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Stanislav Mazurenko, University of Liverpool, GB, talk 1655 <o:p></o:p></span><span style="font-size:10.0pt">
Co-Authors: Tuomo Valkonen, C. Clason<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"><o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">Fast Converging Stochastic Optimization Algorithms (Continuous Optimization)<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
RandomM - We 3:15pm-4:45pm, Format: 3x30 min <br>Room: Salle KC6 Building: K, Intermediate 1, Zone: 10 <o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
Invited Session 213<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
Organizer: Francis Bach, INRIA - ENS, FR&nbsp;</span></b> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">1 - Bridging the Gap between Constant Step Size SGD and Markov Chains<o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Aymeric Dieuleveut, EPFL, CH, talk 1102 <o:p></o:p></span><span style="font-size:10.0pt">
Co-Authors: Alain Durmus, Francis Bach <o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">2 - Stochastic Optimization for Large Scale Optimal Transport</span><span style="font-size:10.0pt">
Speaker: Aude Genevay, ENS, FR, talk 888<o:p></o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">3 - Variance Reduced Methods via Sketching&nbsp;</span><span style="font-size:10.0pt;color:red">
Speaker: Robert Gower, Telecom Paristech, FR, talk 859 <o:p></o:p></span><span style="font-size:10.0pt;color:red">
Co-Authors: Peter Richtárik, Francis Bach<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"><o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">Non-Convex and Second-order Methods in Machine Learning (Continuous Optimization)<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
RandomM - We 5:00pm-6:30pm, Format: 4x20 min <br>Room: Salle KC6 Building: K, Intermediate 1, Zone: 10 <o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b>
<b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">Invited Session 33</span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
Organizer: Martin Takáč, Lehigh University, US <o:p></o:p></span></b> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">1 - Escaping Saddles with Stochastic Algorithms <o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Aurelien Lucchi, ETH Zurich, CH, talk 531<o:p></o:p></span></p>
          <p class="MsoNormal" style="mso-outline-level:1"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">2 - Convergence Rate of Expectation-Maximization <o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Reza Babanezhad, UBC, CA, talk 1135 <o:p></o:p></span><span style="font-size:10.0pt">
Co-Authors: Raunak Kumar, Mark Schmidt <o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">3 - Parameter-free nonsmooth convex stochastic optimization through coin betting<o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Francesco Orabona, Stony Brook University, US, talk 1108<o:p></o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">4 - SGD and Hogwild! Convergence Without the Bounded Gradients Assumption<o:p></o:p></span><span style="font-size:10.0pt;color:red">
Speaker: Martin Takáč, Lehigh University, US, talk 1342 <o:p></o:p></span><span style="font-size:10.0pt;color:red">
Co-Authors: Lam Nguyen, Phuong Nguyen, Marten van Dijk, Peter Richtárik, Katya Scheinberg <o:p></o:p></span> </p>
          <p class="MsoNormal"><b style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt"></span></b><b style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt">First-order methods for large-scale convex problems (Specific Models, Algorithms, and Software Learning) <o:p></o:p></span></b><b style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt">
Th 8:30am-10:30am, Format: 4x30 min <br>Room: FABRE Building: J, Ground Floor, Zone: 8 <o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt"><br>Invited Session 316<o:p></o:p></span></b><b style="mso-bidi-font-weight:normal"><span style="font-size:10.0pt">
Organizer: Stephen Vavasis, University of Waterloo, CA <o:p></o:p></span></b></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">1 - A single potential governing convergence of CG, AG and geometric descent<o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Stephen Vavasis, University of Waterloo, CA, talk 582 <o:p></o:p></span><span style="font-size:10.0pt">
Co-Authors: Sahar Karimi<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">2 - Robust Accelerated Gradient Method<o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Mert Gurbuzbalaban, Rutgers University, US, talk 1106<o:p></o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">3 - Randomized methods for convex feasibility problems and applications to ML<o:p></o:p></span><span style="font-size:10.0pt;color:red">
Speaker: Peter Richtárik, KAUST, SA, talk 385<o:p></o:p></span><span style="font-size:10.0pt;color:red">
Co-Authors: Ion Necoara, Andrei Patrascu<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">4 - Bregman Divergence for Stochastic Variance Reduction<o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Yaoliang Yu, University of Waterloo, CA, talk 937 <o:p></o:p></span><span style="font-size:10.0pt">
Co-Authors: Xinhua Zhang, Zhan Shi<o:p></o:p></span><span style="font-size:10.0pt"><o:p> </o:p></span><b><span style="font-size:10.0pt"></span><span style="mso-bidi-font-weight: normal"><span style="font-size:10.0pt"></span></span></b></p>
          <p class="MsoNormal"><b><span style="mso-bidi-font-weight:
normal"><span style="font-size:10.0pt">Recent Advances in Coordinate Descent and Constrained Problems (Continuous Optimization)<br>RandomM - Fr 9:00am-10:30am, Format: 3x30 min
Room: Salle KC6 Building: K, Intermediate 1, Zone: 10
Invited Session 208
Organizer: Ion Necoara, Univ. Politehnica Bucharest, RO</span></span></b><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red"></span> </p>
          <span style="font-size:10.0pt; color:red"></span>
          <p class="MsoNormal"><span style="font-size:10.0pt; color:red">1 - Convergence Analysis of Inexact Randomized Iterative Methods<o:p></o:p></span><span style="font-size:10.0pt;color:red">
Speaker: Nicolas Loizou, University of Edinburgh, GB, talk 835 <o:p></o:p></span><span style="font-size:10.0pt;color:red">
Co-Authors: Peter Richtárik<o:p></o:p></span> </p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt; color:red">2 - A Stochastic Penalty Model for Optimization with Many Convex Constraints<o:p></o:p></span><span style="font-size:10.0pt;color:red">
Speaker: Konstantin Mishchenko, KAUST, SA, talk 1264
Co-Authors: Peter Richtárik, Ion Necoara<o:p></o:p></span></p>
          <p class="MsoNormal"><span style="font-size:10.0pt"><o:p></o:p></span><span style="font-size:10.0pt">3 - Random coordinate descent methods for linearly constrained convex optimization<o:p></o:p></span><span style="font-size:10.0pt">
Speaker: Ion Necoara, Univ. Politehnica Bucharest, RO, talk 809
</span><span style="font-size:10.0pt">Co-Authors: Martin Takáč </span></p>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 29, 2018</h3>
          <h1> Best DS3 poster award for Samuel Horváth</h1>
          <br>
          <span class="important"></span><a
            href="https://vcc.kaust.edu.sa/Pages/Horvath.aspx">Samuel
            Horváth</a> attended the <a
            href="http://www.ds3-datascience-polytechnique.fr">Data
            Science Summer School</a> (<span style="box-sizing:
            border-box; margin: 0px; padding: 0px; border: 0px; outline:
            0px; font-size: 14px; vertical-align: baseline; caret-color:
            rgb(102, 102, 102); color: rgb(102, 102, 102); font-family:
            &quot;Open Sans&quot;, Helvetica, Arial, Lucida, sans-serif;
            font-style: normal; font-variant-caps: normal;
            letter-spacing: normal; orphans: auto; text-align: center;
            text-indent: 0px; text-transform: none; white-space: normal;
            widows: auto; word-spacing: 0px; -webkit-text-size-adjust:
            auto; -webkit-text-stroke-width: 0px; text-decoration: none;
            font-weight: 400; background-position: 0px 0px;
            background-repeat: initial initial;">DS</span><span
            style="box-sizing: border-box; margin: 0px; padding: 0px;
            border: 0px; outline: 0px; font-size: 14px; vertical-align:
            baseline; caret-color: rgb(102, 102, 102); color: rgb(102,
            102, 102); font-family: &quot;Open Sans&quot;, Helvetica,
            Arial, Lucida, sans-serif; font-style: normal;
            font-variant-caps: normal; letter-spacing: normal; orphans:
            auto; text-align: center; text-indent: 0px; text-transform:
            none; white-space: normal; widows: auto; word-spacing: 0px;
            -webkit-text-size-adjust: auto; -webkit-text-stroke-width:
            0px; text-decoration: none; font-weight: 400;
            background-position: 0px 0px; background-repeat: initial
            initial;"><sup style="box-sizing: border-box; margin: 0px;
              padding: 0px; border: 0px; outline: 0px; font-size:
              smaller; vertical-align: super; position: relative;
              height: 0px; line-height: 1; bottom: 0.3em !important;
              background-position: 0px 0px; background-repeat: initial
              initial;">3</sup></span>), which took place during June
          25-29, 2018, at École Polytechnique in Paris, France. Based on
          the <a href="http://www.ds3-datascience-polytechnique.fr">event

























































































































































































            website</a>, the event gathered 500 participants from 34
          countries and 6 continents, out of which 290 were MS and PhD
          students and postdocs, and 110 professionals. Selected
          guest/speaker names (out of 41): Cédric Villani, Nicoló
          Cesa-Bianchi, Mark Girolami, Yann Lecun, Suvrit Sra,
          Jean-Philippe Vert, Adrian Weller, Marco Cuturi, Arthur
          Gretton, and Andreas Krause.<br>
          <br>
          <span style="box-sizing: border-box; margin: 0px; padding:
            0px; border: 0px; outline: 0px; font-size: 14px;
            vertical-align: baseline; caret-color: rgb(102, 102, 102);
            color: rgb(102, 102, 102); font-family: &quot;Open
            Sans&quot;, Helvetica, Arial, Lucida, sans-serif;
            font-style: normal; font-variant-caps: normal;
            letter-spacing: normal; orphans: auto; text-align: center;
            text-indent: 0px; text-transform: none; white-space: normal;
            widows: auto; word-spacing: 0px; -webkit-text-size-adjust:
            auto; -webkit-text-stroke-width: 0px; text-decoration: none;
            font-weight: 400; background-position: 0px 0px;
            background-repeat: initial initial;"></span>The event also
          included a best-poster competition, with an impressive total
          of <a
            href="http://www.ds3-datascience-polytechnique.fr/posters/">170

























































































































































































            posters</a>. Samuel's poster won the <span
            class="important">Best DS</span><span class="important"><font
              color="#ff9966"><span style="box-sizing: border-box;
                margin: 0px; padding: 0px; border: 0px none; outline:
                0px none; font-size: 14px; vertical-align: baseline;
                font-family: &quot;Open
                Sans&quot;,Helvetica,Arial,Lucida,sans-serif;
                font-style: normal; font-variant-caps: normal;
                letter-spacing: normal; text-align: center; text-indent:
                0px; text-transform: none; white-space: normal;
                word-spacing: 0px; -moz-text-size-adjust: auto;
                -webkit-text-stroke-width: 0px; text-decoration: none;
                font-weight: 400; background-position: 0px 0px;"></span><span
                style="box-sizing: border-box; margin: 0px; padding:
                0px; border: 0px none; outline: 0px none; font-size:
                14px; vertical-align: baseline; font-family: &quot;Open
                Sans&quot;,Helvetica,Arial,Lucida,sans-serif;
                font-style: normal; font-variant-caps: normal;
                letter-spacing: normal; text-align: center; text-indent:
                0px; text-transform: none; white-space: normal;
                word-spacing: 0px; -moz-text-size-adjust: auto;
                -webkit-text-stroke-width: 0px; text-decoration: none;
                font-weight: 400; background-position: 0px 0px;"><sup
                  style="box-sizing: border-box; margin: 0px; padding:
                  0px; border: 0px; outline: 0px; font-size: smaller;
                  vertical-align: super; position: relative; height:
                  0px; line-height: 1; bottom: 0.3em !important;
                  background-position: 0px 0px; background-repeat:
                  initial initial;">3</sup></span></font> Poster Award.</span><font
            color="#ff9966"> </font>The poster, entitled <i>Nonconvex
            variance reduced optimization with arbitrary sampling</i>,
          is based on a paper of the same title, joint work with me, and
          currently under review.<br>
          <br>
          Here is the poster:<br>
          <br>
          <a
href="http://www.ds3-datascience-polytechnique.fr/wp-content/uploads/2018/06/DS3-342.pdf"><img
              src="posters/Poster-DS3-small.png" alt="Poster"
              width="700" border="2" height="526"></a><br>
          <br>
          And here is the award:<br>
          <br>
          <img src="docs/best_poster_DS3.jpg" alt="" width="600"
            height="348"><br>
          <br>
          This first prize carries a 500 EUR cash award.<br>
          <br>
          Samuel: Congratulations!!! <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>June 18, 2018</h3>
          <h1>I am visiting Edinburgh</h1>
          <br>
          <span class="important"></span>I am now in Edinburgh for a
          week. On Tuesday, I am giving a talk in the <a
            href="http://www.anc.ed.ac.uk/events/aggregator">ANC Seminar</a>
          (School of Informatics), and on Wednesday I am giving the same
          talk in the <a
            href="http://www.maths.ed.ac.uk/ERGO/abstracts/2018-06-richtarik.html">ERGO

























































































































































































            Seminar</a> (School of Mathematics). <br>
          <br>
          <span style="font-style: italic;"> </span> <span
            style="font-style: italic;"> <br>
          </span>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>June 15, 2018</h3>
          <h1>New paper</h1>
          <br>
          <span class="important"></span><span class="important">New
            paper out: </span><a
            href="https://arxiv.org/abs/1806.05633">"Improving SAGA via
            a probabilistic interpolation with gradient descent"</a> -
          joint work with <a href="http://www.adelbibi.com">Adel Bibi</a>,
          <a href="https://vcc.kaust.edu.sa/Pages/Sailanbayev.aspx">Alibek

























































































































































































            Sailanbayev</a>, <a href="http://www.bernardghanem.com">Bernard

























































































































































































            Ghanem</a> and <a
            href="https://perso.telecom-paristech.fr/rgower/">Robert
            Mansel Gower</a>.<br>
          <br>
          Abstract:<span style="font-style: italic;"> We develop and
            analyze a new algorithm for empirical risk minimization,
            which is the key paradigm for training supervised machine
            learning models. Our method---SAGD---is based on a
            probabilistic interpolation of SAGA and gradient descent
            (GD). In particular, in each iteration we take a gradient
            step with probability $q$ and a SAGA step with probability
            $1−q$. We show that, surprisingly, the total expected
            complexity of the method (which is obtained by multiplying
            the number of iterations by the expected number of gradients
            computed in each iteration) is minimized for a non-trivial
            probability $q$. For example, for a well conditioned problem
            the choice $q=1/(n−1)^2$, where $n$ is the number of data
            samples, gives a method with an overall complexity which is
            better than both the complexity of GD and SAGA. We further
            generalize the results to a probabilistic interpolation of
            SAGA and minibatch SAGA, which allows us to compute both the
            optimal probability and the optimal minibatch size. While
            the theoretical improvement may not be large, the practical
            improvement is robustly present across all synthetic and
            real data we tested for, and can be substantial. Our
            theoretical results suggest that for this optimal minibatch
            size our method achieves linear speedup in minibatch size,
            which is of key practical importance as minibatch
            implementations are used to train machine learning models in
            practice. This is the first time linear speedup in minibatch
            size is obtained for a variance reduced gradient-type method
            by directly solving the primal empirical risk minimization
            problem.</span><br>
          <span style="font-style: italic;"> </span> <span
            style="font-style: italic;"> <br>
            <br>
          </span>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>June 10, 2018</h3>
          <h1>10th traditional youth school in control, information
            &amp; optimization</h1>
          <br>
          <span class="important"></span>I am in Voronovo, Russia,
          attending the Traditional Youth School in <a
            href="https://cs.hse.ru/tradschool/2018/">"Control,
            Information and Optimization"</a> organized by <a
            href="http://lab7.ipu.ru/eng/people/polyak.html">Boris
            Polyak</a>&nbsp; and Elena Gryazina. This is the 10th
          edition of the school. I will be teaching a 3h module on
          stochastic methods in optimization and machine learning.<br>
          <br>
          <i>Update 1: </i>Slides from my two talks: <a
            href="talks/2018%20Voronovo%20-%20TALK%201.pdf">TALK 1</a>,
          <a href="talks/2018%20Voronovo%20-%20TALK%202.pdf">TALK 2</a>.<i><br>
            <br>
          </i> <i>Update 2:</i> <a
            href="https://www.hse.ru/en/staff/nikitadoikov">Nikita
            Doikov</a> won the Best Talk Award for the paper <a
            href="https://arxiv.org/abs/1802.04084">"Randomized Block
            Cubic Newton Method"</a>, to appear in ICML 2018.<br>
          <span style="font-style: italic;"> </span> <span
            style="font-style: italic;"> <br>
            <br>
          </span>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>June 1, 2018</h3>
          <h1>Jingwei Liang @ KAUST</h1>
          <br>
          <span class="important"></span><a
            href="http://www.damtp.cam.ac.uk/people/jl993/">Jingwei
            Liang</a> (Cambridge) is visiting me at KAUST.<br>
          <br>
          <span style="font-style: italic;"> </span> <span
            style="font-style: italic;"> <br>
          </span>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>May 21, 2018</h3>
          <h1>Adil Salim @ KAUST</h1>
          <br>
          <span class="important"></span><a
            href="https://adil-salim.github.io">Adil Salim</a> (Télécom
          ParisTech) is visiting me at KAUST this week.<br>
          <br>
          <span style="font-style: italic;"> </span> <span
            style="font-style: italic;"> <br>
          </span>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>May 21, 2018</h3>
          <h1>New paper</h1>
          <br>
          <span class="important"></span><span class="important">New
            paper out: </span><a
            href="https://arxiv.org/abs/1805.07962">"A nonconvex
            projection method for robust PCA"</a> - joint work with <a
            href="https://vcc.kaust.edu.sa/Pages/Dutta.aspx">Aritra
            Dutta</a> and <a
            href="https://vcc.kaust.edu.sa/Pages/Hanzely.aspx">Filip
            Hanzely</a>.<br>
          <br>
          Abstract:<span style="font-style: italic;"> Robust principal
            component analysis (RPCA) is a well-studied problem with the
            goal of decomposing a matrix into the sum of low-rank and
            sparse components. In this paper, we propose a nonconvex
            feasibility reformulation of RPCA problem and apply an
            alternating projection method to solve it. To the best of
            our knowledge, we are the first to propose a method that
            solves RPCA problem without considering any objective
            function, convex relaxation, or surrogate convex
            constraints. We demonstrate through extensive numerical
            experiments on a variety of applications, including shadow
            removal, background estimation, face detection, and galaxy
            evolution, that our approach matches and often significantly
            outperforms current state-of-the-art in various ways.</span><br>
          <br>
          <span style="font-style: italic;"> </span> <span
            style="font-style: italic;"> <br>
          </span>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>May 19, 2018</h3>
          <h1> NIPS deadline over!</h1>
          <br>
          <span class="important"></span>The NIPS deadline is over now.
          Me and my group members will probably spend a few days
          sleeping...<span style="caret-color: rgb(51, 51, 51); color:
            rgb(51, 51, 51); font-family: Verdana, Arial,
            &quot;Trebuchet MS&quot;, sans-serif, Georgia, Courier,
            &quot;Times New Roman&quot;, serif; font-size: 14px;
            font-style: normal; font-variant-caps: normal; font-weight:
            normal; letter-spacing: normal; orphans: auto; text-align:
            start; text-indent: 0px; text-transform: none; white-space:
            normal; widows: auto; word-spacing: 0px;
            -webkit-text-size-adjust: auto; -webkit-text-stroke-width:
            0px; background-color: rgb(255, 255, 255); text-decoration:
            none; display: inline !important; float: none;"><span
              class="Apple-converted-space"><br>
            </span></span><br>
          <span style="font-style: italic;"> </span> <span
            style="font-style: italic;"> <br>
          </span>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>May 11, 2018</h3>
          <h1> Two paper accepted to ICML</h1>
          <br>
          <span class="important"></span>We have got two papers accepted
          to ICML 2018:<br>
          <br>
          1) <a href="https://arxiv.org/abs/1802.04084" style="margin:
            0px; padding: 0px; color: rgb(51, 102, 153);
            text-decoration: none; font-family: Verdana, Arial,
            &quot;Trebuchet MS&quot;, sans-serif, Georgia, Courier,
            &quot;Times New Roman&quot;, serif; font-size: 14px;
            font-style: normal; font-variant-caps: normal; font-weight:
            normal; letter-spacing: normal; orphans: auto; text-align:
            start; text-indent: 0px; text-transform: none; white-space:
            normal; widows: auto; word-spacing: 0px;
            -webkit-text-size-adjust: auto; -webkit-text-stroke-width:
            0px;">Randomized block cubic Newton method</a> (with Nikita
          Doikov)<br>
          <br>
          2) <a href="https://arxiv.org/abs/1802.03801" style="margin:
            0px; padding: 0px; color: rgb(51, 102, 153);
            text-decoration: none; font-family: Verdana, Arial,
            &quot;Trebuchet MS&quot;, sans-serif, Georgia, Courier,
            &quot;Times New Roman&quot;, serif; font-size: 14px;
            font-style: normal; font-variant-caps: normal; font-weight:
            normal; letter-spacing: normal; orphans: auto; text-align:
            start; text-indent: 0px; text-transform: none; white-space:
            normal; widows: auto; word-spacing: 0px;
            -webkit-text-size-adjust: auto; -webkit-text-stroke-width:
            0px;">SGD and Hogwild! convergence without the bounded
            gradients assumption</a><span style="caret-color: rgb(51,
            51, 51); color: rgb(51, 51, 51); font-family: Verdana,
            Arial, &quot;Trebuchet MS&quot;, sans-serif, Georgia,
            Courier, &quot;Times New Roman&quot;, serif; font-size:
            14px; font-style: normal; font-variant-caps: normal;
            font-weight: normal; letter-spacing: normal; orphans: auto;
            text-align: start; text-indent: 0px; text-transform: none;
            white-space: normal; widows: auto; word-spacing: 0px;
            -webkit-text-size-adjust: auto; -webkit-text-stroke-width:
            0px; background-color: rgb(255, 255, 255); text-decoration:
            none; display: inline !important; float: none;"><span
              class="Apple-converted-space"> (with </span></span><span
            style="caret-color: rgb(51, 51, 51); color: rgb(51, 51, 51);
            font-family: Verdana, Arial, &quot;Trebuchet MS&quot;,
            sans-serif, Georgia, Courier, &quot;Times New Roman&quot;,
            serif; font-size: 14px; font-style: normal;
            font-variant-caps: normal; font-weight: normal;
            letter-spacing: normal; orphans: auto; text-align: start;
            text-indent: 0px; text-transform: none; white-space: normal;
            widows: auto; word-spacing: 0px; -webkit-text-size-adjust:
            auto; -webkit-text-stroke-width: 0px; background-color:
            rgb(255, 255, 255); text-decoration: none; display: inline
            !important; float: none;"><span
              class="Apple-converted-space"><span style="caret-color:
                rgb(51, 51, 51); color: rgb(51, 51, 51); font-family:
                Verdana, Arial, &quot;Trebuchet MS&quot;, sans-serif,
                Georgia, Courier, &quot;Times New Roman&quot;, serif;
                font-size: 14px; font-style: normal; font-variant-caps:
                normal; font-weight: normal; letter-spacing: normal;
                orphans: auto; text-align: start; text-indent: 0px;
                text-transform: none; white-space: normal; widows: auto;
                word-spacing: 0px; -webkit-text-size-adjust: auto;
                -webkit-text-stroke-width: 0px; background-color:
                rgb(255, 255, 255); text-decoration: none; display:
                inline !important; float: none;">Lam M. Nguyen, Phuong
                Ha Nguyen, Marten van Dijk, Katya Scheinberg and Martin
                Takáč</span>)<br>
            </span></span><br>
          <span style="font-style: italic;"> </span> <span
            style="font-style: italic;"> <br>
          </span>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>May 1, 2018</h3>
          <h1> New paper</h1>
          <br>
          <span class="important"></span><span class="important">New
            paper out: </span><a
            href="https://arxiv.org/abs/1805.02632">"Stochastic
            quasi-gradient methods: variance reduction via Jacobian
            sketching"</a> - joint work with <a
            href="https://perso.telecom-paristech.fr/rgower/">Robert
            Gower</a> and <a href="http://www.di.ens.fr/%7Efbach/">Francis

























































































































































































            Bach.</a><br>
          <br>
          Abstract:<span style="font-style: italic;"> We develop a new
            family of variance reduced stochastic gradient descent
            methods for minimizing the average of a very large number of
            smooth functions. Our method---JacSketch---is motivated by
            novel developments in randomized numerical linear algebra,
            and operates by maintaining a stochastic estimate of a
            Jacobian matrix composed of the gradients of individual
            functions. In each iteration, JacSketch efficiently updates
            the Jacobian matrix by first obtaining a random linear
            measurement of the true Jacobian through (cheap) sketching,
            and then projecting the previous estimate onto the solution
            space of a linear matrix equation whose solutions are
            consistent with the measurement. The Jacobian estimate is
            then used to compute a variance-reduced unbiased estimator
            of the gradient, followed by a stochastic gradient descent
            step. Our strategy is analogous to the way quasi-Newton
            methods maintain an estimate of the Hessian, and hence our
            method can be seen as a stochastic quasi-gradient method.
            Indeed, quasi-Newton methods project the current Hessian
            estimate onto a solution space of a linear equation
            consistent with a certain linear (but non-random)
            measurement of the true Hessian. Our method can also be seen
            as stochastic gradient descent applied to a controlled
            stochastic optimization reformulation of the original
            problem, where the control comes from the Jacobian
            estimates.<br>
            <br>
            We prove that for smooth and strongly convex functions,
            JacSketch converges linearly with a meaningful rate dictated
            by a single&nbsp; convergence theorem which applies to
            general sketches. We also provide a refined convergence
            theorem which applies to a smaller class of sketches,
            featuring a novel proof technique based on a stochastic
            Lyapunov function. This enables us to obtain sharper
            complexity results for variants of JacSketch with importance
            sampling. By specializing our general approach to
            specific&nbsp; sketching strategies, JacSketch reduces to
            the celebrated stochastic average gradient (SAGA) method,
            and its several existing and many new minibatch, reduced
            memory, and importance sampling variants. Our rate for SAGA
            with importance sampling is the current best-known rate for
            this method, resolving a conjecture by Schmidt et al (2015).
            The rates we obtain for minibatch SAGA are also superior to
            existing rates. Moreover, we obtain the first minibatch SAGA
            method with importance sampling.</span><br>
          <br>
          <span style="font-style: italic;"> </span> <span
            style="font-style: italic;"> <br>
          </span>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>April 29, 2018</h3>
          <h1> Seminar talks at University of Birmingham and Warwick</h1>
          <br>
          <span class="important"></span>I am on my way to Birmingham,
          and then Coventry. I will be giving a talk at the <a
href="https://warwick.ac.uk/fac/cross_fac/dimap/seminars/#010518Richtarik">DIMAP</a>
          seminar (DIMAP = Centre for Discrete Mathematics and its
          Applications), <a href="https://warwick.ac.uk">University of
            Warwick</a>, on "Stochastic Quasi-Gradient Methods: Variance
          Reduction via Jacobian Sketching". The talk is based on joint
          work with <a
            href="https://perso.telecom-paristech.fr/rgower/">Robert M.
            Gower</a> and <a href="http://www.di.ens.fr/%7Efbach/">Francis

























































































































































































            Bach</a>.<br>
          <br>
          <span style="font-style: italic;"> </span> <span
            style="font-style: italic;"> <br>
          </span>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3> </h3>
          <h3><br>
          </h3>
          <h3>April 25, 2018</h3>
          <h1>Teaching Saudi math olympiad contestants</h1>
          <br>
          <span class="important"></span>Today and tomorrow I am
          teaching a mini-course on "Optimization for Machine Learning"<span
            style="font-style: italic;"> </span>for students from
          various Saudi universities who were previously contestants<span
            style="font-style: italic;"> </span>in <a
            href="https://en.wikipedia.org/wiki/KFUPM_mathematics_olympiad">Saudi


























































































































































National



Mathematical

























































































































































































            Olympiad</a><span style="font-style: italic;"> </span>and/or
























































































































































































          <a
            href="https://www.imo-official.org/country_individual_r.aspx?code=SAU">IMO</a>.<span
            style="font-style: italic;"> </span>Several<span
            style="font-style: italic;"> </span>current contestants are
          attending as well.<span style="font-style: italic;"> </span><br>
          <br>
          This is a collaborative effort with <a
            href="https://www.kaust.edu.sa/en/study/faculty/diogo-gomes">Diogo

























































































































































































            Gomes</a>, who is teaching a "Mathematica" mini-course. <br>
          <br>
          <span style="font-style: italic;"> </span> <span
            style="font-style: italic;"> <br>
          </span>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>April 18, 2018</h3>
          <h1>New paper</h1>
          <br>
          <span class="important"></span><span class="important">New
            paper out: </span><a
            href="https://arxiv.org/abs/1804.06252">"Weighted low-rank
            approximation of matrices and background modeling"</a> -
          joint work with <a
            href="https://vcc.kaust.edu.sa/Pages/Dutta.aspx">Aritra
            Dutta</a> and <a
            href="https://sciences.ucf.edu/math/people/li-xin-2/">Xin Li</a>.<br>
          <br>
          Abstract:<span style="font-style: italic;"> We primarily study
            a special a weighted low-rank approximation of matrices and
            then apply it to solve the background modeling problem. We
            propose two algorithms for this purpose: one operates in the
            batch mode on the entire data and the other one operates in
            the batch-incremental mode on the data and naturally
            captures more background variations and computationally more
            effective. Moreover, we propose a robust technique that
            learns the background frame indices from the data and does
            not require any training frames. We demonstrate through
            extensive experiments that by inserting a simple weight in
            the Frobenius norm, it can be made robust to the outliers
            similar to the L1 norm. Our methods match or outperform
            several state-of-the-art online and batch background
            modeling methods in virtually all quantitative and
            qualitative measures.<br>
            <br>
          </span> <span style="font-style: italic;"> <br>
          </span>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <h3><br>
          </h3>
          <h3>April 16, 2018</h3>
          <h1>I am giving a seminar talk @ KAUST</h1>
          <br>
          I am giving a talk today at the CS Graduate Seminar at KAUST.
          I will be talking about <a
            href="https://arxiv.org/abs/1801.04873">"Randomized Methods
            for Convex Feasibility Problems"</a>. This is joint work
          with Ion Necoara and Andrei Patrascu.<br>
          <span style="font-style: italic;"><br>
            <br>
          </span>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="36"></h3>
          <br>
          <h3>April 5, 2018</h3>
          <h1>Postdoc and research scientist vacancies</h1>
          <br>
          My lab has openings for&nbsp;<span class="important">postdoc</span>
          (straight after PhD, or a few years after PhD) and&nbsp;<span
            class="important">research scientist</span> (several to many
          years after PhD; similar to a RS position at big data
          companies such as Google, Microsoft Research, Amazon, Baidu,
          Tencent, Facebook) positions. <br>
          <br>
          <span style="font-weight: bold;">Relevant areas:</span>
          machine learning theory, optimization, algorithms, high
          performance computing, deep learning, randomized and
          stochastic algorithms, federated learning, computer vision,
          machine learning systems, data science, applied mathematics,
          theoretical computer science. Contact me by email if
          interested. Please send your CV (including publication
          record), a brief statement of interest, 3 reference letters
          (and PhD transcript for postdoc applicants).<br>
          <br>
          <span style="font-weight: bold;">Place of work:</span> <a
            href="https://www.kaust.edu.sa/en">KAUST</a>. Outstanding
          working conditions.<br>
          <br>
          <span style="font-weight: bold;">Starting date:</span> Fall
          2018 (flexible). <br>
          <br>
          <span style="font-weight: bold;">Contract duration:</span>
          based on agreement (e.g., 1-3 years).<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>April 2, 2018</h3>
          <h1> Dominik's PhD thesis online</h1>
          <br>
          <a href="http://www.dominikcsiba.com">Dominik Csiba's</a> PhD
          thesis <a href="https://arxiv.org/abs/1804.00437">"Data
            sampling strategies in stochastic algorithms for empirical
            risk minimization"</a> is online now. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>March 2, 2018</h3>
          <h1>Vacation</h1>
          <br>
          I am on vacation this week. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>March 23, 2018</h3>
          <h1>Konstantin and Filip @ INFORMS Opt</h1>
          <br>
          <a href="https://konstmish.github.io">Konstantin</a> and <a
            href="https://vcc.kaust.edu.sa/Pages/Hanzely.aspx">Filip</a>
          are attending the <a
href="https://www.informs.org/Meetings-Conferences/INFORMS-Conference-Calendar/2018-INFORMS-Optimization-Society-Conference">2018
INFORMS



Optimization

























































































































































































            Society Conference</a> in Denver, Colorado. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>March 21, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important"></span><span class="important">New
            paper out: </span><a
            href="https://arxiv.org/abs/1803.07374">"Fastest rates for
            stochastic mirror descent methods"</a> - joint work with <a
            href="https://vcc.kaust.edu.sa/Pages/Hanzely.aspx">Filip
            Hanzely</a>.<br>
          <br>
          Abstract:<span style="font-style: italic;"> Relative
            smoothness - a notion introduced by Birnbaum et al. (2011)
            and rediscovered by Bauschke et al. (2016) and Lu et al.
            (2016) - generalizes the standard notion of smoothness
            typically used in the analysis of gradient type methods. In
            this work we are taking ideas from well studied field of
            stochastic convex optimization and using them in order to
            obtain faster algorithms for minimizing relatively smooth
            functions. We propose and analyze two new algorithms:
            Relative Randomized Coordinate Descent (relRCD) and Relative
            Stochastic Gradient Descent (relSGD), both generalizing
            famous algorithms in the standard smooth setting. The
            methods we propose can be in fact seen as variants of
            stochastic mirror descent. One of them, relRCD is the first
            stochastic mirror descent algorithm with a linear
            convergence rate. </span> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>March 18, 2018</h3>
          <h1> A Student from TUM Doing her MS Thesis Under my
            Supervision</h1>
          <br>
          <span class="important"></span>Sarah Sachs, a master student
          from <a href="https://www.tum.de/en/homepage/">Technical
            University Munich (TUM)</a>, arrived at KAUST today. She
          will spend six months at KAUST (until early September) as a
          visiting student in my group, and will write her master's
          thesis under my supervision. In her thesis she is focusing on
          randomized optimization algorithms. Welcome!<br>
          <br>
          Sarah's bachelor thesis at TUM focused on approximation of the
          infimal convolution for non-convex functions. She&nbsp;
          previously worked on finding efficiently computable stopping
          criteria for ADMM and the Chambolle-Pock algorithm applied to
          LP relaxations of ILPs with integral extreme points. She is
          generally interested in optimization with applications to
          computer vision.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>March 4, 2018</h3>
          <h1>Konstantin @ Cambridge &amp; Vatican<br>
          </h1>
          <br>
          <span class="important"></span><a
            href="https://konstmish.github.io">Konstantin Mishchenko</a>
          is visiting the Cambridge Image Analysis group of <a
            href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">Carola-Bibiane

























































































































































































            Schönlieb</a> at the University of Cambridge. During March
          8-11 he is participating in <a href="https://vhacks.org">VHacks</a>,
          the first ever hackathon at the Vatican. <br>
          <br>
          Aritra and El Houcine are also travelling. <br>
          <br>
          Update (March 19): Konstantin, El Houcine and Aritra are back.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 25, 2018</h3>
          <h1> Visiting "Matfyz"</h1>
          <br>
          <span class="important"></span>I am on my way to Bratislava,
          Slovakia. Tomorrow, I am giving a <a
href="https://fmph.uniba.sk/detail-novinky/back_to_page/fakulta-matematiky-fyziky-a-informatiky-uk/article/seminar-z-matematickej-statistiky-peter-richtarik-2622018/">statistics
seminar



talk

























































































































































































            at "Matfyz"</a> - School of Mathematics, Physics and
          Informatics, Comenius<span style="font-style: italic;"> </span>University.<span
            style="font-style: italic;"><br>
            <br>
          </span>Title: On stochastic algorithms in linear algebra,
          optimization and machine learning<br>
          Place: FMFI UK, M/XII<span style="font-style: italic;"><br>
          </span>Date:<span style="font-style: italic;"> </span>Monday,
          February 26, 2018<span style="font-style: italic;"><br>
          </span>Time:<span style="font-style: italic;"> </span>09:50am<span
            style="font-style: italic;"><br>
            <br>
          </span>If anyone is interested in MS / PhD / postdocs /
          research scientist positions at <a
            href="https://www.kaust.edu.sa/en">KAUST</a>, I will be
          available to talk to you after the talk.<br>
          <span style="font-style: italic;"> <br>
          </span> <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 20, 2018</h3>
          <h1>Optimization &amp; Big Data 2018: Videos are Online</h1>
          <br>
          Videos of the talks from the <a
            href="https://obd.kaust.edu.sa">KAUST Research Workshop on
            Optimization and Big Data</a> are now available. <a
href="http://mediasite.kaust.edu.sa/Mediasite/Catalog/catalogs/mediasiteadmin-kaust-research-workshop-on-optimization-and-big-data---2018">They
can



be

























































































































































































            found here.</a><br>
          <br>
          Comment: At the moment the videos are accessible to KAUST
          community only, they will soon be available globally.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 13, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1802.03801">"SGD and Hogwild!
            convergence without the bounded gradients assumption"</a> -
          joint work with Lam M. Nguyen, Phuong Ha Nguyen, Marten van
          Dijk, Katya Scheinberg and Martin Takáč.<br>
          <br>
          Abstract:<span style="font-style: italic;"> Stochastic
            gradient descent (SGD) is the optimization algorithm of
            choice in many machine learning applications such as
            regularized empirical risk minimization and training deep
            neural networks. The classical analysis of convergence of
            SGD is carried out under the assumption that the norm of the
            stochastic gradient is uniformly bounded. While this might
            hold for some loss functions, it is always violated for
            cases where the objective function is strongly convex. In
            (Bottou et al., 2016) a new analysis of convergence of SGD
            is performed under the assumption that stochastic gradients
            are bounded with respect to the true gradient norm. Here we
            show that for stochastic problems arising in machine
            learning such bound always holds. Moreover, we propose an
            alternative convergence analysis of SGD with diminishing
            learning rate regime, which is results in more relaxed
            conditions that those in (Bottou et al., 2016). We then move
            on the asynchronous parallel setting, and prove convergence
            of the Hogwild! algorithm in the same regime, obtaining the
            first convergence results for this method in the case of
            diminished learning rate. </span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 12, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1802.04079">"Accelerated
            stochastic matrix inversion: general theory and speeding up
            BFGS rules for faster second-order optimization"</a> - joint
          work with Robert M. Gower, Filip Hanzely and Sebastian Stich.<br>
          <br>
          Abstract:<span style="font-style: italic;"> We present the
            first accelerated randomized algorithm for solving linear
            systems in Euclidean spaces. One essential problem of this
            type is the matrix inversion problem. In particular, our
            algorithm can be specialized to invert positive definite
            matrices in such a way that all iterates (approximate
            solutions) generated by the algorithm are positive definite
            matrices themselves. This opens the way for many
            applications in the field of optimization and machine
            learning. As an application of our general theory, we
            develop the first accelerated (deterministic and stochastic)
            quasi-Newton updates. Our updates lead to provably more
            aggressive approximations of the inverse Hessian, and lead
            to speed-ups over classical non-accelerated rules in
            numerical experiments. Experiments with empirical risk
            minimization show that our rules can accelerate training of
            machine learning models. </span> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 10, 2018</h3>
          <h1>New Paper</h1>
          <span class="important"><br>
            New paper out: </span><a
            href="https://arxiv.org/abs/1802.04084">"Randomized block
            cubic Newton method"</a> - joint work with Nikita Doikov.<br>
          <br>
          Abstract:<span style="font-style: italic;"> We study the
            problem of minimizing the sum of three convex functions: a
            differentiable, twice-differentiable and a non-smooth term
            in a high dimensional setting. To this effect we propose and
            analyze&nbsp; a randomized block&nbsp; cubic Newton (RBCN)
            method, which in each iteration builds a model of the
            objective function formed as the sum of the {\em natural}
            models of its three components: a linear model with a
            quadratic regularizer for the differentiable term, a
            quadratic model with a cubic regularizer for the twice
            differentiable term, and perfect (proximal)&nbsp; model for
            the nonsmooth term. Our method in each iteration minimizes
            the model over a random subset of&nbsp; blocks of the search
            variable. RBCN is the first algorithm with these properties,
            generalizing several existing methods, matching the best
            known bounds in all special cases. We establish ${\cal
            O}(1/\epsilon)$, ${\cal O}(1/\sqrt{\epsilon})$ and ${\cal
            O}(\log (1/\epsilon))$ rates under different assumptions on
            the component functions. Lastly, we show numerically that
            our method outperforms the state-of-the-art on a variety of
            machine learning problems, including cubically regularized
            least-squares, logistic regression with constraints, and
            Poisson regression. </span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 10, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1802.03703">"Stochastic spectral
            and conjugate descent methods"</a> - joint work with Dmitry
          Kovalev, <a
            href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard

























































































































































































            Gorbunov</a> and Elnur Gasanov.<br>
          <br>
          Abstract:<span style="font-style: italic;"> The
            state-of-the-art methods for solving optimization problems
            in big dimensions are variants of randomized coordinate
            descent (RCD). In this paper we introduce a fundamentally
            new type of acceleration strategy for RCD based on the
            augmentation of the set of coordinate directions by a few
            spectral or conjugate directions. As we increase the number
            of extra directions to be sampled from, the rate of the
            method improves, and interpolates between the linear rate of
            RCD and a linear rate independent of the condition number.
            We develop and analyze also inexact variants of these
            methods where the spectral and conjugate directions are
            allowed to be approximate only. We motivate the above
            development by proving several negative results which
            highlight the limitations of RCD with importance sampling.</span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 5, 2018</h3>
          <h1><span class="important"></span>Optimization &amp; Big Data
            2018 Started</h1>
          <span class="important"><br>
            OBD 2018 is starting!</span> The <a
            href="https://obd.kaust.edu.sa">KAUST Workshop on
            Optimization and Big Data</a> just started. We have 19
          amazing speakers and 21 deluxe e-posters lined up. <br>
          <br>
          Update (February 12): Thanks for all who participated in the
          workshop, thanks you to this was an excellent event! Group
          photos:<br>
          <br>
          <img style="width: 750px; height: 438px;" alt="KAUST Research
            Workshop on Optimization and Big Data"
            src="imgs/OBD2018a.jpg"><br>
          <br>
          <img style="width: 750px; height: 438px;" alt="KAUST Research
            Workshop on Optimization and Big Data"
            src="imgs/OBD2018b.jpg"><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 4, 2018</h3>
          <h1> Optimization &amp; Big Data 2018</h1>
          <br>
          <a href="https://obd.kaust.edu.sa">KAUST Research Workshop on
            Optimization and Big Data</a> is starting tomorrow! We have
          19 amazing speakers, and 21 deluxe poster talks and <a
            href="http://epostersonline.com/obd2018/">ePoster
            presentations</a>. <br>
          <br>
          This year, <a href="https://coral.ise.lehigh.edu/terlaky/">Tamás

























































































































































































            Terlaky (Lehigh)</a> is the keynote speaker. <br>
          <br>
          Thanks to the KAUST Office for Sponsored Research, The Alan
          Turing Institute and KICP.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>February 1, 2018</h3>
          <h1> Nicolas @ KAUST</h1>
          <br>
          <a href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a> is back at KAUST on a research visit. Welcome! <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 28, 2018</h3>
          <h1>Aritra, Alibek and Samuel @ EPFL</h1>
          <br>
          Aritra Dutta (postdoc), Alibek Sailanbayev (MS/PhD student)
          and Samuel Horvath (MS/PhD student) are attending <a
            href="https://www.appliedmldays.org">Applied Machine
            Learning Days</a> at EPFL, Lausanne, Switzerland. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 27, 2018</h3>
          <h1> Two new MS Students and a new Intern</h1>
          <br>
          Let me welcome Dmitry Kovalev and Elnur Gasanov (master
          students visiting from MIPT, Moscow) and Slavomír Hanzely
          (undergraduate student at Comenius University), who arrived at
          KAUST about a week ago and are working with me as interns.
          They will be here for about a month. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 18, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1801.05661">"A randomized
            exchange algorithm for computing optimal approximate designs
            of experiments"</a> - joint work with <a
            href="http://www.iam.fmph.uniba.sk/ospm/Harman/index-sk.htm">Radoslav

























































































































































































            Harman</a> and <a
            href="http://www.iam.fmph.uniba.sk/ospm/Filova/index-sk.htm">Lenka

























































































































































































            Filová</a>.<br>
          <br>
          Abstract:<span style="font-style: italic;"> We propose a class
            of subspace ascent methods for computing optimal approximate
            designs that covers both existing as well as new and more
            efficient algorithms. Within this class of methods, we
            construct a simple, randomized exchange algorithm (REX).
            Numerical comparisons suggest that the performance of REX is
            comparable or superior to the performance of
            state-of-the-art methods across a broad range of problem
            structures and sizes. We focus on the most commonly used
            criterion of D-optimality that also has applications beyond
            experimental design, such as the construction of the minimum
            volume ellipsoid containing a given set of datapoints. For
            D-optimality, we prove that the proposed algorithm converges
            to the optimum. We also provide formulas for the optimal
            exchange of weights in the case of the criterion of
            A-optimality. These formulas enable one to use REX for
            computing A-optimal and I-optimal designs.</span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 16, 2018</h3>
          <h1>New Intern, Visitor and Postdoc</h1>
          <br>
          I was travelling and am back at KAUST now. <br>
          <br>
          Let me welcome <a
            href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard

























































































































































































            Gorbunov</a> (a master's student visiting from MIPT, Moscow;
          will be here until Feb 8), <a
            href="http://www.damtp.cam.ac.uk/user/me404/">Matthias
            Ehrhardt</a> (visiting from Cambridge, UK, until February
          10) and <a href="http://maiage.jouy.inra.fr/?q=fr/bergou">Elhoucine

























































































































































































            Bergou</a> (new postdoc in my group, starting today). <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>
          <h3>January 15, 2018</h3>
          <h1>New Paper</h1>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1801.04873">"Randomized
            projection methods for convex feasibility problems:
            conditioning and convergence rates"</a> - joint work with <a
            href="http://acse.pub.ro/person/ion-necoara/">Ion Necoara</a>
          and <a
            href="https://www.researchgate.net/profile/Andrei_Patrascu2">Andrei

























































































































































































            Patrascu</a>.<br>
          <br>
          Abstract: <span style="font-style: italic;">Finding a point
            in the intersection of a collection of closed convex sets,
            that is the convex feasibility problem, represents the main
            modeling strategy for many computational problems. In this
            paper we analyze new stochastic reformulations of the convex
            feasibility problem in order to facilitate the development
            of new algorithmic schemes. We also analyze the conditioning
            problem parameters using certain (linear) regularity
            assumptions on the individual convex sets. Then, we
            introduce a general random projection algorithmic framework,
            which extends to the random settings many existing
            projection schemes, designed for the general convex
            feasibility problem. Our general random projection algorithm
            allows to project simultaneously on several sets, thus
            providing great flexibility in matching the implementation
            of the algorithm on the parallel architecture at hand. Based
            on the conditioning parameters, besides the asymptotic
            convergence results, we also derive explicit sublinear and
            linear convergence rates for this general algorithmic
            framework.</span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <h1> Old News</h1>
          <br>
          <a href="i_oldnews.html">Read old news</a> (2017 and earlier)<br>
          <br>
        </div>
        <div id="sidebar">
          <h6> [8/2019] Sign up for my <a
              href="https://piazza.com/kaust.edu.sa/fall2019/cs390ff/home">CS






















































              390FF (Big Data Optimization) class</a> (Fall 2019). </h6>
          <br>
          <br>
          <h6>[9/2018] A <a
href="https://www.youtube.com/watch?v=gjgEck0zU7w&amp;list=PLgKuh-lKre13E9dXSsif4KsGoFp4bjubd&amp;index=9">YouTube






















































              video</a> of my talk at the Simons Institute on the
            JacSketch algorithm. </h6>
          <br>
          <br>
          <h6> [8/2017] Video recording of my 5 hour mini-course on
            "Randomized Optimization Methods" delivered at the Data
            Science Summer School (DS3) at École Polytechnique: Parts <a
href="https://comm.medias.polytechnique.fr/videos/richtarik_1_randonopti/">1</a>,
            <a
href="https://comm.medias.polytechnique.fr/videos/richtarik_2_randonopti/">2</a>,
            <a
href="https://comm.medias.polytechnique.fr/videos/richtarik_3_randonopti_08033/">3</a>,
            <a
href="https://comm.medias.polytechnique.fr/videos/richtarik_4_randonoptimization/">4</a>,
            <a
href="https://comm.medias.polytechnique.fr/videos/richtarik_5_randonoptimization/">5</a>
          </h6>
          <br>
          <br>
          <h6> [3/2017] I have taken up an Associate Professor position
            at <a href="https://www.kaust.edu.sa/en">KAUST</a>. I am on
            leave from Edinburgh. </h6>
          <br>
          <br>
          <h6> [12/2016] Video from my <a
              href="https://events.yandex.ru/lib/talks/4294/">talk at
              Yandex </a>entitled "Empirical Risk Minimization:
            Complexity, Duality, Sampling, Sparsity and Big Data". </h6>
          <br>
          <br>
          <h6> [10/2016] My Alan Turing Institute talk on Stochastic
            Dual Ascent for Solving Linear Systems is now on <a
              href="https://www.youtube.com/watch?v=RbkhWrTbrKs">YouTube</a>.
          </h6>
          <br>
          <br>
          <h6>[9/2015] <a
              href="talks/2015-09-Toulouse-Summer-School-Optimization.pdf">Slides</a>
            from a 6hr course on "Optimization in Machine Learning",
            Toulouse, France. </h6>
          <br>
          <br>
          <h6> [7/2015] ICML Tutorial (joint with Mark Schmidt) on <span
              class="important">Modern Convex Optimization Methods for
              Large-scale ERM:</span> <a
              href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_I.pdf">Part































































































































































































              I</a>, <a
              href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_II.pdf">Part































































































































































































              II</a> </h6>
          <br>
          <br>
          <h6> [2/2014] A <a
              href="https://www.youtube.com/watch?v=0sHOfqhCZw0">YouTube
              video</a> of a talk on the APPROX algorithm delivered at
            PreMoLab in Moscow. </h6>
          <br>
          <br>
          <h6> [10/2013] A <a
              href="http://www.youtube.com/watch?v=IQgnstB0n2E#t=538">YouTube































































































































































































              video</a> of a talk I gave at the Simons Institute
            workshop on <a
              href="http://simons.berkeley.edu/workshops/bigdata2013-2">parallel






















































              and distributed optimization and inference.</a> </h6>
        </div>
        <div style="clear: both;"> </div>
      </div>
    </div>
  </body>
</html>
