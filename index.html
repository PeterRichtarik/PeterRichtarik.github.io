<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <!--[if lt IE 9]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
    <link rel="stylesheet" href="style.css">
    <title>Peter Richtarik</title>
    <link rel="alternate" type="application/rss+xml" title="Peter
      Richtarik" href="http://rsspect.com/rss/42611.xml">
    <script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-37355274-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();



  </script>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta name="Title" content="">
    <meta name="Keywords" content="">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="ProgId" content="Word.Document">
    <meta name="Generator" content="Microsoft Word 14">
    <meta name="Originator" content="Microsoft Word 14">
    <link rel="File-List"
href="file://localhost/Users/hydra/Library/Caches/TemporaryItems/msoclip/0clip_filelist.xml">
    <link rel="themeData"
href="file://localhost/Users/hydra/Library/Caches/TemporaryItems/msoclip/0clip_themedata.xml">
    <style>
<!--
 /* Font Definitions */
@font-face
	{font-family:Times;
	panose-1:2 0 5 0 0 0 0 0 0 0;
	mso-font-charset:0;
	mso-generic-font-family:auto;
	mso-font-pitch:variable;
	mso-font-signature:3 0 0 0 1 0;}
@font-face
	{font-family:"ＭＳ 明朝";
	panose-1:0 0 0 0 0 0 0 0 0 0;
	mso-font-charset:128;
	mso-generic-font-family:roman;
	mso-font-format:other;
	mso-font-pitch:fixed;
	mso-font-signature:1 134676480 16 0 131072 0;}
@font-face
	{font-family:"ＭＳ 明朝";
	panose-1:0 0 0 0 0 0 0 0 0 0;
	mso-font-charset:128;
	mso-generic-font-family:roman;
	mso-font-format:other;
	mso-font-pitch:fixed;
	mso-font-signature:1 134676480 16 0 131072 0;}
@font-face
	{font-family:Cambria;
	panose-1:2 4 5 3 5 4 6 3 2 4;
	mso-font-charset:0;
	mso-generic-font-family:auto;
	mso-font-pitch:variable;
	mso-font-signature:3 0 0 0 1 0;}
 /* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{mso-style-unhide:no;
	mso-style-qformat:yes;
	mso-style-parent:"";
	margin:0cm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	font-family:Cambria;
	mso-ascii-font-family:Cambria;
	mso-ascii-theme-font:minor-latin;
	mso-fareast-font-family:"ＭＳ 明朝";
	mso-fareast-theme-font:minor-fareast;
	mso-hansi-font-family:Cambria;
	mso-hansi-theme-font:minor-latin;
	mso-bidi-font-family:"Times New Roman";
	mso-bidi-theme-font:minor-bidi;
	mso-ansi-language:EN-US;}
.MsoChpDefault
	{mso-style-type:export-only;
	mso-default-props:yes;
	font-family:Cambria;
	mso-ascii-font-family:Cambria;
	mso-ascii-theme-font:minor-latin;
	mso-fareast-font-family:"ＭＳ 明朝";
	mso-fareast-theme-font:minor-fareast;
	mso-hansi-font-family:Cambria;
	mso-hansi-theme-font:minor-latin;
	mso-bidi-font-family:"Times New Roman";
	mso-bidi-theme-font:minor-bidi;
	mso-ansi-language:EN-US;}
@page WordSection1
	{size:612.0pt 792.0pt;
	margin:72.0pt 90.0pt 72.0pt 90.0pt;
	mso-header-margin:36.0pt;
	mso-footer-margin:36.0pt;
	mso-paper-source:0;}
div.WordSection1
	{page:WordSection1;}
--</style>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  </head>
  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <script src="table_header.js"></script> </div>
      </div>
      <ul class="menu">
        <li><a class="active" href="index.html">News</a></li>
        <li><a href="i_oldnews.html">Old News</a></li>
        <li><a href="i_papers.html">Papers</a></li>
        <li><a href="i_talks.html">Talks</a></li>
        <li><a href="i_events.html">Events</a></li>
        <li><a href="i_seminar.html">Seminar</a></li>
        <li><a href="i_software.html">Code</a></li>
        <li><a href="i_team.html">Team</a></li>
        <li><a href="i_join.html">Join</a></li>
        <li><a href="i_bio.html">Bio</a></li>
        <li><a href="i_teaching.html">Teaching</a></li>
        <li><a href="i_consulting.html">Consulting</a></li>
      </ul>
      <div id="wrapper" class="main">
        <div id="content">
          <h3>April 10, 2019</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1904.05115">"Stochastic
            distributed learning with gradient quantization and variance
            reduction"</a> - joint work with <a
            href="https://samuelhorvath.github.io">Samuel Horváth,</a> <a
            href="https://www.dmitry-kovalev.com">Dmitry Kovalev,</a> <a
            href="https://konstmish.github.io">Konstantin Mishchenko,</a>
          and <a href="https://sstich.ch">Sebastian Stich.</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>April 9, 2019</h3>
          <br>
          <a href="https://www.hse.ru/en/org/persons/219293044">Alexey
            Kroshnin</a> arrived at KAUST today and will stay here until
          the end of April. Alexey's research interests include
          fundamental theory of optimal transport, geometry of
          Wasserstein spaces, Wasserstein barycenters, dynamical systems
          on Wasserstein spaces, probability theory, measure theory,
          functional analysis and computational complexity theory.<br>
          <br>
          Alexey will work with <a href="https://konstmish.github.io">Konstantin





































            Mishchenko</a> and me on randomized methods for feasibility
          problems. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>April 8, 2019</h3>
          <br>
          <a href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a> arrived at KAUST today and will stay here until
          mid-May. He is finishing writing up his PhD thesis, and plans
          to defend in the Summer. Once he is done with the thesis, we
          will work do some work towards NeurIPS 2019. Nicolas got
          several job offers and chose to join <a
            href="https://mila.quebec">MILA</a> as a postdoc in
          September 2019. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>March 19, 2019</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1903.07971">"Convergence
            analysis of inexact randomized iterative methods"</a> -
          joint work with <a
            href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou.</a><br>
          <br>
          Abstract: <i>In this paper we present a convergence rate
            analysis of inexact variants of several randomized iterative
            methods. Among the methods studied are: stochastic gradient
            descent, stochastic Newton, stochastic proximal point and
            stochastic subspace ascent. A common feature of these
            methods is that in their update rule a certain sub-problem
            needs to be solved exactly. We relax this requirement by
            allowing for the sub-problem to be solved inexactly. In
            particular, we propose and analyze inexact randomized
            iterative methods for solving three closely related
            problems: a convex stochastic quadratic optimization
            problem, a best approximation problem and its dual, a
            concave quadratic maximization problem. We provide iteration
            complexity results under several assumptions on the
            inexactness error. Inexact variants of many popular and some
            more exotic methods, including randomized block Kaczmarz,
            randomized Gaussian Kaczmarz and randomized block coordinate
            descent, can be cast as special cases. Numerical experiments
            demonstrate the benefits of allowing inexactness.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>March 18, 2019</h3>
          <br>
          As of today, <a
            href="https://vcc.kaust.edu.sa/Pages/Kovalev.aspx">Dmitry
            Kovalev</a> is visiting Moscow - he will stay there for two
          weeks and will give two research talks while there (one in
          Boris Polyak's group and another at MIPT).<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>March 17, 2019</h3>
          <br>
          <a href="https://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu (The
            University of Hong Kong)</a> is visiting me at KAUST this
          week. She will stay for a week, and will give the Machine
          Learning Hub seminar on Thursday.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>March 9, 2019</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1903.06701">"Scaling distributed
            machine learning with in-network aggregation"</a> - joint
          work with <a
            href="https://ecrc.kaust.edu.sa/Pages/Sapio.aspx">Amedeo
            Sapio</a>, <a href="https://mcanini.github.io">Marco Canini</a>,
          <a href="https://www.chenyuho.com">Chen-Yu Ho</a>, <a
            href="https://www.microsoft.com/en-us/research/people/jacnels/">Jacob













































            Nelson</a>, <a
            href="https://www.kaust.edu.sa/en/study/faculty/panagiotis-kalnis">Panos













































            Kalnis</a>, <a
            href="https://www.linkedin.com/in/changhoon-chang-kim-b3394317/">Changhoon













































            Kim</a>, <a
            href="https://www.cs.washington.edu/people/faculty/arvind">Arvind













































            Krishnamurthy</a>, <a
            href="https://scholar.google.com/citations?user=okIQd4oAAAAJ&amp;hl=en">Masoud













































            Moshref</a>, and <a
            href="https://www.microsoft.com/en-us/research/people/dports/">Dan












































            R. K. Ports.</a><br>
          <br>
          Abstract: <i>Training complex machine learning models in
            parallel is an increasingly important workload. We
            accelerate distributed parallel training by designing a
            communication primitive that uses a programmable switch
            dataplane to execute a key step of the training process. Our
            approach, SwitchML, reduces the volume of exchanged data by
            aggregating the model updates from multiple workers in the
            network. We co-design the switch processing with the
            end-host protocols and ML frameworks to provide a robust,
            efficient solution that speeds up training by up to 300%,
            and at least by 20% for a number of real-world benchmark
            models.</i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>March 9, 2019</h3>
          <br>
          Ľubomír Baňas (Bielefeld) is arriving today at KAUST for a
          research visit; he will stay for a week. He will give an <a
href="https://cemse.kaust.edu.sa/events/Pages/AMCS-seminar--Numerical-approximation-of-the-stochastic-Cahn-Hilliard-equation-and-the-%28stochastic%29-Hele-Shaw-probl-125059.aspx">AMCS














































            seminar talk on Wednesday. </a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>March 4, 2019</h3>
          <br>
          My former intern, Atal Sahu (IIT Kanpur), joined KAUST as an
          MS student in the group of <a
            href="https://mcanini.github.io">Marco Canini</a>. <br>
          <br>
          Atal: Welcome back!<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>February 23, 2019</h3>
          <br>
          I have accepted an invite to serve as a Senior Program
          Committee Member at the 28th International Joint Conference on
          Artificial Intelligence (<a href="https://ijcai19.org">IJCAI
            2019</a>). The conference will take place in Macao, China,
          during August 10-16, 2019. The first IJCAI conference was held
          in 1969.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>February 20, 2019</h3>
          <br>
          I am in Vienna, visiting the <span style="caret-color:
            rgb(51, 51, 51); color: rgb(51, 51, 51); font-family:
            Verdana, Arial, &quot;Trebuchet MS&quot;, sans-serif,
            Georgia, Courier, &quot;Times New Roman&quot;, serif;
            font-size: 14px; font-style: normal; font-variant-caps:
            normal; font-weight: normal; letter-spacing: normal;
            orphans: auto; text-align: start; text-indent: 0px;
            text-transform: none; white-space: normal; widows: auto;
            word-spacing: 0px; -webkit-text-size-adjust: auto;
            -webkit-text-stroke-width: 0px; background-color: rgb(255,
            255, 255); text-decoration: none; display: inline
            !important; float: none;"><a href="https://www.esi.ac.at">Erwin






















































              Schrödinger International Institute for Mathematics and
              Physics (ESI)</a> which is the hosting a program on <a
href="https://www.esi.ac.at/activities/events/2019/modern-maximal-monotone-operator-theory">Modern






















































              Maximal Monotone Operator Theory: From Nonsmooth
              Optimization to Differential Inclusions</a></span>.<br>
          <br>
          On February 22 I am teaching a one-day (5 hrs) doctoral course
          on randomized methods in convex optimization. I offered two
          possible courses to the students, and they picked (almost
          unanimously) <a
            href="https://www.esi.ac.at/activities/events/2019/files/richtarik">this






















































            one</a>.<br>
          <br>
          During February 25-March 1, I am attending the workshop <a
href="https://www.univie.ac.at/projektservice-mathematik/e/?event=nualnoop">Numerical






















































            Algorithms in Nonsmooth Optimization</a>. My talk is on
          February 26; I am speaking about the <a
href="https://papers.nips.cc/paper/7478-sega-variance-reduction-via-gradient-sketching">"SEGA"






















































            paper (NeurIPS 2018)</a> - joint work with <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          and <a href="https://konstmish.github.io">Konstantin
            Mishchenko</a>. My SEGA slides are here (click on the image
          to get the pdf file):<br>
          <br>
          <a href="talks/Talk-SEGA.pdf"><img
              src="imgs/SEGA-cover-slide.png" alt="Click to download the
              slides" width="600" border="2" height="447"></a><br>
          <br>
          <div align="center"><i>&nbsp;</i><br>
          </div>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>February 18, 2019</h3>
          <br>
          As of today, Konstantin Mishchenko is visiting Martin Jaggi's
          <a href="https://mlo.epfl.ch">Machine Learning and
            Optimization Laboratory at EPFL</a>. He will stay there for
          a month.<i><br>
            <br>
            Update (March 17): Konstantin is back at KAUST now.<br>
          </i> <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>February 12, 2019</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1902.03591">"Stochastic three
            points method for unconstrained smooth minimization"</a> -
          joint work with <a
            href="https://vcc.kaust.edu.sa/Pages/Bergou.aspx">El Houcine
            Bergou</a> and <a
            href="https://eduardgorbunov.github.io/index.html">Eduard
            Gorbunov</a>.<br>
          <br>
          Abstract: <i>In this paper we consider the unconstrained
            minimization problem of a smooth function in R^n in a
            setting where only function evaluations are possible. We
            design a novel randomized direct search method based on
            stochastic three points (STP) and analyze its complexity. At
            each iteration, STP generates a random search direction
            according to a certain fixed probability law. Our
            assumptions on this law are very mild: roughly speaking, all
            laws which do not concentrate all measure on any halfspace
            passing through the origin will work. For instance, we allow
            for the uniform distribution on the sphere and also
            distributions that concentrate all measure on a positive
            spanning set. Given a current iterate x, STP compares the
            objective function at three points: x, x+αs and x−αs, where
            α&gt;0 is a stepsize parameter and s is the random search
            direction. The best of these three points is the next
            iterate. We analyze the method STP under several stepsize
            selection schemes (fixed, decreasing, estimated through
            finite differences, etc). We study non-convex, convex and
            strongly convex cases.&nbsp; We also propose a parallel
            version for STP, with iteration complexity bounds which do
            not depend on the dimension n.<br>
            <br>
          </i>Comment:<i> The paper was finalized in March 2018; but we
            only put it online now.<br>
          </i> <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>February 11, 2019</h3>
          <br>
          I always have research internships available in my group @ <a
            href="https://www.kaust.edu.sa/en">KAUST</a> throughout the
          year for outstanding and highly motivated students. If you are
          from Europe, USA, Canada, Australia or New Zealand, you are
          eligible for the <a
href="https://vsrp.kaust.edu.sa/Pages/Topics%20in%20Machine%20Learning%20and%20Optimization.aspx">Visiting




























































            Student Research Program (VSRP).</a> These internships are a
          minimum 3 months and a maximum 6 months in duration. We have a
          different internship program dedicated to applicants from
          elsewhere. Shorter internships are possible with this program.
          Drop me an email if you are interested in working with me,
          explaining why you are interested, attaching your CV and
          complete transcript of grades. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>February 8, 2019</h3>
          <br>
          This is my research group: <br>
          <br>
          <a href="imgs/Richtarik-group-2019-02-medium.jpeg"><img
              src="imgs/Richtarik-group-2019-02-small.png" alt="My
              research group (February 2019)" width="700" border="0"
              height="353"></a><br>
          <br>
          People on the photo: <br>
          <br>
          <i>Postdocs:</i> Aritra Dutta, El-Houcine Bergou, Xun Qian <br>
          <br>
          <i>PhD students:</i> Filip Hanzely, Konstantin Mishchenko,
          Alibek Sailanbayev, Samuel Horváth <br>
          <br>
          <i>MS/PhD students:</i> Elnur Gasanov, Dmitry Kovalev <br>
          <br>
          <i>interns:</i> Eduard Gorbunov, Dmitry Kamzolov, Igor
          Sokolov, Egor Shulgin, Vladislav Elsukov (all belong to my
          group at MIPT where I am a visiting professor), Ľudovít
          Horváth (from Comenius University)<br>
          <br>
          Comment: Nicolas Loizou (Edinburgh) is not on the photo; we
          will photoshop him in once he comes for a visit in April... <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>February 4, 2019</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1902.01272">"A stochastic
            derivative-free optimization method with importance
            sampling"</a> - joint work with <a
            href="http://www.adelbibi.com">Adel Bibi</a>, <a
            href="https://vcc.kaust.edu.sa/Pages/Bergou.aspx">El Houcine
            Bergou</a>, <a href="http://ozansener.net">Ozan Sener</a>
          and <a
            href="https://www.kaust.edu.sa/en/study/faculty/bernard-ghanem">Bernard


























            Ghanem</a>.<br>
          <br>
          Abstract: <i>We consider the problem of unconstrained
            minimization of a smooth objective function in R^n in a
            setting where only function evaluations are possible. While
            importance sampling is one of the most popular techniques
            used by machine learning practitioners to accelerate the
            convergence of their models when applicable, there is not
            much existing theory for this acceleration in the
            derivative-free setting. In this paper, we propose an
            importance sampling version of the stochastic three points
            (STP) method proposed by Bergou et al. and derive new
            improved complexity results on non-convex, convex and
            λ-strongly convex functions. We conduct extensive
            experiments on various synthetic and real LIBSVM datasets
            confirming our theoretical results. We further test our
            method on a collection of continuous control tasks on
            several MuJoCo environments with varying difficulty. Our
            results suggest that STP is practical for high dimensional
            continuous control problems. Moreover, the proposed
            importance sampling version results in a significant sample
            complexity improvement.</i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>January 27, 2019</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1901.09437">"99% of parallel
            optimization is inevitably a waste of time"</a> - joint work
          with <a href="https://konstmish.github.io">Konstantin
            Mishchenko</a> and <a
            href="https://fhanzely.github.io/index.html">Filip Hanzely</a>.<br>
          <br>
          Abstract: <i>It is well known that many optimization methods,
            including SGD, SAGA, and Accelerated SGD for
            over-parameterized models, do not scale linearly in the
            parallel setting. In this paper, we present a new version of
            block coordinate descent that solves this issue for a number
            of methods. The core idea is to make the sampling of
            coordinate blocks on each parallel unit independent of the
            others. Surprisingly, we prove that the optimal number of
            blocks to be updated by each of $n$ units in every iteration
            is equal to $m/n$, where $m$ is the total number of blocks.
            As an illustration, this means that when $n=100$ parallel
            units are used, 99% of work is a waste of time. We
            demonstrate that with $m/n$ blocks used by each unit the
            iteration complexity often remains the same. Among other
            applications which we mention, this fact can be exploited in
            the setting of distributed optimization to break the
            communication bottleneck. Our claims are justified by
            numerical experiments which demonstrate almost a perfect
            match with our theory on a number of datasets.</i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>January 26, 2019</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1901.09269">"Distributed
            learning with compressed gradient differences"</a> - joint
          work with <a href="https://konstmish.github.io">Konstantin
            Mishchenko</a>, <a
            href="https://eduardgorbunov.github.io/index.html">Eduard
            Gorbunov</a> and <a href="http://mtakac.com">Martin Takáč</a>.<br>
          <br>
          Abstract: <i>Training very large machine learning models
            requires a distributed computing approach, with
            communication of the model updates often being the
            bottleneck. For this reason, several methods based on the
            compression (e.g., sparsification and/or quantization) of
            the updates were recently proposed, including QSGD (Alistarh
            et al., 2017), TernGrad (Wen et al., 2017), SignSGD
            (Bernstein et al., 2018), and DQGD (Khirirat et al., 2018).
            However, none of these methods are able to learn the
            gradients, which means that they necessarily suffer from
            several issues, such as the inability to converge to the
            true optimum in the batch mode, inability to work with a
            nonsmooth regularizer, and slow convergence rates. In this
            work we propose a new distributed learning
            method---DIANA---which resolves these issues via compression
            of gradient differences. We perform a theoretical analysis
            in the strongly convex and nonconvex settings and show that
            our rates are vastly superior to existing rates. Our
            analysis of block quantization and differences between l2
            and l∞ quantization closes the gaps in theory and practice.
            Finally, by applying our analysis technique to TernGrad, we
            establish the first convergence rate for this method.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>January 26, 2019</h3>
          <br>
          <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          and <a href="https://www.aritradutta.com">Aritra Dutta</a>
          are on their way to <a
            href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a>,
          to be held during Jan 27-Feb 1, 2019 in Honolulu, Hawaii.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>January 25, 2019</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1901.09401">"SGD: general
            analysis and improved rates"</a> - joint work with <a
            href="https://perso.telecom-paristech.fr/rgower/">Robert
            Mansel Gower</a>, <a
            href="https://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
            Loizou</a>, <a href="https://qianxunk.github.io">Xun Qian</a>,
          <a href="http://www.ali-sa.org">Alibek Sailanbayev</a> and <a
            href="https://www.linkedin.com/in/egor-shulgin-a34373127">Egor


























            Shulgin</a>.<br>
          <br>
          Abstract: <i>We propose a general yet simple theorem
            describing the convergence of SGD under the arbitrary
            sampling paradigm. Our theorem describes the convergence of
            an infinite array of variants of SGD, each of which is
            associated with a specific probability law governing the
            data selection rule used to form minibatches. This is the
            first time such an analysis is performed, and most of our
            variants of SGD were never explicitly considered in the
            literature before. Our analysis relies on the recently
            introduced notion of expected smoothness and does not rely
            on a uniform bound on the variance of the stochastic
            gradients. By specializing our theorem to different
            mini-batching strategies, such as sampling with replacement
            and independent sampling, we derive exact expressions for
            the stepsize as a function of the mini-batch size. With this
            we can also determine the mini-batch size that optimizes the
            total complexity, and show explicitly that as the variance
            of the stochastic gradient evaluated at the minimum grows,
            so does the optimal mini-batch size. For zero variance, the
            optimal mini-batch size is one. Moreover, we prove
            insightful stepsize-switching rules which describe when one
            should switch from a constant to a decreasing stepsize
            regime.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>January 24, 2019</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1901.08689">"Don’t jump through
            hoops and remove those loops: SVRG and Katyusha are better
            without the outer loop"</a> - joint work with Dmitry Kovalev
          and <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.<br>
          <br>
          Abstract: <i>The stochastic variance-reduced gradient method
            (SVRG) and its accelerated variant (Katyusha) have attracted
            enormous attention in the machine learning community in the
            last few years due to their superior theoretical properties
            and empirical behaviour on training supervised machine
            learning models via the empirical risk minimization
            paradigm. A key structural element in both of these methods
            is the inclusion of an outer loop at the beginning of which
            a full pass over the training data is made in order to
            compute the exact gradient, which is then used to construct
            a variance-reduced estimator of the gradient. In this work
            we design loopless variants of both of these methods. In
            particular, we remove the outer loop and replace its
            function by a coin flip performed in each iteration designed
            to trigger, with a small probability, the computation of the
            gradient. We prove that the new methods enjoy the same
            superior theoretical convergence properties as the original
            methods. However, we demonstrate through numerical
            experiments that our methods have substantially superior
            practical behavior.<br>
            <br>
          </i> <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1901.08669">"SAGA with arbitrary
            sampling"</a> - joint work with <a
            href="https://qianxunk.github.io">Xun Qian</a> and <a
            href="https://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a>.<br>
          <br>
          Abstract: <i>We study the problem of minimizing the average
            of a very large number of smooth functions, which is of key
            importance in training supervised learn- ing models. One of
            the most celebrated methods in this context is the SAGA
            algorithm of Defazio et al. (2014). Despite years of
            research on the topic, a general-purpose version of SAGA—one
            that would include arbitrary importance sampling and
            minibatching schemes—does not exist. We remedy this
            situation and propose a general and flexible variant of SAGA
            following the arbitrary sampling paradigm. We perform an
            iteration complexity analysis of the method, largely
            possible due to the construction of new stochastic Lyapunov
            functions. We establish linear convergence rates in the
            smooth and strongly convex regime, and under a quadratic
            functional growth condition (i.e., in a regime not assuming
            strong convexity). Our rates match those of the primal-dual
            method Quartz (Qu et al., 2015) for which an arbitrary
            sampling analysis is available, which makes a significant
            step towards closing the gap in our understanding of
            complexity of primal and dual methods for finite sum
            problems.</i><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>January 15, 2019</h3>
          <br>
          <a href="https://vcc.kaust.edu.sa/Pages/Bergou.aspx">El
            Houcine Bergou's</a> 1 year postdoc contract in my group
          ended; he now a postdoc in <a
            href="https://www.kaust.edu.sa/en/study/faculty/panagiotis-kalnis">Panos



















































            Kalnis</a>' group here at KAUST. I am looking forward to
          further collaboration with El Houcine and Panos.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>January 14, 2019</h3>
          <br>
          <a href="https://icml.cc/Conferences/2019">ICML</a> deadline
          is upon us (on Jan 23)... Everyone in my group is working hard
          towards the deadline. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>January 10, 2019</h3>
          <br>
          I've been asked to lead an Aritificial Intelligence Committee
          at KAUST whose role is to prepare a strategic plan for growing
          AI research and activities at KAUST over the next 5 years.
          This will be a substantial investment, and will involve a
          large number of new faculty, research scientist, postdoc and
          PhD and MS/PhD positions; investment into computing
          infrastructure and more. (The committee started its work in
          2018; I am positing the news with some delay...)<br>
          <br>
          Independently to this, Bernard Ghanem, Marco Canini, Panos
          Kalnis and me have established the <a
            href="https://ml.kaust.edu.sa">Machine Learning Hub at KAUST</a>,
          with the aim to advance ML research and training activities
          for the benefit of the entire KAUST community. The website is
          only visible from within the KAUST network at the moment. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>January 6, 2019</h3>
          <br>
          I am back at KAUST. <a
            href="http://maiage.jouy.inra.fr/?q=fr/bergou">El Houcine</a>,
          <a href="https://konstmish.github.io">Konstantin</a> and <a
            href="https://qianxunk.github.io">Xun</a> are here. <a
            href="https://www.aritradutta.com">Aritra</a> is on his way
          to <a href="http://wacv19.wacv.net">WACV 2019, Hawaii</a>. <a
            href="https://samuelhorvath.github.io">Samuel</a> and <a
            href="https://fhanzely.github.io/index.html">Filip</a> will
          come back tomorrow. <a
            href="https://vcc.kaust.edu.sa/Pages/Sailanbayev.aspx">Alibek</a>
          and <a href="https://vcc.kaust.edu.sa/Pages/Gasanov.aspx">Elnur</a>
          are arriving soon, too.<br>
          <br>
          I will have several interns/research visitors from <a
href="https://mipt.ru/science/visiting_prof/piter-rikhtarik-peter-richtarik.php">my
















































































            group at MIPT</a> visiting me at KAUST during
          January-February: <br>
          <br>
          - <a href="https://www.researchgate.net/profile/Egor_Shulgin">Egor

















            Shulgin</a> (Jan 6 - Feb 21) <br>
          - Dmitry Kamzolov (Jan 10 - Feb 18)<br>
          - Vladislav Elsukov (Jan 11 - Feb 15) <br>
          - <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>
          (Jan 13 - Feb 24) <br>
          - Igor Sokolov (Jan 18 - Feb 25)<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>January 3, 2019</h3>
          <br>
          I am visiting <a
            href="http://www.iam.fmph.uniba.sk/ospm/Harman/">Radoslav
            Harman</a> @ Comenius University, Slovakia.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <a href="i_oldnews.html">Read old news</a> (2018 and earlier)<br>
          <br>
        </div>
        <div id="sidebar">
          <h6>[9/2018] A <a
href="https://www.youtube.com/watch?v=gjgEck0zU7w&amp;list=PLgKuh-lKre13E9dXSsif4KsGoFp4bjubd&amp;index=9">YouTube


              video</a> of my talk at the Simons Institute on the
            JacSketch algorithm. </h6>
          <br>
          <br>
          <h6> [8/2017] Video recordings of my 5 hour mini-course on
            "Randomized Optimization Methods" delivered at the Data
            Science Summer School (DS3) at École Polytechnique: Parts <a
href="https://comm.medias.polytechnique.fr/videos/richtarik_1_randonopti/">1</a>,
            <a
href="https://comm.medias.polytechnique.fr/videos/richtarik_2_randonopti/">2</a>,
            <a
href="https://comm.medias.polytechnique.fr/videos/richtarik_3_randonopti_08033/">3</a>,
            <a
href="https://comm.medias.polytechnique.fr/videos/richtarik_4_randonoptimization/">4</a>,
            <a
href="https://comm.medias.polytechnique.fr/videos/richtarik_5_randonoptimization/">5</a>
          </h6>
          <br>
          <br>
          <h6> [3/2017] I have taken up an Associate Professor position
            at <a href="https://www.kaust.edu.sa/en">KAUST</a>. I am on
            leave from Edinburgh. </h6>
          <br>
          <br>
          <h6> [12/2016] Video from my <a
              href="https://events.yandex.ru/lib/talks/4294/">talk at
              Yandex </a>entitled "Empirical Risk Minimization:
            Complexity, Duality, Sampling, Sparsity and Big Data". </h6>
          <br>
          <br>
          <h6> [10/2016] My Alan Turing Institute talk on Stochastic
            Dual Ascent for Solving Linear Systems is now on <a
              href="https://www.youtube.com/watch?v=RbkhWrTbrKs">YouTube</a>.
          </h6>
          <br>
          <br>
          <h6>[9/2015] <a
              href="talks/2015-09-Toulouse-Summer-School-Optimization.pdf">Slides</a>
            from a 6hr course on "Optimization in Machine Learning",
            Toulouse, France. </h6>
          <br>
          <br>
          <h6> [7/2015] ICML Tutorial (joint with Mark Schmidt) on <span
              class="important">Modern Convex Optimization Methods for
              Large-scale ERM:</span> <a
              href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_I.pdf">Part


              I</a>, <a
              href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_II.pdf">Part


              II</a> </h6>
          <br>
          <br>
          <h6> [2/2014] A <a
              href="https://www.youtube.com/watch?v=0sHOfqhCZw0">YouTube
              video</a> of a talk on the APPROX algorithm delivered at
            PreMoLab in Moscow. </h6>
          <br>
          <br>
          <h6> [10/2013] A <a
              href="http://www.youtube.com/watch?v=IQgnstB0n2E#t=538">YouTube


              video</a> of a talk I gave at the Simons Institute
            workshop on <a
              href="http://simons.berkeley.edu/workshops/bigdata2013-2">parallel


              and distributed optimization and inference.</a> </h6>
        </div>
        <div style="clear: both;"> </div>
      </div>
      <div id="footer">
        <script src="x_footer.js"></script> </div>
    </div>
  </body>
</html>
