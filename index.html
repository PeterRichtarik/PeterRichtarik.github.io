<!DOCTYPE html>

<html>

  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="style.css">
    <title>Peter Richtarik</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-168147887-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-168147887-1');
    </script>
  </head>


  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <script src="table_header.js"></script> </div>
      </div>
      <ul class="menu">
        <li><a class="active" href="index.html">News</a></li>
        <li><a href="i_oldnews.html">Old News</a></li>
        <li><a href="i_papers.html">Papers</a></li>
        <li><a href="i_talks.html">Talks</a></li>
        <li><a href="i_videotalks.html">Video Talks</a></li>
        <li><a href="i_events.html">Events</a></li>
      <!--  <li><a href="i_seminar.html">Seminar</a></li> !-->
        <li><a href="i_software.html">Code</a></li>
        <li><a href="i_team.html">Team</a></li>
        <li><a href="i_join.html">Join</a></li>
        <li><a href="i_bio.html">Bio</a></li>
        <li><a href="i_teaching.html">Teaching</a></li>
        <li><a href="i_consulting.html">Consulting</a></li>
      </ul>
      <div id="wrapper" class="main">
        <div id="content">


<h3>October 5, 2023</h3>  
  
<h1> A paper accepted to TMLR </h1>
 
The paper 

<ul>
<li> Rustem Islamov, Xun Qian, Slavomír Hanzely, Mher Safaryan, and Peter Richtárik. Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation, <a href="https://arxiv.org/abs/2206.03588">arXiv:2206.03588</a>, 2022,</li>
</ul>
was just accepted to <a href="https://openreview.net/forum?id=NekBTCKJ1H">Transactions on Machine Learning Research</a>.

<br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>October 3, 2023</h3>  
  
<h1> New VSRP Intern: Georg Meinhardt </h1>
 
Georg is studying towards an MSc in Mathematical Sciences at the University of Oxford, currently ranked #2 in the program. In his studies, he focuses on probability, deep learning and combinatorics.

<br>
<br>

Prior to this, Georg obtained two separate BSc degrees from the University of Bonn: one in mathematics, and another one in computer science. He focused on combinatorial optimization, algorithm design and probability theory. In mathematics, he was ranked in the top 3% in his cohort.
Georg is the recipient of the "Studienstiftung des deutschen Volkes" Scholarship, awarded for outstanding academic potential to top 2% of students.
<br>
<br>

Some random facts about Georg:
<ul>
<li>
In 2019, he coauthored a paper: I. Messaris, A. Ascoli, G.S. Meinhardt, R. Tetzlaff, L.O. Chua "Mem-Computing CNNs with Bistable-Like Memristors”, 2019 IEEE International Symposium on Circuits and Systems (ISCAS), 2019  </li>

<li> At Oxford, he studied "Geometric Deep Learning" (final project: Transfer Learning in Neural Algorithmic Reasoning) and "Theories of Deep Learning" (final project: Modifying Attention Heads of BERT)
</li>

<li> The title of his MSc thesis is "Analytical Lower Bounds on the Probability of at Least m out of N Events“
</li>

<li> In his BSc thesis, he worked on Branch-and-Price Algorithm for Vehicle Routing. Thesis title: "Lower Bounds for the Vehicle Routing Problem in a Branch-and-Price Framework“
</li>

<li> He speaks Chinese (approx. CEFR B1) and Polish (CEFR A1)
</li>

<li> He likes rowing (he is a member of the Lady Margaret Hall Boat Club)
</li>

<li> He is a member of the "Young European Federalists" society
</li>

<li> Georg was a "Micro Intern" at EcoSync, Oxford, where he worked as a Junior Data and AI Developer for time series prediction with deep learning (GluonTS)
</li>

<li> He was also a Summer intern at Oliver Wyman, Data and Analytics, Berlin, where he worked on expanding a stress testing framework of a large Australasian Bank
</li>

<li> During 2016-2019, Georg obtained a "Vordiplom" in Information Systems Engineering from Technical University of Dresden. He studied Computer Science and Electrical Engineering.
</li>

<li> Georg likes traveling, plays the viola, photography & snowboarding (maybe he will expand his skills to sand-dunes-boarding here in Saudi?)
</li>
 
</ul>


Georg, welcome!!!
<br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>September 28, 2023</h3>  
  
<h1> ICLR 2024 </h1>
 
The deadline for submitting papers to ICLR 2024 passed; my team and I submitted a few papers. Let's see how it will pan out.
<br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>September 26, 2023</h3>  
  
<h1> A paper accepted to SIMODS </h1>
 
The paper 
<ul>
<li> Haoyu Zhao, Konstantin Burlachenko, Zhize Li, and Peter Richtárik. Faster Rates for Compressed Federated Learning with Client-Variance Reduction, <a href="https://arxiv.org/abs/2112.13097">arXiv:2112.13097</a>, 2021,</li>
</ul>
was accepted to SIAM Journal on Mathematics of Data Science (SIMODS).

<br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 



<h3>September 25, 2023</h3>  
  
<h1> RDIA </h1>
 
I've applied for an RDIA grant. The Research, Development and Innovation Authority (RDIA) is a newly established funding agency in Saudi Arabia.

<br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>September 24, 2023</h3>  
  
  <h1> Paper accepted to TMLR </h1>
 
The paper 

<ul>
<li> Alexander Tyurin, Lukang Sun, Konstantin Burlachenko, and Peter Richtárik. Sharper Rates and Flexible Framework for Nonconvex SGD with Client and Data Sampling, <a href="https://arxiv.org/abs/2206.02275">arXiv:2206.02275</a>, 2022,</li>
</ul>
was accepted to Transactions on Machine Learning Research (TMLR).

<br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 




 
   <h3>August 21, 2023</h3>  
  
  <h1> Paper accepted to JOTA </h1>
 
The paper 

<ul>
<li> Ahmed Khaled, Othmane Sebbouh, Nicolas Loizou, Robert M. Gower, and Peter Richtárik. Unified analysis of stochastic gradient methods for composite convex and smooth optimization, <a href="https://arxiv.org/abs/2006.11573">arXiv:2006.11573</a>, 2020,</li>
</ul>
was accepted to Journal of Optimization Theory and Applications (JOTA).

    <br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 

    <h3>August 16, 2023</h3>  
  
  <h1> New MS/PhD student: Artem Riabinin </h1>
 
Artem Riabinin is a new MS/PhD student in my lab about to arrive to KAUST on August 16! Artem studied in the Mathematics Department, Faculty of Physics, Lomonosov Moscow State University, with a GPA of 4.88/5.00. His past research interests and experience lie in applied mathematics, including numerical methods, inverse ill-posed problems and their applications.  For example, he was involved with the processing of images obtained by laser radars.
<br><br>

Artem developed and delivered two courses aimed at training of school students for experimental round of All-Russian Olympiad in Physics.

<br><br>
Artem's successes in various competitions:
<ul>
<li>Phystech Olympiad in Physics, 2019 (winner) </li>
<li>Phystech Olympiad in Mathematics, 2018 (prize winner) </li>
<li>City Physics Olympiad, N. Novgorod, 2018 (2nd place) </li>
<li>Regional stage of the All Russian Olympiad in Physics, 2018 (prize winner)</li>
</ul>

Artem, welcome to the team!
<br>

        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


<h3>August 14, 2023</h3>  
  
<h1> New PhD student: Arto Maranjyan </h1>


<a href="https://artomaranjyan.github.io">Artavazd "Arto" Maranjyan</a> has just arrived to KAUST -- he is joining my team as a PhD student. He is the recipient of the prestigious KAUST Dean Award (a 6,000 USD per year top-up on the already generous KAUST Fellowship, given to a handful of the best incoming students to KAUST).
<br><br>

Arto has has MSc degree in Applied Statistics and Data Science from Yerevan State University. His MSc thesis entitled <a href="https://drive.google.com/file/d/1Eu9QypLzcw5T4jmBzisD5V60hGRWyT0P/view">On local training methods</a> was based on a <a href="https://arxiv.org/abs/2210.16402">joint paper</a> he worked on during his internship co-supervised by my postdoc Mher Safaryan and myself at KAUST during the second half of 2022. Prior to this, Arto obtained his  BSc degree in Informatics and Applied Mathematics  from Yerevan State University. His BSc thesis  <a href="https://drive.google.com/file/d/18VkkGXtqfky94HpInyID9mgzPryVdiaz/view">"On the convergence of series in classical systems</a> was supervised by Martin Grigoryan. Arto got an Outstanding Final Project Award for this  thesis (awarded to 6 students among more than 250 students).

<br><br>
Arto coauthored 4 papers before the start of his PhD:
<ul>
<li>Martin Grigoryan, Anna Kamont, Artavazd Maranjyan. Menshov-type theorem for divergence sets of sequences of localized operators, Journal of Contemporary Mathematical Analysis, vol. 58, no. 2, pp. 81–92, 2023. </li>
<li>Artavazd Maranjyan, Mher Safaryan, Peter Richtárik. GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity, arXiv:2210.16402, 2022.</li>
<li>Martin Grigoryan, Artavazd Maranjyan. On the divergence of Fourier series in the general Haar system, Armenian Journal of Mathematics, vol. 13, p. 1–10, Sep. 2021.</li>
<li>Rigran Grigoryan, Artavazd Maranjyan. On the unconditional convergence of Faber-Schauder series in L1, Proceedings of the YSU A: Physical and Mathematical Sciences, vol. 55, no. 1 (254), pp. 12–19, 2021.</li>
</ul>

Arto, welcome to the team!
<br>

        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 





    <h3>August 2-21, 2023</h3>  
  
  <h1> NeurIPS "vacation" </h1>
 
Most of my August is spent on writing NeurIPS rebuttals and on NeurIPS Area Chair work. What a good way to spend the Summer.  Hawaii comes distant second.
    <br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br> 


   
  <h3>August 10, 2023</h3>  
  
  <h1> Numerische Mathematik </h1>
 
 I accepted an invite to join the <a href="https://www.springer.com/journal/211/editors">Editorial Board</a> of <a href="https://www.springer.com/journal/211">Numerische Mathematik</a>.     <br>
        
 <br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>   


       
 <h3>July 22, 2023</h3>

<h1> ICML @ O'ahu, Hawaii </h1>

I am leaving San Francisco for Honolulu to attend ICML 2023. KAUST has a booth, so please stop by! We have 10 papers at the event: 2 conference and 8 workshop papers.<br> <br>

Conference: 
<ul>
<li> <a href="https://proceedings.mlr.press/v202/sadiev23a">High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance</a> (Abdurakhmon Sadiev, Marina Danilova, Eduard Gorbunov, Samuel Horváth, Gauthier Gidel, Pavel Dvurechensky, Alexander Gasnikov, Peter Richtárik)</li>
<li> <a href="https://proceedings.mlr.press/v202/gruntkowska23a">EF21-P and Friends: Improved Theoretical Communication Complexity for Distributed Optimization with Bidirectional Compression</a> (Kaja Gruntkowska, Alexander Tyurin, Peter Richtárik) </li>
</ul>

<br>
Workshops:
<ul>
<li>  <a href="https://arxiv.org/abs/2305.13082">Sketch-and-Project Meets Newton Method: Global O(1/k^2) Convergence with Low-Rank Updates  (Slavomír Hanzely) </li>
<li> <a href="https://arxiv.org/abs/2306.16484">Towards a Better Theoretical Understanding of Independent Subnetwork Training</a> (Egor Shulgin, Peter Richtárik) </li>
<li> <a href="https://arxiv.org/abs/2301.06806">Convergence of First-Order Algorithms for Meta-Learning with Moreau Envelopes</a> (Konstantin Mishchenko, Slavomír Hanzely, Peter Richtárik) </li>
<li>  <a href="https://arxiv.org/abs/2306.03240">Improving Accelerated Federated Learning with Compression and Importance Sampling </a>  (Michał Grudzień, Grigory Malinovsky, Peter Richtárik)</li>
<li> <a href="https://arxiv.org/abs/2302.03662">Federated Learning with Regularized Client Participation</a> (Grigory Malinovsky, Samuel Horváth, Konstantin Burlachenko, Peter Richtárik) </li>
<li> <a href="https://arxiv.org/abs/2305.15155">Momentum Provably Improves Error Feedback!</a> (Ilyas Fatkhullin, Alexander Tyurin, Peter Richtárik) </li>
<li> <a href="https://arxiv.org/abs/2206.07021">Federated Optimization Algorithms with Random Reshuffling and Gradient Compression</a> (Abdurakhmon Sadiev, Grigory Malinovsky, Eduard Gorbunov, Igor Sokolov, Ahmed Khaled, Konstantin Burlachenko, Peter Richtárik)  </li>
<li> <a href="https://arxiv.org/abs/2303.04622">ELF: Federated Langevin Algorithms with Primal, Dual and Bidirectional Compression</a> (Avetik Karagulyan, Peter Richtárik)  </li>
</ul>

<br>
Several members of my team attended as well, including Avetik Karagulyan, Yury Demidovich, Samuel Horváth, Slavomír Hanzely, Egor Shulgin, Igor Sokolov, as well as former interns and coauthors of two of the above papers, Kaja Gruntkowska and Ilyas Fatkhullin.
<br> <br>

We all had a great time. I spoiled by breaking my hand on the last day in Hawaii. Will need 2 months for the hand to heal.  
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>   


        
<h3>July 19, 2023</h3>

<h1> Federated and Collaborative Learning Workshop @ Berkeley </h1>

I am attending the <a href="https://simons.berkeley.edu/workshops/federated-collaborative-learning/schedule">Federated and Collaborative Learning</a> workshop organized by John Duchi, Nika Haghtalab, Peter Kairouz, Virginia Smith, Nati Srebro, and Kunal Talwar, taking place at the Simons Institute at UC Berkeley during July 19-20, 2023. My talk is on Day 2 of the event. This is a scoping event prior to applying for a semester-long program on Federated Learning at the Simons Institute.<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>   

        
        
<h3>July 9, 2023</h3>

<h1>Eastern European Machine Learning (EEML) Summer School </h1>

I am one of the lecturers at <a href="https://www.eeml.eu">EEML</a>, which is taking place during July 10-15, 2023, in beautiful Košice, Slovakia. <br> <br>

Eastern European Machine Learning (EEML) summer school is a one-week summer school around core topics regarding machine learning and artificial intelligence. The summer school includes both lectures and practical sessions (labs) to improve the theoretical and practical understanding of these topics. The school is organized in English and is aimed in particular at graduate students, although it is open to anyone interested in the topic. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>        


<h3>July 1, 2023</h3>

<h1>Trip to Yerevan, Armenia</h1>

I am on my way to Yerevan Armenia, to attend the conference <a href="http://mathconf.sci.am/">Mathematics in Armenia: Advances and Perspectives</a>, which is a celebration of the 80th anniversary of the foundation of the Armenian National Academy of Sciences. I am giving a plenary talk on July 5.  <br> <br>

Several members of my team are giving talks, too.
<ul>
<li> Abdurakhmon Sadiev, High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance</li>
<li> Egor Shulgin, Towards a Better Theoretical Understanding of Independent Subnetwork
Training</li>
<li> Slavomír Hanzely, A Second-Order Optimization Method with Low-Rank Updates and Global O(1/k^2) Convergence Rate</li>
<li> Avetik Karagulyan, ELF: Federated Langevin Algorithms with Primal, Dual and Bidirectio-
nal Compression</li>
<li> Grigorii Malinovskii, Can 5th Generation Local Training Methods Support Client Sampling? Yes!</li>
<li> Artavazd Maranjyan, GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity</li>
</ul>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>June 28, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2306.16484">
"Towards a Better Theoretical Understanding of Independent Subnetwork Training"</a> -
joint work with
 <a href="https://shulgin-egor.github.io">Egor Shulgin</a>.
 <br>
  <br>
  
We take a closer theoretical look at Independent Subnetwork Training (IST), which is a recently proposed and highly effective technique for solving the aforementioned problems. We identify fundamental differences between IST and alternative approaches, such as distributed methods with compressed communication, and provide a precise analysis of its optimization performance on a quadratic model.
  
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2306.16484">
<img alt="Towards a Better Theoretical Understanding of Independent Subnetwork Training" src="imgs/IST.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>June 15, 2023</h3>

<h1>Aramco course endgame</h1>

Four more days of teaching (June 15-18) my Introduction to Machine Learning course for Aramco MS students. This the second half of an intensive 48 hours long course delivered over 8 days. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>June 5, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2306.03240">
"Improving Accelerated Federated Learning with Compression and Importance Sampling"</a> -
joint work with
 <a href="https://scholar.google.com/citations?user=vN2ALVYAAAAJ&hl=en">Michał Grudzień</a>, and
 <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>.
 <br>
  <br>
 
We found out how to combine local training, client sampling and communication compression correctly in a single method.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2306.03240">
<img alt="Improving Accelerated Federated Learning with Compression and Importance Sampling" src="imgs/5GCS-CC.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 4, 2023</h3>

<h1>Aramco course halftime</h1>

After full 4 days of teaching (June 1-4), I have week plus to prepare for further and final 4 days. This is an intensive 48 hours long course delivered over 8 days. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 31, 2023</h3>

<h1>Teaching @ Aramco</h1>

On my way to <a href="https://www.google.com/maps/place/Dammam/@26.3622887,48.7235797,8z/data=!4m6!3m5!1s0x3e361d32276b3403:0xefd901ec7a5e5676!8m2!3d26.4206828!4d50.0887943!16zL20vMDJmZ3Z6?entry=ttu">Dammam!</a> I am teaching an "Introduction to Machine Learning" course within an MS in Data Science program offered by KAUST to selected employees of Saudi Aramco. My TAs: Alexander Tyurin and Rafal Szlendak. Four days of teaching in a row, starting tomorrow. Some break. And then again, four more days. Will be fun. Will be tiring.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>May 30, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.18929">
"Clip21: Error Feedback for Gradient Clipping"</a> -
joint work with
 <a href="https://richtarik.org/i_team.html">Sarit Khirirat</a>, 
 <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>, 
 <a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth</a>,
  <a href="https://rustem-islamov.github.io">Rustem Islamov</a>,
  and
 <a href="https://uwaterloo.ca/scholar/karray">Fakhri Karray</a>.
 <br>
  <br>
 
 We found out how to correct for the error caused by gradient clipping.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.18929">
<img alt="Clip21: Error Feedback for Gradient Clipping" src="imgs/Clip21.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 29, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.18627">
"Global-QSGD: Practical Floatless Quantization for Distributed Learning with Theoretical Guarantees"</a> -
joint work with
 <a href="https://jihaoxin.github.io">Jihao Xin</a>, <a href="https://mcanini.github.io">Marco Canini</a> and <a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth</a>.
 
 <br>
  <br>
 
 We found out how to make gradient quantization all-reduce friendly.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.18627">
<img alt="Global-QSGD: Practical Floatless Quantization for Distributed Learning with Theoretical Guarantees" src="imgs/Global-QSGD.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 28, 2023</h3>

<h1>Back @ KAUST</h1>

I am back at KAUST now. Biggest news: I have a new glass whiteboard in my office covering an entire wall. Awesome! <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 26, 2023</h3>

<h1>Talk @ Slovak Academy of Sciences</h1>

After a week, I am visiting the Institute of Informatics of the Slovak Academy of Sciences again. They seem to be very interested in Federated Learning.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>May 25, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.16296">
"A Guide Through the Zoo of Biased SGD"</a> -
joint work with
 <a href="https://combgeo.org/en/members/yury-demidovich/">Yury Demidovich</a>, <a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky</a> and <a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a>.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.16296">
<img alt="A Guide Through the Zoo of Biased SGD" src="imgs/BiasedSGD.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 24, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.15264">
"Error Feedback Shines when Features are Rare"</a> -
joint work with
 <a href="https://elnurgasanov.com">Elnur Gasanov</a> and <a href="https://burlachenkok.github.io">Konstantin Burlachenko</a>.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.15264">
<img alt="Error Feedback Shines when Features are Rare" src="imgs/EF21-sparse.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 24, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.15155">
"Momentum Provably Improves Error Feedback!"</a> -
joint work with
 <a href="https://ai.ethz.ch/about-us/people/ilyas-fatkhullin.html">Ilyas Fatkhullin</a> and <a href="https://k3nfalt.github.io/">Alexander Tyurin</a>.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.15155">
<img alt="Momentum Provably Improves Error Feedback!" src="imgs/EF21-SGDM.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 23, 2023</h3>

<h1>Talk @ Apple</h1>

Today I am giving a talk at Apple, targeting their Federated Learning team. My talk title:  "On 5th Generation of Local Training Methods in Federated Learning". 

<br>

<br>
<a href="talks/TALK-2023-05-23-Apple.pdf"><img alt="My talk." src="imgs/2023-05-23-Apple.png" width="720"></a>
<br>

<a href="talks/TALK-2023-05-23-Apple.pdf">Here are the slides.</a>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 22, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.13170">
"Explicit Personalization and Local Training: Double Communication Acceleration in Federated Learning"</a> -
joint work with
 <a href="https://kaiyi.me">Kai Yi</a> and <a href="https://lcondat.github.io">Laurent Condat</a>.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.13170">
<img alt="Explicit Personalization and Local Training: Double Communication Acceleration in Federated Learning" src="imgs/Scafflix.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 21, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.12387">
"Optimal Time Complexities of Parallel Stochastic Optimization Methods Under a Fixed Computation Model"</a> -
joint work with
 <a href="https://k3nfalt.github.io">Alexander Tyurin</a>.
 
<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.12387">
<img alt="Optimal Time Complexities of Parallel Stochastic Optimization Methods Under a Fixed Computation Model" src="imgs/RennalaSGD.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 21, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.12379">
"2Direction: Theoretically Faster Distributed Training with Bidirectional Communication Compression"</a> -
joint work with
 <a href="https://k3nfalt.github.io">Alexander Tyurin</a>.

<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.12379">
<img alt="2Direction: Theoretically Faster Distributed Training with Bidirectional Communication Compression" src="imgs/2Direction.png" width="500">
</a>
</center>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 21, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2305.12568">
"Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization"</a> -
joint work with
Hanmin Li and <a href="https://avetx.github.io">Avetik Karagulyan</a>.

<br>
<br>
<center>
<a href="https://arxiv.org/abs/2305.12568">
<img alt="Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization" src="imgs/Det-CGD.png" width="500">
</a>
</center>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 19, 2023</h3>

<h1>Talk @ Slovak Academy of Sciences</h1>

I am visiting the Institute of Informatics of the Slovak Academy of Sciences to give a research seminar talk.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 17, 2023</h3>

<h1>NeurIPS 2023 Deadline</h1>

Finalizing our submissions! I'll need to sleep for 24 hours after this...
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 15, 2023</h3>

<h1>Research Talk @ KInIT</h1>

Today, I am giving a research talk at the <a href="https://kinit.sk/">Kempelen Institute of Intelligent Technologies (KInIT)</a> in Bratislava, Slovakia. I will talk about some new results on adaptive stepsizes in optimization.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

        
<h3>May 14, 2023</h3>

<h1>Exam Week @ KAUST</h1>

It's the exam week at KAUST. Students in my Federated Learning (CS 331) class are finalizing their project reports.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>April 29, 2023</h3>

<h1>ICLR 2023 @ Kigali, Rwanda</h1>

I am heading to Kigali, Rwanda, to attend <a href="https://iclr.cc/Conferences/2023">ICLR 2023.</a> Several people from  my team are going, too, including Laurent Condat, Alexander Tyurin, Egor Shulgin, Slavomír Hanzely, and former members Samuel Horváth, Eduard Gorbunov, Adil Salim and Ahmed Khaled. 
<br>
<br>
We will present three papers: 1) <a href="https://openreview.net/forum?id=VA1YpcNr7ul">DASHA</a>, 2) <a href="https://openreview.net/forum?id=pfuqQQCB34">Byz-MARINA</a>, and 3) <a href="https://openreview.net/forum?id=cB4N3G5udUS">RandProx</a>. 
<br>
<br>
Update: I have seen mountain gorillas!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

        
<h3>April 23, 2023</h3>

<h1>Talk @ Qualcomm AI Research</h1>

Later today, I am giving a research talk in the DistributedML Seminar @ Qualcomm AI Research.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

        
        
<h3>March 8, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2303.04622">
"ELF: Federated Langevin Algorithms with Primal, Dual and Bidirectional Compression"</a> -
joint work with
<a href="https://avetx.github.io">Avetik Karagulyan</a>.

<br>
<br>
Abstract:
<i>
Federated sampling algorithms have recently gained great popularity in the community of machine learning and statistics. This paper studies variants of such algorithms called Error Feedback Langevin algorithms (ELF). In particular, we analyze the combinations of EF21 and EF21-P with the federated Langevin Monte-Carlo. We propose three algorithms: P-ELF, D-ELF, and B-ELF that use, respectively, primal, dual, and bidirectional compressors. We analyze the proposed methods under Log-Sobolev inequality and provide non-asymptotic convergence guarantees.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


        
<h3>February 20, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2302.09832">
"TAMUNA: Accelerated Federated Learning with Local Training and Partial Participation"</a> -
joint work with
<a href="https://lcondat.github.io/">Laurent Condat</a> and
<a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky</a>.

<br>
<br>
Abstract:
<i>
In federated learning, a large number of users are involved in a global learning task, in a collaborative way. They alternate local computations and communication with a distant server. Communication, which can be slow and costly, is the main bottleneck in this setting. To accelerate distributed gradient descent, the popular strategy of local training is to communicate less frequently; that is, to perform several iterations of local computations between the communication steps. A recent breakthrough in this field was made by Mishchenko et al. (2022): their Scaffnew algorithm is the first to probably benefit from local training, with accelerated communication complexity. However, it was an open and challenging question to know whether the powerful mechanism behind Scaffnew would be compatible with partial participation, the desirable feature that not all clients need to participate to every round of the training process. We answer this question positively and propose a new algorithm, which handles local training and partial participation, with state-of-the-art communication complexity.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 1, 2023</h3>

<h1>New Research Intern: Dinis Seward (Oxford)</h1>

Dinis Douglas Guerreiro Seward just joined our team as a <a href="https://vsrp.kaust.edu.sa/">VSRP intern</a>. Dinis studies towards an MSc degree in Mathematical Modelling and Scientific Computing at the University of Oxford. Prior to this, he obtained a BSc degree in Applied Mathematics from Universidade de Lisboa, Portugal.
<br>
<br>

Besides mathematics and optimization, Dinis is interested in graph theory, mathematical biology and artificial intelligence. Dinis' numerous accomplishments include:
<ul>

<li>St. Peter’s College Foundation Graduate Award, 2021 (Awarded to cover expenses during graduate studies)</li>

<li>Scholarship New Talents in Artificial Intelligence, Calouste Gulbenkian Foundation, 2019-2020 (10-month research scholarship awarded to 8 promising BSc students nationwide with research potential in the field of Artificial Intelligence.)</li>

<li>IGC Summer School Scholarship, 2019</li>

<li>Merit Diploma for outstanding academic performance, 2019 (Awarded for outstanding academic performance during the 2017-18 academic year)</li>

<li>Erasmus+ Scholarship, 2018 (Awarded by the European Commission to students admitted into the Erasmus+ exchange programme)</li>

<li>Scholarship New Talents in Mathematics, Calouste Gulbenkian Foundation, 2017-2018 (10-month research scholarship awarded to 20 promising BSc students nationwide pursuing degrees with strong mathematical background)</li>

<li>FCIENCIAS.ID Award - Honourable Mention (Awarded to the best students in the first year of a BSc degree at Faculdade de Ciencias da Universidade de Lisboa)</li>

<li>Academic Merit Scholarship (Scholarship covering a 1-year tuition fee. Awarded for outstanding academic performance during the 2016-17 academic year)</li>

<li>Merit Diploma, 2018 (Awarded for outstanding academic performance during the 2016-17 academic year)</li>
</ul>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


        
<h3>February 8, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2302.03662">
"Federated Learning with Regularized Client Participation"</a> -
joint work with
<a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky</a>,
<a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth</a>, and
<a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a>.

<br>
<br>
Abstract:
<i>
Federated Learning (FL) is a distributed machine learning approach where multiple clients work together to solve a machine learning task. One of the key challenges in FL is the issue of partial participation, which occurs when a large number of clients are involved in the training process. The traditional method to address this problem is randomly selecting a subset of clients at each communication round. In our research, we propose a new technique and design a novel regularized client participation scheme. Under this scheme, each client joins the learning process every R communication rounds, which we refer to as a meta epoch. We have found that this participation scheme leads to a reduction in the variance caused by client sampling. Combined with the popular FedAvg algorithm (McMahan et al., 2017), it results in superior rates under standard assumptions. For instance, the optimization term in our main convergence bound decreases linearly with the product of the number of communication rounds and the size of the local dataset of each client, and the statistical term scales with step size quadratically instead of linearly (the case for client sampling with replacement), leading to better convergence rate O(1/T^2) compared to O(1/T), where T is the total number of communication rounds. Furthermore, our results permit arbitrary client availability as long as each client is available for training once per each meta epoch..
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


        

<h3>February 2, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2302.00999">
"High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance"</a> -
joint work with
<a href="https://scholar.google.com/citations?user=R-xZRIAAAAAJ">Abdurakhmon Sadiev</a>,
<a href="https://marinadanya.github.io">Marina Danilova</a>,
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>,
<a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth</a>,
<a href="https://gauthiergidel.github.io">Gauthier Gidel</a>,
<a href="http://wias-berlin.de/people/dvureche/">Pavel Dvurechensky</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.

<br>
<br>
Abstract:
<i>
During recent years the interest of optimization and machine learning communities in high-probability convergence of stochastic optimization methods has been growing. One of the main reasons for this is that high-probability complexity bounds are more accurate and less studied than in-expectation ones. However, SOTA high-probability non-asymptotic convergence results are derived under strong assumptions such as the boundedness of the gradient noise variance or of the objective's gradient itself. In this paper, we propose several algorithms with high-probability convergence results under less restrictive assumptions. In particular, we derive new high-probability convergence results under the assumption that the gradient/operator noise has bounded central α-th moment for α∈(1,2] in the following setups: (i) smooth non-convex / Polyak-Lojasiewicz / convex / strongly convex / quasi-strongly convex minimization problems, (ii) Lipschitz / star-cocoercive and monotone / quasi-strongly monotone variational inequalities. These results justify the usage of the considered methods for solving problems that do not fit standard functional classes studied in stochastic optimization.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 26, 2023</h3>

<h1>ICML 2023 Deadline</h1>

The <a href="https://icml.cc/Conferences/2023">ICML 2023</a> deadline is today, we are all working towards winning tickets to Hawaii!


<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 25, 2023</h3>

<h1>New PhD Student: Ivan Ilin</h1>


Ivan Ilin is joining my team as a PhD student, starting in Spring 2023. He arrived to KAUST today! <br> <br>

Ivan studied BS and MS programs in Automation of Physical
and Technical Research at <a href="https://en.wikipedia.org/wiki/Novosibirsk_State_University">Novosibirsk State University</a> since 2017. According to US News, Novosibirsk State University ranks #5 in Russia,
after Lomonosov Moscow State University (1), Moscow Institute of Physics & Technology (2), National Research Nuclear University MEPhI (3),
and National Research University Higher School of Economics (4), and before Tomsk State University (6), Sechenov First Moscow State Medical
University (7), Saint Petersburg State University (8), Peter the Great St. Petersburg Polytechnic University (9), ITMO University (10),
and Skolkovo Institute of Science & Technology (11). <br> <br>

His work experience includes:
<ul>
<li>Undergraduate Research Assistant, Budker Institute of Nuclear Physics, Novosibirsk, Russia, 2020</li>
<li>Deep learning Junior Researcher, ExpaSoft, Novosibirsk, Russia, since 2020</li>
<li>Lavrentyev Institute of Hydrodynamics, Novosibirsk, Russia, 2018-2019</li>
</ul>

Ivan has so far been interested ML and DL in general, and in these topics in particular: image generation and recognition, NLP,
voice generation, application of ML and DL in games, foundation and advanced 3d graphics generation, physics simulations,
game design, advanced programmed animations by manim or other libraries, product and gadget design, advertisement. <br> <br>

In 2016-2017, Ivan was the captain of the Russian team in International Young Physicists Tournament in Singapore/Russia.<br><br>

Links to Ivan's:
<ul>
<li>website: <a href="http://ilinblog.ru">http://ilinblog.ru</a></li>
<li>youtube channel: <a href="https://www.youtube.com/c/vectozavr">https://www.youtube.com/c/vectozavr</a></li>
<li>online math school project: <a href="https://vectozavr.ru">https://vectozavr.ru</a></li>
<li>facebook: <a href="https://www.facebook.com/ivan.ilin.12935">https://www.facebook.com/ivan.ilin.12935</a></li>
</ul>

Ivan wrote several articles, they can be found on his website in the "Мои статьи и исследования" section.<br><br>

Ivan, Welcome to KAUST and to the team!!!


<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 22, 2023</h3>

<h1>New Semester</h1>

The Spring 2023 semester at KAUST started today. I am teaching CS 332: Federated Learning.


<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 21, 2023</h3>
<h1>Papers Accepted to AISTATS 2023 and ICLR 2023</h1>


<br>
We've had several papers accepted to <a href="http://aistats.org/aistats2023/">26th International Conference on Artificial Intelligence and Statistics (AISTATS 2023) </a>
and <a href="https://iclr.cc/Conferences/2023">11th International Conference on Learning Representations (ICLR 2023)</a>.
Here they are:

<br><br>

<br>
<img alt="AISTATS" src="imgs/AISTATS_logo.png" height="100">
<br>

<a href="https://scholar.google.com/citations?hl=en&user=vN2ALVYAAAAJ">Michał Grudzień*</a>, <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky*</a> and Peter Richtárik<br>
<b>Can 5th Generation Local Training Methods Support Partial Client Participation? Yes!</b>  <br>
Accepted to AISTATS 2023  <a href="https://arxiv.org/abs/2212.14370">[arXiv]</a> <br>
<br>

<a href="https://lukangsun.github.io">Lukang Sun*</a>, <a href="https://avetx.github.io">Avetik Karagulyan*</a> and Peter Richtárik<br>
<b>Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition</b> <br>
Accepted to AISTATS 2023 <a href="https://arxiv.org/abs/2206.00508">[arXiv]</a> <br>
<br>

<a href="https://qianxunk.github.io">Xun Qian*</a>, <a href="http://hendrydong.github.io">Hanze Dong</a>, <a href="http://tongzhang-ml.org">Tong Zhang</a> and Peter Richtárik<br>
<b>Catalyst Acceleration of Error Compensated Methods Leads to Better Communication Complexity</b> <br>
Accepted to AISTATS 2023 <a href="https://arxiv.org/abs/2301.09893">[arXiv]</a> <br>
<br>

<br>
<img alt="ICLR" src="imgs/ICLR_logo.png" height="100">
<br>

<a href="https://k3nfalt.github.io">Alexander Tyurin*</a> and Peter Richtárik<br>
<b>DASHA: Distributed Nonconvex Optimization with Communication Compression and Optimal Oracle Complexity</b> <br>
Accepted to ICLR 2023 <a href="https://arxiv.org/abs/2202.01268">[arXiv]</a> <a href="https://openreview.net/forum?id=VA1YpcNr7ul">[OpenReview]</a> <br>
<br>

<a href="https://eduardgorbunov.github.io/">Eduard Gorbunov*</a>, <a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth*</a>, <a href="https://gauthiergidel.github.io/">Gauthier Gidel</a> and Peter Richtárik<br>
<b>Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top</b> <br>
Accepted to ICLR 2023 <a href="https://arxiv.org/abs/2206.00529">[arXiv]</a> <a href="https://openreview.net/forum?id=pfuqQQCB34">[OpenReview]</a> <br>
<br>

<a href="https://lcondat.github.io/">Laurent Condat*</a> and Peter Richtárik<br>
<b>RandProx: Primal-Dual Optimization Algorithms with Randomized Proximal Updates</b> <br>
Accepted to ICLR 2023 <a href="https://arxiv.org/abs/2207.12891">[arXiv]</a>  <a href="https://openreview.net/forum?id=cB4N3G5udUS">[OpenReview]</a> <br>
<br>


<br>
(*) Members of my <a href="https://richtarik.org/i_team.html">Optimization and Machine Learning Lab</a> at KAUST.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>January 20, 2023</h3>

<h1>Visiting Rice University</h1>

I am giving a <a href="https://events.rice.edu/#!view/event/event_id/340417">CMOR Special Lecture</a> at Rice University today, and meeting several people, including
Shiqian Ma, Anastasios (Tasos) Kyrillidis,  Sebastian Perez-Salazar, Matthias Heinkenschloss, Eric Chi, Jingfeng Wu,
César A. Uribe,  Teng Zhang and Illya Hicks. Looking forward to the conversations!


<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 17, 2023</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2301.06806">
"Convergence of First-Order Algorithms for Meta-Learning with Moreau Envelopes"</a> -
joint work with
<a href="https://www.konstmish.com">Konstantin Mishchenko</a> and
<a href="https://slavomir-hanzely.github.io">Slavomír Hanzely</a>.


<br>
<br>
Abstract:
<i>
In this work, we consider the problem of minimizing the sum of Moreau envelopes of given functions, which has previously appeared in the context of meta-learning and personalized federated learning. In contrast to the existing theory that requires running subsolvers until a certain precision is reached, we only assume that a finite number of gradient steps is taken at each iteration. As a special case, our theory allows us to show the convergence of First-Order Model-Agnostic Meta-Learning (FO-MAML) to the vicinity of a solution of Moreau objective. We also study a more general family of first-order algorithms that can be viewed as a generalization of FO-MAML. Our main theoretical achievement is a theoretical improvement upon the inexact SGD framework. In particular, our perturbed-iterate analysis allows for tighter guarantees that improve the dependency on the problem's conditioning. In contrast to the related work on meta-learning, ours does not require any assumptions on the Hessian smoothness, and can leverage smoothness and convexity of the reformulation based on Moreau envelopes. Furthermore, to fill the gaps in the comparison of FO-MAML to the Implicit MAML (iMAML), we show that the objective of iMAML is neither smooth nor convex, implying that it has no convergence guarantees based on the existing theory.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 6, 2023</h3>

<h1>Paper Accepted to TMLR</h1>

The paper <a href="https://arxiv.org/abs/2002.03329">Better Theory for SGD in the Nonconvex World</a>, joint work with <a href="https://rka97.github.io">Ahmed Khaled</a>, was accepted to <a href="https://openreview.net/forum?id=AU4qHN2VkS&referrer=%5BTMLR%5D(%2Fgroup%3Fid%3DTMLR)">TMLR</a>. I have taught the key result from this paper in my CS331 (Stochastic Gradient Descent Methods) course at KAUST for a couple years already!

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>December 29, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2212.14370">
"Can 5th Generation Local Training Methods Support Client Sampling? Yes!"</a> -
joint work with
<a href="https://www.linkedin.com/in/michał-grudzień-2141a2198/?originalSubdomain=uk">Michał Grudzień</a> and
<a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky</a>.


<br>
<br>
Abstract:
<i>
The celebrated FedAvg algorithm of McMahan et al. (2017) is based on three components: client sampling (CS), data sampling (DS) and local training (LT). While the first two are reasonably well understood, the third component, whose role is to reduce the number of communication rounds needed to train the model, resisted all attempts at a satisfactory theoretical explanation. Malinovsky et al. (2022) identified four distinct generations of LT methods based on the quality of the provided theoretical communication complexity guarantees. Despite a lot of progress in this area, none of the existing works were able to show that it is theoretically better to employ multiple local gradient-type steps (i.e., to engage in LT) than to rely on a single local gradient-type step only in the important heterogeneous data regime. In a recent breakthrough embodied in their ProxSkip method and its theoretical analysis, Mishchenko et al. (2022) showed that LT indeed leads to provable communication acceleration for arbitrarily heterogeneous data, thus jump-starting the 5th generation of LT methods. However, while these latest generation LT methods are compatible with DS, none of them support CS. We resolve this open problem in the affirmative. In order to do so, we had to base our algorithmic development on new algorithmic and theoretical foundations.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>December 13, 2022</h3>

<h1>Vacation</h1>

I am on vacation until the end of the year. This includes the 3rd place World Cup match!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>December 3, 2022</h3>

<h1>Attending "Optimization in the Big Data Era" workshop in Singapore</h1>

I am flying to Singapore (my first time ever!) to attend the <a href="https://ims.nus.edu.sg/events/optimization-in-the-big-data-era/">Optimization in the Big Data Era</a> workshop at the  Institute for Mathematical Sciences, National University of Singapore, organized
by <a href="http://pages.cs.wisc.edu/~swright/">Stephen J. Wright</a> (University of Wisconsin), <a href="https://www.polyu.edu.hk/ama/profile/dfsun/">Defeng Sun</a> (The Hong Kong Polytechnic University) and <a href="http://ww1.math.nus.edu.sg/staff.aspx?s=mattohkc">Kim Chuan Toh</a>
(National University of Singapore). The event was planned long ago, but got delayed because of the Covid 19 situation. It's finally taking place now!

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>November 26, 2022</h3>

<h1>NeurIPS 2022 @ New Orleans</h1>

Several members of my lab are attending  the <a href="https://nips.cc/Conferences/2022">36th Annual Conference on Neural Information Processing Systems (NeurIPS 2022)</a> in New Orleans.
We have 12 papers accepted. In addition, we presented 3 workshop papers.

<br><br>
Our NeurIPS 2022 conference papers:
<br>
<br>

<br>
<img alt="KAUST Optimization and Machine Learning Lab" src="imgs/OML-richtarik-NeurIPS2022.jpeg" width="720">
<br>


<b> 1) "Accelerated Primal-Dual Gradient Method for Smooth and Convex-Concave Saddle-Point Problems with Bilinear Coupling"</b>
<a href="https://arxiv.org/abs/2112.15199">[arXiv]</a>  -
joint work with
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 2) "The First Optimal Algorithm for Smooth and Strongly-Convex-Strongly-Concave Minimax Optimization"</b>
<a href="https://arxiv.org/abs/2205.05653">[arXiv]</a>  -
the work of
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 3) "A Damped Newton Method Achieves Global $O(1/k^2)$ and Local Quadratic Convergence Rate"</b> -
joint work with
<a href="https://slavomir-hanzely.github.io/">Slavomir Hanzely (*)</a>
<a href="https://www.researchgate.net/profile/Dmitry-Kamzolov">Dmitry Kamzolov</a>
<a href="http://dmivilensky.ru/">Dmitry Pasechnyuk</a>
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>, and
<a href="https://mtakac.com/">Martin Takáč</a>.
<br>
<br>

<b> 4) "Variance Reduced ProxSkip: Algorithm, Theory and Application to Federated Learning"</b>
<a href="https://arxiv.org/abs/2207.04338">[arXiv]</a>  -
joint work with
<a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky (*)</a> and
<a href="https://kaiyi.me/">Kai Yi (*)</a>.
<br>
<br>

<b> 5) "Theoretically Better and Numerically Faster Distributed Optimization with Smoothness-Aware Quantization Techniques"</b>
<a href="https://arxiv.org/abs/2106.03524">[arXiv]</a>  -
joint work with
<a href="https://bokun-wang.github.io/">Bokun Wang (*)</a> and
<a href="https://mher-safaryan.github.io/">Mher Safaryan (*)</a>.
<br>
<br>

<b> 6) "Communication Acceleration of Local Gradient Methods via an Accelerated Primal-Dual Algorithm with an Inexact Prox"</b>
<a href="https://arxiv.org/abs/2207.03957">[arXiv]</a>  -
joint work with
<a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">Abdurakhmon Sadiev (*)</a> and
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a>.
<br>
<br>

<b> 7) "Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees"</b>
<a href="https://arxiv.org/abs/2110.03313">[arXiv]</a>  -
joint work with
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://www.hse.ru/en/staff/yhn112">Michael Diskin</a>,
<a href="https://scholar.google.com/citations?user=930PERsAAAAJ&hl=en">Max Ryabinin</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 8) "BEER: Fast $O(1/T)$ Rate for Decentralized Nonconvex Optimization with Communication Compression"</b>
<a href="https://arxiv.org/abs/2201.13320">[arXiv]</a>  -
joint work with
<a href="https://hyzhao.me/">Haoyu Zhao</a>,
<a href="https://www.lti.cs.cmu.edu/people/222218029/boyue-li">Boyue Li</a>,
<a href="https://zhizeli.github.io/">Zhize Li (*)</a> and
<a href="https://users.ece.cmu.edu/~yuejiec/">Yuejie Chi</a>.
<br>
<br>

<b> 9) "The First Optimal Acceleration of High-Order Methods in Smooth Convex Optimization"</b>
<a href="https://arxiv.org/abs/2205.09647">[arXiv]</a>  -
the work of
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 10) "Optimal Gradient Sliding and its Application to Optimal Distributed Optimization Under Similarity"</b>
<a href="https://arxiv.org/abs/2205.15136">[arXiv]</a>  -
the work of
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a>,
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://scholar.google.be/citations?user=9Dapoy8AAAAJ&hl=fr">Ekaterina Borodich</a>,
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a> and
<a href="https://engineering.purdue.edu/~gscutari/">Gesualdo Scutari</a>.
<br>
<br>

<b> 11) "Optimal Algorithms for Decentralized Stochastic Variational Inequalities"</b>
<a href="https://arxiv.org/abs/2202.02771">[arXiv]</a>  -
joint work with
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a>,
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">Abdurakhmon Sadiev (*)</a>,
<a href="https://engineering.purdue.edu/~gscutari/"> Michael Persiianov</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 12) "EF-BV: A Unified Theory of Error Feedback and Variance Reduction Mechanisms for Biased and Unbiased Compression in
Distributed Optimization"</b>
<a href="https://arxiv.org/abs/2205.04180">[arXiv]</a>  -
joint work with
<a href="https://lcondat.github.io/">Laurent Condat (*)</a> and
<a href="https://kaiyi.me/">Kai Yi (*)</a>.
<br>
<br>

<br><br>
(*) Members of my Optimization and Machine Learning Lab at KAUST.
<br><br>

<br><br>
Our NeurIPS 2022 workshop papers:
<br>
<br>

<b> 13) "Certified Robustness in Federated Learning" @ Federated Learning NeurIPS 2022 Workshop</b>
<a href="https://arxiv.org/abs/2206.02535">[arXiv]</a>  -
joint work with
<a href="https://motasemalfarra.netlify.app">Motasem Alfarra</a>,
<a href="https://www.juancprzs.com">Juan C. Pérez</a>,
<a href="https://shulgin-egor.github.io">Egor Shulgin</a>, and
<a href="https://www.bernardghanem.com">Bernard Ghanem</a>
<br>
<br>

<b> 14) "Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation" @ HOOML NeurIPS 2022 Workshop</b>
<a href="https://arxiv.org/abs/2206.03588">[arXiv]</a>  -
joint work with
<a href="https://rustem-islamov.github.io/">Rustem Islamov</a>,
<a href="https://qianxunk.github.io/">Xun Qian</a>,
<a href="https://slavomir-hanzely.github.io/">Slavomír Hanzely</a>, and
<a href="https://mher-safaryan.github.io/">Mher Safaryan</a>.
<br>
<br>

<b> 15) "RandProx: Primal-Dual Optimization Algorithms with Randomized Proximal Updates" @ OPT NeurIPS 2022 Workshop</b>
<a href="https://arxiv.org/abs/2207.12891">[arXiv]</a>  -
joint work with
<a href="https://lcondat.github.io/">Laurent Condat</a>.
<br>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>November 16, 2022</h3>

<h1>Teaching at Saudi Aramco</h1>

Just arrived to Al Khobar, a Saudi Arabian city on the Arabian Gulf. I will be teaching "Introduction to Optimization" in a
KAUST-Aramco MS program that started this year. My PhD students <a href="https://cemse.kaust.edu.sa/amcs/people/person/igor-sokolov">Igor Sokolov</a> and
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> are here with me as TAs. Four full days of teaching (6 hrs a day), and then four
more at the end of November. Should be fun!
</a>

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>November 9-10, 2022</h3>

<h1>Attending 2022 Workshop on Federated Learning and Analytics</h1>


As in the past years, this year I have again been invited to attend a workshop organized by the Google Federated Learning team entitled
"2022 Workshop on Federated Learning and Analytics". While this is an invite-only event, I can share my slides.

<br>
<a href="talks/TALK-2022-11-10-Google-FL-Workshop.pdf">
<img alt="" src="imgs/FL2022-leading-slide.png" width="700">
</a>
<br>

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>November 2, 2022</h3>

<h1>Teaching at Saudi Aramco</h1>

Just arrived to Al Khobar, a Saudi Arabian city on the Arabian Gulf. I will be teaching "Introduction to Optimization" in a
KAUST-Aramco MS program that started this year. My PhD students <a href="https://cemse.kaust.edu.sa/amcs/people/person/igor-sokolov">Igor Sokolov</a> and
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> are here with me as TAs. Four full days of teaching (6 hrs a day), and then four
more at the end of November. Should be fun!
</a>

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 31, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2211.00188">
"Adaptive Compression for Communication-Efficient Distributed Training"</a> -
joint work with
<a href="https://www.makarenko.co/">Maksim Makarenko</a>,
<a href="https://elnurgasanov.com/">Elnur Gasanov</a>,
<a href="https://rustem-islamov.github.io/">Rustem Islamov</a>, and
<a href="https://scholar.google.com/citations?user=g0CzD50AAAAJ&hl=en">Abdurakhmon Sadiev</a>.


<br>
<br>
Abstract:
<i>
We propose Adaptive Compressed Gradient Descent (AdaCGD) - a novel optimization algorithm for communication-efficient training of supervised machine learning models with adaptive compression level. Our approach is inspired by the recently proposed three point compressor (3PC) framework of Richtarik et al. (2022), which includes error feedback (EF21), lazily aggregated gradient (LAG), and their combination as special cases, and offers the current state-of-the-art rates for these methods under weak assumptions. While the above mechanisms offer a fixed compression level, or adapt between two extremes only, our proposal is to perform a much finer adaptation. In particular, we allow the user to choose any number of arbitrarily chosen contractive compression mechanisms, such as Top-K sparsification with a user-defined selection of sparsification levels K, or quantization with a user-defined selection of quantization levels, or their combination. AdaCGD chooses the appropriate compressor and compression level adaptively during the optimization process. Besides i) proposing a theoretically-grounded multi-adaptive communication compression mechanism, we further ii) extend the 3PC framework to bidirectional compression, i.e., we allow the server to compress as well, and iii) provide sharp convergence bounds in the strongly convex, convex and nonconvex settings. The convex regime results are new even for several key special cases of our general mechanism, including 3PC and EF21. In all regimes, our rates are superior compared to all existing adaptive compression methods.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 31, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2211.00140">
"A Damped Newton Method Achieves Global O(1/k^2) and Local Quadratic Convergence Rate"</a> -
joint work with
<a href="https://slavomir-hanzely.github.io">Slavomír Hanzely</a>,
<a href="https://scholar.google.com/citations?user=7ezSoS-hLVAC&hl=en">Dmitry Kamzolov</a>,
<a href="http://dmivilensky.ru">Dmitry Pasechnyuk</a>,
<a href="https://scholar.google.com/citations?user=AmeE8qkAAAAJ">Alexander Gasnikov</a>, and
<a href="https://mtakac.com">Martin Takáč</a>.


<br>
<br>
Abstract:
<i>
In this paper, we present the first stepsize schedule for Newton method resulting in fast global and local convergence guarantees. In particular, a) we prove an $O(1/k^2)$ global rate, which matches the state-of-the-art global rate of cubically regularized Newton method of Polyak and Nesterov (2006) and of regularized Newton method of Mishchenko (2021) and Doikov and Nesterov (2021), b) we prove a local quadratic rate, which matches the best-known local rate of second-order methods, and c) our stepsize formula is simple, explicit, and does not require solving any subproblem. Our convergence proofs hold under affine-invariance assumptions closely related to the notion of self-concordance. Finally, our method has competitive performance when compared to existing baselines, which share the same fast global convergence guarantees.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 28, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2210.16402">
"GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity"</a> -
joint work with
<a href="https://www.linkedin.com/in/arto-maranjyan/?originalSubdomain=am">Artavazd Maranjyan</a> and <a href="https://mher-safaryan.github.io">Mher Safaryan</a>.


<br>
<br>
Abstract:
<i>
In this work, we study distributed optimization algorithms that reduce the high communication costs of synchronization by allowing clients to perform multiple local gradient steps in each communication round. Recently, Mishchenko et al. (2022) proposed a new type of local method, called ProxSkip, that enjoys an accelerated communication complexity without any data similarity condition. However, their method requires all clients to call local gradient oracles with the same frequency. Because of statistical heterogeneity, we argue that clients with well-conditioned local problems should compute their local gradients less frequently than clients with ill-conditioned local problems. Our first contribution is the extension of the original ProxSkip method to the setup where clients are allowed to perform a different number of local gradient steps in each communication round. We prove that our modified method, GradSkip, still converges linearly, has the same accelerated communication complexity, and the required frequency for local gradient computations is proportional to the local condition number. Next, we generalize our method by extending the randomness of probabilistic alternations to arbitrary unbiased compression operators and considering a generic proximable regularizer. This generalization, GradSkip+, recovers several related methods in the literature. Finally, we present an empirical study to confirm our theoretical claims.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>October 24, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2210.13277">
"Provably Doubly Accelerated Federated Learning: The First Theoretically Successful Combination of Local Training and Compressed Communication"</a> -
joint work with
<a href="https://lcondat.github.io">Laurent Condat</a> and <a href="https://kinit.sk/member/ivan-agarsky/">Ivan Agarský</a>.


<br>
<br>
Abstract:
<i>
In the modern paradigm of federated learning, a large number of users are involved in a global learning task, in a collaborative way. They alternate local computations and two-way communication with a distant orchestrating server. Communication, which can be slow and costly, is the main bottleneck in this setting. To reduce the communication load and therefore accelerate distributed gradient descent, two strategies are popular: 1) communicate less frequently; that is, perform several iterations of local computations between the communication rounds; and 2) communicate compressed information instead of full-dimensional vectors. In this paper, we propose the first algorithm for distributed optimization and federated learning, which harnesses these two strategies jointly and converges linearly to an exact solution, with a doubly accelerated rate: our algorithm benefits from the two acceleration mechanisms provided by local training and compression, namely a better dependency on the condition number of the functions and on the dimension of the model, respectively.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 6, 2022</h3>

<h1>Visiting MBZUAI in Abu Dhabi</h1>

I am on my way to Abu Dhabi to visit <a href="https://mbzuai.ac.ae">MBZUAI</a>, where I am an Adjunct Professor.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 5, 2022</h3>

<h1>Talk at the One World Seminar Series on the Mathematics of Machine Learning</h1>

Today, I am  giving a virtual (Zoom) talk at the <a href="https://www.oneworldml.org/home">One World Seminar Series on the Mathematics of Machine Learning</a>.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>October 2, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2210.00462">
"Improved Stein Variational Gradient Descent with Importance Weights"</a> -
joint work with
<a href="https://lukangsun.github.io">Lukang Sun</a>.


<br>
<br>
Abstract:
<i>
Stein Variational Gradient Descent~(SVGD) is a popular sampling algorithm used in various machine learning tasks. It is well known that SVGD arises from a discretization of the kernelized gradient flow of the Kullback-Leibler divergence $D_{KL}(\cdot\mid\pi)$, where $\pi$ is the target distribution. In this work, we propose to enhance SVGD via the introduction of  importance weights, which leads to a new method for which we coin the name $\beta$-SVGD. In the continuous time and infinite particles regime, the time for this flow to converge to the equilibrium distribution $\pi$, quantified by the Stein Fisher information, depends on $\rho_0$ and $\pi$ very weakly. This is very different from the kernelized gradient flow of Kullback-Leibler divergence, whose time complexity depends on $D_{KL}(\rho_0\mid\pi)$. Under certain assumptions, we provide a descent lemma for the population limit $\beta$-SVGD, which covers the descent lemma for the population limit SVGD when $\beta\to 0$. We also illustrate the advantages of $\beta$-SVGD over SVGD by simple experiments.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>September 30, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2209.15218">
"EF21-P and Friends: Improved Theoretical Communication Complexity for Distributed Optimization with Bidirectional Compression"</a> -
joint work with
Kaja Gruntkowska and <a href="https://k3nfalt.github.io">Alexander Tyurin</a>.


<br>
<br>
Abstract:
<i>
The starting point of this paper is the discovery of a novel and simple error-feedback mechanism, which we call EF21-P, for dealing with the error introduced by a contractive compressor. Unlike all prior works on error feedback, where compression and correction operate in the dual space of gradients, our mechanism operates in the primal space of models. While we believe that EF21-P may be of interest in many situations where it is often advantageous to perform model perturbation prior to the computation of the gradient (e.g., randomized smoothing and generalization), in this work we focus our attention on its use as a key building block in the design of communication-efficient distributed optimization methods supporting bidirectional compression. In particular, we employ EF21-P as the mechanism for compressing and subsequently error-correcting the model broadcast by the server to the workers. By combining EF21-P with suitable methods performing worker-to-server compression, we obtain novel methods supporting bidirectional compression and enjoying new state-of-the-art theoretical communication complexity for convex and nonconvex problems. For example, our bounds are the first that manage to decouple the variance/error coming from the workers-to-server and server-to-workers compression, transforming a multiplicative dependence to an additive one. In the convex regime, we obtain the first bounds that match the theoretical communication complexity of gradient descent. Even in this convex regime, our algorithms work with biased gradient estimators, which is non-standard and requires new proof techniques that may be of independent interest. Finally, our theoretical results are corroborated through suitable experiments.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 29, 2022</h3>

<h1>ICLR 2023</h1>

The ICLR 2023 submission deadline is over. Time to rest for a day before moving on to AISTATS 2023...


<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 16, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2209.07883">
"Minibatch Stochastic Three Points Method for Unconstrained Smooth Minimization"</a> -
joint work with
<a href="https://www.linkedin.com/in/soumia-boucherouite/?originalSubdomain=ma">Soumia Boucherouite</a>,
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>,
and <a href="https://ehbergou.github.io">El Houcine Bergou</a>.


<br>
<br>
Abstract:
<i>
In this paper, we propose a new zero order optimization method called minibatch stochastic three points (MiSTP) method to solve an unconstrained minimization problem in a setting where only an approximation of the objective function evaluation is possible. It is based on the recently proposed stochastic three points (STP) method (Bergou et al., 2020). At each iteration, MiSTP generates a random search direction in a similar manner to STP, but chooses the next iterate based solely on the approximation of the objective function rather than its exact evaluations. We also analyze our method's complexity in the nonconvex and convex cases and evaluate its performance on multiple machine learning tasks.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 15, 2021</h3>
<h1>Papers Accepted to NeurIPS 2022</h1>
<br>
We've had several papers accepted to the <a href="https://nips.cc/Conferences/2022">36th Annual Conference on Neural Information Processing Systems (NeurIPS 2022)</a>, which will run during November 28-December 3, 2022 in New Orleans, USA.

<br><br>
Here they are:

<br>
<br>
<img alt="KAUST Optimization and Machine Learning Lab" src="imgs/OML-richtarik-NeurIPS2022.jpeg" width="720">
<br>
<br>


<b> 1) "Accelerated Primal-Dual Gradient Method for Smooth and Convex-Concave Saddle-Point Problems with Bilinear Coupling"</b>
<a href="https://arxiv.org/abs/2112.15199">[arXiv]</a>  -
joint work with
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 2) "The First Optimal Algorithm for Smooth and Strongly-Convex-Strongly-Concave Minimax Optimization"</b>
<a href="https://arxiv.org/abs/2205.05653">[arXiv]</a>  -
the work of
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 3) "A Damped Newton Method Achieves Global $O(1/k^2)$ and Local Quadratic Convergence Rate"</b> -
joint work with
<a href="https://slavomir-hanzely.github.io/">Slavomir Hanzely (*)</a>
<a href="https://www.researchgate.net/profile/Dmitry-Kamzolov">Dmitry Kamzolov</a>
<a href="http://dmivilensky.ru/">Dmitry Pasechnyuk</a>
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>, and
<a href="https://mtakac.com/">Martin Takáč</a>.
<br>
<br>

<b> 4) "Variance Reduced ProxSkip: Algorithm, Theory and Application to Federated Learning"</b>
<a href="https://arxiv.org/abs/2207.04338">[arXiv]</a>  -
joint work with
<a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky (*)</a> and
<a href="https://kaiyi.me/">Kai Yi (*)</a>.
<br>
<br>

<b> 5) "Theoretically Better and Numerically Faster Distributed Optimization with Smoothness-Aware Quantization Techniques"</b>
<a href="https://arxiv.org/abs/2106.03524">[arXiv]</a>  -
joint work with
<a href="https://bokun-wang.github.io/">Bokun Wang (*)</a> and
<a href="https://mher-safaryan.github.io/">Mher Safaryan (*)</a>.
<br>
<br>

<b> 6) "Communication Acceleration of Local Gradient Methods via an Accelerated Primal-Dual Algorithm with an Inexact Prox"</b>
<a href="https://arxiv.org/abs/2207.03957">[arXiv]</a>  -
joint work with
<a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">Abdurakhmon Sadiev (*)</a> and
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a>.
<br>
<br>

<b> 7) "Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees"</b>
<a href="https://arxiv.org/abs/2110.03313">[arXiv]</a>  -
joint work with
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://www.hse.ru/en/staff/yhn112">Michael Diskin</a>,
<a href="https://scholar.google.com/citations?user=930PERsAAAAJ&hl=en">Max Ryabinin</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 8) "BEER: Fast $O(1/T)$ Rate for Decentralized Nonconvex Optimization with Communication Compression"</b>
<a href="https://arxiv.org/abs/2201.13320">[arXiv]</a>  -
joint work with
<a href="https://hyzhao.me/">Haoyu Zhao</a>,
<a href="https://www.lti.cs.cmu.edu/people/222218029/boyue-li">Boyue Li</a>,
<a href="https://zhizeli.github.io/">Zhize Li (*)</a> and
<a href="https://users.ece.cmu.edu/~yuejiec/">Yuejie Chi</a>.
<br>
<br>

<b> 9) "The First Optimal Acceleration of High-Order Methods in Smooth Convex Optimization"</b>
<a href="https://arxiv.org/abs/2205.09647">[arXiv]</a>  -
the work of
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 10) "Optimal Gradient Sliding and its Application to Optimal Distributed Optimization Under Similarity"</b>
<a href="https://arxiv.org/abs/2205.15136">[arXiv]</a>  -
the work of
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a>,
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://scholar.google.be/citations?user=9Dapoy8AAAAJ&hl=fr">Ekaterina Borodich</a>,
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a> and
<a href="https://engineering.purdue.edu/~gscutari/">Gesualdo Scutari</a>.
<br>
<br>

<b> 11) "Optimal Algorithms for Decentralized Stochastic Variational Inequalities"</b>
<a href="https://arxiv.org/abs/2202.02771">[arXiv]</a>  -
joint work with
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev (*)</a>,
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">Abdurakhmon Sadiev (*)</a>,
<a href="https://engineering.purdue.edu/~gscutari/"> Michael Persiianov</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
<br>
<br>

<b> 12) "EF-BV: A Unified Theory of Error Feedback and Variance Reduction Mechanisms for Biased and Unbiased Compression in
Distributed Optimization"</b>
<a href="https://arxiv.org/abs/2205.04180">[arXiv]</a>  -
joint work with
<a href="https://lcondat.github.io/">Laurent Condat (*)</a> and
<a href="https://kaiyi.me/">Kai Yi (*)</a>.
<br>
<br>

<br><br>
(*) Members of my Optimization and Machine Learning Lab at KAUST.
<br><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>September 13, 2022</h3>

<h1>Talk at the Mathematics and Applications Colloquium</h1>

I am giving a talk at the <a href="https://cemse.kaust.edu.sa/events/event/resolution-theoretical-question-related-nature-local-training-federated-learning">Mathematics and Applications Colloquium</a> at KAUST.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 12, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2209.05148">
"Personalized Federated Learning with Communication Compression"</a> -
joint work with
<a href="https://ehbergou.github.io">El Houcine Bergou</a>,
<a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a>,
and <a href="http://www.aritradutta.com">Aritra Dutta</a>.


<br>
<br>
Abstract:
<i>
In contrast to training traditional machine learning (ML) models in data centers, federated learning (FL) trains ML models over local datasets contained on resource-constrained heterogeneous edge devices. Existing FL algorithms aim to learn a single global model for all participating devices, which may not be helpful to all devices participating in the training due to the heterogeneity of the data across the devices. Recently, Hanzely and Richtárik (2020) proposed a new formulation for training personalized FL models aimed at balancing the trade-off between the traditional global model and the local models that could be trained by individual devices using their private data only. They derived a new algorithm, called Loopless Gradient Descent (L2GD), to solve it and showed that this algorithms leads to improved communication complexity guarantees in regimes when more personalization is required. In this paper, we equip their L2GD algorithm with a bidirectional compression mechanism to further reduce the communication bottleneck between the local devices and the server. Unlike other compression-based algorithms used in the FL-setting, our compressed L2GD algorithm operates on a probabilistic communication protocol, where communication does not happen on a fixed schedule. Moreover, our compressed L2GD algorithm maintains a similar convergence rate as vanilla SGD without compression. To empirically validate the efficiency of our algorithm, we perform diverse numerical experiments on both convex and non-convex problems and using various compression techniques.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 28, 2022</h3>
<h1>The Fall 2022 Semester Begins!</h1>
<br>


I am back at KAUST - the Fall semester begins! I am teaching CS 331: Stochastic Gradient Descent Methods.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 15, 2022</h3>
<h1>New Research Intern: Wenzi (Tom) Fang</h1>
<br>


I am hereby welcoming <a href="https://wenzhifang.github.io">Wenzhi (Tom) Fang</a> to my team as a (remote) VS research intern! His internship started today.
Tom is an MS student in Communication and Information Systems at University of the Chinese Academy of Sciences / ShanghaiTech University.
He obtained a BS degree in Communication Engineering from Shanghai University. In the past, his research focus was on optimization of the
physical layer of wireless communication. At present, he is more interested in optimization theory and federated learning.
<br>
<br>

Tom co-authored several papers, including:
<ul>
<li>W. Fang, Y. Jiang, Y. Shi, Y. Zhou, W. Chen, and K. Letaief, “Over-the-Air Computation via Reconfigurable Intelligent Surface,” IEEE Transactions on Communications, vol. 69, no. 12, pp. 8612-8626, Dec. 2021</li>
<li>W. Fang, Y. Zou, H. Zhu, Y. Shi, and Y. Zhou, “Optimal Receive Beamforming for Over-the-Air Computation,” in Proc. IEEE SPAWC, Virtual Conferences, Sept. 2021</li>
<li>W. Fang, M. Fu, K. Wang, Y. Shi, and Y. Zhou, “Stochastic Beamforming for Reconfigurable Intelligent Surface Aided Over-the-Air Computation,” in Proc. IEEE Globecom, Virtual Conference, Dec. 2020.</li>
<li>W. Fang, M. Fu, Y. Shi, and Y. Zhou, “Outage Minimization for Intelligent Reflecting Surface Aided MISO Communication Systems via Stochastic Beamforming,” in Proc. IEEE SAM, Virtual Conference, Jun. 2020.</li>
<li>W. Fang, Ziyi Yu, Yuning Jiang, Yuanming Shi, Colin N. Jones, and Yong Zhou, “Communication-Efficient Stochastic Zeroth-Order Optimization for Federated Learning,” submitted to IEEE Transactions on Signal Processing</li>
</ul>


<br>
In 2021, Tom won a China National Scholarship (0.2% acceptance rate nationwide). In 2017 he won a 1st prize in the China National Undergraduate
Electronic Design Competition.

<br>
<br>

Here is Tom's:
<ul>
<li><a href="https://scholar.google.com/citations?user=XdUxykMAAAAJ&hl=en">Google Scholar Profile</a></li>
<li><a href="https://wenzhifang.github.io">Website</a></li>
<li><a href="https://wenzhifang.github.io/photos/CV_fangwenzhi.pdf">CV</a></li>
</ul>



<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 10, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2208.05287">
"Adaptive Learning Rates for Faster Stochastic Gradient Methods"</a> -
joint work with
<a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth</a> and <a href="https://www.konstmish.com">Konstantin Mishchenko</a>.


<br>
<br>
Abstract:
<i>
In this work, we propose new adaptive step size strategies that improve several stochastic gradient methods. Our first method (StoPS) is based on the classical Polyak step size (Polyak, 1987) and is an extension of the recent development of this method for the stochastic optimization-SPS (Loizou et al., 2021), and our second method, denoted GraDS, rescales step size by "diversity of stochastic gradients". We provide a theoretical analysis of these methods for strongly convex smooth functions and show they enjoy deterministic-like rates despite stochastic gradients. Furthermore, we demonstrate the theoretical superiority of our adaptive methods on quadratic objectives. Unfortunately, both StoPS and GraDS depend on unknown quantities, which are only practical for the overparametrized models. To remedy this, we drop this undesired dependence and redefine StoPS and GraDS to StoP and GraD, respectively. We show that these new methods converge linearly to the neighbourhood of the optimal solution under the same assumptions. Finally, we corroborate our theoretical claims by experimental validation, which reveals that GraD is particularly useful for deep learning optimization.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>July 30, 2022</h3>
<h1>On my way back to KAUST</h1>
<br>


I am now on my way back to KAUST. I'll stay for about a week and then take up some vacation.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>July 26, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2207.12891">
"RandProx: Primal-Dual Optimization Algorithms with Randomized Proximal Updates"</a> -
joint work with
<a href="https://lcondat.github.io">Laurent Condat</a>.


<br>
<br>
Abstract:
<i>
Proximal splitting algorithms are well suited to solving large-scale nonsmooth optimization problems, in particular those arising in machine learning. We propose a new primal-dual algorithm, in which the dual update is randomized; equivalently, the proximity operator of one of the function in the problem is replaced by a stochastic oracle. For instance, some randomly chosen dual variables, instead of all, are updated at each iteration. Or, the proximity operator of a function is called with some small probability only. A nonsmooth variance-reduction technique is implemented so that the algorithm finds an exact minimizer of the general problem involving smooth and nonsmooth functions, possibly composed with linear operators. We derive linear convergence results in presence of strong convexity; these results are new even in the deterministic case, when our algorithms reverts to the recently proposed Primal-Dual Davis-Yin algorithm. Some randomized algorithms of the literature are also recovered as particular cases (e.g., Point-SAGA). But our randomization technique is general and encompasses many unbiased mechanisms beyond sampling and probabilistic updates, including compression. Since the convergence speed depends on the slowest among the primal and dual contraction mechanisms, the iteration complexity might remain the same when randomness is used. On the other hand, the computation complexity can be significantly reduced. Overall, randomness helps getting faster algorithms. This has long been known for stochastic-gradient-type algorithms, and our work shows that this fully applies in the more general primal-dual setting as well.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 14, 2022</h3>

<h1>11 Team Members Among Top 10% ICML 2022 Reviewers!</h1>

Thanks to my former and current team members Konstantin Mishchenko, Rafał Szlendak, Samuel Horváth, Igor Sokolov,
Alexander Tyurin, Abdurakhmon Sadiev, Laurent Condat, Ahmed Khaled, Eduard Gorbunov, Elnur Gasanov and Egor Shulgin for their
excellet reviewing efforts for ICML 2022 which resulted in them being recognized as <a href="https://icml.cc/Conferences/2022/Reviewers">Outstanding
(Top 10%) Reviewers at ICML 2022!</a>

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>July 12, 2022</h3>

<h1>New York, Baltimore, Houston and Los Angeles</h1>

Starting today, I'll be  on a tour of the US, giving a few talks and visiting/attending several places and conferences,
including the Flatiron Institute in New York City, ICML in Baltimore, Rice University in Houston, and Los Angeles. <br> <br>

At ICML,
I gave two talks: one on our ProxSkip paper "ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!" [<a href="https://proceedings.mlr.press/v162/mishchenko22b.html">paper</a>] [<a href="slides/ProxSkip-ICML-slides.pdf">slides</a>] [poster]
[<a href="https://www.youtube.com/watch?v=OMVSzgsd7ZY">71 min talk</a>] [<a href="https://www.youtube.com/watch?v=yIF_l2g8rGY">90 min talk</a>], and the other one on our
3PC paper "3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation" [<a href="https://proceedings.mlr.press/v162/richtarik22a.html">paper</a>] [<a href="https://icml.cc/media/PosterPDFs/ICML%202022/b6417f112bd27848533e54885b66c288_r5C1iLn.png">poster</a>] [<a href="slides/3PC-ICML-slides.pdf">slides</a>].
My team had three more accepted papers: "A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1" presented by <a href="https://adil-salim.github.io">Adil Salim</a>
[<a href="https://proceedings.mlr.press/v162/salim22a.html">paper</a>] [<a href="https://icml.cc/media/icml-2022/Slides/17594.pdf">slides</a>],
"FedNL: Making Newton-Type Methods Applicable to Federated Learning" presented by <a href="https://mher-safaryan.github.io">Mher Safaryan</a> [<a href="https://proceedings.mlr.press/v162/safaryan22a.html">paper</a>] [<a href="https://icml.cc/media/PosterPDFs/ICML%202022/7385db9a3f11415bc0e9e2625fae3734.png">poster</a>] and
"Proximal and Federated Random Reshuffling" presented by <a href="https://www.akhaled.org">Ahmed Khaled</a> [<a href="https://proceedings.mlr.press/v162/mishchenko22a.html">paper</a>].
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 9, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2207.04338">
"Variance Reduced ProxSkip: Algorithm, Theory and Application to Federated Learning"</a> -
joint work with
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> and <a href="https://kaiyi.me">Kai Yi</a>.


<br>
<br>
Abstract:
<i>
We study distributed optimization methods based on the local training (LT) paradigm: achieving communication efficiency
by performing richer local gradient-based training on the clients before  parameter averaging.  Looking back at the progress
of the field, we identify 5 generations of LT methods: 1) heuristic, 2) homogeneous, 3) sublinear, 4) linear, and 5) accelerated.
The 5th generation, initiated by the ProxSkip method of Mishchenko, Malinovsky, Stich and Richtárik (2022) and its analysis,
is characterized by the first theoretical confirmation that  LT is a communication  acceleration mechanism. Inspired by this recent
progress, we contribute to the 5th generation of LT methods by showing that it is possible to enhance them further using
variance reduction. While all previous theoretical results for LT methods ignore the cost of local work altogether, and are framed
purely in terms of the number of communication rounds, we show that our methods can be substantially faster in terms of the
total training cost than the state-of-the-art method ProxSkip in theory and practice in the regime when local computation is
sufficiently expensive. We characterize this threshold theoretically, and confirm our theoretical predictions with empirical results.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 7, 2022</h3>
<h1>New Research Intern: Michał Grudzień (Oxford)</h1>
<br>


Let us all welcome <a href="https://www.linkedin.com/in/michał-grudzień-2141a2198/?originalSubdomain=uk">Michał Grudzień</a> to the team as a new VSRP research intern! Michal arrived to KAUST yesterday.
Michał tudies toward an MA degree in Mathematics and Statistics at Oxford University. He is the recipient of the
<a href="https://www.ox.ac.uk/admissions/undergraduate/fees-and-funding/oxford-support/palgrave-brown-scholarship-non-uk">Palgrave Brown Scholarship</a>. Prior to this, Michal studied in Warsaw, Poland, at the famous Stanislaw Staszic Lyceum.
Some of Michał's achievements:
<ul>
<li>Member of a project for Oxford "Engineers without borders" society (classification and segmentation on Kaggle), 2021-now </li>
<li> Research Internship with Yaodong Yang on bilevel optimization, 2021</li>
<li>Finalist and Laureate in National Mathematical Olympiad, Poland, 2018 and 2019</li>
<li>Volunteer for Women in Tech Summit II, 2018-2019</li>
<li>Semi-Finalist in the Physics Olympiad, Poland, 2018</li>
<li>Laureate and Finalist in Junior Mathematical Olympiad, Poland, 2016 and 2017</li>
</ul>

Michał has an "obsessive passion for machine learning and data analysis". Besides being native in Polish and fluent in English,
he speaks a bit of Japanese, Chinese and Spanish. Also interestingly, Michal watched 300 anime shows in middle school. So, open with
this subject at your own peril! Most importantly though, Michał can benchpress 125kg. I need a proof of that though. Who volunteers
to join him in the Gym and video-tape this super-human feat?

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 6, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2207.03957">
"Communication Acceleration of Local Gradient Methods via an Accelerated Primal-Dual Algorithm with Inexact Prox"</a> -
joint work with
<a href="https://scholar.google.com/citations?user=R-xZRIAAAAAJ&hl=ru">Abdurakhmon Sadiev</a> and <a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>.


<br>
<br>
Abstract:
<i>
  Inspired by a recent breakthrough of Mishchenko et al. (2022), who for the first time showed that local gradient steps
  can lead to provable communication acceleration, we propose an alternative algorithm which obtains the same communication
  acceleration as their method (ProxSkip). Our approach is very different, however: it is based on the celebrated method of
  Chambolle and Pock (2011), with several nontrivial modifications: i) we allow for an inexact computation of the prox operator
  of a certain smooth strongly convex function via a suitable gradient-based method (e.g., GD, Fast GD or FSFOM), ii) we perform
  a careful modification of the dual update step in order to retain linear convergence. Our general results offer the new
  state-of-the-art rates for the class of strongly convex-concave saddle-point problems with bilinear coupling characterized
  by the absence of smoothness in the dual function. When applied to federated learning, we obtain a theoretically better
  alternative to ProxSkip: our method requires fewer local steps (O(κ^{1/3}) or O(κ^{1/4}), compared to O(κ^{1/2}) of ProxSkip),
  and performs a deterministic number of local steps instead. Like ProxSkip, our method can be applied to optimization over a
  connected network, and we obtain theoretical improvements here as well.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 27, 2022</h3>

<h1>Samuel Horváth defended his PhD thesis today!</h1>

Today, my PhD student <a href="https://samuelhorvath.github.io">Samuel Horváth</a>
<br>
<br>
<img alt="Dr Samuel Horváth" src="imgs/Samuel.jpg" width="200">
<br>
<br>

defended his PhD thesis entitled <a href="https://arxiv.org/abs/2207.00392">Better Methods and Theory for Federated Learning: Compression, Client Selection, and Heterogeneity.</a> <br><br>


His PhD thesis committee members were:
<a href="https://www.avestimehr.com/">Salman Avestimehr</a> (University of Southern California and FedML), <a href="https://mcanini.github.io/">Marco Canini</a> (KAUST),
<a href="https://www.kaust.edu.sa/en/study/faculty/marc-genton">Marc G. Genton</a> (KAUST),
<a href="https://ai.facebook.com/people/michael-rabbat">Mike Rabbat</a> (Facebook AI Research) and myself as the advisor and committee chair.
<br>

<br>
<img alt="" src="imgs/Samuel-defense-Zoom.png" width="700">
<br>
<br>

Samuel joined my <a href="https://richtarik.org/i_team.html">Optimization and Machine Learning Lab</a> at KAUST in August 2017 as an MS/PhD student, right after completing his BS in Financial Mathematics at <a href="https://uniba.sk/en/">Comenius University, Bratislava,
Slovakia</a>, ranked #1 in the program. He obtained his MS degree in Statistics at KAUST in December 2018.

<br>
<br>
Interestingly, Samuel came to KAUST just 5 months after I joined KAUST in March 2017!
<br>
<br>

Samuel has gone a long way since then; <a href="https://samuelhorvath.github.io/files/cv.pdf">look at his CV</a>!  His post-2017 accomplishments include:
<br>
<br>

<ul>
<li> Accepted the position of an <b>Assistant Prof</b> at <a href="https://mbzuai.ac.ae">MBZUAI, Abu Dhabi, United Arab Emirates</a>, about to start in Fall 2022</li>
<li> Co-authored <a href="https://scholar.google.com/citations?user=k252J7kAAAAJ">20 papers during his PhD</a></li>
<li> h index = 11, with 600+ citations according to <a href="https://scholar.google.com/citations?user=k252J7kAAAAJ">Google Scholar</a> </li>
<li> 3 industrial internships during his PhD (Amazon AWS, Samsung AI and Facebook) </li>
<li> <b>Al‐Kindi Statistics Research Student Award</b> (research award for top Statistics PhD student at KAUST), 2021 </li>
<li> <b>Spotlight Paper at NeurIPS 2021</b> for the first-author paper <a href="https://openreview.net/forum?id=4fLr7H5D_eT">FjORD: Fair and Accurate Federated Learning Under Heterogeneous Targets with Ordered Dropout</a> </li>
<li> Progress Towards PhD rated as “Outstanding”, KAUST, in years 2019, 2020 and 2021 </li>
<li> <b>Best Paper Award (+ 1,888 USD cash prize) at the NeurIPS 2020 Spicy Federated Learning Workshop</b> for the paper <a href="https://openreview.net/pdf?id=vYVI1CHPaQg">A Better Alternative to Error Feedback for Communication‐Efficient Distributed Learning</a>, later published in ICLR 2021</li>
<li> <b>Best Reviewer Award (Top 10%) @ NeurIPS 2020</b> </li>
<li> <b>Spotlight paper at the NeuriPS 2020 Optimization for Machine Learning Workshop</b> for the first-authored paper <a href="https://epubs.siam.org/doi/abs/10.1137/21M1394308?journalCode=sjmdaq">Adaptivity of Stochastic Gradient Methods for Nonconvex Optimization</a>, later published by SIAM J on Mathematics of Data Science </a>
<li> <b>Oral Paper at the NeuriPS 2020 Workshop on Scalability, Privacy, and Security in Federated Learning</b> for the paper <a href="https://arxiv.org/abs/2002.12410">On Biased Compression for Distributed Learning</a> </li>
<li> Accepted to Machine Learning Summer School (MLSS) 2020, Tübingen (acceptance rate 180/1300+); also accepted to MLSS 2020 Indonesia </li>
<li> <b>Top Reviewer Award @ NeurIPS 2019</b> </li>
<li> <b>Best Poster Prize</b> for the paper <a href="https://arxiv.org/abs/1904.05115">Stochastic Distributed Learning with Gradient Quantization and Variance Reduction</a> at the Control, Information and Optimization Summer School, Voronovo, Russia (organized by B. Polyak). The poster was presented by coauthor D. Kovalev.
<li> 157th/4049, IEEEXtreme 24‐Hour Programming Competition 12.0 joint with Dmitry Kovalev, 2018 </li>
<li> <b>Best Poster Prize</b> (+ 500 EUR cash prize) for a poster based on the paper <a href="http://proceedings.mlr.press/v97/horvath19a.html">Nonconvex Variance Reduced Optimization with Arbitrary Sampling</a>, awarded as one of 2 out of total 170 posters presented at the Data Science Summer School (DS3), École Polytechnique, 2018  </li>
<li> 131st /3,350 place, IEEEXtreme 24‐Hour Programming Competition 11.0 joint with Konstantin Mishchenko, 2017 </li>
</ul>

<br>
Samuel is very unique in that he is at the same time highly talented (in science and sports), hard-working, has impeccable work ethics, and is incredibly humble and down-to-earth. An amazing person to work with!

<br> <br>
Dr Samuel Horváth, and soon-to-become Prof Samuel Horváth: all the best in your new adventures in Abu Dhabi!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>June 21, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.10452">
"Shifted Compression Framework: Generalizations and Improvements"</a> - joint work with
<a href="https://shulgin-egor.github.io">Egor Shulgin</a>. An earlier version of this paper appeared
in <a href="https://opt-ml.org/papers/2021/paper20.pdf">OPT2021: 13th Annual Workshop on Optimization for Machine Learning.</a>
The final version was recently accepted to the <a href="https://www.auai.org/uai2022/">38th Conference on Uncertainty in Artificial Intelligence (UAI 2022)</a>.


<br>
<br>
Abstract:
<i>
Communication is one of the key bottlenecks in the distributed training of large-scale machine learning models, and lossy compression of exchanged information, such as stochastic gradients or models, is one of the most effective instruments to alleviate this issue. Among the most studied compression techniques is the class of unbiased compression operators with variance bounded by a multiple of the square norm of the vector we wish to compress. By design, this variance may remain high, and only diminishes if the input vector approaches zero. However, unless the model being trained is overparameterized, there is no a-priori reason for the vectors we wish to compress to approach zero during the iterations of classical methods such as distributed compressed SGD, which has adverse effects on the convergence speed. Due to this issue, several more elaborate and seemingly very different algorithms have been proposed recently, with the goal of circumventing this issue. These methods are based on the idea of compressing the difference between the vector we would normally wish to compress and some auxiliary vector which changes throughout the iterative process. In this work we take a step back, and develop a unified framework for studying such methods, conceptually, and theoretically. Our framework incorporates methods compressing both gradients and models, using unbiased and biased compressors, and sheds light on the construction of the auxiliary vectors. Furthermore, our general framework can lead to the improvement of several existing algorithms, and can produce new algorithms. Finally, we performed several numerical experiments which illustrate and support our theoretical findings.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>June 20, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.09709">
"A Note on the Convergence of Mirrored Stein Variational Gradient Descent under (L0,L1)−Smoothness Condition"</a> - joint work with
<a href="https://lukangsun.github.io/">Lukang Sun</a>.


<br>
<br>
Abstract:
<i>
In this note, we establish a descent lemma for the population limit Mirrored Stein Variational Gradient Method (MSVGD).
This descent lemma does not rely on the path information of MSVGD but rather on a simple assumption for the mirrored distribution.
Our analysis demonstrates that MSVGD can be applied to a broader class of constrained sampling problems with non-smooth V. We also
investigate the complexity of the population limit MSVGD in terms of dimension d.</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 18, 2022</h3>
<h1>New Research Intern: Omar Shaikh Omar (U of Washington)</h1>
<br>

Please join me in welcoming <a href="https://www.linkedin.com/in/omar-shaikh-omar-998970176/">Omar Shaikh Omar</a> to our team as a <a href="https://kgsp.kaust.edu.sa">KGSP (KAUST Gifted Student Program)</a>
research intern! Omar just arrived to KAUST and his KGSP orientation is tomorrow! Omar is a CS undergraduate student at the <a href="https://www.washington.edu">University of Washington</a>.
<br>
<br>
Selected distinctions, experience & interests:
<ul>
<li>Student researcher at the University of Washington STAR Lab (http://www.uwstarlab.org), 2021</li>
<li>Deployed two implementations of the YOLO v4 on Jetson Nano and used models on video streams of traﬀic intersections to detect pedestrians both crossing and waiting to cross </li>
<li>Trained custom YOLO v4 model for pedestrian detection using data from the Caltech Pedestrian Detection Benchmark and the MIOVision Traﬀic Camera Dataset </li>
<li>Student researcher at the Washington eXperimental Mathematics Lab (WXML), 2020-2021 </li>
<li>Constructed guesses for compact regions based on a subset of the roots of complex polynomials to find a local analog of the Gauss-Lucas Theorem and disproved the eligibility of multiple guesses </li>
<li>Built a Python library that simulates the process of finding counterexamples for guesses and creates animations of the process using matplotlib, scipy, and sympy </li>
<li>Presented at the Brown University’s Symposium for Undergraduates in the Mathematical Sciences (SUMS), and at the WXML conference </li>
<li>Data Science Intern at Uliza (https://www.uliza.org), 2020</li>
<li>Developed the idea for a spell checker for the African language Luganda with a team of programmers and linguists </li>
<li>Wrote several specification documents for data collection and a terminal interface </li>
<li>Scrapped the Luganda Bible and an English/Luganda dictionary for a total of 80,000+ unique words and 70,000+ sentences </li>
<li>Built core spell checker in Python by connecting an nltk part of speech tagger, a lemmatizer built specifically for Luganda, and an edit-distance suggestion system </li>
<li>Top 500 among US and Canada Universities, Putnam Mathematical Competition, 2019</li>
<li>Volunteer Teaching Assistance at Math Circle, 2019-2020 </li>
<li>Bronze medal in the Balkan Mathematical Olympiad, 2018</li>
</ul>


<br>
Omar, welcome!
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>June 16, 2022</h3>
<h1>New Research Intern: Kaja Gruntkowska (Warwick)</h1>
<br>

Please join me in welcoming Kaja Gruntkowska to the team as a <a href="https://vsrp.kaust.edu.sa/about-vsrp">VSRP intern</a>!!!
Kaja is a 3rd year undergraduate student of
<a href="https://warwick.ac.uk/study/undergraduate/courses/mathsstatsbsc/">Mathematics and Statistics at the University of Warwick, UK</a>.
Kaja will arrive at KAUST later this month. A few remarks about Kaja's current and past experience and successes:

<ul>
<li>one of the top students in her cohort at Warwick (top 3 in 2020-2021)</li>
<li>received the Academic Excellence Prize for overall performance at Warwick, 2020-2021</li>
<li>was a finalist of Polish Statistics Olympiad – 1st place winner on voivodeship stage, 2019</li>
<li>won 2nd place at "First Step to Fields Medal", 2019</li>
<li>won 3rd place at Jagiellonian Mathematical Tournament, 2018</li>
<li>participated (twice) at the Congress of Young Polish Mathematicians, 2016 and 2018</li>
<li>co-organized team Lodz for ‘Naboj Junior’, 2018</li>
<li>led extracurricular Maths classes at Lodz University of Technology High School, 2018-2019 </li>
</ul>



Kaja likes tutoring high school students in mathematics, acts as a mentor coordinator at Warwick, and is interested in sport and food sustainability.

<br>

<br>
Kaja, welcome!
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 15, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.07021">
"Federated Optimization Algorithms with Random Reshuffling and Gradient Compression"</a> - joint work with
<a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">Abdurakhmon Sadiev</a>,
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>,
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>,
<a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a>,
<a href="https://rka97.github.io">Ahmed Khaled</a>,
and
<a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a>.

<br>
<br>
Abstract:
<i>
Gradient compression is a popular technique for improving communication complexity of stochastic first-order methods in distributed training of machine learning models. However, the existing works consider only with-replacement sampling of stochastic gradients. In contrast, it is well-known in practice and recently confirmed in theory that stochastic methods based on without-replacement sampling, e.g., Random Reshuffling (RR) method, perform better than ones that sample the gradients with-replacement. In this work, we close this gap in the literature and provide the first analysis of methods with gradient compression and without-replacement sampling. We first develop a distributed variant of random reshuffling with gradient compression (Q-RR), and show how to reduce the variance coming from gradient quantization through the use of control iterates. Next, to have a better fit to Federated Learning applications, we incorporate local computation and propose a variant of Q-RR called Q-NASTYA. Q-NASTYA uses local gradient steps and different local and global stepsizes. Next, we show how to reduce compression variance in this setting as well. Finally, we prove the convergence results for the proposed methods and outline several settings in which they improve upon existing algorithms.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 12, 2022</h3>

<h1>Conference on the Mathematics of Complex Data</h1>

I have just arrived to Stockholm, Sweden, to attend the <a href="https://www.mathdatalab.org/">Conference on the Mathematics of Complex Data.</a>
This event was supposed to happen in 2020, but as postponed due to the Covid-19 pandemic. My talk is scheduled for Thursday, June 16.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 7, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.03588">
"Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation"</a> - joint work with
<a href="https://rustem-islamov.github.io">Rustem Islamov</a>, <a href="https://qianxunk.github.io">Xun Qian</a>, <a href="https://slavomir-hanzely.github.io">Slavomír Hanzely</a> and <a href="https://mher-safaryan.github.io">Mher Safaryan</a>.

<br>
<br>
Abstract:
<i>
Despite their high computation and communication costs, Newton-type methods remain an appealing option for distributed training due to their robustness against ill-conditioned convex problems. In this work, we study ommunication compression and aggregation mechanisms for curvature information in order to reduce these costs while preserving theoretically superior local convergence guarantees. We prove that the recently developed class of three point compressors (3PC) of Richtarik et al. [2022] for gradient communication can be generalized to Hessian communication as well. This result opens up a wide variety of communication strategies, such as contractive compression} and lazy aggregation, available to our disposal to compress prohibitively costly curvature information. Moreover, we discovered several new 3PC mechanisms, such as adaptive thresholding and Bernoulli aggregation, which require reduced communication and occasional Hessian computations. Furthermore, we extend and analyze our approach to bidirectional communication compression and partial device participation setups to cater to the practical considerations of applications in federated learning. For all our methods, we derive fast condition-number-independent local linear and/or superlinear convergence rates. Finally, with extensive numerical evaluations on convex optimization problems, we illustrate that our designed schemes achieve state-of-the-art communication complexity compared to several key baselines using second-order information.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 6, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.02535">
"Certified Robustness in Federated Learning"</a> - joint work with
<a href="https://scholar.google.com/citations?user=caAyffEAAAAJ&hl=en">Motasem Alfarra</a>, <a href="https://juancprzs.github.io">Juan C. Pérez</a>, <a href="https://shulgin-egor.github.io">Egor Shulgin</a> and <a href="https://www.bernardghanem.com">Bernard Ghanem</a>.

<br>
<br>
Abstract:
<i>
Federated learning has recently gained significant attention and popularity due to its effectiveness in training machine learning models on distributed data privately. However, as in the single-node supervised learning setup, models trained in federated learning suffer from vulnerability to imperceptible input transformations known as adversarial attacks, questioning their deployment in security-related applications. In this work, we study the interplay between federated training, personalization, and certified robustness. In particular, we deploy randomized smoothing, a widely-used and scalable certification method, to certify deep networks trained on a federated setup against input perturbations and transformations. We find that the simple federated averaging technique is effective in building not only more accurate, but also more certifiably-robust models, compared to training solely on local data. We further analyze personalization, a popular technique in federated training that increases the model's bias towards local data, on robustness. We show several advantages of personalization over both~(that is, only training on local data and federated training) in building more robust models with faster training. Finally, we explore the robustness of mixtures of global and local~(\ie personalized) models, and find that the robustness of local models degrades as they diverge from the global model
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 5, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.02275">
"Sharper Rates and Flexible Framework for Nonconvex SGD with Client and Data Sampling"</a> -
joint work with
<a href="https://k3nfalt.github.io">Alexander Tyurin</a>, <a href="https://lukangsun.github.io">Lukang Sun</a> and <a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a>.

<br>
<br>
Abstract:
<i>
We revisit the classical problem of finding an approximately stationary point of the average of n smooth and possibly nonconvex functions.
The optimal complexity of stochastic first-order methods in terms of the number of gradient evaluations of individual functions is
O(n+√n/ε), attained by the optimal SGD methods 𝖲𝖯𝖨𝖣𝖤𝖱 [Fang et al, NeurIPS 2018] and 𝖯𝖠𝖦𝖤 [Zhize et al, ICML 2021], for example,
where ε is the error tolerance. However, i) the big-O notation hides crucial dependencies on the smoothness constants associated with the functions,
and ii) the rates and theory in these methods assume simplistic sampling mechanisms that do not offer any flexibility. In this work we remedy the
situation. First, we generalize the 𝖯𝖠𝖦𝖤 algorithm so that it can provably work with virtually any (unbiased) sampling mechanism. This is particularly
useful in federated learning, as it allows us to construct and better understand the impact of various combinations of client and data sampling strategies.
Second, our analysis is sharper as we make explicit use of certain novel inequalities that capture the intricate interplay between the smoothness
constants and the sampling procedure. Indeed, our analysis is better even for the simple sampling procedure analyzed in the 𝖯𝖠𝖦𝖤 paper.
However, this already improved bound can be further sharpened by a different sampling scheme which we propose. In summary, we provide the most
general and most accurate analysis of optimal SGD in the smooth nonconvex regime. Finally, our theoretical findings are supposed with carefully
designed experiments.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 2, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.00920">
"Federated Learning with a Sampling Algorithm under Isoperimetry"</a> -
joint work with
<a href="https://lukangsun.github.io">Lukang Sun</a> and <a href="https://adil-salim.github.io">Adil Salim</a>.

<br>
<br>
Abstract:
<i>
Federated learning uses a set of techniques to efficiently distribute the training of a machine learning algorithm across several devices, who own the training data. These techniques critically rely on reducing the communication cost -- the main bottleneck -- between the devices and a central server. Federated learning algorithms usually take an optimization approach: they are algorithms for minimizing the training loss subject to communication (and other) constraints. In this work, we instead take a Bayesian approach for the training task, and propose a communication-efficient variant of the Langevin algorithm to sample a posteriori. The latter approach is more robust and provides more knowledge of the \textit{a posteriori} distribution than its optimization counterpart. We analyze our algorithm without assuming that the target distribution is strongly log-concave. Instead, we assume the weaker log Sobolev inequality, which allows for nonconvexity.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 1, 2022</h3>
<h1>New Research Intern: Arto Maranjyan (Yerevan State University)</h1>
<br>


Artavazd "Arto" Maranjyan [<a href="https://www.researchgate.net/profile/Artavazd-Maranjyan">ResearchGate</a>]
[<a href="https://publons.com/researcher/4267694/artavazd-maranjyan/">publons</a>]
[<a href="https://www.linkedin.com/in/arto-maranjyan/?originalSubdomain=am">LinkedIn</a>] joined my Optimization and Machine Learning Lab as
a <a href="https://vsrp.kaust.edu.sa">VSRP intern</a>. His internship started on June 1, and will last for 6 months. Arto is a first year MS
student in Applied Mathematics and Statistics at <a href="https://en.wikipedia.org/wiki/Yerevan_State_University">Yerevan State University, Armenia</a>.
Parts of his BS thesis entitled
"On the Convergence of Series in Classical Systems", supervised by Prof. Martin Grigoryan, have been published as separate papers:
<ul>
<li> On the divergence of Fourier series in the general Haar system, <a href="http://armjmath.sci.am/index.php/ajm/article/view/533">Armenian Journal of Mathematics 13(6):1-10, 2021</a>,</li>
<li> On the unconditional convergence of Faber-Schauder series in L1, <a href="https://journals.ysu.am/index.php/proceedings-phys-math/article/view/vol55_no1_2021_pp012-019">Proceedings of the
  Yerevan State University 55(254):12-19, 2021</a>,</li>
<li> Problems of recovering from the Fourier-Vilenkin series (unpublished).</li>
</ul>

Arto was one of 6 out of 250 YSU students receiving an Outstanding Final Project Award. Arto gained practical and theoretical knowledge in machine
learning, deep learning, natural language processing, and computer vision during a year-long AI training with the
<a href="https://fast.foundation">Foundation for Armenian Science and
Technology (FAST)</a>.
<br>

<br>
Welcome to the team!!!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 1, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.00529">
"Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top"</a> -
joint work with
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>, <a href="https://samuelhorvath.github.io">Samuel Horváth</a> and <a href="https://gauthiergidel.github.io">Gauthier Gidel</a>.

<br>
<br>
Abstract:
<i>
Byzantine-robustness has been gaining a lot of attention due to the growth of the interest in collaborative and federated learning.
However, many fruitful directions, such as the usage of variance reduction for achieving robustness and communication compression
for reducing communication costs, remain weakly explored in the field. This work addresses this gap and proposes Byz-VR-MARINA --
a new Byzantine-tolerant method with variance reduction and compression. A key message of our paper is that variance reduction is
key to fighting Byzantine workers more effectively. At the same time, communication compression is a bonus that makes the process
more communication efficient. We derive theoretical convergence guarantees for Byz-VR-MARINA outperforming previous state of the art
for general non-convex and Polyak-Lojasiewicz loss functions. Unlike the concurrent Byzantine-robust methods with variance reduction
and/or compression, our complexity results are tight and do not rely on restrictive assumptions such as boundedness of the gradients
or limited compression. Moreover, we provide the first analysis of a Byzantine-tolerant method supporting non-uniform sampling of
stochastic gradients. Numerical experiments corroborate our theoretical findings.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 1, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2206.00508">
"Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition"</a> -
joint work with
<a href="https://lukangsun.github.io">Lukang Sun</a> and <a href="https://avetx.github.io">Avetik Karagulyan</a>.

<br>
<br>
Abstract:
<i>
Stein Variational Gradient Descent (SVGD) is an important alternative to the Langevin-type algorithms for sampling from probability
distributions of the form π(x)∝exp(−V(x)). In the existing theory of Langevin-type algorithms and SVGD, the potential function V is
often assumed to be L-smooth. However, this restrictive condition excludes a large class of potential functions such as polynomials
of degree greater than 2. Our paper studies the convergence of the SVGD algorithm for distributions with (L0,L1)-smooth potentials.
This relaxed smoothness assumption was introduced by Zhang et al. [2019a] for the analysis of gradient clipping algorithms. With the
help of trajectory-independent auxiliary conditions, we provide a descent lemma establishing that the algorithm decreases the KL divergence
at each iteration and prove a complexity bound for SVGD in the population limit in terms of the Stein Fisher information.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 31, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2205.15580">
"A Computation and Communication Efficient Method for Distributed Nonconvex Problems in the Partial Participation Setting"</a> -
joint work with
<a href="https://k3nfalt.github.io">Alexander Tyurin</a>.

<br>
<br>
Abstract:
<i>
We present a new method that includes three key components of distributed optimization and federated learning: variance reduction of stochastic gradients, compressed communication, and partial participation. We prove that the new method has optimal oracle complexity and state-of-the-art communication complexity in the partial participation setting. Moreover, we observe that "1 + 1 + 1 is not 3": by mixing variance reduction of stochastic gradients with compressed communication and partial participation, we do not obtain a fully synergetic effect. We explain the nature of this phenomenon, argue that this is to be expected, and propose possible workarounds.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>May 30, 2022</h3>

<h1>Teaching in Vienna</h1>

During May 30-June 3, I am teaching a course on <a href="https://ufind.univie.ac.at/en/course.html?lv=390015&semester=2022S">Stochastic
Gradient Descent Methods</a> at the <a href="https://vgsco.univie.ac.at">Vienna Graduate School of Computational Optimization (VGSCO).</a>
About 20 PhD students, postdocs and even some professors from 4 Austrian universities (U Wien, IST Austria, TU Wien, WU Wien) are attending.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>May 15, 2022</h3>

<h1>Five Papers Accepted to ICML 2022</h1>
<br>

We've had several several papers accepted to the <a href="https://icml.cc/Conferences/2022/">International Conference on Machine Learning (ICML 2022).</a>
Here they are:<br><br>


<b> 1) "Proximal and Federated Random Reshuffling"</b> <a href="https://arxiv.org/abs/2102.06704">[arXiv]</a> <a href="https://www.youtube.com/watch?v=hsGty6dPAKU">[video]</a>  -
joint work with
<a href="https://konstmish.github.io">Konstantin Mishchenko</a> and
<a href="https://rka97.github.io">Ahmed Khaled</a>.

<br>
<br>
<br>

<b> 2) "FedNL: Making Newton-Type Methods Applicable to Federated Learning"</b>
<a href="https://arxiv.org/abs/2106.02969">[arXiv]</a> <a href="https://www.youtube.com/watch?v=_VYCEWT17R0">[video]</a>  -
joint work with
<a href="https://mher-safaryan.github.io/">Mher Safaryan</a>,
<a href="https://rustem-islamov.github.io/">Rustem Islamov</a>, and
<a href="https://qianxunk.github.io/">Xun Qian</a>.

<br>
<br>
<br>

<b> 3) "A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1"</b>
<a href="https://arxiv.org/abs/2106.03076">[arXiv]</a>  -
joint work with
<a href="https://lukangsun.github.io">Lukang Sun</a> and
<a href="https://adil-salim.github.io">Adil Salim</a>.

<br>
<br>
<br>

<b> 4) "ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!"</b>
<a href="https://arxiv.org/abs/2202.09357">[arXiv]</a> <a href="https://www.youtube.com/watch?v=OMVSzgsd7ZY">[video]</a> -
joint work with
<a href="https://konstmish.github.io">Konstantin Mishchenko</a>,
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>, and
<a href="https://www.sstich.ch">Sebastian Stich</a>.

<br>
<br>
<br>

<b> 5) "3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation"</b>
<a href="https://arxiv.org/abs/2202.00998">[arXiv]</a>  -
joint work with
<a href="https://cemse.kaust.edu.sa/amcs/people/person/igor-sokolov">Igor Sokolov</a>,
<a href="https://ai.ethz.ch/people/ilyas-fatkhullin.html">Ilyas Fatkhullin</a>,
<a href="https://elnurgasanov.com">Elnur Gasanov</a>,
<a href="https://zhizeli.github.io">Zhize Li</a>, and
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 15, 2022</h3>

<h1>Stochastic Numerics and Statistical Learning Workshop</h1>

Today I gave a talk at <a href="https://cemse.kaust.edu.sa/events/event/snsl-workshop">"Stochastic Numerics and Statistical
  Learning: Theory and Applications Workshop"</a>.
  I spoke about <a href="https://arxiv.org/abs/2202.09357">ProxSkip paper</a> [<a href="https://richtarik.org/talks/2022-ProxSkip-Lagrange_Workshop.pdf">slides</a>]
  [<a href="https://www.youtube.com/watch?v=OMVSzgsd7ZY">video</a>]. The good news from today is that the paper got accepted
  to ICML.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 10, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2205.04180">
"EF-BV: A Unified Theory of Error Feedback and Variance Reduction Mechanisms for Biased and Unbiased Compression in Distributed Optimization"</a> -
joint work with
<a href="https://lcondat.github.io">Laurent Condat</a> and
<a href="https://kaiyi.me">Kai Yi</a>.

<br>
<br>
Abstract:
<i>
In distributed or federated optimization and learning, communication between the different computing units is often the bottleneck, and gradient compression is a widely used technique for reducing the number of bits sent within each communication round of iterative methods. There are two classes of compression operators and separate algorithms making use of them. In the case of unbiased random compressors with bounded variance (e.g., rand-k), the DIANA algorithm of Mishchenko et al. [2019], which implements a variance reduction technique for handling the variance introduced by compression, is the current state of the art. In the case of biased and contractive compressors (e.g., top-k), the EF21 algorithm of Richtárik et al. [2021], which implements an error-feedback mechanism for handling the error introduced by compression, is the current state of the art. These two classes of compression schemes and algorithms are distinct, with different analyses and proof techniques. In this paper, we unify them into a single framework and propose a new algorithm, recovering DIANA and EF21 as particular cases. We prove linear convergence under certain conditions. Our general approach works with a new, larger class of compressors, which includes unbiased and biased compressors as particular cases, and has two parameters, the bias and the variance. These gives a finer control and allows us to inherit the best of the two worlds: biased compressors, whose good performance in practice is recognized, can be used. And independent randomness at the compressors allows to mitigate the effects of compression, with the convergence rate improving when the number of parallel workers is large. This is the first time that an algorithm with all these features is proposed. Our approach takes a step towards better understanding of two so-far distinct worlds of communication-efficient distributed learning.
</i>
<br>




<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>May 10, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2205.03914">
"Federated Random Reshuffling with Compression and Variance Reduction"</a> -
joint work with <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>.

<br>
<br>
Abstract:
<i>
Random Reshuffling (RR), which is a variant of Stochastic Gradient Descent (SGD) employing sampling without replacement, is an immensely popular method for training supervised machine learning models via empirical risk minimization. Due to its superior practical performance, it is embedded and often set as default in standard machine learning software. Under the name FedRR, this method was recently shown to be applicable to federated learning (Mishchenko et al.,2021), with superior performance when compared to common baselines such as Local SGD. Inspired by this development, we design three new algorithms to improve FedRR further: compressed FedRR and two variance reduced extensions: one for taming the variance coming from shuffling and the other for taming the variance due to compression. The variance reduction mechanism for compression allows us to eliminate dependence on the compression parameter, and applying additional controlled linear perturbations for Random Reshuffling, introduced by Malinovsky et al.(2021) helps to eliminate variance at the optimum. We provide the first analysis of compressed local methods under standard assumptions without bounded gradient assumptions and for heterogeneous data, overcoming the limitations of the compression operator. We corroborate our theoretical results with experiments on synthetic and real data sets.
</i>
<br>




<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>







<h3>April 27, 2022</h3>

<h1>New Paper</h1>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2204.13169">
"FedShuffle: Recipes for Better Use of Local Work in Federated Learning"</a> -
joint work with
<a href="https://samuelhorvath.github.io">Samuel Horváth</a>,
<a href="https://sites.google.com/view/maziar">Maziar Sanjabi</a>,
<a href="https://linxiaolx.github.io">Lin Xiao</a>, and
<a href="https://ai.facebook.com/people/michael-rabbat">Michael Rabbat</a>.

<br>
<br>
Abstract:
<i>
The practice of applying several local updates before aggregation across clients has been empirically shown to be a successful approach to overcoming the communication bottleneck in Federated Learning (FL). In this work, we propose a general recipe, FedShuffle, that better utilizes the local updates in FL, especially in the heterogeneous regime. Unlike many prior works, FedShuffle does not assume any uniformity in the number of updates per device. Our FedShuffle recipe comprises four simple-yet-powerful ingredients: 1) local shuffling of the data, 2) adjustment of the local learning rates, 3) update weighting, and 4) momentum variance reduction (Cutkosky and Orabona, 2019). We present a comprehensive theoretical analysis of FedShuffle and show that both theoretically and empirically, our approach does not suffer from the objective function mismatch that is present in FL methods which assume homogeneous updates in heterogeneous FL setups, e.g., FedAvg (McMahan et al., 2017). In addition, by combining the ingredients above, FedShuffle improves upon FedNova (Wang et al., 2020), which was previously proposed to solve this mismatch. We also show that FedShuffle with momentum variance reduction can improve upon non-local methods under a Hessian similarity assumption. Finally, through experiments on synthetic and real-world datasets, we illustrate how each of the four ingredients used in FedShuffle helps improve the use of local updates in FL.
</i>
<br>




<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 27, 2022</h3>

<h1>Two Seminar Talks: Baidu and PSU</h1>

I gave two talks on the ProxSkip method [<a href="https://arxiv.org/abs/2202.09357">paper</a>] today: the first talk at the
"Seminar Series in Cognitive Computing at Baidu Research", invited by <a href="https://www.linkedin.com/in/ping-li-a4624389/">Ping Li</a>
[<a href="talks/TALK-2022-04-ProxSkip-Baidu.pdf">my slides</a>], and the second talk at the "Department of
Mathematics Colloqium, Penn State University", invited by
<a href="https://www.personal.psu.edu/jxx1/">Jinchao Xu</a> [<a href="talks/TALK-2022-04-ProxSkip-PSU.pdf">my slides</a>].
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>April 25, 2022</h3>

<h1>Lagrange Workshop on Federated Learning</h1>

Today I am giving a talk entitled <a href="https://arxiv.org/abs/2202.09357">"ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!"</a> at the Lagrange Workshop on Federated Learning, organized by Eric Moulines, Merouane Debbah and Samson Lasaulce as a
"Lagrange Mathematics and Computing Research Center" event. The talk is based on this paper in which we resolve an important open problem in the field of federated learning. In particular, we show that local gradient steps can provably lead to
communication acceleration.

<br>
<br>
<a href="imgs/WS-FL_FLyer.pdf"><img alt="" src="imgs/WS-FL_FLyer.png" width="750"></a>
<br>
<br>

The workshop is a virtual meeting; anyone can join via Zoom. I am looking forward to listening to many interesting talks, including one
by my former student <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>.
<br>

<br>
Here are <a href="talks/2022-ProxSkip-Lagrange_Workshop.pdf">my slides.</a>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>April 7, 2022</h3>

<h1>Apple Workshop on Privacy Preserving Machine Learning</h1>

On Tuesday and Wednesday earlier this week I attended and gave a talk at (an invite-only) workshop on "Privacy Preserving Machine Learning" organized by Apple.
The program started at 7pm my time (morning California time) and lasted until after midnight. Yes, I felt very very tired near the end...
In any case, a nice event.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>April 3, 2022</h3>

<h1>Positions in my Optimization and Machine Learning Lab</h1>

I always have openings for outstanding individuals to join my team as interns, MS/PhD students, PhD students, postdocs and research scientists.
If you are interested in a position, please fill out this <a href="https://apply.interfolio.com/105097">interfolio application form</a>.


<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>March 31, 2022</h3>

<h1>Talk on ProxSkip at the AMCS/STAT Graduate Seminar at KAUST</h1>

Today I have a talk about "ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!"
at the <a href="https://cemse.kaust.edu.sa/events/event/proxskip-yes-local-gradient-steps-provably-lead-communication-acceleration-finally">AMCS/STAT Graduate Seminar</a> here at KAUST. The talk is based on a
joint paper with <a href="https://konstmish.github.io">Konstantin Mishchenko</a>,  <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> and <a href="https://sstich.ch">Sebastian Stich</a>.
I will give the same talk at the <a href="https://sites.google.com/view/one-world-seminar-series-flow/future-talks">Federated Learning One World (FLOW) seminar</a> on May 4, 2022.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>March 24, 2022</h3>

<h1>Transactions on Machine Learning Research Accepting Submissions!</h1>

<a href="https://jmlr.org/tmlr/index.html">Transactions on Machine Learning Research</a> is a new venue for the dissemination of
machine learning research. Yours truly is one of a large number (approx 150) of <a href="https://jmlr.org/tmlr/editorial-board.html">Action
  Editors</a>. You can read <a href="https://jmlr.org/tmlr/ae-guide.html">here</a> in detail what
the responsibilities of an Action Editor are. We are delighted to let you know that TMLR is now <a href="https://jmlr.org/tmlr/news/2022/launch.html">accepting submissions!</a>
<br> <br>


An excerpt from the TMLR website: <i>"TMLR emphasizes technical correctness over subjective significance, to ensure that we facilitate scientific discourse on topics that are deemed less significant by contemporaries but may be important in the future. TMLR caters to the shorter format manuscripts that are usually submitted to conferences, providing fast turnarounds and double blind reviewing. We employ a rolling submission process, shortened review period, flexible timelines, and variable manuscript length, to enable deep and sustained interactions among authors, reviewers, editors and readers. This leads to a high level of quality and rigor for every published article.
TMLR does not accept submissions that have any overlap with previously published work. TMLR maximizes openness and transparency by hosting the review process on OpenReview."</i>
<br> <br>

I have high hopes that TMLR will combine the benefits of a fast conference-like publication process with the high reviewing standards of journals.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 21, 2022</h3>

<h1>Talk on PermK at the CS Graduate Seminar at KAUST</h1>

Today I gave a talk about "Permutation compressors for provably faster distributed nonconvex optimization"
at the <a href="https://cemse.kaust.edu.sa/events/event/permutation-compressors-provably-faster-distributed-nonconvex-optimization">Computer Science Graduate Seminar</a> here at KAUST. The talk is based on
joint paper with <a href="https://www.linkedin.com/in/rafał-szlendak-552936220">Rafał Szlendak</a> and <a href="https://k3nfalt.github.io/">Alexander Tyurin</a> recently accepted to <a href="https://iclr.cc">ICLR 2022</a>. I gave the same talk at the Federated Learning One World (FLOW) seminar in February; this one was recorded and is on <a href="https://www.youtube.com/watch?v=aMEj2pkGrcY">YouTube</a>.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 13, 2022</h3>

<h1>Rising Stars in AI Symposium at KAUST</h1>

A <a href="https://cemse.kaust.edu.sa/ai/aii-symp-2022">Rising Stars in AI Symposium</a> is taking place at KAUST during March 13-15, 2022. This event is organized by
KAUST's <a href="https://cemse.kaust.edu.sa/ai">AI Initiative</a>, led by <a href="https://cemse.kaust.edu.sa/ai/people/person/jurgen-schmidhuber">Jürgen Schmidhuber</a>.
Several people from my team are giving talks: <a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a>, <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>,
<a href="https://k3nfalt.github.io">Alexander Tyurin</a>, <a href="https://elnurgasanov.com">Elnur Gasanov</a>, <a href="https://mher-safaryan.github.io">Mher Safaryan</a>,
<a href="https://shulgin-egor.github.io">Egor Shulgin</a>, <a href="https://samuelhorvath.github.io">Samuel Horváth</a> and <a href="https://zhizeli.github.io">Zhize Li</a>. My former PhD
student <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> is also giving a talk.

<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>March 12, 2022</h3>

<h1>Back to KAUST</h1>

After spending a few weeks in Europe, I have just arrived back to KAUST.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 20, 2022</h3>

<h1>Dagstuhl</h1>

As of today, and until February 25, I am at Dagstuhl, Germany, attending a Seminar on
 the <a href="https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=22081">Theory of Randomized Optimization Heuristics</a>.

 <br>


 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>


<h3>February 19, 2022</h3>

<h1>New Paper</h1>

<a href="papers/ProxSkip.pdf"><img alt="" src="imgs/ProxSkip-thmb.png" height="1000"></a>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2202.09357">
"ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!"</a> -
joint work with
<a href="https://konstmish.github.io/">Konstantin Mishchenko</a>,
<a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky</a> and
<a href="http://sstich.ch/">Sebastian Stich</a>.

<br>
<br>
Abstract:
<i>
We introduce ProxSkip---a surprisingly simple and provably efficient method for minimizing the sum of a smooth ($f$) and an expensive nonsmooth proximable ($\psi$) function. The canonical approach to solving such problems is via the proximal gradient descent (ProxGD) algorithm, which is based on the evaluation of the gradient of $f$ and the prox operator  of $\psi$ in each iteration. In this work we are specifically interested in the regime in which the evaluation of prox is costly relative to the evaluation of the gradient, which is the case in many applications. ProxSkip allows for the expensive prox operator to be skipped in most iterations: while its iteration complexity is $\cO(\kappa \log \nicefrac{1}{\varepsilon})$, where $\kappa$ is the condition number of $f$, the number of prox evaluations is $\cO(\sqrt{\kappa} \log \nicefrac{1}{\varepsilon})$ only. Our main motivation comes from federated learning, where evaluation of the gradient operator corresponds to taking a local GD step independently on all devices, and evaluation of prox corresponds to (expensive) communication in the form of gradient averaging. In this context, ProxSkip offers an effective <i>acceleration</i> of communication complexity. Unlike other local gradient-type methods, such as FedAvg, Scaffold, S-Local-GD and FedLin, whose theoretical communication complexity is worse than, or at best matching, that of vanilla GD in the heterogeneous data regime, we obtain a provable and large improvement without any heterogeneity-bounding assumptions.
</i>
<br>




<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 19, 2022</h3>

<h1>Dagstuhl Seminar</h1>
<br>
I am on my way to Schloss Dagstuhl, Germany, to attend a week-long seminar on the <a href="https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=22081">Theory
of Randomized Optimization Heuristics</a>.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 9, 2022</h3>

<h1>Talk @ FLOW </h1>
<br>

I just gave a talk at the <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">Federated Learning One World (FLOW)</a>  seminar.
I spoke about <a href="https://arxiv.org/abs/2110.03300">"Permutation compressors for provably faster distributed nonconvex optimization"</a> -
a paper recently accepted to ICLR 2022.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 8, 2022</h3>

<h1>New Paper</h1>
<br>


<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2202.02771">
"Optimal Algorithms for Decentralized Stochastic Variational Inequalities"</a> -
joint work with
<a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>,
<a href="https://anbeznosikov.github.io/">Aleksandr Beznosikov</a>,
<a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">Abdurakhmon Sadiev</a>,
Michael Persiianov, and
<a href="https://www.researchgate.net/profile/Alexander-Gasnikov">Alexander Gasnikov</a>.

<br>
<br>
Abstract:
<i>
Variational inequalities are a formalism that includes games, minimization, saddle point, and equilibrium problems as special cases. Methods for variational inequalities are therefore universal approaches for many applied tasks, including machine learning problems. This work concentrates on the decentralized setting, which is increasingly important but not well understood. In particular, we consider decentralized stochastic (sum-type) variational inequalities over fixed and time-varying networks. We present lower complexity bounds for both communication and local iterations and construct optimal algorithms that match these lower bounds. Our algorithms are the best among the available literature not only in the decentralized stochastic case, but also in the decentralized deterministic and non-distributed stochastic cases. Experimental results confirm the effectiveness of the presented algorithms.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 7, 2022</h3>

<h1>Talk @ Machine Learning NeEDS Mathematical Optimization Seminar</h1>
<br>

Today I am giving a talk within the <a href="https://congreso.us.es/mlneedsmo/">Machine Learning NeEDS Mathematical Optimization</a>
virtual seminar series. This is the opening talk of the third season of the seminar. I spoke about
<a href="https://arxiv.org/abs/2110.03300">"Permutation compressors for provably faster distributed nonconvex optimization"</a> -
paper recently accepted to ICLR 2022.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 2, 2022</h3>

<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2202.01268">
"Distributed nonconvex optimization with communication compression, optimal oracle complexity, and no client synchronization"</a> -
joint work with
<a href="https://k3nfalt.github.io">Alexander Tyurin</a>.

<br>
<br>
Abstract:
<i>
We develop and analyze DASHA: a new family of methods for nonconvex distributed optimization problems. When the local functions at the nodes have a finite-sum
or an expectation form, our new methods, DASHA-PAGE and DASHA-SYNC-MVR, improve the theoretical oracle and communication complexity of the previous state-of-the-art method MARINA by
Gorbunov et al. (2020). In particular, to achieve an $\varepsilon$-stationary point, and considering the random sparsifier RandK as an
example, our methods compute the optimal number of gradients $O(\sqrt{m}/(\varepsilon\sqrt{n}))$ and $O(\sigma/(\varepsilon^{3/2}n))$ in finite-sum and
expectation form cases, respectively, while maintaining the SOTA communication complexity $O(d/(\varepsilon \sqrt{n}))$. Furthermore, unlike
MARINA, the new methods DASHA, DASHA-PAGE and DASHA-MVR send compressed vectors only and never synchronize the nodes, which makes them more practical for federated learning. We extend our results to the case when the functions satisfy the Polyak-Łojasiewicz condition.
Finally, our theory is corroborated in practice: we see a significant improvement in experiments with nonconvex classification and training of deep learning models.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 2, 2022</h3>
<h1>New Research Intern: Abdurakhmon Sadiev (MIPT)</h1>
<br>


Abdurakhmon Sadiev joined my Optimization and Machine Learning Lab as a research intern. He arrived
today at KAUST, and will be here for 5-6 months.
Abdurakhmon is a final-year MS student in Applied Mathematics at <a href="https://mipt.ru">MIPT</a>; his
advisor there is <a href="http://www.mathnet.ru/eng/person27590">Alexander Gasnikov</a>. He has received
several scholarships and awards during his studies, including:

<ul>
<li> 3rd Degree Prof. Andrei Raigorodskii Personal Scholarship (2021),</li>
<li> Increased State Academic Scholarship for 4 year bachelor and master students at MIPT (2020), and</li>
<li> Abramov Scholarship for 1-3 year bachelor students with the best grades at MIPT (2018).</li>
</ul>

During his MIPT studies, he was a teaching assistant at MIPT for Functional Analysis (Department of Advanced Mathematics)
and Methods of Optimal Control (Department of Mathematical Fundamentals of Control).
<br>
<br>
Abdurakhmon is interested in min-max / saddle-point problems, derivative-free methods and federated learning.
He has coauthored a number of papers, most of which can be
found on his <a href="https://scholar.google.com/citations?hl=en&user=R-xZRIAAAAAJ">Google Scholar</a>
and <a href="https://www.researchgate.net/profile/Abdurakhmon-Sadiev">ResearchGate</a> profiles:
<ul>
<li>Gradient-Free Methods for Saddle-Point Problem</li>
<li>Zeroth-Order Algorithms for Smooth Saddle-Point Problems</li>
<li>Solving Smooth Min-Min and Min-Max Problems by Mixed Oracle Algorithms</li>
<li>Decentralized Personalized Federated Min-Max Problems</li>
<li>Decentralized Personalized Federated Learning: Lower Bounds and Optimal Algorithm for All Personalization Modes</li>
<li>Decentralized and Personalized Federated Learning</li>
<li>Gradient-Free Methods with Inexact Oracle for Convex-Concave Stochastic Saddle-Point Problem</li>
<li>Optimal Algorithms for Decentralized Stochastic Variational Inequalities</li>
</ul>
<br>
Welcome to the team!!!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 2, 2022</h3>

<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2202.00998">
"3PC: Three point compressors for communication-efficient distributed training and a better theory for lazy aggregation"</a> -
joint work with
<a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a>,
<a href="https://ai.ethz.ch/people/ilyas-fatkhullin.html">Ilyas Fatkhullin</a>,
<a href="https://elnurgasanov.com/">Elnur Gasanov</a>,
<a href="https://zhizeli.github.io/">Zhize Li</a>, and
<a href="https://eduardgorbunov.github.io/">Eduard Gorbunov</a>.

<br>
<br>
Abstract:
<i>
We propose and study a new class of gradient communication mechanisms for communication-efficient training -- three point compressors (3PC) -- as well as efficient distributed nonconvex optimization algorithms that can take advantage of them. Unlike most established approaches, which rely on a static compressor choice (e.g., Top-K), our class allows the compressors to {\em evolve} throughout the training process, with the aim of improving the theoretical communication complexity and practical efficiency of the underlying methods. We show that our general approach can recover the recently proposed state-of-the-art error feedback mechanism EF21 (Richtárik et al., 2021) and its theoretical properties as a special case, but also leads to a number of new efficient methods. Notably, our approach allows us to improve upon the state of the art in the algorithmic and theoretical foundations of the lazy aggregation literature (Chen et al., 2018). As a by-product that may be of independent interest, we provide a new and fundamental link between the lazy aggregation and error feedback literature. A special feature of our work is that we do not require the compressors to be unbiased.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 28, 2022</h3>

<h1>ICML 2022</h1>
<br>

The ICML 2022 submission deadline is over, I'll be sleeping for the rest of the month.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 26, 2022</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2201.11066">
"Server-side stepsizes and sampling without replacement provably help in federated optimization"</a> -
joint work with
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> and
<a href="https://konstmish.github.io">Konstantin Mishchenko</a>.
<br>
<br>
Abstract:
<i>
We present a theoretical study of server-side optimization in federated learning. Our results are the first to show that the widely popular heuristic of scaling the client updates with an extra parameter is very useful in the context of Federated Averaging (FedAvg) with local passes over the client data. Each local pass is performed without replacement using Random Reshuffling, which is a key reason we can show improved complexities. In particular, we prove that whenever the local stepsizes are small, and the update direction is given by FedAvg in conjunction with Random Reshuffling over all clients, one can take a big leap in the obtained direction and improve rates for convex, strongly convex, and non-convex objectives. In particular, in non-convex regime we get an enhancement of the rate of convergence from $O(\epsilon^{-3})$ to $O(\epsilon^{-2})$. This result is new even for Random Reshuffling performed on a single node. In contrast, if the local stepsizes are large, we prove that the noise of client sampling can be controlled by using a small server-side stepsize. To the best of our knowledge, this is the first time that local steps provably help to overcome the communication bottleneck. Together, our results on the advantage of large and small server-side stepsizes give a formal justification for the practice of adaptive server-side optimization in federated learning. Moreover, we consider a variant of our algorithm that supports partial client participation, which makes the method more practical.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 23, 2022</h3>

<h1>The Spring 2022 Semester at KAUST Started</h1>
<br>

The Spring 2022 semester at KAUST started today; I am teaching CS 332: Federated Learning.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 20, 2022</h3>

<h1>Three Papers Accepted to ICLR 2022</h1>
<br>

We've had several three papers accepted to the <a href="https://iclr.cc/">International Conference on Learning Representations 2022.</a>
Here they are:<br><br>


<b> 1) "IntSGD: Floatless Compression of Stochastic Gradients"</b> <a href="https://arxiv.org/abs/2102.08374">[arXiv]</a>  -
joint work with
<a href="https://konstmish.github.io">Konstantin Mishchenko</a>,
<a href="https://dblp.org/pid/207/1922.html">Bokun Wang</a>, and
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>.

<br>
<br>
<br>

<b> 2) "Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information"</b>
<a href="https://arxiv.org/abs/2109.05198">[arXiv]</a>  -
joint work with
<a href="https://coral.ise.lehigh.edu/maj316/">Majid Jahani</a>,
<a href="https://scholar.google.ru/citations?user=tdyw7IAAAAAJ&hl=en">Sergey Rusakov</a>,
<a href="https://coral.ise.lehigh.edu/zhs310/">Zheng Shi</a>,
<a href="https://www.stat.berkeley.edu/~mmahoney/">Michael W. Mahoney</a>, and
<a href="https://mtakac.com/">Martin Takáč</a>.

<br>
<br>
<br>

<b> 3) "Permutation Compressors for Provably Faster Distributed Nonconvex Optimization"</b>
<a href="https://arxiv.org/abs/2110.03300">[arXiv]</a>  -
joint work with
<a href="https://uk.linkedin.com/in/rafa%C5%82-szlendak-552936220">Rafal Szlendak</a> and
<a href="https://k3nfalt.github.io/">Alexander Tyurin.</a>

<br>



<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 18, 2022</h3>
<h1>Three Papers Accepted to AISTATS 2022</h1>
<br>

We've had several three papers accepted to <a href="http://aistats.org/aistats2022/">The 25th International Conference on Artificial Intelligence and Statistics.</a>

Here they are:<br><br>


<b> 1) "An Optimal Algorithm for Strongly Convex Minimization under Affine Constraints"</b> <a href="https://arxiv.org/abs/2102.11079">[arXiv]</a>  -
joint work with
<a href="https://adil-salim.github.io/">Adil Salim</a>,
<a href="https://lcondat.github.io/">Laurent Condat</a>, and
<a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>.

<br>
<br>
<br>

<b> 2) "FLIX: A Simple and Communication-Efficient Alternative to Local Methods in Federated Learning"</b>
<a href="https://arxiv.org/abs/2111.11556">[arXiv]</a>  -
joint work with
<a href="http://elnurgasanov.com/">Elnur Gasanov</a>,
<a href="https://rka97.github.io/">Ahmed Khaled</a>, and
<a href="https://samuelhorvath.github.io/">Samuel Horváth</a>.

<br>
<br>
<br>

<b> 3) "Basis Matters: Better Communication-Efficient Second Order Methods for Federated Learning"</b>
<a href="https://arxiv.org/abs/2111.01847">[arXiv]</a>  -
joint work with
<a href="https://qianxunk.github.io/">Xun Qian</a>,
<a href="https://rustem-islamov.github.io/">Rustem Islamov</a>, and
<a href="https://mher-safaryan.github.io/">Mher Safaryan</a>.

<br>



<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 11, 2022</h3>

<h1>Paper Accepted to SIAM Journal on Mathematics of Data Science</h1>
<br>

The paper <a href="https://arxiv.org/abs/2002.05359">"Adaptivity of stochastic
  gradient methods for nonconvex optimization"</a>, joint work with
  <a href="https://samuelhorvath.github.io">Samuel Horváth</a> and
<a href="https://lihualei71.github.io">Lihua Lei</a>, and
<a href="https://people.eecs.berkeley.edu/~jordan/">Michael I. Jordan</a>
was accepted to
<a href="https://www.frontiersin.org/journals/signal-processing">SIAM Journal on Mathematics
  of Data Science (SIMODS).
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>December 31, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2112.15199">
  "Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling"</a> -
joint work with
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> and
<a href="https://zhizeli.github.io">Alexander Gasnikov</a>.
<br>
<br>
Abstract: <i>
  In this paper we study a convex-concave saddle-point problem min_x max_y f(x) + y^T A x − g(y), where f(x) and g(y) are smooth and convex functions.
  We propose an Accelerated Primal-Dual Gradient Method for solving this problem which (i) achieves an optimal
  linear convergence rate in the strongly-convex-strongly-concave regime matching the lower complexity bound
  (Zhang et al., 2021) and (ii) achieves an accelerated linear convergence rate in the case when only one of
  the functions f(x) and g(y) is strongly convex or even none of them are. Finally, we obtain a
  linearly-convergent algorithm for the general smooth and convex-concave saddle point problem min_x max_y F(x,y)
  without requirement of strong convexity or strong concavity.
</i>

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>December 24, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2112.13097">
  "Faster rates for compressed federated learning with client-variance reduction"</a> -
joint work with
<a href="http://hyzhao.me">Haoyu Zhao</a>,
<a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a> and
<a href="https://zhizeli.github.io">Zhize Li</a>.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>December 7, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://dl.acm.org/doi/pdf/10.1145/3488659.3493775">"FL_PyTorch: optimization research simulator for federated learning"</a> -
joint work with
<a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a> and
<a href="https://samuelhorvath.github.io">Samuel Horváth.</a>
<br>
<br>
Abstract: <i>
Federated Learning (FL) has emerged as a promising technique for edge devices to collaboratively learn a shared machine learning model while keeping training data locally on the device, thereby removing the need to store and access the full data in the cloud. However, FL is difficult to implement, test and deploy in practice considering heterogeneity in common edge device settings, making it fundamentally hard for researchers to efficiently prototype and test their optimization algorithms. In this work, our aim is to alleviate this problem by introducing FL_PyTorch : a suite of open-source software written in python that builds on top of one the most popular research Deep Learning (DL) framework PyTorch. We built FL_PyTorch as a research simulator for FL to enable fast development, prototyping and experimenting with new and existing FL optimization algorithms. Our system supports abstractions that provide researchers with a sufficient level of flexibility to experiment with existing and novel approaches to advance the state-of-the-art. Furthermore, FL_PyTorch is a simple to use console system, allows to run several clients simultaneously using local CPUs or GPU(s), and even remote compute devices without the need for any distributed implementation provided by the user. FL_PyTorch also offers a Graphical User Interface. For new methods, researchers only provide the centralized implementation of their algorithm. To showcase the possibilities and usefulness of our system, we experiment with several well-known state-of-the-art FL algorithms and a few of the most common FL datasets.
</i>
<br>
<br>
The paper is published in the <a href="https://dl.acm.org/doi/pdf/10.1145/3488659.3493775">Proceedings of the 2nd ACM International Workshop on Distributed Machine Learning.</a>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>December 7, 2021</h3>
<h1>Oral Talk at NeurIPS 2021</h1>
<br>

Today I gave an oral talk at NeurIPS about the <a href="https://arxiv.org/abs/2106.05203">EF21 method.</a>  Come to our poster on Thursday! A longer version of the talk is on <a href="https://www.youtube.com/watch?v=rjWze5rcSUM&feature=emb_logo">YouTube.</a>
<br>

<br>
<img alt="" src="imgs/EF21-neurips.png" width="600">
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>November 24, 2021</h3>
<h1>KAUST-GSAI Workshop</h1>
<br>

Today and tomorrow I am attending (and giving a talk at) the <a href="http://ai.ruc.edu.cn/english/gsainews/20211119100.html">KAUST-GSAI Joint Workshop on Advances in AI</a>.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>November 22, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2111.11556">"FLIX: A Simple and Communication-Efficient Alternative to Local Methods in Federated Learning"</a> -
joint work with
<a href="http://elnurgasanov.com">Elnur Gasanov</a>,
<a href="https://rka97.github.io">Ahmed Khaled</a> and
<a href="https://samuelhorvath.github.io">Samuel Horváth.</a>
<br>
<br>
Abstract: <i>
Federated Learning (FL) is an increasingly popular machine learning paradigm in which multiple nodes try to collaboratively learn under privacy, communication and multiple heterogeneity constraints. A persistent problem in federated learning is that it is not clear what the optimization objective should be: the standard average risk minimization of supervised learning is inadequate in handling several major constraints specific to federated learning, such as communication adaptivity and personalization control. We identify several key desiderata in frameworks for federated learning and introduce a new framework, FLIX, that takes into account the unique challenges brought by federated learning. FLIX has a standard finite-sum form, which enables practitioners to tap into the immense wealth of existing (potentially non-local) methods for distributed optimization. Through a smart initialization that does not require any communication, FLIX does not require the use of local steps but is still provably capable of performing dissimilarity regularization on par with local methods. We give several algorithms for solving the FLIX formulation efficiently under communication constraints. Finally, we corroborate our theoretical results with extensive experimentation.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>November 17, 2021</h3>
<h1>Samuel, Dmitry and Grigory won the 2021 CEMSE Research Excellence Award!</h1>
<br>

Today I am very proud and happy! Three of my students won the CEMSE Research Excellence Award at KAUST: <a href="https://samuelhorvath.github.io">Samuel Horváth</a> (Statistics PhD student),
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> (Computer Science PhD student) and
<a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> (Applied Math and Computing Sciences MS student). The Statistics award is also known as the "Al-Kindi Research Excellence Award".
<br>
<br>

The award comes with a 1,000 USD cash prize for each. Congratulations to all of you, well deserved!
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>November 10, 2021</h3>
<h1>Talk at the SMAP Colloquium at the University of Portsmouth, United Kingdom</h1>
<br>

Today I gave a 1hr research talk on the EF21 method at the SMAP Colloquium, University of Portsmouth, UK.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




          <h3>November 2, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2111.01847">"Basis Matters: Better Communication-Efficient Second Order Methods for Federated Learning"</a> -
          joint work with
          <a href="https://qianxunk.github.io">Xun Qian</a>,
          <a href="https://rustem-islamov.github.io">Rustem Islamov</a> and
          <a href="https://mher-safaryan.github.io">Mher Safaryan.</a>
          <br>
          <br>
          Abstract: <i>
          Recent advances in distributed optimization have shown that Newton-type methods with proper communication compression mechanisms can guarantee fast local rates and low communication cost compared to first order methods. We discover that the communication cost of these methods can be further reduced, sometimes dramatically so, with a surprisingly simple trick: {\em Basis Learn (BL)}. The idea is to transform the usual representation of the local Hessians via a change of basis in the space of matrices and apply compression tools to the new representation. To demonstrate the potential of using custom bases, we design a new Newton-type method (BL1), which reduces communication cost via both {\em BL} technique and bidirectional compression mechanism. Furthermore, we present two alternative extensions (BL2 and BL3) to partial participation to accommodate federated learning applications. We prove local linear and superlinear rates independent of the condition number. Finally, we support our claims with numerical experiments by comparing several first and second~order~methods.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>November 1, 2021</h3>
          <h1>Talk at the CS Graduate Seminar at KAUST</h1>
          <br>

          Today I am giving <a href="https://cemse.kaust.edu.sa/events/event/ef21-new-simpler-theoretically-better-and-practically-faster-error-feedback">a talk in the CS Graduate Seminar at KAUST.</a>
           <br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>October 25, 2021</h3>
          <h1>Talk at KInIT</h1>
          <br>

          Today at 15:30 I am giving a research talk  at the <a href="https://kinit.sk">Kempelen Institute of Intelligent Technologies (KInIT)</a>, Slovakia.
           <br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>October 22, 2021</h3>
          <h1>Talk at "Matfyz"</h1>
          <br>

      Today at 15:30 I am giving a <a href="https://fmph.uniba.sk/detail-novinky/back_to_page/fakulta-matematiky-fyziky-a-informatiky-uk/article/seminar-zo-strojoveho-ucenia-peter-richtarik-22102021/">talk in the machine learning seminar at "Matfyz"</a>, Comenius University, Slovakia. I will talk about the paper
      <a href="https://arxiv.org/abs/2106.05203">"EF21: A new, simpler, theoretically better, and practically faster error feedback"</a> which was recently accepted to <a href="https://nips.cc/Conferences/2021">NeurIPS 2021</a> as an <span class="important">oral paper</span>
      (less than 1% acceptance rate from more than 9000 paper submissions). The paper is joint work with <a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a> and
      <a href="https://scholar.google.com/citations?user=G2OzFpIAAAAJ&hl=en">Ilyas Fatkhullin.</a>
          <br>

          <br>
          <img alt="" src="imgs/EF21-first-slide.png" width="700" >

          <br>  <br>

          With an extended set of coauthors, we have recently written a follow up paper with many major extensions of the EF21 method; you may wish
          to look at this as well:
          <a href="https://arxiv.org/abs/2110.03294">"EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback".</a> This second paper is joint work with
          <a href="https://scholar.google.com/citations?user=G2OzFpIAAAAJ&hl=en">Ilyas Fatkhullin</a>, <a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a>,
           <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>, and <a href="https://zhizeli.github.io">Zhize Li.</a>
           <br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>October 20, 2021</h3>
          <h1>Paper Accepted to Frontiers in Signal Processing</h1>
          <br>

      The paper <a href="https://www.frontiersin.org/articles/10.3389/frsip.2021.776825/abstract">"Distributed proximal splitting algorithms with rates and acceleration"</a>, joint work with <a href="https://lcondat.github.io">Laurent Condat</a> and
      <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a>, was accepted to <a href="https://www.frontiersin.org/journals/signal-processing">Frontiers in Signal Processing, section Signal Processing for Communications.</a> The paper is
      is a part of a special issue ("research topic" in the language of Frontiers) dedicated to <a href="https://www.frontiersin.org/research-topics/21002/distributed-signal-processing-and-machine-learning-for-communication-networks#articles">"Distributed Signal Processing and Machine Learning for Communication Networks".</a>
          <br>



          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>October 15, 2021</h3>
          <h1>2 Students Received the 2021 NeurIPS Outstanding Reviewer Award</h1>
          <br>

        Congratulations to <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> and <a href = "https://konstmish.github.io">Konstantin Mishchenko</a> who received the <a href="https://nips.cc/Conferences/2021/ProgramCommittee">2021 NeurIPS Outstanding Reviewer Award</a> given to the top 8% reviewers, as judged
        by the conference chairs!
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



<h3>October 9, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2110.03313">"Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees"</a> -
joint work with <a href="https://anbeznosikov.github.io">Aleksandr Beznosikov</a>, <a href="https://www.hse.ru/en/staff/yhn112">Michael Diskin</a>, <a href="https://www.hse.ru/en/staff/mryabinin">Max Ryabinin</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.<br>
<br>
Abstract: <i>
Variational inequalities in general and saddle point problems in particular are increasingly relevant in machine learning applications, including adversarial learning, GANs, transport and robust optimization. With increasing data and problem sizes necessary to train high performing models across these and other applications, it is necessary to rely on parallel and distributed computing. However, in distributed training, communication among the compute nodes is a key bottleneck during training, and this problem is exacerbated for high dimensional and over-parameterized models models. Due to these considerations, it is important to equip existing methods with strategies that would allow to reduce the volume of transmitted information during training while obtaining a model of comparable quality. In this paper, we present the first theoretically grounded distributed methods for solving variational inequalities and saddle point problems using compressed communication: MASHA1 and MASHA2. Our theory and methods allow for the use of both unbiased (such as RandK; MASHA1) and contractive (such as TopK; MASHA2) compressors. We empirically validate our conclusions using two experimental setups: a standard bilinear min-max problem, and large-scale distributed adversarial training of transformers.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>October 8, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2110.03300">"Permutation Compressors for Provably Faster Distributed Nonconvex Optimization"</a> -
joint work with <a href="https://www.linkedin.com/in/rafał-szlendak-552936220">Rafał Szlendak</a> and <a href="https://k3nfalt.github.io">Alexander Tyurin</a>.<br>
<br>
Abstract: <i>
We study the MARINA method of Gorbunov et al (ICML, 2021) -- the current state-of-the-art distributed non-convex optimization method in terms of
theoretical communication complexity. Theoretical superiority of this method can be largely attributed to two sources: the use of a carefully
engineered biased stochastic gradient estimator, which leads to a reduction in the number of communication rounds, and  the reliance on
independent stochastic communication compression operators, which leads to a reduction in the number of  transmitted bits within each
 communication round. In this paper we i) extend the theory of MARINA to support a much wider class of potentially correlated
 compressors, extending the reach of the method beyond the classical independent compressors setting, ii) show that a new quantity, for which we
 coin the name Hessian variance, allows us to significantly refine the original analysis of MARINA without any additional assumptions, and
 iii) identify a special class of correlated compressors based on the idea of random  permutations, for which we coin the term PermK, the use of
 which leads to  $O(\sqrt{n})$ (resp. $O(1 + d/\sqrt{n})$) improvement in the theoretical communication complexity of MARINA in the low Hessian
 variance regime when $d\geq n$ (resp. $d \leq n$), where $n$ is the number of workers and $d$ is the number of parameters describing the model
 we are learning. We corroborate our theoretical results with carefully engineered synthetic experiments with minimizing the average of nonconvex
 quadratics, and on autoencoder training with the MNIST dataset.

</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>October 7, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2110.03294">"EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback"</a> -
joint work with
<a href="https://scholar.google.com/citations?user=G2OzFpIAAAAJ&hl=en">Ilyas Fatkhullin</a>,
<a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a>,
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>
and
<a href="https://zhizeli.github.io">Zhize Li</a>.<br>
<br>
Abstract: <i>
First proposed by Seide et al (2014) as a heuristic, error feedback (EF) is a very popular mechanism for enforcing convergence of distributed gradient-based
optimization methods enhanced with communication compression strategies based on the application of contractive compression operators. However, existing
theory of EF relies on very strong assumptions (e.g., bounded gradients), and provides pessimistic convergence rates (e.g., while the best known rate for
EF in the smooth nonconvex regime, and when full gradients are compressed, is O(1/T^{2/3}), the rate of gradient descent in the same regime is O(1/T).
Recently, Richt\'{a}rik et al (2021) proposed a new error feedback mechanism,
EF21, based on the construction of a Markov compressor induced by a contractive compressor. EF21 removes the aforementioned theoretical deficiencies
of EF and at the same time works better in practice. In this work we propose six practical extensions of EF21: partial participation, stochastic
approximation, variance reduction, proximal setting, momentum and bidirectional compression. Our extensions are supported by strong convergence
theory in the smooth nonconvex and also Polyak-Łojasiewicz regimes. Several of these techniques were never analyzed in conjunction with EF before,
and in cases where they were (e.g., bidirectional compression), our rates are vastly superior.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 5, 2021</h3>
<h1>2020 COAP Best Paper Award</h1>
<br>
We have just received this email:
<br><br>
<i>
  Your paper "Momentum and stochastic momentum for
stochastic gradient, Newton, proximal point and subspace descent methods"
published in Computational  Optimization and Applications  was voted by the
editorial board as the best paper appearing in the journal in 2020.
There were 93 papers in  the 2020 competition. Congratulations!
</i>
<br><br>
The <a href="https://link.springer.com/article/10.1007/s10589-020-00220-z">paper</a> is joint work with <a href="https://nicolasloizou.github.io">Nicolas Loizou.</a>

<br>
<br>
<img alt="" src="imgs/COAP-best-paper.png" width="600" >
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>October 4, 2021</h3>
<h1>Konstantin Mishchenko Defended his PhD Thesis</h1>
<br>

<a href="https://konstmish.github.io">Konstantin Mishchenko</a> defended his PhD thesis "On Seven Fundamental Optimization Challenges in Machine Learning" today.
<br> <br>

Having started in Fall 2017 (I joined KAUST in March of the same year), Konstantin is my second PhD student to graduate from KAUST. Konstantin
has done some absolutely remarkable research, described by the committee (<a href="http://optml.mit.edu">Suvrit Sra</a>, <a href="https://www.math.ucla.edu/~wotaoyin/">Wotao Yin</a>,
<a href="https://dblp.org/pid/23/2168.html">Lawrence Carin</a>, <a href="http://www.bernardghanem.com">Bernard Ghanem</a> and myself) in the following way:
"The committee commends Konstantin Mishchenko on his outstanding achievements, including research creativity, depth of technical/mathematical results, volume of published work,
service to the community, and a particularly lucid presentation and defense of his thesis".

<br> <br>
Konstantin wrote more than 20 papers and his works attracted more than 500 citations during his PhD. Konstantin's next destination is a postdoctoral fellowship
position with <a href="https://www.di.ens.fr/~aspremon/">Alexander d'Aspremont</a> and <a href="https://www.di.ens.fr/~fbach/">Francis Bach</a> at INRIA.
Congratulations, Konstantin!

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>September 29, 2021</h3>
<h1>Papers Accepted to NeurIPS 2021</h1>
<br>
We've had several papers accepted to the <a href="https://nips.cc/Conferences/2021">35th Annual Conference on Neural Information Processing Systems (NeurIPS 2021)</a>, which will be run virtually during December 6-14, 2021.
Here they are:<br><br>


<b> 1) "EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback"</b> <a href="https://arxiv.org/abs/2106.05203">[arXiv]</a>  -
joint work with
<a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a> and
<a href="https://scholar.google.com/citations?user=G2OzFpIAAAAJ&hl=en">Ilyas Fatkhullin.</a><br>
<br>
This paper was accepted as an <span class="important">ORAL PAPER (less than 1% of all submissions).</span>
<br><br>

Further links:
<ul>
  <li><a href="https://www.youtube.com/watch?v=rjWze5rcSUM&feature=emb_logo">Long 57 min YouTube talk by me at the FLOW seminar</a> </li>
  <li><a href="https://drive.google.com/file/d/1zNS7wJgd1XSxnK7jTItcGvluTLLs49oL/view?usp=sharing">slides</a></li>
</ul>
<br>

<b> 2) "CANITA: Faster Rates for Distributed Convex Optimization with Communication Compression"</b>
<a href="https://arxiv.org/abs/2107.09461">[arXiv]</a>  -
joint work with
<a href="https://zhizeli.github.io">Zhize Li.</a><br>
<br>
<br>

<b> 3) "Smoothness Matrices Beat Smoothness Constants: Better Communication Compression Techniques for Distributed Optimization"</b>
<a href="https://arxiv.org/abs/2102.07245">[arXiv]</a>  -
joint work with
<a href="https://mher-safaryan.github.io">Mher Safaryan</a> and <a href="https://fhanzely.github.io/index.html">Filip Hanzely.</a><br>
<br>
<br>

<b> 4) "Error Compensated Distributed SGD can be Accelerated"</b>
<a href="https://arxiv.org/abs/2010.00091">[arXiv]</a>  -
joint work with
<a href="https://qianxunk.github.io">Xun Qian</a> and <a href="http://tongzhang-ml.org">Tong Zhang.</a><br>
<br>
<br>

<b> 5) "Lower Bounds and Optimal Algorithms for Smooth and Strongly Convex Decentralized Optimization Over Time-Varying Networks"</b>
<a href="https://arxiv.org/abs/2106.04469">[arXiv]</a>  -
joint work with
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
<a href="http://elnurgasanov.com">Elnur Gasanov</a> and
<a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov.</a><br>
<br>
<br>

<b> 6) "FjORD: Fair and Accurate Federated Learning Under Heterogeneous Targets with Ordered Dropout"</b>
<a href="https://arxiv.org/abs/2102.13451">[arXiv]</a>  -
the work of
<a href="https://samuelhorvath.github.io">Samuel Horváth</a>,
<a href="https://stevelaskaridis.github.io">Stefanos Laskaridis</a>,
Mario Almeida,
<a href="https://leontiadis.net">Ilias Leontiadis</a>,
<a href="https://steliosven10.github.io">Stylianos I. Venieris </a> and
<a href="http://niclane.org">Nicholas D. Lane.</a>
<br>
<br>
<br>

<b> 7) "Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices"</b>
<a href="https://arxiv.org/abs/2103.03239">[arXiv]</a>  -
the work of
<a href="https://www.hse.ru/en/staff/mryabinin">Max Ryabinin</a>,
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>,
<a href="https://research.yandex.com/people/610925">Vsevolod Plokhotnyuk</a> and
<a href="https://www.cs.toronto.edu/~pekhimenko/">Gennady Pekhimenko.</a>





<br>
<br>



<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 23, 2021</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2109.10049">"Error Compensated Loopless SVRG, Quartz, and SDCA for Distributed Optimization"</a> -
joint work with <a href="https://qianxunk.github.io">Xun Qian</a>, <a href="https://www.researchgate.net/profile/Hanze-Dong">Hanze Dong</a>, and <a href="http://tongzhang-ml.org">Tong Zhang</a>.<br>
<br>

Abstract: <i>
The communication of gradients is a key bottleneck in distributed training of large scale machine learning models. In order to reduce the communication cost, gradient compression (e.g., sparsification and quantization) and error compensation techniques are often used. In this paper, we propose and study three new efficient methods in this space: error compensated loopless SVRG method (EC-LSVRG), error compensated Quartz (EC-Quartz), and error compensated SDCA (EC-SDCA). Our method is capable of working with any contraction compressor (e.g., TopK compressor), and we perform analysis for convex optimization problems in the composite case and smooth case for EC-LSVRG. We prove linear convergence rates for both cases and show that in the smooth case the rate has a better dependence on the parameter associated with the contraction compressor. Further, we show that in the smooth case, and under some certain conditions, error compensated loopless SVRG has the same convergence rate as the vanilla loopless SVRG method. Then we show that the convergence rates of EC-Quartz and EC-SDCA in the composite case are as good as EC-LSVRG in the smooth case. Finally, numerical experiments are presented to illustrate the efficiency of our methods.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>September 11, 2021</h3>
<h1>New Paper</h1>
 <br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2109.05198">"Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information"</a> -
joint work with <a href="https://coral.ise.lehigh.edu/maj316/">Majid Jahani</a>, <a href="https://scholar.google.com/citations?user=tdyw7IAAAAAJ&hl=lv">Sergey Rusakov</a>, <a href="https://coral.ise.lehigh.edu/zhs310/">Zheng Shi</a>,
<a href="https://www.stat.berkeley.edu/~mmahoney/">Michael W. Mahoney</a>, and
<a href="https://mtakac.com">Martin Takáč</a>.<br>
 <br>
Abstract: <i> We present a novel adaptive optimization algorithm for large-scale machine learning problems. Equipped with a low-cost estimate of local curvature and Lipschitz smoothness, our method dynamically adapts the search direction and step-size. The search direction contains gradient information preconditioned by a well-scaled diagonal preconditioning matrix that captures the local curvature information. Our methodology does not require the tedious task of learning rate tuning, as the learning rate is updated automatically without adding an extra hyperparameter. We provide convergence guarantees on a comprehensive collection of optimization problems, including convex, strongly convex, and nonconvex problems, in both deterministic and stochastic regimes. We also conduct an extensive empirical evaluation on standard machine learning problems, justifying our algorithm's versatility and demonstrating its strong performance compared to other start-of-the-art first-order and second-order methods.
</i>

 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>August 29, 2021</h3>
 <h1>Fall 2021 Semester Started</h1>

 The Fall semester has started at KAUST today; I am teaching CS 331: Stochastic Gradient Descent Methods.
 <br> <br>

 <i>Brief course blurb:</i> Stochastic gradient descent (SGD) in one or another of its many variants is the workhorse method for training modern supervised machine learning models. However, the world of SGD methods is vast and expanding, which makes it hard for practitioners and even experts to understand its landscape and inhabitants. This course is a mathematically rigorous and comprehensive introduction to the field, and is based on the latest results and insights.
 The course develops a convergence and complexity theory for serial, parallel, and distributed variants of SGD, in the strongly convex, convex and nonconvex setup, with randomness coming from sources such as subsampling and compression. Additional topics such as acceleration via Nesterov momentum or curvature information will be covered as well.
 A substantial part of the course offers a unified analysis of a large family of variants of SGD which have so far required different intuitions, convergence analyses, have different applications, and which have been developed separately in various communities. This framework includes methods with and without the following tricks, and their combinations: variance reduction, data sampling, coordinate sampling, arbitrary sampling, importance sampling, mini-batching, quantization, sketching, dithering and sparsification.
 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>August 11, 2021</h3>
 <h1>New Paper</h1>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2108.04755">"FedPAGE: A Fast Local Stochastic Gradient Method for Communication-Efficient Federated Learning"</a> -
 joint work with <a href="http://hyzhao.me">Haoyu Zhao</a> and <a href="https://zhizeli.github.io">Zhize Li</a>.<br>
 <br>

 Abstract: <i> Federated Averaging (FedAvg, also known as Local-SGD) [McMahan et al., 2017] is a classical federated learning algorithm in which
   clients run multiple local SGD steps before communicating their update to an orchestrating server.  We propose a new federated learning algorithm,
   FedPAGE,  able to further reduce the communication complexity by utilizing the recent optimal PAGE method [Li et al., 2021] instead of plain
   SGD in FedAvg. We show that FedPAGE uses much fewer communication rounds than previous local methods for both federated convex and nonconvex
   optimization. Concretely, 1) in the convex setting, the number of communication rounds of FedPAGE is $O(\frac{N^{3/4}}{S\epsilon})$, improving
   the best-known result $O(\frac{N}{S\epsilon})$ of SCAFFOLD [Karimireddy et al., 2020] by a factor of $N^{1/4}$, where $N$ is the total number
   of clients (usually is very large in federated learning), $S$ is the sampled subset of clients in each communication round, and $\epsilon$ is
   the target error; 2) in the nonconvex setting, the number of communication rounds of FedPAGE is $O(\frac{\sqrt{N}+S}{S\epsilon^2})$, improving
   the best-known result  $O(\frac{N^{2/3}}{S^{2/3}\epsilon^2})$ of SCAFFOLD by a factor of $N^{1/6}S^{1/3}$ if the sampled clients $S\leq \sqrt{N}$.
   Note that in both settings, the communication cost for each round is the same for both FedPAGE and SCAFFOLD. As a result, FedPAGE achieves new
   state-of-the-art results in terms of communication complexity for both federated convex and nonconvex optimization.
 </i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>




 <h3>July 20, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2107.09461">"CANITA: Faster Rates for Distributed Convex Optimization with Communication Compression"</a> -
 joint work with <a href="https://zhizeli.github.io">Zhize Li</a>.<br>
 <br>
 <br>
 In this work we develop and analyze the first distributed gradient method  capable in the convex regime of benefiting from
 communication compression and acceleration/momentum at the same time. The strongly convex regime was first handled in the <a href="http://proceedings.mlr.press/v119/li20g.html">ADIANA
 paper (ICML 2020)</a>, and the nonconvex regime in the <a href="http://proceedings.mlr.press/v139/gorbunov21a.html">MARINA paper (ICML 2021)</a>.
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>July 20, 2021</h3>
 <h1>Talk at SIAM Conference on Optimization</h1>
 <br>

 Today I gave a talk in the <a href="https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=71575">Recent Advancements in Optimization Methods for Machine Learning - Part I of III</a> minisymposium
 at the <a href="https://www.siam.org/conferences/cm/conference/op21">SIAM Conference on Optimization.</a> The conference was originally supposed to take place in Hong Kong in 2020, but due to the Covid-19
 situation, this was not to be. Instead, the event is happening this year, and virtually. I was on the organizing committee for the conference, jointly resposible for inviting plenary and tutorial speakers.
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>July 12, 2021</h3>
 <h1>Optimization Without Borders Conference (Nesterov 65)</h1>
 <br>

 Today I am giving a talk at the <a href="https://cs.hse.ru/hdilab/opti/">"Optimization Without Borders"</a> conference, organized in the honor of <a href="https://en.wikipedia.org/wiki/Yurii_Nesterov">Yurii Nesterov's</a> 65th Birthday. This is a hybrid
 event, with online and offline participants. The offline part takes place at the <a href="https://siriusuniversity.ru">Sirius University</a> in Sochi, Russia.
 <br>    <br>
 Other speakers at the event (in order of giving talks at the event): Gasnikov, Nesterov, myself, Spokoiny, Mordukhovich, Bolte, Belomestny, Srebro,
 Zaitseva, Protasov, Shikhman, d'Aspremont, Polyak, Taylor, Stich, Teboulle, Lasserre, Nemirovski,  Vorobiev,
 Yanitsky, Bakhurin, Dudorov, Molokov, Gornov, Rogozin, Hildebrand, Dvurechensky, Moulines, Juditsky,
 Sidford, Tupitsa, Kamzolov, and Anikin.

 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>July 5, 2021</h3>
 <h1>New Postdoc: Alexander Tyurin</h1>
 <br>

 <a href="https://k3nfalt.github.io">Alexander Tyurin</a> has joined my
 Optimization and Machine Learning lab as a postdoc. Welcome!!!

<br> <br>
 Alexander obtained his PhD from
 the Higher School of Economics (HSE) in December 2020, under the supervision of <a href="https://scholar.google.ru/citations?user=AmeE8qkAAAAJ">Alexander
 Gasnikov</a>, with the thesis "Development of a method for solving structural
 optimization problems". His 15 research papers can be found on <a href="https://scholar.google.com/citations?user=Es8-xocAAAAJ">Google Scholar</a>. He has a masters degree in CS from
 HSE (2017), with a GPA of 9.84 / 10, and a BS degree in Computational Mathematics and Cybernetics from
 Lomonosov Moscow State University, with a GPA of 4.97 / 5.

<br> <br>
  During his studies, and for a short period of time after his PhD,
 Alexander worked as a research and development engineer in the Yandex self-driving cars team, where he was developing real-time algorithms for dynamic and static objects detection in a perception team
 for self-driving cars Using lidar (3D point clouds) and cameras (images) sensors.
His primary responsibilities there ranged from the creation of datasets, throught research (Python, SQL, MapReduce) and implementation of the
proposed algorithms (C++). Prior to this, he was a Research Engineer at VisionLabs in Moscow where he developed a face recognition
algorithm that achieved a top 2 result in the FRVT NIST international competition.

 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>July 4, 2021</h3>
 <h1>Two New Interns</h1>
 <br>

 Two new people joined my team as Summer research interns:
 <br><br>

 <b>Rafał Szlendak</b> joined as an undergraduate intern.  Rafal is studying towards a <a href="https://warwick.ac.uk/study/undergraduate/courses/mathsstatsbsc/">BSc degree in Mathematics and Statistics</a> at the <a href="https://warwick.ac.uk">University of Warwick</a>, United Kingdom.
 He was involved in a research project entitled "Properties and characterisations of sequences generated
by weighted context-free grammars with one terminal symbol". Among Rafal’s successes belong
 <br>
 - Ranked #1 in the Mathematics and Statistics Programme at Warwick, 2020
 <br>
 - Finalist, Polish National Mathematical Olympiad, 2017 and 2018
 <br>
 - Member of MATEX: an experimental mathematics programme for gifted students. This high school was ranked the top high school in Poland in the 2019 Perspektywy ranking.



<br>
<br>
 <b>Muhammad Harun Ali Khan</b> joined as an undergraduate intern. Harun is a US citizen of Pakistani ancestry, and studies towards a <a href="https://www.imperial.ac.uk/study/ug/courses/mathematics-department/mathematics-bsc/">BSc degree in Mathematics</a> at <a href="https://www.imperial.ac.uk">Imperial College London</a>. He has interests in
 number theory, artificial intelligence and doing mathematics via the <a href="https://en.wikipedia.org/wiki/Lean_(proof_assistant)">Lean proof assistant</a>. Harun is the Head of Imperial College mathematics competition problem selection committee.
 Harun has been active in various mathematics competitions at high school and university level. Some of his most notable recognitions and awards include
 <br>
 - <a href="https://www.imc-math.org.uk/?act=results&by=sum&year=2020">2nd Prize, International Mathematics Competition for University Students, 2020</a>
 <br>
 - Imperial College UROP Prize (for formalizing Fibonacci Squares in Lean)
 <br>
 - Imperial College Mathematics Competition, First Place in First Round
 <br>
 - <a href="https://www.imo-official.org/participant_r.aspx?id=27280">Bronze Medal, International Mathematical Olympiad, United Kingdom, 2019</a>
 <br>
 - <a href="https://www.apmo-official.org/country_report/PAK/2019">Bronze Medal, Asian Pacific Mathematics Olympiad, 2019</a>
 <br>
 - Honorable Mention, International Mathematical Olympiad, Romania, 2018
 <br>
 - Honorable Mention, International Mathematical Olympiad, Brazil, 2017
<br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>


 <h3>June 9, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2106.05203">"EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback"</a> -
 joint work with <a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a> and Ilyas Fatkhullin.<br>
 <br>

 Abstract: <i>
Error feedback (EF), also known as error compensation, is an immensely popular convergence stabilization mechanism in the context of distributed training of supervised machine learning models enhanced by the use of contractive communication compression mechanisms, such as Top-k. First proposed by Seide et al (2014) as a heuristic, EF resisted any theoretical understanding until recently [Stich et al., 2018, Alistarh et al., 2018]. However, all existing analyses either i) apply to the single node setting only, ii) rely on very strong and often unreasonable assumptions, such global boundedness of the gradients, or iterate-dependent assumptions that cannot be checked a-priori and may not hold in practice, or iii) circumvent these issues via the introduction of additional unbiased compressors, which increase the communication cost. In this work we fix all these deficiencies by proposing and analyzing a new EF mechanism, which we call EF21, which consistently and substantially outperforms EF in practice. Our theoretical analysis relies on standard assumptions only, works in the distributed heterogeneous data setting, and leads to better and more meaningful rates. In particular, we prove that EF21 enjoys a fast O(1/T) convergence rate for smooth nonconvex problems, beating the previous bound of O(1/T^{2/3}), which was shown a bounded gradients assumption. We further improve this to a fast linear rate for PL functions, which is the first linear convergence result for an EF-type method not relying on unbiased compressors. Since EF has a large number of applications where it reigns supreme, we believe that our 2021 variant, EF21, can a large impact on the practice of communication efficient distributed learning.
</i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>June 8, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2106.04469">"Lower Bounds and Optimal Algorithms for Smooth and Strongly Convex Decentralized Optimization Over Time-Varying Networks"</a> -
 joint work with <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="http://elnurgasanov.com">Elnur Gasanov</a> and <a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.<br>
 <br>

 Abstract: <i>
We consider the task of minimizing the sum of smooth and strongly convex functions stored in a decentralized manner across the nodes of a communication network whose links are allowed to change in time. We solve two fundamental problems for this task. First, we establish the first lower bounds on the number of decentralized communication rounds and the number of local computations required to find an ϵ-accurate solution. Second, we design two optimal algorithms that attain these lower bounds: (i) a variant of the recently proposed algorithm ADOM (Kovalev et al., 2021) enhanced via a multi-consensus subroutine, which is optimal in the case when access to the dual gradients is assumed, and (ii) a novel algorithm, called ADOM+, which is optimal in the case when access to the primal gradients is assumed. We corroborate the theoretical efficiency of these algorithms by performing an experimental comparison with existing state-of-the-art methods.
</i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>June 6, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2106.03524">"Smoothness-Aware Quantization Techniques"</a> -
 joint work with Bokun Wang, and <a href="https://mher-safaryan.github.io">Mher Safaryan</a>.<br>
 <br>

 Abstract: <i>
Distributed machine learning has become an indispensable tool for training large supervised machine learning models. To address the high communication costs of distributed training, which is further exacerbated by the fact that modern highly performing models are typically overparameterized, a large body of work has been devoted in recent years to the design of various compression strategies, such as sparsification and quantization, and optimization algorithms capable of using them. Recently, Safaryan et al (2021) pioneered a dramatically different compression design approach: they first use the local training data to form local "smoothness matrices", and then propose to design a compressor capable of exploiting the smoothness information contained therein. While this novel approach leads to substantial savings in communication, it is limited to sparsification as it crucially depends on the linearity of the compression operator. In this work, we resolve this problem by extending their smoothness-aware compression strategy to arbitrary unbiased compression operators, which also includes sparsification. Specializing our results to quantization, we observe significant savings in communication complexity compared to standard quantization. In particular, we show theoretically that block quantization with n blocks outperforms single block quantization, leading to a reduction in communication complexity by an O(n) factor, where n is the number of nodes in the distributed system. Finally, we provide extensive numerical evidence that our smoothness-aware quantization strategies outperform existing quantization schemes as well the aforementioned smoothness-aware sparsification strategies with respect to all relevant success measures: the number of iterations, the total amount of bits communicated, and wall-clock time.
</i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>




 <h3>June 6, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2106.03076">"Complexity Analysis of Stein Variational Gradient Descent under Talagrand's Inequality T1"</a> -
 joint work with
 <a href="https://adil-salim.github.io">Adil Salim</a>, and <a href="https://lukangsun.github.io">Lukang Sun</a>.<br>
 <br>

 Abstract: <i>
We study the complexity of Stein Variational Gradient Descent (SVGD), which is an algorithm to sample from π(x)∝exp(−F(x)) where F smooth and nonconvex.
We provide a clean complexity bound for SVGD in the population limit in terms of the
Stein Fisher Information (or squared Kernelized Stein Discrepancy), as a function of the dimension of the problem d and the desired accuracy ε.
Unlike existing work, we do not make any assumption on the trajectory of the algorithm. Instead, our key assumption is that the target distribution
satisfies Talagrand's inequality T1.
</i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>June 6, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2106.03056">"MURANA: A Generic Framework for Stochastic Variance-Reduced Optimization"</a> -
 joint work with
 <a href="https://lcondat.github.io">Laurent Condat</a>.<br>
 <br>

 Abstract: <i>
We propose a generic variance-reduced algorithm, which we call MUltiple RANdomized Algorithm (MURANA), for minimizing a sum of several smooth functions plus a regularizer, in a sequential or distributed manner. Our method is formulated with general stochastic operators, which allow us to model various strategies for reducing the computational complexity. For example, MURANA supports sparse activation of the gradients, and also reduction of the communication load via compression of the update vectors. This versatility allows MURANA to cover many existing randomization mechanisms within a unified framework. However, MURANA also encodes new methods as special cases. We highlight one of them, which we call ELVIRA, and show that it improves upon Loopless SVRG.
 </i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>June 5, 2021</h3>
 <h1>New Paper</h1>
 <br>

 <span class="important">New paper out: </span>
 <a href="https://arxiv.org/abs/2106.02969">"FedNL: Making Newton-Type Methods Applicable to Federated Learning"</a> -
 joint work with
 <a href="https://mher-safaryan.github.io">Mher Safaryan</a>,
 <a href="https://rustem-islamov.github.io">Rustem Islamov</a> and <a href="https://qianxunk.github.io">Xun Qian</a>.<br>
 <br>

 Abstract: <i>
  Inspired by recent work of Islamov et al (2021), we propose a family of Federated Newton Learn (FedNL) methods, which we believe is a marked step in the direction of making second-order methods applicable to FL. In contrast to the aforementioned work, FedNL employs a different Hessian learning technique which i) enhances privacy as it does not rely on the training data to be revealed to the coordinating server, ii) makes it applicable beyond generalized linear models, and iii) provably works with general contractive compression operators for compressing the local Hessians, such as Top-K or Rank-R, which are vastly superior in practice. Notably, we do not need to rely on error feedback for our methods to work with contractive compressors. Moreover, we develop FedNL-PP, FedNL-CR and FedNL-LS, which are variants of FedNL that support partial participation, and globalization via cubic regularization and line search, respectively, and FedNL-BC, which is a variant that can further benefit from bidirectional compression of gradients and models, i.e., smart uplink gradient and smart downlink model compression. We prove local convergence rates that are independent of the condition number, the number of training data points, and compression variance. Our communication efficient Hessian learning technique provably learns the Hessian at the optimum. Finally, we perform a variety of numerical experiments that show that our FedNL methods have state-of-the-art communication complexity when compared to key baselines.
 </i>
 <br>

 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>June 5, 2021</h3>
 <h1>Finally, the NeurIPS Month of Deadlines is Over</h1>

 <br>
 I've been silent here for a while due a stream of NeurIPS deadlines (abstract, paper, supplementary material). Me and my fantastic <a href="https://richtarik.org/i_team.html">team</a> can rest a bit now! <br>


 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



 <h3>May 10, 2021</h3>
 <h1>Papers Accepted to ICML 2021</h1>

 We've had several papers accepted to the <a href="https://icml.cc/Conferences/2021">International Conference on Machine Learning (ICML 2021)</a>, which will be run virtually during July 18-24, 2021.
 Here they are:<br><br>


 <b> 1) "MARINA: Faster Non-convex Distributed Learning with Compression"</b> <a href="https://arxiv.org/abs/2102.07845">[arXiv]</a> [<a href="http://proceedings.mlr.press/v139/gorbunov21a.html">ICML</a>]  - joint work with
 <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>,
 <a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a> and <a href="https://zhizeli.github.io">Zhize Li</a>.<br>
 <br>

 Abstract: <i>We develop and analyze MARINA: a new communication efficient method for non-convex distributed learning over heterogeneous datasets. MARINA employs a novel communication compression strategy based on the compression of gradient differences which is reminiscent of but different from the strategy employed in the DIANA method of Mishchenko et al (2019). Unlike virtually all competing distributed first-order methods, including DIANA, ours is based on a carefully designed biased gradient estimator, which is the key to its superior theoretical and practical performance. To the best of our knowledge, the communication complexity bounds we prove for MARINA are strictly superior to those of all previous first order methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and PP-MARINA. The first method is designed for the case when the local loss functions owned by clients are either of a finite sum or of an expectation form, and the second method allows for partial participation of clients -- a feature important in federated learning. All our methods are superior to previous state-of-the-art methods in terms of the oracle/communication complexity. Finally, we provide convergence analysis of all methods for problems satisfying the Polyak-Lojasiewicz condition.
 </i>

 <br>
 <br>

 More material:
 <ul>
   <li><a href="https://www.youtube.com/watch?v=o5MwC4DYbGE&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=34">Short 5 min YouTube talk by Konstantin</a></li>
   <li><a href="https://www.youtube.com/watch?v=bj0E94Siq74">Long 70 min YouTube talk by Eduard delivered at the FLOW seminar</a> </li>
   <li><a href="">poster</a></li>
 </ul>
 <br>



 <b> 2) "PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization"</b> <a href="https://arxiv.org/abs/2008.10898">[arXiv]</a> <a href="http://proceedings.mlr.press/v139/li21a.html">[ICML]</a>  - joint work with
 <a href="https://zhizeli.github.io">Zhize Li</a>, <a href="https://cemse.kaust.edu.sa/people/person/hongyan-bao">Hongyan Bao</a>, and <a href="https://cemse.kaust.edu.sa/people/person/xiangliang-zhang">Xiangliang Zhang</a>.


 <br>
 <br>

 Abstract: <i>In this paper, we propose a novel stochastic gradient estimator---ProbAbilistic Gradient Estimator (PAGE)---for nonconvex optimization. PAGE is easy to implement as it is designed via a small adjustment to vanilla SGD:
   in each iteration, PAGE uses the vanilla minibatch SGD update with probability p or reuses the previous gradient with a small adjustment, at a much lower computational cost, with probability 1−p. We give a simple formula for the
   optimal choice of p. We prove tight lower bounds for nonconvex problems, which are of independent interest. Moreover, we prove matching upper bounds both in the finite-sum and online regimes, which establish that PAGE is an optimal
   method. Besides, we show that for nonconvex functions satisfying the Polyak-Łojasiewicz (PL) condition, PAGE can automatically switch to a faster linear convergence rate. Finally, we conduct several deep learning experiments (e.g.,
   LeNet, VGG, ResNet) on real datasets in PyTorch, and the results demonstrate that PAGE not only converges much faster than SGD in training but also achieves the higher test accuracy, validating our theoretical results and confirming
   the practical superiority of PAGE. </i>
   <br>
 <br>

 More material:
 <ul>
   <li><a href="https://www.youtube.com/watch?v=_K3XPxN-vdk&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=29">Short 5 min YouTube talk by Zhize</a></li>
 </ul>


 <br>


 <b> 3) "Distributed Second Order Methods with Fast Rates and Compressed Communication" </b> <a href="https://arxiv.org/abs/2102.07158">[arXiv]</a> <a href="http://proceedings.mlr.press/v139/islamov21a.html">[ICML]</a> - joint work with  <a
 href="https://rustem-islamov.github.io">Rustem Islamov</a> and <a href="https://qianxunk.github.io">Xun Qian</a>.<br>
 <br>

 Abstract: <i>
 We develop several new communication-efficient second-order methods for distributed optimization. Our first method, NEWTON-STAR, is a variant of Newton's method from which it inherits its fast local quadratic rate. However, unlike Newton's method, NEWTON-STAR enjoys the same per iteration communication cost as gradient descent. While this method is impractical as it relies on the use of certain unknown parameters characterizing the Hessian of the objective function at the optimum, it serves as the starting point which enables us design practical variants thereof with strong theoretical guarantees. In particular, we design a stochastic sparsification strategy for learning the unknown parameters in an iterative fashion in a communication efficient manner. Applying this strategy to NEWTON-STAR leads to our next method, NEWTON-LEARN, for which we prove local linear and superlinear rates independent of the condition number. When applicable, this method can have dramatically superior convergence behavior when compared to state-of-the-art methods. Finally, we develop a globalization strategy using cubic regularization which leads to our next method, CUBIC-NEWTON-LEARN, for which we prove global sublinear and linear convergence rates, and a fast superlinear rate. Our results are supported with experimental results on real datasets, and show several orders of magnitude improvement on baseline and state-of-the-art methods in terms of communication complexity.
  </i> <br>
 <br>

 More material:
 <ul>
   <li><a href="https://www.youtube.com/watch?v=iSKBZXlaoWo&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=41">Short 5 min YouTube talk by Rustem</a></li>
   <li><a href="https://www.youtube.com/watch?v=jqgyjN5kLqo">Long 80 min YouTube talk by myself delivered at the FLOW seminar</a></li>
   <li><a href="https://richtarik.org/talks/TALK-2021-Newton-Learn.pdf">my FLOW talk slides</a></li>
   <li><a href="https://richtarik.org/posters/Poster-Newton-Learn.pdf">poster</a></li>
 </ul>
 <br>

 <b> 4) "Stochastic Sign Descent Methods: New Algorithms and Better Theory" </b>  <a href="https://arxiv.org/abs/1905.12938">[arXiv]</a> <a href="http://proceedings.mlr.press/v139/safaryan21a.html">[ICML]</a>  - joint work with <a href="https://mher-safaryan.github.io">Mher Safaryan</a>. <br>
 <br>

 Abstract: <i>
 Various gradient compression schemes have been proposed to mitigate the communication cost in distributed training of large scale machine learning models. Sign-based methods, such as signSGD, have recently been gaining popularity because of their simple compression rule and connection to adaptive gradient methods, like ADAM. In this paper, we analyze sign-based methods for non-convex optimization in three key settings: (i) standard single node, (ii) parallel with shared data and (iii) distributed with partitioned data. For single machine case, we generalize the previous analysis of signSGD relying on intuitive bounds on success probabilities and allowing even biased estimators. Furthermore, we extend the analysis to parallel setting within a parameter server framework, where exponentially fast noise reduction is guaranteed with respect to number of nodes, maintaining 1-bit compression in both directions and using small mini-batch sizes. Next, we identify a fundamental issue with signSGD to converge in distributed environment. To resolve this issue, we propose a new sign-based method, Stochastic Sign Descent with Momentum (SSDM), which converges under standard bounded variance assumption with the optimal asymptotic rate. We validate several aspects of our theoretical findings with numerical experiments.
 </i> <br>
 <br>

 More material:
 <ul>
   <li><a href="https://opt-ml.org/posters/2020/poster_14.png">poster</a></li>
 </ul>
 <br>


 <b> 5) "ADOM: Accelerated Decentralized Optimization Method for Time-Varying Networks"</b> <a href="https://arxiv.org/abs/2102.09234">[arXiv] </a> <a href="http://proceedings.mlr.press/v139/kovalev21a.html">[ICML] </a>
 - joint work with
 <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://shulgin-egor.github.io">Egor Shulgin</a>,
  <a
 href="https://scholar.google.com/citations?hl=ru&user=sEjyzkgAAAAJ">Alexander Rogozin</a> and <a href="https://www.hse.ru/en/org/persons/49503846">Alexander Gasnikov</a>.
 <br>
 <br>

 Abstract: <i>We propose ADOM - an accelerated method for smooth and strongly convex decentralized optimization over time-varying networks. ADOM uses a dual oracle, i.e., we assume access to the gradient of the Fenchel conjugate of the individual loss functions. Up to a constant factor, which depends on the network structure only, its communication complexity is the same as that of accelerated Nesterov gradient method (Nesterov, 2003). To the best of our knowledge, only the algorithm of Rogozin et al. (2019) has a convergence rate with similar properties. However, their algorithm converges under the very restrictive assumption that the number of network changes can not be greater than a tiny percentage of the number of iterations. This assumption is hard to satisfy in practice, as the network topology changes usually can not be controlled. In contrast, ADOM merely requires the network to stay connected throughout time.
 </i><br>

 More material:
 <ul>
   <li>  <a href="https://www.youtube.com/watch?v=jO3t4eZFdkc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=37">Short 5 min YouTube talk by Egor</a> </li>
   <li>  <a href="posters/Poster-ADOM-ICML-2021.pdf">poster</a> </li>
 </ul>



 <br>
 <br>
 <img alt="" src="imgs/fancy-line.png" width="196" height="36">
 <br>
 <br>



<h3>April 29, 2021</h3>
<h1>Paper Accepted to IEEE Transactions on Information Theory</h1>

<br>
Our paper <a href="https://arxiv.org/abs/1905.08645">Revisiting randomized gossip algorithms: general framework, convergence rates and novel block and accelerated protocols</a>, joint work with
<a href="https://nicolasloizou.github.io">Nicolas Loizou</a>, was accepted to IEEE Transactions on Information Theory.<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 28, 2021</h3>
<h1>KAUST Conference on Artificial Intelligence: 17 Short (up to 5 min) Talks by Members of my Team!</h1>
<br>

Today and tomorrow I am attending the <a href="https://cemse.kaust.edu.sa/ai/aii-conf-2021">KAUST Conference on Artificial Intelligence</a>. Anyone can attend for free by watching the <a href="https://kaust.zoom.us/j/96464686903">LIVE Zoom webinar stream.</a> Today I have given a short 20
  min talk today entitled "Recent Advances in Optimization for Machine Learning". Here are my slides:


  <br>

  <br>
  <a href="slides/Talk-1-KAUST-AI-Conference-2021.pdf"><img alt="" src="imgs/KAUST-AI-Conference-2021-Talk-1-Richtarik.png" width="700"></a>
  <br>


<br>


   I will deliver another 20 min talk tomorrow, entitled "On Solving a Key Challenge in Federated Learning: Local Steps, Compression
  and Personalization". Here are the slides:


    <br>

    <br>
    <a href="slides/Talk-2-KAUST-AI-Conference-2021.pdf"><img alt="" src="imgs/KAUST-AI-Conference-2021-Talk-2-Richtarik.png" width="700"></a>
    <br>


  <br>


<br>
<br>


  More importantly, 17 members (research scientists, postdocs, PhD students,  MS students and interns) of the "Optimization and Machine Learning Lab"  that I lead at KAUST have
  prepared short videos on selected recent papers they co-athored. This includes 9 papers from 2021, 7 papers from 2020 and 1 paper from 2019. Please check out their video talks!
  Here they are:

  <br>
  <br>


A talk by Konstantin Burlachenko (<a href="https://arxiv.org/abs/2102.07845">paper</a>):
<br>
<a href="https://www.youtube.com/watch?v=o5MwC4DYbGE&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=33"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Burlachenko.png" width="350"></a>
<br>

A talk by Laurent Condat (<a href="http://proceedings.mlr.press/v119/malinovskiy20a.html">paper</a>):
<br>
<a href="https://www.youtube.com/watch?v=-QgQp5HnWQY&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=27"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Condat.png" width="350"></a>
<br>

A talk by Eduard Gorbunov (<a href="https://papers.nips.cc/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html">paper</a>):
<br>
<a href="https://www.youtube.com/watch?v=6Hpt6hbzgjU&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=37"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Gorbunov.png" width="350"></a>
<br>

A talk by Filip Hanzely (<a href="http://proceedings.mlr.press/v130/gorbunov21a.html">paper</a>):
<br>
<a href="https://www.youtube.com/watch?v=u_KoimUuc6k&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Hanzely.png" width="350"></a>
<br>

A talk by Slavomir Hanzely:
<br>
<a href="https://www.youtube.com/watch?v=YkZeROHXahc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=34"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Slavomir-Hanzely.png" width="350"></a>
<br>

A talk by Samuel Horvath:
<br>
<a href="https://www.youtube.com/watch?v=M3zHT_qieB4&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=32"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Horvath.png" width="350"></a>
<br>

A talk by Rustem Islamov:
<br>
<a href="https://www.youtube.com/watch?v=iSKBZXlaoWo&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=40"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Islamov.png" width="350"></a>
<br>

A talk by Ahmed Khaled:
<br>
<a href="https://www.youtube.com/watch?v=aHAU6OYNoKA&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=39"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Khaled.png" width="350"></a>
<br>

A talk by Dmitry Kovalev:
<br>
<a href="https://www.youtube.com/watch?v=0bAgav0x-8U&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=31"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Kovalev.png" width="350"></a>
<br>

A talk by Zhize Li:
<br>
<a href="https://www.youtube.com/watch?v=_K3XPxN-vdk&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=28"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Li.png" width="350"></a>
<br>

A talk by Grigory Malinovsky:
<br>
<a href="https://www.youtube.com/watch?v=DK9CJmz6SR8&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=35"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Malinovsky.png" width="350"></a>
<br>

A talk by Konstantin Mishchenko:
<br>
<a href="https://www.youtube.com/watch?v=0ZZY5Y_6fd4&list=PLC28kDljnOrh4XxzWpOFBIu8IHjJzmKQs"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Mishchenko.png" width="350"></a>
<br>

A talk by Xun Qian:
<br>
<a href="https://www.youtube.com/watch?v=QMJDOtm9wxk&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=29"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Qian.png" width="350"></a>
<br>

A talk by Mher Safaryan:
<br>
<a href="https://www.youtube.com/watch?v=vSD-smU0JjE&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=45"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Safaryan.png" width="350"></a>
<br>

A talk by Adil Salim:
<br>
<a href="https://www.youtube.com/watch?v=jz_ylyAhkL8&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=30"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Salim.png" width="350"></a>
<br>

A talk by Egor Shulgin:
<br>
<a href="https://www.youtube.com/watch?v=jO3t4eZFdkc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=36"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Shulgin.png" width="350"></a>
<br>

A talk by Bokun Wang:
<br>
<a href="https://www.youtube.com/watch?v=-YjXDdwkeqc&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=38"><img alt="" src="imgs/KAUST-AI-Conference-2021-Spotlight-Talk-Wang.png" width="350"></a>
<br>




<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




          <h3>April 21, 2021</h3>
          <h1>Area Editor for Journal of Optimization Theory and Applications</h1>
          <br>

      I have just become an <a href="https://www.springer.com/journal/10957/editors">Area Editor</a> for <a href="https://www.springer.com/journal/10957">Journal on Optimization Theory and Applications (JOTA)</a>, representing the area
      "Optimization and Machine Learning". Consider sending your best optimizaiton for machine learning papers to JOTA! We aim to provide fast and high quality reviews. <br> <br>

      Established in 1967, JOTA is one of the oldest optimization journals. For example, Mathematical Programming was established in 1972, SIAM J on Control and Optimization in 1976, and SIAM J on Optimization in 1991.

        <br><br>

       According to <a href="https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=phy_mathematicaloptimization">Google Scholar Metrics</a>, JOTA is one of the top optimization journals: <br>

       <br>
       <a href="https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=phy_mathematicaloptimization"><img alt="" src="imgs/JOTA-GoogleScholarMetrics-2021-small.png" width="750"></a>
       <br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>April 22, 2021</h3>
          <h1>Talk at AMCS/STAT Graduate Seminar at KAUST</h1>
          <br>

        Today I gave a talk entitled "Distributed second order methods with fast rates and compressed communication" at
        the AMCS/STAT Graduate Seminar at KAUST. Here is the <a href="https://cemse.kaust.edu.sa/events/event/distributed-second-order-methods-fast-rates-and-compressed-communication">official KAUST blurb.</a> I talked about
        the paper <a href="https://arxiv.org/abs/2102.07158">Distributed Second Order Methods with Fast Rates and Compressed Communication</a>.
        This is joint work with my fantastic intern <a href="https://rustem-islamov.github.io">Rustem Islamov</a> (KAUST and MIPT) and fantastic
        postdoc <a href="https://qianxunk.github.io">Xun Qian</a> (KAUST). <br>



        <br>
        <br>
        <img alt="" src="imgs/fancy-line.png" width="196" height="36">
        <br>
        <br>



        <h3>April 19, 2021</h3>
        <h1>New Paper</h1>
        <br>

        <span class="important">New paper out: </span>
        <a href="https://arxiv.org/abs/2104.09342">"Random Reshuffling with Variance Reduction: New Analysis and Better Rates"</a> -
        joint work with
        <a href="https://grigory-malinovsky.github.io">Grigory Malinovsky</a> and
        <a href="https://cemse.kaust.edu.sa/people/person/alibek-sailanbayev">Alibek Sailanbayev</a>.<br>
        <br>

        Abstract: <i>
         Virtually all state-of-the-art methods for training supervised machine learning models are variants of SGD enhanced with a number of additional tricks, such as minibatching, momentum, and adaptive stepsizes. One of the tricks that works so well in practice that it is used as default in virtually all widely used machine learning software is {\em random reshuffling (RR)}. However, the practical benefits of RR have until very recently been eluding attempts at being satisfactorily explained using theory. Motivated by recent development due to Mishchenko, Khaled and Richt\'{a}rik (2020), in this work we provide the first analysis of SVRG under Random Reshuffling (RR-SVRG) for general finite-sum problems. First, we show that RR-SVRG converges linearly with the rate $O(\kappa^{3/2})$ in the strongly-convex case, and can be improved further to $O(\kappa)$ in the big data regime (when $n > O(\kappa)$), where $\kappa$ is the condition number. This improves upon the previous best rate $O(\kappa^2)$ known for a variance reduced RR method in the strongly-convex case due to Ying, Yuan and Sayed (2020). Second, we obtain the first sublinear rate for general convex problems. Third, we establish similar fast rates for Cyclic-SVRG and Shuffle-Once-SVRG. Finally, we develop and analyze a more general variance reduction scheme for RR, which allows for less frequent updates of the control variate. We corroborate our theoretical results with suitably chosen experiments on synthetic and real datasets.
        </i>
        <br>

        <br>
        <br>
        <img alt="" src="imgs/fancy-line.png" width="196" height="36">
        <br>
        <br>




          <h3>April 14, 2021</h3>
          <h1>Talk at FLOW</h1>
          <br>

        Today I am giving a talk entitled "Beyond Local and Gradient Methods for Federated Learning" at
        the <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">Federated Learning One World Seminar (FLOW).</a>
        After a brief motivation spent on bashing gradient and local methods, I will talk about
        the paper <a href="https://arxiv.org/abs/2102.07158">Distributed Second Order Methods with Fast Rates and Compressed Communication</a>.
        This is joint work with my fantastic intern <a href="https://rustem-islamov.github.io">Rustem Islamov</a> (KAUST and MIPT) and fantastic
        postdoc <a href="https://qianxunk.github.io">Xun Qian</a> (KAUST).



         <br><br>
         The talk was recorded and is now available on      YouTube:


         <br>
         <a href="https://www.youtube.com/watch?v=jqgyjN5kLqo"><img alt="" src="imgs/Newton-Learn-FLOW-small.png" width="750"></a>
         <br>




          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>April 13, 2021</h3>
          <h1>Three Papers Presented to AISTATS 2021</h1>
            <br>
We've had three papers accepted to <a href="https://aistats.org/aistats2021/">The 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)</a>. The conference will be held virtually over the next few days; during April 13-15, 2021. Here are the papers:<br><br>


1. <a href="http://proceedings.mlr.press/v130/kovalev21a.html">A linearly convergent algorithm for decentralized optimization: sending less bits for free!</a>, joint work with  <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://scholar.google.com/citations?user=ldJpvE8AAAAJ&hl=en">Anastasia Koloskova</a>, <a href="https://people.epfl.ch/martin.jaggi/?lang=en">Martin Jaggi</a>, and <a href="https://www.sstich.ch">Sebastian U. Stich</a>.<br><br>

<i>Abstract:</i>  Decentralized optimization methods enable on-device training of machine learning models without a central coordinator. In many scenarios communication between devices is energy demanding and time consuming and forms the bottleneck of the entire system.
We propose a new randomized first-order method which tackles the communication bottleneck by applying randomized compression operators to the communicated messages. By combining our scheme with a new variance reduction technique that progressively throughout the iterations reduces the adverse effect of the injected quantization noise, we obtain the first scheme that converges linearly on strongly convex decentralized problems while using compressed communication only.
We prove that our method can solve the problems without any increase in the number of communications compared to the baseline which does not perform any communication compression while still allowing for a significant compression factor which depends on the conditioning of the problem and the topology of the network. Our key theoretical findings are supported by numerical experiments. <br> <br>

<a href="posters/Poster-Decentralized-DIANA.pdf"> <img alt="" src="posters/Poster-Decentralized-DIANA-small.png" width="750" ></a>
<br> <br>

2. <a href="http://proceedings.mlr.press/v130/gorbunov21a.html">Local SGD: unified theory and new efficient methods</a>, joint work with  <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> and <a href="https://fhanzely.github.io">Filip Hanzely</a>.<br><br>

<i>Abstract:</i> We present a unified framework for analyzing local SGD methods in the convex and strongly convex regimes for distributed/federated training of supervised machine learning models. We recover several known methods as a special case of our general framework, including Local-SGD/FedAvg, SCAFFOLD, and several variants of SGD not originally designed for federated learning. Our framework covers both the identical and heterogeneous data settings, supports both random and deterministic number of local steps, and can work with a wide array of local stochastic gradient estimators, including shifted estimators which are able to adjust the fixed points of local iterations for faster convergence. As an application of our framework, we develop multiple novel FL optimizers which are superior to existing methods. In particular, we develop the first linearly converging local SGD method which does not require any data homogeneity or other strong assumptions. <br> <br>


<a href="posters/Poster-Local-SGD-AISTATS-2021.pdf"> <img alt="" src="posters/Poster-Local-SGD-AISTATS-2021-small.png" width="750" ></a>
<br> <br>





3. <a href="http://proceedings.mlr.press/v130/horvath21a.html">Hyperparameter transfer learning with adaptive complexity</a>, joint work with  <a href="https://samuelhorvath.github.io">Samuel Horváth</a> and <a href="https://aaronkl.github.io">Aaron Klein</a>, and <a href="http://www0.cs.ucl.ac.uk/staff/c.archambeau/">Cedric Archambeau</a>.<br><br>

<i>Abstract:</i>  Bayesian optimization (BO) is a data-efficient approach to automatically tune the hyperparameters of machine learning models. In practice, one frequently has to solve similar hyperparameter tuning problems sequentially. For example, one might have to tune a type of neural network learned across a series of different classification problems. Recent work on multi-task BO exploits knowledge gained from previous hyperparameter tuning tasks to speed up a new tuning task. However, previous approaches do not account for the fact that BO is a sequential decision making procedure. Hence, there is in general a mismatch between the number of evaluations collected in the current tuning task compared to the number of evaluations accumulated in all previously completed tasks. In this work, we enable multi-task BO to compensate for this mismatch, such that the transfer learning procedure is able to handle different data regimes in a principled way. We propose a new multi-task BO method that learns a set of ordered, non-linear basis functions of increasing complexity via nested drop-out and automatic relevance determination. Experiments on a variety of hyperparameter tuning problems show that our method improves the sample efficiency of recently published multi-task BO methods.
<br>

<a href="posters/Poster-ABRAC-AISTATS-2021.pdf"> <img alt="" src="posters/Poster-ABRAC-AISTATS-2021-small.png" width="750" ></a>



          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>





          <h3>April 7, 2021</h3>
          <h1>Talk at All Russian Seminar in Optimization</h1>
          <br>

        Today I am giving a talk at the <a href="http://www.mathnet.ru/php/conference.phtml?&eventID=31&confid=1794&option_lang=eng">All Russian Seminar in Optimization.</a> I am talking about the paper <a href="https://arxiv.org/abs/2102.07158">Distributed Second Order Methods with Fast Rates and Compressed Communication</a>, which is joint work with
        <a href="https://rustem-islamov.github.io">Rustem Islamov (KAUST and MIPT)</a> and <a href="https://qianxunk.github.io">Xun Qian (KAUST)</a>.


         <br><br>
         The talk was recorded and uploaded to  YouTube:

         <br>
         <a href="https://www.youtube.com/watch?v=_1q-vt0nu44"><img alt="" src="imgs/Newton-Learn-All-Russian-seminar-small.png" width="750"></a>
         <br>



        <br>

        Here are the <a href="talks/TALK-2021-Newton-Learn.pdf">slides from my talk</a>, and here is a
          <a href="posters/Poster-Newton-Learn.pdf"> poster </a> that will son be presented by Rustem Islamov at the <a href="https://sites.google.com/ucsd.edu/cedo/">NSF-TRIPODS Workshop on Communication Efficient Distributed Optimization.</a>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>March 24, 2021</h3>
          <h1>Mishchenko and Gorbunov: ICLR 2021 Outstanding Reviewer Award</h1>
          <br>

          Congratulations <a href="https://konstmish.github.io/">Konstantin Mishchenko</a> and
          <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> for receiving an Outstanding
          Reviewer Award from ICLR 2021! I wish the reviews we get for our papers were as good (i.e., insighful,
          expert and thorough) as the reviews Konstantin and Eduard are writing.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>March 19, 2021</h3>
          <h1>Area Chair for NeurIPS 2021</h1>
          <br>



          I will serve as an Area Chair for NeurIPS 2021, to be held during December 6-14, 2021 virtually ( = same location as last year ;-).


          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>





          <h3>March 1, 2021</h3>
          <h1>New PhD Student: Lukang Sun</h1>
          <br>


        Lukang Sun has joined my group as a PhD student. Welcome!!!

        Lukang has an MPhil degree in mathematics form Nanjing University, China (2020), and a BA in mathematics from Jilin University, China (2017). His thesis (written in Chinese) was on the topic of
        "Harmonic functions on metric measure spaces". In this work, Lukang proposed some novel methods using optimal transport theory to generalize some results from Riemannian manifolds to metric measure
        spaces. Lukang has held visiting/exchange/temporary positions at the Hong Kong University of Science and Technology, Georgia Institute of Technology, and the Chinese University of Hong Kong.


          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>February 22, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.11079">"An Optimal Algorithm for Strongly Convex Minimization under Affine Constraints"</a> -
          joint work with
          <a href="https://adil-salim.github.io">Adil Salim</a>,
          <a href="https://lcondat.github.io">Laurent Condat</a> and
          <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>.<br>
          <br>

          Abstract: <i>
           Optimization problems under affine constraints appear in various areas of machine learning. We consider the task of
           minimizing a smooth strongly convex function $F(x)$ under the affine constraint $Kx=b$, with an oracle providing evaluations
           of the gradient of $F$ and matrix-vector multiplications by $K$ and its transpose. We provide lower bounds on the number of
           gradient computations and matrix-vector multiplications to achieve a given accuracy. Then we propose an accelerated
           primal--dual algorithm achieving these lower bounds. Our algorithm is the first optimal algorithm for this class of
           problems.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 19, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.09700">"AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods"</a> -
          joint work with
          <a href="https://coral.ise.lehigh.edu/zhs310/">Zheng Shi</a>,
          <a href="https://nicolasloizou.github.io">Nicolas Loizou</a> and
          <a href="https://engineering.lehigh.edu/faculty/martin-takac">Martin Takáč</a>.<br>
          <br>

          Abstract: <i>
           We present an adaptive stochastic variance reduced method with an implicit approach for adaptivity.
           As a variant of SARAH, our method employs the stochastic recursive gradient yet adjusts step-size
           based on local geometry. We provide convergence guarantees for finite-sum minimization problems
           and show a faster convergence than SARAH can be achieved if local geometry permits. Furthermore,
           we propose a practical, fully adaptive variant, which does not require any knowledge of local geometry
           and any effort of tuning the hyper-parameters. This algorithm implicitly computes step-size and
           efficiently estimates local Lipschitz smoothness of stochastic functions. The numerical experiments
           demonstrate the algorithm's strong performance compared to its classical counterparts and other
           state-of-the-art first-order methods.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>February 18, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.09234">"ADOM: Accelerated Decentralized Optimization Method for Time-Varying Networks"</a> -
          joint work with <a href="http://dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://shulgin-egor.github.io">Egor Shulgin</a>, <a href="https://scholar.google.com/citations?hl=ru&user=sEjyzkgAAAAJ">Alexander Rogozin</a>, and
          <a href="https://mipt.ru/education/chairs/dm/staff/gasnikov-aleksandr-vladimirovich.php">Alexander Gasnikov</a>.<br>
          <br>

          Abstract: <i>
          We propose ADOM - an accelerated method for smooth and strongly convex decentralized optimization over time-varying networks.
          ADOM uses a dual oracle, i.e., we assume access to the gradient of the Fenchel conjugate of the individual loss functions.
          Up to a constant factor, which depends on the network structure only, its communication complexity is the same as that of
          accelerated Nesterov gradient method (Nesterov, 2003). To the best of our knowledge, only the algorithm of Rogozin et al. (2019)
          has a convergence rate with similar properties. However, their algorithm converges under the very restrictive assumption that
          the number of network changes can not be greater than a tiny percentage of the number of iterations. This assumption is hard
          to satisfy in practice, as the network topology changes usually can not be controlled. In contrast, ADOM merely requires the
          network to stay connected throughout time.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 16, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.08374">"IntSGD: Floatless Compression of Stochastic Gradients"</a> -
          joint work with <a href="http://konstmish.github.io">Konstantin Mishchenko</a>, and
          <a href="https://scholar.google.com/citations?user=H9GqvAYAAAAJ&hl=en">Bokun Wang</a> and <a href="http://dmitry-kovalev.com">Dmitry Kovalev</a>.<br>
          <br>

          Abstract: <i>
          We propose a family of lossy integer compressions for Stochastic Gradient Descent (SGD) that do not communicate a single float. This is achieved by multiplying floating-point vectors with a number known to every device and then rounding to an integer number. Our theory shows that the iteration complexity of SGD does not change up to constant factors when the vectors are scaled properly. Moreover, this holds for both convex and non-convex functions, with and without overparameterization. In contrast to other compression-based algorithms, ours preserves the convergence rate of SGD even on non-smooth problems. Finally, we show that when the data is significantly heterogeneous, it may become increasingly hard to keep the integers bounded and propose an alternative algorithm, IntDIANA, to solve this type of problems.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>





          <h3>February 16, 2021</h3>
          <h1>Talk at MBZUAI</h1>
          <br>

          Today I gave a research seminar talk at <a href="https://mbzuai.ac.ae">MBZUAI.</a> I spoke about randomized second order methods.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>





          <h3>February 15, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.07845">"MARINA: Faster Non-Convex Distributed Learning with Compression"</a> -
          joint work with <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>, and
          <a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a> and <a href="https://zhizeli.github.io">Zhize Li</a>.<br>
          <br>

          Abstract: <i>
          We develop and analyze MARINA: a new communication efficient method for non-convex distributed learning over heterogeneous datasets. MARINA employs a novel communication compression strategy based on the compression of gradient differences which is reminiscent of but different from the strategy employed in the DIANA method of Mishchenko et al (2019). Unlike virtually all competing distributed first-order methods, including DIANA, ours is based on a carefully designed biased gradient estimator, which is the key to its superior theoretical and practical performance. To the best of our knowledge, the communication complexity bounds we prove for MARINA are strictly superior to those of all previous first order methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and PP-MARINA. The first method is designed for the case when the local loss functions owned by clients are either of a finite sum or of an expectation form, and the second method allows for partial participation of clients -- a feature important in federated learning. All our methods are superior to previous state-of-the-art methods in terms of the oracle/communication complexity. Finally, we provide convergence analysis of all methods for problems satisfying the Polyak-Lojasiewicz condition.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 14, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.07245">"Smoothness Matrices Beat Smoothness Constants: Better Communication Compression Techniques for Distributed Optimization"</a> -
          joint work with <a href="https://mher-safaryan.github.io">Mher Safaryan</a>, and
          <a href="https://www.ttic.edu/faculty/hanzely/">Filip Hanzely</a>.<br>
          <br>

          Abstract: <i>
           Large scale distributed optimization has become the default tool for the training of supervised machine
           learning models with a large number of parameters and training data. Recent advancements in the field
           provide several mechanisms for speeding up the training, including compressed communication, variance
           reduction and acceleration. However, none of these methods is capable of exploiting the inherently rich data-dependent smoothness
           structure of the local losses beyond standard smoothness constants. In this paper, we argue that when
           training supervised models, smoothness matrices -- information-rich generalizations of the ubiquitous
           smoothness constants -- can and should be exploited for further dramatic gains, both in theory and practice.
           In order to further alleviate the communication burden inherent in distributed optimization, we propose a
           novel communication sparsification strategy that can take full advantage of the smoothness matrices
           associated with local losses. To showcase the power of this tool, we describe how our sparsification
           technique can be adapted to three distributed optimization algorithms -- DCGD, DIANA and ADIANA -- yielding
           significant savings in terms of communication complexity. The new methods always outperform the baselines,
           often dramatically so.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 13, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.07158">"Distributed Second Order Methods with Fast Rates and Compressed Communication"</a> -
          joint work with <a href="https://rustem-islamov.github.io">Rustem Islamov</a>, and
          <a href="https://qianxunk.github.io">Xun Qian</a>.<br>
          <br>

          Abstract: <i>
           We develop several new communication-efficient second-order methods for distributed optimization. Our first method, NEWTON-STAR, is a variant of Newton's method from which it inherits its fast local quadratic rate. However, unlike Newton's method, NEWTON-STAR enjoys the same per iteration communication cost as gradient descent. While this method is impractical as it relies on the use of certain unknown parameters characterizing the Hessian of the objective function at the optimum, it serves as the starting point which enables us design practical variants thereof with strong theoretical guarantees. In particular, we design a stochastic sparsification strategy for learning the unknown parameters in an iterative fashion in a communication efficient manner. Applying this strategy to NEWTON-STAR leads to our next method, NEWTON-LEARN, for which we prove local linear and superlinear rates independent of the condition number. When applicable, this method can have dramatically superior convergence behavior when compared to state-of-the-art methods. Finally, we develop a globalization strategy using cubic regularization which leads to our next method, CUBIC-NEWTON-LEARN, for which we prove global sublinear and linear convergence rates, and a fast superlinear rate. Our results are supported with experimental results on real datasets, and show several orders of magnitude improvement on baseline and state-of-the-art methods in terms of communication complexity.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>February 12, 2021</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://arxiv.org/abs/2102.06704">"Proximal and Federated Random Reshuffling"</a> -
          joint work with <a href="https://konstmish.github.io">Konstantin Mishchenko</a>, and
          <a href="https://rka97.github.io">Ahmed Khaled</a>.<br>
          <br>

          Abstract: <i>
            Random Reshuffling (RR), also known as Stochastic Gradient Descent (SGD) without replacement, is a popular and
            theoretically grounded method for finite-sum minimization. We propose two new algorithms: Proximal and Federated
            Random Reshuffing (ProxRR and FedRR). The first algorithm, ProxRR, solves composite convex finite-sum minimization
            problems in which the objective is the sum of a (potentially non-smooth) convex regularizer and an average of n smooth
            objectives. We obtain the second algorithm, FedRR, as a special case of ProxRR applied to a reformulation of distributed
            problems with either homogeneous or heterogeneous data. We study the algorithms' convergence properties with constant
            and decreasing stepsizes, and show that they have considerable advantages over Proximal and Local SGD. In particular,
            our methods have superior complexities and ProxRR evaluates the proximal operator once per epoch only. When the proximal
            operator is expensive to compute, this small difference makes ProxRR up to n times faster than algorithms that evaluate
            the proximal operator in every iteration. We give examples of practical optimization tasks where the proximal operator
            is difficult to compute and ProxRR has a clear advantage. Finally, we corroborate our results with experiments on real
            data sets.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>February 10, 2021</h3>
          <h1>Best Paper Award @ NeurIPS SipcyFL 2020</h1>
  <br>
    Super happy about this surprise prize; and huge congratulations to my outstanding student and collaborator <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.
    The paper was recently accepted to <a href="https://openreview.net/forum?id=vYVI1CHPaQg">ICLR 2021</a>, check it out!

<br>
<br>
<img alt="" src="imgs/NeurIPS-2020-SpicyFL-Prize.jpeg" width="600" >
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>January 24, 2021</h3>
          <h1>Spring 2021 Semester Starts at KAUST</h1>
  <br>
As of today, the Spring semester starts at KAUST. The timing of this every year conflicts with the endgame before the ICML submission deadline, and this year is no different. Except for Covid-19.
I am teaching <a href="https://piazza.com/kaust.edu.sa/spring2021/cs332">CS 332: Federated Learning</a> on Sundays and Tuesdays. The first class is today.
  <br>



          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>January 23, 2021</h3>
          <h1>Three Papers Accepted to AISTATS 2021</h1>
            <br>
We've had some papers accepted to <a href="https://aistats.org/aistats2021/">The 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)</a>. The conference will be held virtually during April 13-15, 2021. Here are the papers:<br><br>


1. <a href="http://proceedings.mlr.press/v130/kovalev21a.html">A linearly convergent algorithm for decentralized optimization: sending less bits for free!</a>, joint work with  <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://scholar.google.com/citations?user=ldJpvE8AAAAJ&hl=en">Anastasia Koloskova</a>, <a href="https://people.epfl.ch/martin.jaggi/?lang=en">Martin Jaggi</a>, and <a href="https://www.sstich.ch">Sebastian U. Stich</a>.<br><br>

<i>Abstract:</i>  Decentralized optimization methods enable on-device training of machine learning models without a central coordinator. In many scenarios communication between devices is energy demanding and time consuming and forms the bottleneck of the entire system.
We propose a new randomized first-order method which tackles the communication bottleneck by applying randomized compression operators to the communicated messages. By combining our scheme with a new variance reduction technique that progressively throughout the iterations reduces the adverse effect of the injected quantization noise, we obtain the first scheme that converges linearly on strongly convex decentralized problems while using compressed communication only.
We prove that our method can solve the problems without any increase in the number of communications compared to the baseline which does not perform any communication compression while still allowing for a significant compression factor which depends on the conditioning of the problem and the topology of the network. Our key theoretical findings are supported by numerical experiments. <br> <br>


2. <a href="http://proceedings.mlr.press/v130/gorbunov21a.html">Local SGD: unified theory and new efficient methods</a>, joint work with  <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> and <a href="https://fhanzely.github.io">Filip Hanzely</a>.<br><br>

<i>Abstract:</i> We present a unified framework for analyzing local SGD methods in the convex and strongly convex regimes for distributed/federated training of supervised machine learning models. We recover several known methods as a special case of our general framework, including Local-SGD/FedAvg, SCAFFOLD, and several variants of SGD not originally designed for federated learning. Our framework covers both the identical and heterogeneous data settings, supports both random and deterministic number of local steps, and can work with a wide array of local stochastic gradient estimators, including shifted estimators which are able to adjust the fixed points of local iterations for faster convergence. As an application of our framework, we develop multiple novel FL optimizers which are superior to existing methods. In particular, we develop the first linearly converging local SGD method which does not require any data homogeneity or other strong assumptions. <br> <br>

3. <a href="http://proceedings.mlr.press/v130/horvath21a.html">Hyperparameter transfer learning with adaptive complexity</a>, joint work with  <a href="https://samuelhorvath.github.io">Samuel Horváth</a> and <a href="https://aaronkl.github.io">Aaron Klein</a>, and <a href="http://www0.cs.ucl.ac.uk/staff/c.archambeau/">Cedric Archambeau</a>.<br><br>

<i>Abstract:</i>  Bayesian optimization (BO) is a data-efficient approach to automatically tune the hyperparameters of machine learning models. In practice, one frequently has to solve similar hyperparameter tuning problems sequentially. For example, one might have to tune a type of neural network learned across a series of different classification problems. Recent work on multi-task BO exploits knowledge gained from previous hyperparameter tuning tasks to speed up a new tuning task. However, previous approaches do not account for the fact that BO is a sequential decision making procedure. Hence, there is in general a mismatch between the number of evaluations collected in the current tuning task compared to the number of evaluations accumulated in all previously completed tasks. In this work, we enable multi-task BO to compensate for this mismatch, such that the transfer learning procedure is able to handle different data regimes in a principled way. We propose a new multi-task BO method that learns a set of ordered, non-linear basis functions of increasing complexity via nested drop-out and automatic relevance determination. Experiments on a variety of hyperparameter tuning problems show that our method improves the sample efficiency of recently published multi-task BO methods.
<br>


          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>January 22, 2021</h3>
          <h1>Paper Accepted to Information and Inference: A Journal of the IMA</h1>
            <br>
          Our paper "Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor”, joint work with  <a href="https://mher-safaryan.github.io">Mher Safaryan</a> and <a href="https://shulgin-egor.github.io">Egor Shulgin</a>,  was accepted to
      <a href="https://academic.oup.com/imaiai">Information and Inference: A Journal of the IMA</a>.<br><br>

    <i>Abstract:</i> In order to mitigate the high communication cost in distributed and federated learning,
    various vector compression schemes, such as quantization, sparsification and dithering, have become
    very popular. In designing a compression method, one aims to communicate as few bits
    as possible, which minimizes the cost per communication round, while at the same time attempting to
    impart as little distortion (variance) to the communicated messages as possible, which minimizes
    the adverse effect of the compression on the overall number of communication rounds. However,
     intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more
     distorted the messages become. We formalize this intuition and prove an  uncertainty principle for
     randomized compression operators, thus quantifying this limitation mathematically, and
     effectively providing asymptotically tight lower bounds on what might be achievable with communication
     compression. Motivated by these developments, we call for the search for the optimal compression
      operator. In an attempt to take a first step in this direction, we consider an unbiased compression
       method inspired by the Kashin representation of vectors, which we call Kashin compression (KC).
       In contrast to all previously proposed compression mechanisms, KC enjoys a dimension independent
       variance bound for which we derive an explicit formula even in the regime when only a few bits
       need to be communicate per each vector entry.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



          <h3>January 12, 2021</h3>
          <h1>Paper Accepted to ICLR 2021</h1>
  <br>
    Our paper "A Better Alternative to Error Feedback for Communication-efficient Distributed Learning'', joint work with  <a href="https://samuelhorvath.github.io">Samuel Horváth</a>,  was accepted to
      <a href="https://openreview.net/forum?id=vYVI1CHPaQg">The 9th International Conference on Learning Representations (ICLR 2021)</a>.<br><br>

    <i>Abstract:</i>  Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed compute systems. A key bottleneck of such systems is the communication overhead for exchanging information across the workers, such as stochastic gradients. Among the many techniques proposed to remedy this issue, one of the most successful is the framework of compressed communication with error feedback (EF).
    EF remains the only known technique that can deal with the error induced by contractive compressors which are not unbiased, such as Top-K.
    In this paper, we propose a new and theoretically and practically better alternative to EF for dealing with contractive compressors. In particular, we propose a construction which can transform any contractive compressor into an induced unbiased compressor. Following this transformation, existing methods able to work with unbiased compressors can be applied. We show that our approach leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. We further extend our results to federated learning with partial participation following an arbitrary
    distribution over the nodes, and demonstrate the benefits thereof. We perform several numerical experiments which validate our theoretical
    findings.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




<h3>January 11, 2021</h3>
<h1>Call for Al-Khwarizmi Doctoral Fellowships (apply by Jan 22, 2021)</h1>
<br>

If you are from Europe and want to apply for a PhD position in my Optimization and Machine Learning group at KAUST, you may wish to apply for the
<a href="https://cemse.kaust.edu.sa/alkhwarizmi">European Science Foundation Al-Khwarizmi Doctoral Fellowship</a>.

<a href="https://cemse.kaust.edu.sa/alkhwarizmi"> <img alt="" src="imgs/al-khwarizmi.png" width="200"></a>

<br>
 Here is the official blurb:
<br>
<br>
"The Al-Khwarizmi Graduate Fellowship scheme invites applications for doctoral fellowships, with the submission deadline of 22 January 2021, 17:00 CET.
The King Abdullah University of Science and Technology (KAUST) in the Kingdom of Saudi Arabia with support from the European Science Foundation (ESF)
launches a competitive doctoral fellowship scheme to welcome students from the European continent for a research journey to a top international university
in the Middle East. The applications will be evaluated via an independent peer-review process managed by the ESF. The selected applicants will be offered
generous stipends and free tuition for Ph.D. studies within one of KAUST academic programs. Strong applicants who were not awarded a Fellowship but passed
KAUST admission requirements will be offered the possibility to join the University as regular Ph.D. students with the standard benefits that include the
usual stipends and free tuition."
<br>
<br>
- Submission deadline = 22 January 2021 @ 17:00 CET <br>
- Duration of the Fellowship = 3 years (extensions may be considered in duly justified cases)<br>
- Annual living allowance/stipend = USD 38,000  (net)<br>
- Approx USD 50,000 annual benefits = free tuition, free student housing on campus, relocation support, and medical and dental coverage<br>
- Each Fellowship includes a supplementary grant of USD 6,000 at the Fellow’s disposal for research-related expenses such as conference attendance<br>
- The applications must be submitted in two steps, with the formal documents and transcripts to be submitted to KAUST Admissions in Step 1, and
the research proposal to be submitted to the ESF in Step 2. Both steps should be completed in parallel before the call deadline.<br>

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




          <h3>December 15, 2020</h3>
          <h1>Vacation</h1>
          <br>
I am on vacation until early January, 2021.
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>December 12, 2020</h3>
          <h1>Paper Accepted to NSDI 2021</h1>
          <br>
Our paper ``Scaling Distributed Machine Learning with In-Network Aggregation'', joint work with  Amedeo Sapio, Marco Canini, Chen-Yu Ho,
Jacob Nelson, Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, and Dan R. K. Ports,  was accepted to
<a href="https://www.usenix.org/conference/nsdi21?ref=infosec-conferences.com">The 18th USENIX Symposium on Networked Systems Design and Implementation
(NSDI '21 Fall)</a>. <br><br>

<i>Abstract:</i>  Training machine learning models in parallel is an increasingly important workload. We accelerate distributed parallel training by designing a communication primitive that uses a programmable switch dataplane to execute a key step of the training process. Our approach, SwitchML, reduces the volume of exchanged data by aggregating the model updates from multiple workers in the network. We co-design the switch processing with the end-host protocols and ML frameworks to provide an efficient solution that speeds up training by up to 5.5× for a number of real-world benchmark models.


          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>December 8, 2020</h3>
          <h1>Fall 2020 Semester at KAUST is Over</h1>
          <br>


          The Fall 2020 semester at KAUST is now over; I've had  alot of fun teaching my CS 331 class (Stochastic Gradient Descent Methods). At the very end I run into some
          LaTeX issues after upgrading to Big Sur on Mac - should not have done that...
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>December 6, 2020</h3>
          <h1>NeurIPS 2020 Started</h1>
          <br>


          Me and the members of my group will be attending <a href="https://neurips.cc/virtual/2020/public/index.html">NeurIPS 2020</a> - the event is starting today.
          Marco Cuturi and me will co-chair the <a href="https://neurips.cc/virtual/2020/public/session_oral_21084.html">Optimization session (Track 21) on Wednesday</a>. I am particularly looking forward to the workshops: <a href="https://opt-ml.org">OPT2020</a>, <a href="https://ppml-workshop.github.io">PPML</a> and <a href="http://128.1.38.43/SpicyFL/2020/">SpicyFL.</a>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>




          <h3>November 24, 2020</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://opt-ml.org/papers/2020/paper_61.pdf">"Error Compensated Loopless SVRG for Distributed Optimization"</a> -
          joint work with <a href="https://qianxunk.github.io/">Xun Qian</a>, <a href="https://www.researchgate.net/profile/Hanze_Dong">Hanze Dong</a>, and
          <a href="http://tongzhang-ml.org/">Tong Zhang</a>.<br>
          <br>

          Abstract: <i>
            A key bottleneck in distributed training of large scale machine learning models is the overhead related
            to communication of gradients. In order to reduce the communicated cost, gradient compression
            (e.g., sparsification and quantization) and error compensation techniques are often used. In this
            paper, we propose and study a new efficient method in this space: error compensated loopless SVRG
            method (L-SVRG). Our method is capable of working with any contraction compressor (e.g., TopK
            compressor), and we perform analysis for strongly convex optimization problems in the composite
            case and smooth case. We prove linear convergence rates for both cases and show that in the smooth
            case the rate has a better dependence on the contraction factor associated with the compressor.
            Further, we show that in the smooth case, and under some certain conditions, error compensated
            L-SVRG has the same convergence rate as the vanilla L-SVRG method. Numerical experiments are
            presented to illustrate the efficiency of our method.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>


          <h3>November 24, 2020</h3>
          <h1>New Paper</h1>
          <br>

          <span class="important">New paper out: </span>
          <a href="https://opt-ml.org/papers/2020/paper_59.pdf">"Error Compensated Proximal SGD and RDA"</a> -
          joint work with <a href="https://qianxunk.github.io/">Xun Qian</a>, <a href="https://www.researchgate.net/profile/Hanze_Dong">Hanze Dong</a>, and
          <a href="http://tongzhang-ml.org/">Tong Zhang</a>.<br>
          <br>

          Abstract: <i>
            Communication cost is a key bottleneck in distributed training of large machine learning models. In
            order to reduce the amount of communicated data, quantization and error compensation techniques
            have recently been studied. While the error compensated stochastic gradient descent (SGD) with
            contraction compressor (e.g., TopK) was proved to have the same convergence rate as vanilla SGD in
            the smooth case, it is unknown in the regularized case. In this paper, we study the error compensated
            proximal SGD and error compensated regularized dual averaging (RDA) with contraction compressor
            for the composite finite-sum optimization problem. Unlike the smooth case, the leading term in the
            convergence rate of error compensated proximal SGD is dependent on the contraction compressor
            parameter in the composite case, and the dependency can be improved by introducing a reference
            point to reduce the compression noise. For error compensated RDA, we can obtain better dependency
            of compressor parameter in the convergence rate. Extensive numerical experiments are presented to
            validate the theoretical results.
          </i>
          <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>



<h3>November 6, 2020</h3>
<h1>ICML 2021 Area Chair</h1>
<br>


I've accepted an invite to serve the machine learning community as an Area Chair for <a href="https://icml.cc/Conferences/2021">ICML 2021.</a>
I'll be a tough (but friendly) Area Chair:  I expect the best from the reviewers and will do all I can to make sure the reviews and reviewer discussion
are as fair and substantial as possible.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>November 3, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2011.02828">"Local SGD: Unified Theory and New Efficient Methods"</a> -
joint work with <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> and <a href="https://fhanzely.github.io/index.html">Filip Hanzely.</a><br>
<br>

Abstract: <i>
We present a unified framework for analyzing local SGD methods in the convex and strongly convex regimes for distributed/federated training of supervised machine learning models. We recover several known methods as a special case of our general framework, including Local-SGD/FedAvg, SCAFFOLD, and several variants of SGD not originally designed for federated learning. Our framework covers both the identical and heterogeneous data settings, supports both random and deterministic number of local steps, and can work with a wide array of local stochastic gradient estimators, including shifted estimators which are able to adjust the fixed points of local iterations for faster convergence. As an application of our framework, we develop multiple novel FL optimizers which are superior to existing methods. In particular, we develop the first linearly converging local SGD method which does not require any data homogeneity or other strong assumptions.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>November 3, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2011.01697">"A Linearly Convergent Algorithm for Decentralized Optimization: Sending Less Bits for Free!"</a> -
joint work with <a href="https://www.dmitry-kovalev.com ">Dmitry Kovalev</a>, <a href="https://scholar.google.com/citations?user=ldJpvE8AAAAJ&hl=en">Anastasia Koloskova</a>,
<a href="https://www.epfl.ch/labs/mlo/">Martin Jaggi</a> and <a href="https://scholar.google.com/citations?user=8l-mDfQAAAAJ&hl=en">Sebastian U. Stich</a>.<br>
<br>

Abstract: <i> Decentralized optimization methods enable on-device training of machine learning models without a central coordinator. In many scenarios communication between devices is energy demanding and time consuming and forms the bottleneck of the entire system. We propose a new randomized first-order method which tackles the communication bottleneck by applying randomized compression operators to the communicated messages. By combining our scheme with a new variance reduction technique that progressively throughout the iterations reduces the adverse effect of the injected quantization noise, we obtain the first scheme that converges linearly on strongly convex decentralized problems while using compressed communication only. We prove that our method can solve the problems without any increase in the number of communications compared to the baseline which does not perform any communication compression while still allowing for a significant compression factor which depends on the conditioning of the problem and the topology of the network. Our key theoretical findings are supported by numerical experiments.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 26, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.13723">"Optimal Client Sampling for Federated Learning"</a> -
joint work with Wenlin Chen, and <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.<br>
<br>

Abstract: <i>
It is well understood that client-master communication can be a primary bottleneck in Federated Learning. In this work, we address this issue with a novel client subsampling scheme, where we restrict the number of clients allowed to communicate their updates back to the master node. In each communication round, all participated clients compute their updates, but only the ones with "important" updates communicate back to the master. We show that importance can be measured using only the norm of the update and we give a formula for optimal client participation. This formula minimizes the distance between the full update, where all clients participate, and our limited update, where the number of participating clients is restricted. In addition, we provide a simple algorithm that approximates the optimal formula for client participation which only requires secure aggregation and thus does not compromise client privacy. We show both theoretically and empirically that our approach leads to superior performance for Distributed SGD (DSGD) and Federated Averaging (FedAvg) compared to the baseline where participating clients are sampled uniformly. Finally, our approach is orthogonal to and compatible with existing methods for reducing communication overhead, such as local methods and communication compression methods.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 23, 2020</h3>
<h1>New Paper (Spotlight @ NeurIPS 2020)</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.12292">"Linearly Converging Error Compensated SGD"</a> -
joint work with <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>, <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
and Dmitry Makarenko.<br>
<br>

Abstract: <i>
In this paper, we propose a unified analysis of variants of distributed SGD with arbitrary compressions and delayed updates. Our framework is general enough to cover different variants of quantized SGD, Error-Compensated SGD (EC-SGD) and SGD with delayed updates (D-SGD). Via a single theorem, we derive the complexity results for all the methods that fit our framework. For the existing methods, this theorem gives the best-known complexity results. Moreover, using our general scheme, we develop new variants of SGD that combine variance reduction or arbitrary sampling with error feedback and quantization and derive the convergence rates for these methods beating the state-of-the-art results. In order to illustrate the strength of our framework, we develop 16 new methods that fit this. In particular, we propose the first method called EC-SGD-DIANA that is based on error-feedback for biased compression operator and quantization of gradient differences and prove the convergence guarantees showing that EC-SGD-DIANA converges to the exact optimum asymptotically in expectation with constant learning rate for both convex and strongly convex objectives when workers compute full gradients of their loss functions. Moreover, for the case when the loss function of the worker has the form of finite sum, we modified the method and got a new one called EC-LSVRG-DIANA which is the first distributed stochastic method with error feedback and variance reduction that converges to the exact optimum asymptotically in expectation with a constant learning rate.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>October 16, 2020</h3>
<h1>Nicolas Loizou runner-up in OR Society Doctoral Award</h1>
<br>


<a href="https://nicolasloizou.github.io/">Nicolas Loizou</a>, my former PhD student (and last student to have graduated from Edinburgh after I left for KAUST), has been selected as a runner-up in the <a href="https://www.theorsociety.com/membership/awards-medals-and-scholarships/the-doctoral-award/previous-awards/">2019
OR Society Doctoral Award</a> competition. Congratuations! <br><br>

Nicolas' PhD thesis: <a href="https://arxiv.org/abs/1909.12176">Randomized Iterative Methods for Linear Systems: Momentum, Inexactness and Gossip</a>


<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 7, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.03246">"Optimal Gradient Compression for Distributed and Federated Learning"</a> -
joint work with <a href="https://alyazeedbasyoni.wixsite.com/blog">Alyazeed Albasyoni</a>, <a href="https://mher-safaryan.github.io">Mher Safaryan</a>,
and <a href="https://lcondat.github.io">Laurent Condat</a>.<br>
<br>

Abstract: <i>
Communicating information, like gradient vectors, between computing nodes in distributed and federated learning is typically an unavoidable burden, resulting in scalability issues. Indeed, communication might be slow and costly. Recent advances in communication-efficient training algorithms have reduced this bottleneck by using compression techniques, in the form of sparsification, quantization, or low-rank approximation. Since compression is a lossy, or inexact, process, the iteration complexity is typically worsened; but the total communication complexity can improve significantly, possibly leading to large computation time savings. In this paper, we investigate the fundamental trade-off between the number of bits needed to encode compressed vectors and the compression error. We perform both worst-case and average-case analysis, providing tight lower bounds. In the worst-case analysis, we introduce an efficient compression operator, Sparse Dithering, which is very close to the lower bound. In the average-case analysis, we design a simple compression operator, Spherical Compression, which naturally achieves the lower bound. Thus, our new compression schemes significantly outperform the state of the art. We conduct numerical experiments to illustrate this improvement.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 5, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.02372">"Lower Bounds and Optimal Algorithms for Personalized Federated Learning"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>, <a href="https://slavomirhanzely.wordpress.com">Slavomír Hanzely</a>,
and <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.<br>
<br>

Abstract: <i>
In this work, we consider the optimization formulation of personalized federated learning recently introduced by Hanzely and Richtárik (2020)
which was shown to give an alternative explanation to the workings of local SGD methods. Our first contribution is establishing the first lower
bounds for this formulation, for both the communication complexity and the local oracle complexity. Our second contribution is the design of
several optimal methods matching these lower bounds in almost all regimes. These are the first provably optimal methods for personalized federated
learning. Our optimal methods include an accelerated variant of FedProx, and an accelerated variance-reduced version of FedAvg / Local SGD. We
demonstrate the practical superiority of our methods through extensive numerical experiments.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>October 2, 2020</h3>
<h1>New Paper </h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.00952">"Distributed Proximal Splitting Algorithms with Rates and Acceleration"</a> -
joint work with <a href="https://lcondat.github.io">Laurent Condat</a> and Grigory Malinovsky.<br>
<br>

Abstract: <i>
We analyze several generic proximal splitting algorithms well suited for large-scale convex nonsmooth optimization. We derive sublinear and linear convergence results with new rates on the function value suboptimality or distance to the solution, as well as new accelerated versions, using varying stepsizes. In addition, we propose distributed variants of these algorithms, which can be accelerated as well. While most existing results are ergodic, our nonergodic results significantly broaden our understanding of primal-dual optimization algorithms.
</i>
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 2, 2020</h3>
<h1>New Paper </h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.00892">"Variance-Reduced Methods for Machine Learning"</a> -
joint work with <a href="https://gowerrobert.github.io">Robert Mansel Gower</a>,
<a href="https://scholar.google.com/citations?user=5BtEUJcAAAAJ&hl=en">Mark Schmidt</a> and <a href="https://www.di.ens.fr/~fbach/">Francis Bach</a>.<br>
<br>


Abstract: <i>
Stochastic optimization lies at the heart of machine learning, and its cornerstone is stochastic gradient descent (SGD), a method introduced over 60 years ago. The last 8 years have seen an exciting new development: variance reduction (VR) for stochastic optimization methods. These VR methods excel in settings where more than one pass through the training data is allowed, achieving a faster convergence than SGD in theory as well as practice. These speedups underline the surge of interest in VR methods and the fast-growing body of work on this topic. This review covers the key principles and main developments behind VR methods for optimization with finite data sets and is aimed at non-expert readers. We focus mainly on the convex setting, and leave pointers to readers interested in extensions for minimizing non-convex functions.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>October 2, 2020</h3>
<h1>New Paper </h1>
<br>

<span class="important">New paper out: </span>
<a href="https://arxiv.org/abs/2010.00091">"Error Compensated Distributed SGD Can Be Accelerated"</a> -
joint work with <a href="https://qianxunk.github.io">Xun Qian</a> and
<a href="https://www.cse.ust.hk/admin/people/faculty/profile/tongzhang">Tong Zhang</a>.<br>
<br>


Abstract: <i>
Gradient compression is a recent and increasingly popular technique for reducing the communication cost in distributed training of large-scale machine learning models. In this work we focus on developing efficient distributed methods that can work for any compressor satisfying a certain contraction property, which includes both unbiased (after appropriate scaling) and biased compressors such as RandK and TopK. Applied naively, gradient compression introduces errors that either slow down convergence or lead to divergence. A popular technique designed to tackle this issue is error compensation/error feedback. Due to the difficulties associated with analyzing biased compressors, it is not known whether gradient compression with error compensation can be combined with Nesterov's acceleration. In this work, we show for the first time that error compensated gradient compression methods can be accelerated. In particular, we propose and study the error compensated loopless Katyusha method, and establish an accelerated linear convergence rate under standard assumptions. We show through numerical experiments that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 30, 2020</h3>
<h1>Eduard Gorbunov Organizes All-Russian Optimization Research Seminar</h1>
<br>

My serial intern and collaborator <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> is the organizer of an <a href="http://www.mathnet.ru/php/conference.phtml?&eventID=31&confid=1794&option_lang=rus">All-Russian
Research Seminar Series on Mathematical Optimization.</a> There have been 14 speakers at this event so far, including Eduard and <a href="https://konstmish.github.io">Konstantin
  Mishchenko.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>September 28, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/1901.09997v4">"Quasi-Newton Methods for Deep Learning: Forget the Past, Just Sample"</a> -
joint work with <a href="https://ioe.engin.umich.edu/people/albert-s-berahas/">Albert S. Berahas</a>,
<a href="https://coral.ise.lehigh.edu/maj316/">Majid Jahani</a>,  and <a href="http://mtakac.com">Martin Takáč</a>.<br>
<br>


Abstract: <i>
We present two sampled quasi-Newton methods for deep learning: sampled LBFGS (S-LBFGS) and sampled LSR1 (S-LSR1). Contrary to the classical variants of these methods that sequentially build Hessian or inverse Hessian approximations as the optimization progresses, our proposed methods sample points randomly around the current iterate at every iteration to produce these approximations. As a result, the approximations constructed make use of more reliable (recent and local) information, and do not depend on past iterate information that could be significantly stale. Our proposed algorithms are efficient in terms of accessed data points (epochs) and have enough concurrency to take advantage of parallel/distributed computing environments. We provide convergence guarantees for our proposed methods. Numerical tests on a toy classification problem as well as on popular benchmarking neural network training tasks reveal that the methods outperform their classical variants.
</i>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>September 22, 2020</h3>
<h1>Adil Salim Giving a Virtual Talk at the Fields Institute</h1>
<br>

<a href="https://adil-salim.github.io">Adil Salim</a> is giving a virtual talk today at the <a href="http://www.fields.utoronto.ca/activities/20-21/dynamical">Second Symposium on Machine Learning and Dynamical Systems</a>, organized at the Fields Institute.
His talk "Primal Dual Interpretation of the Proximal Gradient Langevin Algorithm", based
on <a href="https://arxiv.org/abs/2006.09270">this paper</a>, is <a href="https://www.youtube.com/watch?v=4lGjecpVWzE">available
on YouTube</a>. <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 30, 2020</h3>
<h1>Fall 2020 Teaching</h1>
<br>

The Fall 2020 semester started. I am teaching CS 331: Stochastic Gradient Descent Methods. <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>September 15, 2020</h3>
<h1>Four New Group Members</h1>
<br>

Four new people joined my team in August/September 2020:
<br><br>

<b>Konstantin Burlachenko</b> joins as a CS PhD student. Konstantin got a Master’s degree in CS and Control Systems from Bauman Moscow State University in 2009. 
He has since worked at a number of companies, most recently as a Senior Developer at Yandex and NVidia and a Principal Engineer at Huawei. Konstantin is
interested in software development, optimization, federated learning, graphics and vision, and forecasting models. Konstantin attended several courses at
Stanford and obtained two graduate certificates [<a href="https://online.stanford.edu/programs/data-models-and-optimization-graduate-certificate">1</a>]
[<a href="https://online.stanford.edu/programs/artificial-intelligence-graduate-certificate">2</a>].

<br>
<br>
<b>Grigory Malinovsky</b> joins as an MS/PhD student in <a href="https://cemse.kaust.edu.sa/amcs">AMCS</a>  after a successful internship at KAUST in early 2020 which led to
the paper <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/5019-Paper.pdf">“From Local SGD to Local Fixed-Point Methods for Federated Learning"</a>,
joint with <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="http://elnurgasanov.me">Elnur Gasanov</a>, Laurent Condat and myself. The paper appeared in ICML 2020.  Grisha has graduated with a BS degree from Moscow Institute of Physics and Technology (MIPT) with a thesis entitled “Averaged Heavy Ball Method” under the supervision of Boris Polyak.
Among Grigory’s successes belong:
<br>
- Abramov’s scholarship for students with the best grades at MIPT, 2016
<br>
- Participant in the final round of All-Russian Physics Olympiad, 2014
<br>
- Bronze medal at International Zhautykov Olympiad in Physics, 2014
<br>
- Prize winner in the final round of All-Russian Physics Olympiad, 2013
<br>
Grisha enjoys basketball, fitness, football and table tennis. He speaks a bit of Tatar.

<br><br>
<b>Igor Sokolov</b> joins as an MS student in <a href="https://cemse.kaust.edu.sa/amcs">AMCS.</a> Igor has a BS degree from MIPT’s Department of Control and
Applied Mathematics. Igor is the recipient of several prizes, including at the Phystech Olympiad in Physics (2014), and regional stage of the All Russian
Olympiad in Physics (2014). He won 2nd place at the Programming Conference (2012 and 2013) and was a winner of the Programming Olympiad (2011); all at the
Computer Training Center. Igor enjoys snowboarding, cycling and jogging. He coauthored a paper which will soon be posted onto arXiv.

<br><br>
<b>Bokun Wang</b> joins as a remote intern and will work in the lab for 6 months. Bokun coauthored several papers, including <a href="https://arxiv.org/abs/2005.01209">
``Riemannian Stochastic Proximal Gradient Methods for Nonsmooth Optimization over the Stiefel Manifold”.</a> He has recently interned with Tong Zhang (HKUST).
Bokun is a graduate student at UC Davis, and has a BS degree in Computer Science from University of Electronic Science and Technology of China.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 25, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2008.10898">"PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization"</a> -
joint work with <a href="https://zhizeli.github.io">Zhize Li</a>, <a href="https://mine.kaust.edu.sa/Pages/Hongyan.aspx">Hongyan Bao</a>,  and <a href="https://scholar.google.com/citations?user=BhRJe4wAAAAJ&hl=en">Xiangliang Zhang</a>.<br>
<br>


Abstract: <i>
In this paper, we propose a novel stochastic gradient estimator---ProbAbilistic Gradient Estimator (PAGE)---for nonconvex optimization. PAGE is easy to implement as it is designed via a small adjustment to vanilla SGD: in each iteration, PAGE uses the vanilla minibatch SGD update with probability p and reuses the previous gradient with a small adjustment, at a much lower computational cost, with probability 1−p. We give a simple formula for the optimal choice of p. We prove tight lower bounds for nonconvex problems, which are of independent interest. Moreover, we prove matching upper bounds both in the finite-sum and online regimes, which establish that Page is an optimal method. Besides, we show that for nonconvex functions satisfying the Polyak-Łojasiewicz (PL) condition, PAGE can automatically switch to a faster linear convergence rate. Finally, we conduct several deep learning experiments (e.g., LeNet, VGG, ResNet) on real datasets in PyTorch, and the results demonstrate that PAGE converges much faster than SGD in training and also achieves the higher test accuracy, validating our theoretical results and confirming the practical superiority of PAGE.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 25, 2020</h3>
<h1>Apple Virtual Workshop on Privacy Preserving Machine Learning</h1>
<br>

I have been invited to give a talk at the "Virtual Workshop on Privacy Preserving Machine Learning", hosted by <a href="https://www.apple.com">Apple</a>. The workshop is a two-day event starting today.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 19, 2020</h3>
<h1>Paper Accepted to SIAM Journal on Scientific Computing</h1>
<br>

The paper <a href="https://arxiv.org/abs/1903.07971">"Convergence Analysis of Inexact Randomized Iterative Methods"</a>, joint with <a href="https://www.maths.ed.ac.uk/~s1461357/">Nicolas Loizou</a>, was accepted to
<a href="https://www.siam.org/publications/journals/siam-journal-on-scientific-computing-sisc">SIAM Journal on Scientific Computing</a>.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>August 12, 2020</h3>
<h1>Paper Accepted to Computational Optimization and Applications</h1>
<br>

The paper <a href="https://arxiv.org/abs/1712.09677">"Momentum and Stochastic Momentum for Stochastic Gradient, Newton,
Proximal Point and Subspace Descent Methods"</a>, joint with <a href="https://www.maths.ed.ac.uk/~s1461357/">Nicolas Loizou</a>, was accepted to
<a href="https://www.springer.com/journal/10589">Computational Optimization and Applications</a>.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>August 8, 2020</h3>
<h1>New Research Intern</h1>
<br>



Wenlin Chen has joined my group as a remote intern until about the end of September 2020. During the internship, Wenlin will be working on communication efficient methods for federated learning.
Wenlin has a BS degree in mathematics from University of Manchester (ranked top 1.5% in the cohort), and is about to start an MPhil in Machine Learning at the University of Cambridge in October 2020.
Wenlin is a coauthor of an ECML 2020 paper entitled <a href="http://www.cs.man.ac.uk/%7Egbrown/publications/ecml2020webb.pdf">To Ensemble or Not Ensemble: When does End-To-End Training Fail?</a>
 where he investigated novel information-theoretic methods of training deep neural network ensembles, focusing on the resulting regularization effects and trade-offs between individual model
 capacity and ensemble diversity. He also conducted large-scale ensemble deep learning experiments using the university’s HPC Cluster CSF3.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>August 7, 2020</h3>
<h1>Senior PC Memeber for IJCAI 2021</h1>
<br>

I've accepted an invite to become  Senior Program Committee member for IJCAI 2021.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>







<h3>August 3, 2020</h3>
<h1>Paper Accepted to SIAM Journal on Optimization</h1>
<br>

The paper <a href="https://arxiv.org/abs/1902.03591">"Stochastic Three Points Method for Unconstrained Smooth Minimization"</a>, joint with <a href="https://ehbergou.github.io">El Houcine Bergou</a>,
<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> was accepted to
<a href="https://www.siam.org/publications/journals/siam-journal-on-optimization-siopt">SIAM Journal on Optimization</a>.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 30, 2020</h3>
<h1>Filip Hanzely Defended his PhD Thesis</h1>
<br>

<a href="http://fhanzely.github.io">Filip Hanzely</a> defended his PhD thesis <a href="http://www.optimization-online.org/DB_FILE/2020/08/7967.pdf">"Optimization for
Supervised Machine Learning:  Randomized Algorithms for Data and Parameters"</a> today.
<br> <br>

Having started in Fall 2017
(I joined KAUST in March the same year), Filip is my first PhD student to graduate from KAUST. He managed to complete his
PhD in less than 3 years, and has done some trully amazing research, described by the committee (<a href="http://pages.cs.wisc.edu/~swright/">Stephen J Wright</a>, <a href="http://tongzhang-ml.org">Tong Zhang</a>,
<a href="https://www.mathematik.rwth-aachen.de/ca/cd/btbp/?allou=1&ikz=11&gguid=0x963949FB4292ED4DA761C2EE9DAD832B">Raul F Tempone</a>,
<a href="http://www.bernardghanem.com">Bernard Ghanem</a> and myself) as "Outstanding work, in all aspects. It is comprehensive as it synthesises various strands of current research, and is almost of
 an encyclopedic coverage. The work develops deep theoretical results, some of which answer long-standing open problems. Overall, highly innovative
 research and excellent thesis narrative and structure".

<br> <br>
Filip's next destination is a faculty position at <a href="https://www.ttic.edu">TTIC</a>. Congratulations, Filip!

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 17, 2020</h3>
<h1>ICML Workshop "Beyond First Order Methods in ML Systems"</h1>
<br>

Today, I have given the openinhg plenary talk at the ICML 2020 Workshop "Beyond First Order Methods in ML Systems".
The slides from my talk "Fast Linear Convergence of Randomized BFGS" are <a href="talks/TALK-2020-07-RBFGS-ICML-workshop.pdf">here.</a>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 12, 2020</h3>
<h1>Attending ICML 2020</h1>
<br>

I am attending ICML 2020 - the event is held virtually during July 12-18. My group members are presenting 5 papers, and I
will give the opening  plenary talk at the <a href="https://sites.google.com/view/optml-icml2020/home">Beyond First Order
  Methods in Machine Learning</a> workshops on Friday.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>July 9, 2020</h3>
<h1>Area Chair for ICLR 2021</h1>
<br>

I've accepted an invite to become an Area Chair for ICLR 2021.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>July 7, 2020</h3>
<h1>Paper Accepted to IEEE Transactions on Signal Processing</h1>
<br>

The paper ``Best Pair Formulation & Accelerated Scheme for Non-convex Principal Component Pursuit'', joint with <a href="https://aritradutta.weebly.com">Aritra Dutta</a>,
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>, and <a href="https://jliang993.github.io">Jingwei Liang</a>, was accepted to
IEEE Transactions on Signal Processing.
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>July 3, 2020</h3>
<h1>Dmitry Kovalev Wins the 2020 Ilya Segalovich Scientific Prize</h1>
<br>



It is a great pleasure to announce that my PhD student <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> is one of nine recipients of the <a href="https://yandex.ru/blog/company/novye-laureaty-premii-imeni-segalovicha">2020 Ilya
Segalovich Scientific Prize for Young Researchers</a>, awarded by Yandex (a Russian equivalent of Google or Baidu) for significant advances in Computer Science. The award is focused on research of particular interest to yandex:
machine learning, computer vision, information search and data analysis, NLP and machine translation and speech synthesis/recognition.
The prize carries a cash award of 350,000 RUB ( = approx 5,000 USD).
<br>
<br>

Dmitry started MS studies at KAUST in Fall 2018 and received his MS degree in December 2019. He is a PhD student since January 2020. In this short period of time,
he has co-authored 17 papers, 15 of which are online (<a href="https://scholar.google.com/citations?user=qHFA5z4AAAAJ">Google Scholar</a>).  In my view, he is one of
the most talented young researchers coming in recent years from Russia. Dmitry's research is insighful, creative and deep.
<br>
<br>

Google translate of the announcement in Russian:
<br>
<br><i>For the second time, we selected the laureates of the Ilya Segalovich Scientific Prize. Yandex marks this award for scientists who have made significant
  advances in computer science. The prize is awarded once a year in two categories: “Young Researchers” and “Scientific Advisers”. The first nomination is for students, undergraduates and graduate students, the second - for their mentors.
Mikhail Bilenko (Head of Machine Intelligence and Research at Yandex) said: "The services and technologies of Yandex are based on science. At the same time, we are interested not only in applied developments, but also in theoretical research. They move the entire industry forward and can lead to impressive results in the future. We established the Segalovich Prize to support students and graduate students who are engaged in machine learning and other promising areas of computer science. Often, talented guys go to work in the industry while still studying. We want them to have the opportunity to continue basic research - with our financial support."
The winners are determined by the award council. It includes Yandex executives and scientists who collaborate with the company, including Ilya Muchnik, professor at Rutgers University in New Jersey, Stanislav Smirnov, professor at the University of Geneva and Fields laureate, and Alexei Efros, professor at the University of California at Berkeley. The size of the prize for young researchers is 350 thousand, and for scientific advisers - 700 thousand rubles.
This year, 12 people became laureates: three supervisors and nine young scientists. When choosing laureates among scientific scientists, we first of all took into account the contribution to community development and youth work. For young researchers, the main criterion is scientific achievements.
All laureates in the nomination “Young Researchers” have already managed to present their work at prestigious international conferences. Proceedings for such conferences are selected and reviewed by the world’s best experts in machine learning and artificial intelligence. If the work was accepted for publication at a conference, this is international recognition.</i><br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 23, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.11773">"Optimal and Practical Algorithms for Smooth and Strongly Convex Decentralized Optimization"</a> -
joint work with <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> and <a href="https://adil-salim.github.io">Adil Salim</a>.<br>
<br>

Abstract: <i>
We consider the task of decentralized minimization of the sum of smooth strongly convex functions stored across the nodes a network. For this problem, lower bounds on the number of gradient computations and the number of communication rounds required to achieve ε accuracy have recently been proven. We propose two new algorithms for this decentralized optimization problem and equip them with complexity guarantees. We show that our first method is optimal both in terms of the number of communication rounds and in terms of the number of gradient computations. Unlike existing optimal algorithms, our algorithm does not rely on the expensive evaluation of dual gradients. Our second algorithm is optimal in terms of the number of communication rounds, without a logarithmic factor. Our approach relies on viewing the two proposed algorithms as accelerated variants of the Forward Backward algorithm to solve monotone inclusions associated with the decentralized optimization problem. We also verify the efficacy of our methods against state-of-the-art algorithms through numerical experiments.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 23, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.11573">"Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization"</a> -
joint work with <a href="https://rka97.github.io">Ahmed Khaled</a>, <a href="https://othmanesebbouh.github.io">Othmane Sebbouh</a>,
<a href="https://www.maths.ed.ac.uk/~s1461357/">Nicolas Loizou</a>, and <a href="https://gowerrobert.github.io">Robert M. Gower</a>.<br>
<br>

Abstract: <i>
We present a unified theorem for the convergence analysis of stochastic gradient algorithms for minimizing a smooth and convex loss plus a convex regularizer.
We do this by extending the unified analysis of Gorbunov, Hanzely & Richtárik (2020) and dropping the requirement that the loss function be strongly convex.
Instead, we only rely on convexity of the loss function. Our unified analysis applies to a host of existing algorithms such as proximal SGD, variance reduced
methods, quantization and some coordinate descent type methods. For the variance reduced methods, we recover the best known convergence rates as special cases.
For proximal SGD, the quantization and coordinate type methods, we uncover new state-of-the-art convergence rates. Our analysis also includes any form of sampling
and minibatching. As such, we are able to determine the minibatch size that optimizes the total complexity of variance reduced methods. We showcase this by obtaining
a simple formula for the optimal minibatch size of two variance reduced methods (\textit{L-SVRG} and \textit{SAGA}). This optimal minibatch size not only improves
the theoretical total complexity of the methods but also improves their convergence in practice, as we show in several experiments.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 23, 2020</h3>
<h1>6th FLOW seminar talk tomorrow</h1>
<br>

<a href="https://www.di.ens.fr/hadrien.hendrikx/">Hadrien Hendrikx</a> will give a talk at the  <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">FLOW seminar tomorrow.</a>
Title of his talk: "Statistical Preconditioning for Federated Learning".
  <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 22, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.11077">"A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning"</a> -
joint work with <a href="https://samuelhorvath.github.io">Samuel Horváth</a>.<br>
<br>

Abstract: <i>
Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed compute systems. A key bottleneck of such systems is the communication overhead for exchanging information across the workers, such as stochastic gradients. Among the many techniques proposed to remedy this issue, one of the most successful is the framework of compressed communication with error feedback (EF). EF remains the only known technique that can deal with the error induced by contractive compressors which are not unbiased, such as Top-K. In this paper, we propose a new and theoretically and practically better alternative to EF for dealing with contractive compressors. In particular, we propose a construction which can transform any contractive compressor into an induced unbiased compressor. Following this transformation, existing methods able to work with unbiased compressors can be applied. We show that our approach leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. We further extend our results to federated learning with partial participation following an arbitrary distribution over the nodes, and demonstrate the benefits thereof. We perform several numerical experiments which validate our theoretical findings.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 18, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.09270">"Primal Dual Interpretation of the Proximal Stochastic Gradient Langevin Algorithm"</a> -
joint work with <a href="https://adil-salim.github.io">Adil Salim</a>.<br>
<br>

Abstract: <i>
We consider the task of sampling with respect to a log concave probability distribution. The potential of the target distribution is assumed to be composite, i.e., written as the sum of a smooth convex term, and a nonsmooth convex term possibly taking infinite values. The target distribution can be seen as a minimizer of the Kullback-Leibler divergence defined on the Wasserstein space (i.e., the space of probability measures). In the first part of this paper, we establish a strong duality result for this minimization problem. In the second part of this paper, we use the duality gap arising from the first part to study the complexity of the Proximal Stochastic Gradient Langevin Algorithm (PSGLA), which can be seen as a generalization of the Projected Langevin Algorithm. Our approach relies on viewing PSGLA as a primal dual algorithm and covers many cases where the target distribution is not fully supported. In particular, we show that if the potential is strongly convex, the complexity of PSGLA is $\cO(1/\varepsilon^2)$ in terms of the 2-Wasserstein distance. In contrast, the complexity of the Projected Langevin Algorithm is $\cO(1/\varepsilon^{12})$ in terms of total variation when the potential is convex.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 17, 2020</h3>
<h1>5th FLOW seminar talk today</h1>
<br>

<a href="https://fhanzely.github.io">Filip Hanzely</a> gave a talk at the  <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">FLOW seminar today.</a>
His <a href="https://drive.google.com/file/d/1X57EMpnO8UXLXpbZrxoxuh53heUmZ2h_/view?usp=sharing">slides</a> and
<a href="https://www.youtube.com/watch?v=N10GwFwpt5I&feature=emb_title">video</a> of the talk can be found
<a href="https://sites.google.com/view/one-world-seminar-series-flow/archive#h.64km7mxogc2u">here.</a>  <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 10, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.07013">"A Unified Analysis of Stochastic Gradient Methods for Nonconvex Federated Optimization"</a> -
joint work with <a href="https://zhizeli.github.io">Zhize Li</a>.<br>
<br>

Abstract: <i>
In this paper, we study the performance of a large family of SGD variants in the smooth nonconvex regime. To this end, we propose a generic and flexible assumption capable of accurate modeling of the second moment of the stochastic gradient. Our assumption is satisfied by a large number of specific variants of SGD in the literature, including SGD with arbitrary sampling, SGD with compressed gradients, and a wide variety of variance-reduced SGD methods such as SVRG and SAGA. We provide a single convergence analysis for all methods that satisfy the proposed unified assumption, thereby offering a unified understanding of SGD variants in the nonconvex regime instead of relying on dedicated analyses of each variant. Moreover, our unified analysis is accurate enough to recover or improve upon the best-known convergence results of several classical methods, and also gives new convergence results for many new methods which arise as special cases. In the more general distributed/federated nonconvex optimization setup, we propose two new general algorithmic frameworks differing in whether direct gradient compression (DC) or compression of gradient differences (DIANA) is used. We show that all methods captured by these two frameworks also satisfy our unified assumption. Thus, our unified convergence analysis also captures a large variety of distributed methods utilizing compressed communication. Finally, we also provide a unified analysis for obtaining faster linear convergence rates in this nonconvex regime under the PL condition.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 12, 2020</h3>
<h1>Plenary Talk at Mathematics of Data Science Workshop</h1>
<br>

Today I gave a plenary talk at the <a href="https://maths-of-data.github.io/plenary-talks/">Mathematics of Data
  Science</a> workshop.  I gave the same talk as the one I gave in April at
the <a href="https://owos.univie.ac.at">One World Optimization Seminar:</a> <a href="https://www.youtube.com/watch?v=HGQkct3db-c">“On Second Order Methods and Randomness”,
which is on YouTube.</a> If you ever wondered what a 2nd order version of SGD should and should not look like, you may want to watch
the video talk. Our stochastic Newton (SN) method converges in 4/3 * n/tau * log 1/epsilon iterations when started
close enough from the solution, where n is the number of functions forming the finite sum we want to minimize, and tau is the minibatch size.
We can choose tau to be any value between 1 and n. Note that unlike all 1st order methods, the rate of SN is
independent of the condition number! 4/n The talk is based on joint work with my fantastic students <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a> and <a href="https://konstmish.github.io">Konstantin Mishchenko</a>:
<a href="https://arxiv.org/abs/1912.01597">“Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates”</a>,
<a href="https://sites.google.com/site/optneurips19/">NeurIPS 2019 Workshop Beyond First Order Methods in ML</a>, 2019.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 10, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2006.05988">"Random Reshuffling: Simple Analysis with Vast Improvements"</a> -
joint work with <a href="https://konstmish.github.io">Konstantin Mishchenko</a> and
<a href="https://rka97.github.io">Ahmed Khaled</a>.<br>
<br>

Abstract: <i>
Random Reshuffling (RR) is an algorithm for minimizing finite-sum functions that utilizes iterative gradient
descent steps in conjunction with data reshuffling. Often contrasted with its sibling Stochastic Gradient
Descent (SGD), RR is usually faster in practice and enjoys significant popularity in convex and non-convex
optimization. The convergence rate of RR has attracted substantial attention recently and, for strongly
convex and smooth functions, it was shown to converge faster than SGD if 1) the stepsize is small, 2) the
gradients are bounded, and 3) the number of epochs is large. We remove these 3 assumptions, improve the
ependence on the condition number from $\kappa^2$ to $\kappa$ (resp.\ from $\kappa$ to $\sqrt{kappa}$) and,
in addition, show that RR has a different type of variance. We argue through theory and experiments that the
new variance type gives an additional justification of the superior performance of RR. To go beyond strong
convexity, we present several results for non-strongly convex and non-convex objectives. We show that in all
cases, our theory improves upon existing literature. Finally, we prove fast convergence of the Shuffle-Once
(SO) algorithm, which shuffles the data only once, at the beginning of the optimization process. Our theory
for strongly-convex objectives tightly matches the known lower bounds for both RR and SO and substantiates
the common practical heuristic of shuffling once or only a few times. As a byproduct of our analysis, we also
get new results for the Incremental Gradient algorithm (IG), which does not shuffle the data at all.
</i> <br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>June 5, 2020</h3>
<h1>NeurIPS Paper Deadline Today</h1>
<br>

The  <a href="https://nips.cc">NeurIPS</a> deadline has passed! Finally, I can relax a bit (= 1 day). Next deadline:
Supplementary Material for NeurIPS, on June 11... <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>June 1, 2020</h3>
<h1>Five Papers Accepted to ICML 2020</h1>
<br>
We've had five papers accepted to <a href="https://icml.cc/Conferences/2020">ICML 2020</a>, which will be run virtually during July 12-18, 2020.
Here they are:<br><br>


1) <a href="https://arxiv.org/abs/2002.04670">"Variance Reduced Coordinate Descent with Acceleration: New
  Method With a Surprising Application to Finite-Sum Problems"</a> - joint work with
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> and
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev.</a> <br>
<br>
Abstract: <i>We propose an accelerated version of stochastic variance reduced coordinate descent -- ASVRCD. As other variance reduced coordinate descent methods such as SEGA or SVRCD, our method can deal with problems that include a non-separable and non-smooth regularizer, while accessing a random block of partial derivatives in each iteration only. However, ASVRCD incorporates Nesterov's momentum, which offers favorable iteration complexity guarantees over both SEGA and SVRCD. As a by-product of our theory, we show that a variant of Allen-Zhu (2017) is a specific case of ASVRCD, recovering the optimal oracle complexity for the finite sum objective.</i><br>
<br>



2) <a href="https://arxiv.org/abs/2002.09526">"Stochastic Subspace Cubic Newton Method"</a> - joint work with
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>,
<a href="https://scholar.google.ru/citations?user=YNBhhjUAAAAJ&hl=en">Nikita Doikov</a> and <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>.
<br>
<br>
Abstract: <i>In this paper, we propose a new randomized second-order optimization
  algorithm---Stochastic Subspace Cubic Newton (SSCN)---for minimizing a high dimensional
  convex function f. Our method can be seen both as a stochastic extension of the
  cubically-regularized Newton method of Nesterov and Polyak (2006), and a second-order
  enhancement of stochastic subspace descent of Kozak et al. (2019). We prove that as we vary
  the minibatch size, the global convergence rate of SSCN interpolates between the rate of
  stochastic coordinate descent (CD) and the rate of cubic regularized Newton, thus giving new
  insights into the connection between first and second-order methods.
  Remarkably, the local convergence rate of SSCN matches the rate of stochastic subspace
  descent applied to the problem of minimizing the quadratic function 0.5 (x−xopt)^T f''(xopt) (x−xopt),
  where xopt is the minimizer of f, and hence depends on the properties of f at the optimum only.
  Our numerical experiments show that SSCN outperforms non-accelerated first-order CD algorithms
  while being competitive to their accelerated variants. </i><br>
<br>



3) <a href="https://arxiv.org/abs/1910.09529">"Adaptive Gradient Descent Without Descent"</a> - work of  <a
href="https://konstmish.github.io">Konstantin Mishchenko</a> and <a href="https://people.epfl.ch/yurii.malitskyi">Yura Malitsky</a>.<br>
<br>
Abstract: <i>We present a strikingly simple proof that two rules are sufficient to automate gradient descent: 1) don't increase the stepsize too fast and 2) don't overstep the local curvature. No need for functional values, no line search, no information about the function except for the gradients. By following these rules, you get a method adaptive to the local geometry, with convergence guarantees depending only on smoothness in a neighborhood of a solution. Given that the problem is convex, our method will converge even if the global smoothness constant is infinity. As an illustration, it can minimize arbitrary continuously twice-differentiable convex function. We examine its performance on a range of convex and nonconvex problems, including matrix factorization and training of ResNet-18. </i> <br>
<br>


4) <a href="https://arxiv.org/abs/2004.01442">
  "From Local SGD to Local Fixed Point Methods for Federated Learning"</a> - joint work with
Grigory Malinovsky,
<a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
<a href="http://elnurgasanov.me">Elnur Gasanov</a>, and
<a href="https://lcondat.github.io">Laurent Condat</a>. <br>

<br>
Abstract: <i>Most algorithms for solving optimization problems or finding saddle points of convex-concave functions are fixed point algorithms. In this work we consider the generic problem of finding a fixed point of an average of operators, or an approximation thereof, in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models the computations done locally on a mobile device. We investigate two strategies to achieve such a consensus: one based on a fixed number of local steps, and the other based on randomized computations. In both cases, the goal is to limit communication of the locally-computed variables, which is often the bottleneck in distributed frameworks. We perform convergence analysis of both methods and conduct a number of experiments highlighting the benefits of our approach.</i> <br>
<br>


5) <a href="https://arxiv.org/abs/2002.11364">"Acceleration for Compressed Gradient Descent in Distributed Optimization"</a>
- joint work with
<a href="https://zhizeli.github.io">Zhize Li</a>, <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
and <a
href="https://qianxunk.github.io">Xun Qian</a>.
<br>
<br>
Abstract: <i>The abstract contains a lot of math symbols, so <a href="https://arxiv.org/abs/2002.11364">look here instead.</a> </i><br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>May 27, 2020</h3>
<h1>NeurIPS Abstract Deadline Today</h1>
<br>

Polishing <a href="https://nips.cc">NeurIPS</a> abstracts... <br >


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>





<h3>May 22, 2020</h3>
<h1>Eduard's ICLR Talk </h1>
<br>

<a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a> presented our paper <a href="https://openreview.net/forum?id=HylAoJSKvH">"A Stochastic Derivative Free Optimization Method with Momentum"</a> at ICLR. This paper is joint work with
Adel Bibi, Ozan Sener, and El Houcine Bergou. Eduard's 5min talk can be found here: <br >

<br>
<a href="https://iclr.cc/virtual_2020/poster_HylAoJSKvH.html"> <img alt="" src="imgs/ICLR2020-Eduard.png" width="700" ></a>
<br>



<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>May 21, 2020</h3>
<h1>"JacSketch" Paper Appeared in Mathematical Programming</h1>
<br>

The paper "Stochastic quasi-gradient methods: variance reduction via Jacobian sketching", joint work
with Robert M. Gower and Francis Bach, just appeared online on the Mathematical Programming
website: <a href="https://link.springer.com/article/10.1007/s10107-020-01506-0">Mathematical Programming, 2020</a>.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 20, 2020</h3>
<h1>Paper Appeared in SIMAX</h1>
<br>

The paper "Stochastic reformulations of linear systems: algorithms and convergence theory", joint work with
Martin Takáč, just appeared online on the SIMAX website: <a href="https://epubs.siam.org/doi/abs/10.1137/18M1179249">SIAM Journal on Matrix Analysis and Applications 41(2):487–524, 2020</a>.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>May 20, 2020</h3>
<h1>2nd FLOW Talk: Blake Woodworth (TTIC)</h1>
<br>

<a href="https://ttic.uchicago.edu/~blake/">Blake Woodworth (TTIC)</a> has given a great talk at the FLOW seminar today. His talk title was "Is Local SGD Better than Minibatch SGD?".
The slides and YouTube video <a href="https://sites.google.com/view/one-world-seminar-series-flow/archive#h.marh0ook3glo">can be found here.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 15, 2020</h3>
<h1>Paper Accepted to UAI 2020</h1>
<br>

Our paper
<a href="https://arxiv.org/abs/1901.09437">"99% of Distributed Optimization is a Waste of Time: The Issue and How to Fix it"</a> -
joint work with <a href="https://konstmish.github.io">Konstantin Mishchenko</a> and
<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>,
was accepted to <a href="http://www.auai.org/uai2020/">Conference on Uncertainty in Artificial Intelligence (UAI 2020)</a>.<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 13, 2020</h3>
<h1>1st FLOW Talk: Ahmed Khaled (Cairo)</h1>
<br>

<a href="https://rka97.github.io">Ahmed Khaled (Cairo)</a> has given his first research talk ever today. Topic: "On the Convergence of Local SGD on Identical and Heterogeneous Data".
It was a great talk - I can't wait to see him give talks in the future. The abstract, link to the relevant papers, slides and YouTube video <a href="https://sites.google.com/view/one-world-seminar-series-flow/archive#h.azhfwca3oax9">are here.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 7, 2020</h3>
<h1>Three Students Attending MLSS 2020</h1>

<br>
<a href="https://samuelhorvath.github.io">Samuel Horváth</a>, <a href = "https://eduardgorbunov.github.io"> Eduard Gorbunov</a> and
<a href="https://shulgin-egor.github.io">Egor Shulgin</a> have been accepted to participate in tis year's <a href="http://mlss.tuebingen.mpg.de/2020/">Machine Learning Summer School (MLSS) in Tübingen, Germany.</a>
As most things this year, the event will be fully virtual. MLSS is highly selective; I am told this year they received more than 1300 applications for 180 spots at the event (less than 14% acceptance rate).
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 5, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2005.01097">"Adaptive Learning of the Optimal Mini-Batch Size of SGD"</a> -
joint work with <a href="https://scholar.google.com/citations?user=caAyffEAAAAJ&hl=en">Motasem Alfarra</a>,
<a href="https://slavomirhanzely.wordpress.com">Slavomír Hanzely</a>,
<a href="https://alyazeedbasyoni.wixsite.com/blog">Alyazeed Albasyoni</a>
and <a href="http://www.bernardghanem.com">Bernard Ghanem</a>.<br>
<br>

Abstract: <i>
Recent advances in the theoretical understandingof SGD (Qian et al., 2019) led to a formula for the optimal mini-batch size minimizing the number of effective data passes, i.e., the number of iterations times the mini-batch size. However, this formula is of no practical value as it depends on the knowledge of the variance of the stochastic gradients evaluated at the optimum. In this paper we design a practical SGD method capable of learning the optimal mini-batch size adaptively throughout its iterations. Our method does this provably, and in our experiments with synthetic and real data robustly exhibits nearly optimal behaviour; that is, it works as if the optimal mini-batch size was known a-priori. Further, we generalize our method to several new mini-batch strategies not considered in the literature before, including a sampling suitable for distributed implementations.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 4, 2020</h3>
<h1>FLOW: Federated Learning One World Seminar</h1>

<br>
Together with <a href = "http://researchers.lille.inria.fr/abellet/">Aurélien Bellet</a> (Inria), <a href = "https://www.cs.cmu.edu/~smithv/">Virginia Smith</a> (Carnegie Mellon) and
<a href= "https://ist.ac.at/en/research/alistarh-group/">Dan Alistarh</a> (IST Austria), we are launching <a href="https://sites.google.com/view/one-world-seminar-series-flow/home">FLOW: Federated Learning One World Seminar.</a>
The seminar will take place on a weekly basis on Wednesdays. All talks will be delivered via Zoom. The first few talks are: <br> <br>
May 13, <a href="https://rka97.github.io">Ahmed Khaled</a> (Cairo): On the Convergence of Local SGD on Identical and Heterogeneous Data <br> <br>
May 20, <a href="http://ttic.uchicago.edu/~blake/">Blake Woodworth</a> (TTIC): Is Local SGD Better than Minibatch SGD? <br> <br>
May 27, <a href="http://papail.io">Dimitris Papailiopoulos</a> (Wisconsin Madison): Robustness in Federated Learning May be Impossible Without an All-knowing Central Authority
 <br> <br>
June 3, No talk due to NeurIPS deadline <br> <br>
June 10, <a href="https://people.epfl.ch/sai.karimireddy">Sai Praneeth Karimireddy</a> (EPFL): Stochastic Controlled Averaging for Federated Learning <br> <br>
June 17, <a href="https://fhanzely.github.io">Filip Hanzely</a> (KAUST): Federated Learning of a Mixture of Global and Local Models: Local SGD and Optimal Algorithms <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>May 3, 2020</h3>
<h1>Talk at the Montréal Machine Learning and Optimization Seminar</h1>

<br>
On Friday this week (May 8), I will give a talk entitled "On Second Order Methods and Randomness" at the  <a href="https://mtl-mlopt.github.io">Montréal Machine
Learning and Optimization (MTL MLOpt) Seminar</a>. This is an online seminar delivered via Google Meet. Starting time: 9am PDT.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>









<h3>April 25, 2020</h3>
<h1>Talk at the One World Optimization Seminar</h1>

<br>
I will give a talk within the <a href="https://owos.univie.ac.at">One World Optimization Seminar</a>
series on Monday, April 27, at 3pm CEST. This is a new exciting initiative, and my talk is only the
second in the series. I will speak about some new results related to second order methods and randomness.
One of the advantages of this new format is that anyone can attend - indeed, attendance is via
<a href="https://zoom.us">Zoom</a>. However, you need to register online in advance in order to get access.
Hope to "see" many of you there!
<br><br>

Update (April 29): The <a href="https://owos.univie.ac.at/fileadmin/user_upload/k_owos/Peter_Richtarik-Stochastic_Newton.pdf">slides</a> and <a href="https://www.youtube.com/watch?v=HGQkct3db-c">video recording</a> of my talk are now available.
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 21, 2020</h3>
<h1>Filip Hanzely Accepted a Position at TTIC</h1>
<br>

<a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> accepted a Research Assistant Professorship at <a href="https://www.ttic.edu">Toyota Technological Institute at Chicago</a> (TTIC).
Filip has written his thesis and will submit it soon. He is expected to graduate this Summer, and will start his new position in Chicago in the Fall. Filip has obatined multiple other offers besides this, including a Tenure-Track Assistant Professorship and a Postdoctoral Fellowship in a top machine learning group.

<br><br>
Congratulations!
<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 20, 2020</h3>
<h1>ICML, UAI and COLT Author Response Deadlines</h1>
<br>

I am busy: <a href="https://icml.cc">ICML</a> and <a href="http://www.auai.org/uai2020/call_for_papers.php">UAI</a> rebuttal deadline is today, and for
<a href="http://learningtheory.org/colt2020/">COLT</a> the deadline is on April 24.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 7, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2004.02635">"Dualize, Split, Randomize: Fast Nonsmooth Optimization Algorithms"</a> -
joint work with <a href="https://adil-salim.github.io">Adil Salim</a>, <a href="https://lcondat.github.io">Laurent Condat</a>, and <a href="https://konstmish.github.io">Konstantin Mishchenko</a>.<br>
<br>

Abstract: <i>
We introduce a new primal-dual algorithm for minimizing the sum of three convex functions, each of which has its own oracle. Namely, the first one is differentiable, smooth and possibly stochastic, the second is proximable, and the last one is a composition of a proximable function with a linear map. Our theory covers several settings that are not tackled by any existing algorithm; we illustrate their importance with real-world applications. By leveraging variance reduction, we obtain convergence with linear rates under strong convexity and fast sublinear convergence under convexity assumptions. The proposed theory is simple and unified by the umbrella of stochastic Davis-Yin splitting, which we design in this work. Finally, we illustrate the efficiency of our method through numerical experiments.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 5, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2004.02163">"On the Convergence Analysis of Asynchronous SGD for Solving Consistent Linear Systems"</a> -
joint work with <a href="https://sands.kaust.edu.sa/authors/atal-narayan-sahu/">Atal Narayan Sahu</a>, <a href="https://www.aritradutta.com">Aritra Dutta</a>, and Aashutosh Tiwari.<br>
<br>

Abstract: <i>
In the realm of big data and machine learning, data-parallel, distributed stochastic algorithms have drawn significant attention in the present days. While the synchronous versions of these algorithms are well
understood in terms of their convergence, the convergence analyses of their asynchronous counterparts are not widely studied.
In this paper, we propose and analyze a  distributed, asynchronous parallel SGD method in light of solving an arbitrary consistent linear system by reformulating the system into a stochastic optimization problem as
studied by Richtárik and Takáč in [35]. We compare the convergence rates of our asynchronous SGD algorithm with the
synchronous parallel algorithm proposed by Richtárik and Takáč in [35] under different choices of the hyperparameters---the stepsize, the damping factor, the number of processors, and the delay factor.
We show that our asynchronous parallel SGD algorithm also enjoys a global linear convergence rate, similar to the "basic method" and the synchronous parallel method in [35] for solving any arbitrary consistent
linear system via stochastic reformulation. We also show that our asynchronous parallel SGD improves upon the "basic method" with a better convergence rate when the number of processors is larger than four.
We further show that this asynchronous approach performs asymptotically better than its synchronous counterpart for certain linear systems. Moreover, for certain linear systems, we compute the minimum number of
processors required for which our asynchronous parallel SGD is better, and find that this number can be as low as two for some ill-conditioned problems.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>April 3, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2004.01442">"From Local SGD to Local Fixed Point Methods for Federated Learning"</a> -
joint work with Grigory Malinovsky, <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="http://elnurgasanov.me">Elnur Gasanov</a>, and <a href="https://lcondat.github.io">Laurent Condat</a>.<br>
<br>

Abstract: <i>
Most algorithms for solving optimization problems or finding saddle points of convex-concave functions are fixed point algorithms.
In this work we consider the generic problem of finding a fixed point of an average of operators, or an approximation thereof,
in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models
the computations done locally on a mobile device. We investigate two strategies to achieve such a consensus: one based on a fixed
number of local steps, and the other based on randomized computations. In both cases, the goal is to limit communication of the
locally-computed variables, which is often the bottleneck in distributed frameworks. We perform convergence analysis of both methods
and conduct a number of experiments highlighting the benefits of our approach.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 10, 2020</h3>
<h1>Area Chair for NeurIPS 2020</h1>
<br>

I will serve as an Area Chair for <a href="https://nips.cc/Conferences/2020/">NeurIPS 2020</a>, to be
held during December 6-12, 2020 in Vancouver, Canada (same location as last year). For those not in the know,
Google Scholar Metrics says that NeurIPS is the #1 conference in AI:
<br>
<br>
<a href="https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=eng_artificialintelligence"> <img alt="" src="imgs/GoogleScholarMetrics-AI.png" width="700" ></a>
<br>
<br>
 The review process has changed this year; here is a short and beautifully produced video explaining the key 5 changes:
 <br>
<br>
<a href="https://www.youtube.com/watch?v=361h6lHZGDg&feature=share"> <img alt="" src="imgs/5changes.jp2" width="700" ></a>
<br>


<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>March 9, 2020</h3>
<h1>Coronavirus at KAUST</h1>
<br>

No, Covid-19 did not catch up with anyone at KAUST yet. Still in luck. However, as in many places, its increasing omnipresence and gravitational pull is felt here as well.

<br>
<br>
For example, as of today, all KAUST lectures are moving online. And for a good reason I think: we have seen in Lombardy what the virus can do when unchecked.
I am teaching my CS 390T Federated Learning course on Sundays (yes - the work week in Saudi spans Sunday-Thursday) and Tuesdays, and hence my first online lecture will take place on Sunday March 15. I hope, at least, as I need
to decide how best to do it.
<br>
<br>

Conference travel has been limited for some time now, but the rules are even  more strict now. This seems less than necessary
as conferences drop like flies anyway. My planned travel between now and May includes a seminar talk at EPFL (Switzerland), a workshop
keynote lecture at King Faisal University (Al-Ahsa, Saudi Arabia), presentation at ICLR (Addis Ababa, Ethiopia), and SIAM Conference on Optimization (Hong Kong)
which I am helping to organize. Most of these events are cancelled, and those that survive will most probably go to sleep soon.

<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 27, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.12410">"On Biased Compression for Distributed Learning"</a> -
joint work with <a href="https://scholar.google.ru/citations?user=hVVJR-sAAAAJ&hl=en">Aleksandr Beznosikov</a>,
<a href="https://samuelhorvath.github.io">Samuel Horváth</a>, and <a href="https://mher-safaryan.github.io">Mher Safaryan</a>.<br>
<br>

Abstract: <i>
In the last few years, various communication  compression techniques have  emerged  as an indispensable  tool helping to alleviate the communication bottleneck in distributed learning.
However, despite the fact biased compressors often show superior performance in practice when compared to the much more studied and understood unbiased compressors,  very little
is known about them. In this work we study three classes of biased compression operators, two of which are new, and their performance when applied to  (stochastic) gradient descent
and distributed (stochastic) gradient descent.  We show for the first time that biased compressors can lead to linear convergence rates both in the single node and distributed settings.
Our distributed SGD method enjoys the ergodic rate $O \left( \frac{\delta L \exp(-K) }{\mu}  + \frac{(C + D)}{K\mu} \right)$, where $\delta$ is a compression parameter which grows when
more compression is applied, $L$ and $\mu$ are the smoothness and strong convexity constants, $C$ captures stochastic gradient noise ($C=0$ if full gradients are computed on each node)
and $D$ captures the variance of the gradients at the optimum ($D=0$ for over-parameterized models).  Further,  via a theoretical study of several synthetic and empirical distributions
of communicated gradients, we shed light on why and by how much  biased compressors outperform  their unbiased variants.  Finally, we  propose a new highly performing biased
compressor---combination of Top-$k$ and natural dithering---which in our experiments outperforms all other compression techniques.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 26, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.11364">"Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization"</a> -
joint work with <a href="https://zhizeli.github.io">Zhize Li</a>,
 <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>, and <a href="https://qianxunk.github.io">Xun Qian</a>.<br>
<br>

Abstract: <i>
Due to the high communication cost in distributed and federated learning problems, methods relying on compression of communicated messages are becoming increasingly popular. While in other contexts the best performing gradient-type methods invariably rely on some form of acceleration/momentum to reduce the number of iterations, there are no methods which combine the benefits of both gradient compression and acceleration. In this paper, we remedy this situation and propose the first accelerated compressed gradient descent (ACGD) methods.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 26, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.11337">"Fast Linear Convergence of Randomized BFGS"</a> -
joint work with <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
 <a href="https://gowerrobert.github.io">Robert M. Gower</a>, and <a href="https://scholar.google.com/citations?hl=ru&user=sEjyzkgAAAAJ">Alexander Rogozin</a>.<br>
<br>

Abstract: <i>
Since the late 1950’s when quasi-Newton methods first appeared, they have become one of the most widely used and
efficient algorithmic paradigms for unconstrained optimization. Despite their immense practical success, there is
little theory that shows why these methods are so efficient. We provide a semi-local rate of convergence for the
randomized BFGS method which can be significantly better than that of gradient descent, finally giving theoretical
evidence supporting the superior empirical performance of the method.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 21, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.09526">"Stochastic Subspace Cubic Newton Method"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>,
 <a href="https://scholar.google.ru/citations?user=YNBhhjUAAAAJ&hl=en">Nikita Doikov</a> and <a href="https://en.wikipedia.org/wiki/Yurii_Nesterov">Yurii Nesterov</a>.<br>
<br>

Abstract: <i>
In this paper, we propose a new randomized second-order optimization algorithm---Stochastic Subspace Cubic Newton (SSCN)---for minimizing a high
dimensional convex function $f$. Our method can be seen both as astochastic extension of the cubically-regularized Newton method of Nesterov
and Polyak (2006), and a second-order enhancement of stochastic subspace descent of Kozak et al. (2019). We prove that as we vary the
minibatch size, the global convergence rate of SSCN interpolates between the rate of stochastic coordinate descent (CD) and the rate of cubic
regularized Newton, thus giving new insights into the connection between first and second-order methods. Remarkably, the local convergence rate
of SSCN matches the rate of stochastic subspace descent applied to the problem of minimizing the quadratic function
$\frac{1}{2} (x-x^*)^\top \nabla^2 f(x^*)(x-x^*)$, where $x^*$ is the minimizer of $f$, and hence depends on the properties of $f$ at the optimum only.
Our numerical experiments show that SSCN outperforms non-accelerated first-order CD algorithms while being competitive to their accelerated variants.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 20, 2020</h3>
<h1>New MS/PhD Student: Egor Shulgin</h1>
<br>

Egor Vladimirovich Shulgin is back at KAUST - now as an MS/PhD student. Welcome!!! <br><br>

Egor has co-authored 4 papers and a book (in Russian) entitled <a href="https://arxiv.org/abs/1907.01060">"Lecture Notes on Stochastic Processes"</a>.
Here are the papers, in reverse chronological order:<br><br>
- <a href="https://arxiv.org/abs/2002.08958">Uncertainty principle for communication compression in distributed and federated
learning and the search for an optimal compressor </a> <br>
- Adaptive catalyst for smooth convex optimization <br>
- <a href="https://arxiv.org/abs/1905.11373">Revisiting stochastic extragradient</a> (AISTATS 2020) <br>
- <a href="http://proceedings.mlr.press/v97/qian19b">SGD: general analysis and improved rates</a> (ICML 2019) <br>
<br>

Egor has a bachelor degree in Applied Mathematics from the Department of
Control and Applied Mathematics at MIPT, Dolgoprudny, Russia. He majored in
Data Analysis. His CV mentions the following as his main subjects: Probability
Theory, Random Processes, Convex Optimization, and Machine Learning.<br>
<br>


Egor’s hobbies, according to his CV, are: hiking, alpine skiing, tennis,
and judo. Notably, this list does not include table tennis. However, I
know for a fact that he is very good in it!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 20, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.08958">"Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor"</a> -
joint work with <a href="https://mher-safaryan.github.io">Mher Safaryan</a> and <a href="https://scholar.google.com/citations?user=XlmSx18AAAAJ&hl=en">Egor Shulgin</a>.<br>
<br>

Abstract: <i>
In order to mitigate the high communication cost in  distributed and federated learning, various vector compression schemes, such as
quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as
few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion
(variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of
communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more
distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators,
thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication
compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first
step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call
{\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension
independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry.
We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication
complexity improvements on previous state of the art.
</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 14, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.05516">"Federated Learning of a Mixture of Global and Local Models"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>.<br>
<br>

Abstract: <i>We propose a new optimization formulation for training federated learning models. The standard
  formulation has the form of an empirical risk minimization problem constructed to find a single global
  model trained from the private data stored across all participating devices. In contrast, our formulation
  seeks an explicit trade-off between this traditional global model and the local models, which can be learned
  by each device from its own private data without any communication. Further, we develop several efficient
  variants of SGD (with and without partial participation and with and without variance reduction) for solving
  the new formulation and prove communication complexity guarantees. Notably, our methods are similar but not
  identical to federated averaging / local SGD, thus shedding some light on the essence of the elusive method.
  In particular, our methods do not perform full averaging steps and instead merely take steps towards averaging.
  We argue for the benefits of this new paradigm for federated learning.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 12, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.05359">"Adaptivity of Stochastic Gradient Methods for Nonconvex Optimization"</a> -
joint work with <a href="https://samuelhorvath.github.io">Samuel Horváth</a>,
<a href="https://statistics.berkeley.edu/people/lihua-lei">Lihua Lei</a> and
<a href="https://people.eecs.berkeley.edu/~jordan/">Michael I. Jordan</a>.<br>
<br>

Abstract: <i>Adaptivity is an important yet under-studied property in modern optimization theory.
  The gap between the state-of-the-art theory and the current practice is striking in that algorithms
  with desirable theoretical guarantees typically involve drastically different settings of hyperparameters,
  such as step-size schemes and batch sizes, in different regimes. Despite the appealing theoretical results,
  such divisive strategies provide little, if any, insight to practitioners to select algorithms that work broadly
  without tweaking the hyperparameters. In this work, blending the "geometrization" technique introduced by
  Lei & Jordan 2016 and the SARAH algorithm of Nguyen et al., 2017, we propose the Geometrized SARAH
  algorithm for non-convex finite-sum and stochastic optimization. Our algorithm is proved to achieve adaptivity
  to both the magnitude of the target accuracy and the Polyak-Łojasiewicz (PL) constant if present. In addition,
  it achieves the best-available convergence rate for non-PL objectives simultaneously while outperforming
  existing algorithms for PL objectives.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 11, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.04670">"Variance Reduced Coordinate Descent with Acceleration: New Method With a
  Surprising Application to Finite-Sum Problems"</a> -
joint work with <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a> and <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>.<br>
<br>

Abstract: <i>We propose an accelerated version of stochastic variance reduced coordinate descent -- ASVRCD. As other
variance reduced coordinate descent methods such as SEGA or SVRCD, our method can deal with problems that include a
non-separable and non-smooth regularizer, while accessing a random block of partial derivatives in each iteration only.
However, ASVRCD incorporates Nesterov's momentum, which offers favorable iteration complexity guarantees over both SEGA
and SVRCD. As a by-product of our theory, we show that a variant of Allen-Zhu (2017) is a specific case of ASVRCD,
recovering the optimal oracle complexity for the finite sum objective.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 10, 2020</h3>
<h1>Konstantin Giving a Series of Talks in the US and UK</h1>

<br>
<a href="https://konstmish.github.io/">Konstantin Mishchenko</a> is giving several talks in New York, London and Oxford. Here is his schedule:
<br>
<br>
<br>

February 10, Facebook Research, New York, "Adaptive Gradient Descent Without Descent"<br><br>
February 12, Deepmind, London, "Adaptive Gradient Descent Without Descent"<br><br>
February 12, UCL Gatsby Computational Neuroscience Unit, London, "Sinkhorn Algorithm as a Special Case of Stochastic Mirror Descent" <br><br>
February 14, Oxford University, Oxford, "Adaptive Gradient Descent Without Descent" <br><br>
February 14, Imperial College London, London, "Sinkhorn Algorithm as a Special Case of Stochastic Mirror Descent" <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>February 9, 2020</h3>
<h1>New Paper</h1>
<br>

<span class="important">New paper out:</span>
<a href="https://arxiv.org/abs/2002.03329">"Better Theory for SGD in the Nonconvex World"</a> -
joint work with <a href="https://rka97.github.io">Ahmed Khaled</a>.<br>
<br>

Abstract: <i>Large-scale nonconvex optimization problems are ubiquitous in modern machine learning,
and among practitioners interested in solving them, Stochastic Gradient Descent (SGD)
reigns supreme. We revisit the analysis of SGD in the nonconvex setting and propose a new
variant of the recently introduced expected smoothness assumption which governs the behaviour
of the second moment of the stochastic gradient. We show that our assumption is both more general
and more reasonable than assumptions made in all prior work. Moreover, our results yield the
optimal $O(\epsilon^{-4})$ rate for finding a stationary point of nonconvex smooth functions, and
recover the optimal $O(\epsilon^{-1})$ rate for finding a global solution if the Polyak-Łojasiewicz
condition is satisfied. We compare against convergence rates under convexity and prove a theorem
on the convergence of SGD under Quadratic Functional Growth and convexity, which might be of
independent interest. Moreover, we perform our analysis in a framework which allows for a detailed
study of the effects of a wide array of sampling strategies and minibatch sizes for finite-sum
optimization problems. We corroborate our theoretical results with experiments on real and
synthetic data.</i> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 8, 2020</h3>
<h1>Interview by Robin.ly for their Leaders in AI Platform</h1>

<br>
In December last year, while attending NeurIPS in Vancouver, I was interviewed by Robin.ly.
The video can be found here<br><br>

<a href="https://www.robinly.info/post/peter-richtarik-kaust-machine-learning-federated-learning-kaust"><img alt="" src="imgs/Leaders_in_AI_video.png" width="600"> </a><br><br>

and a podcast out of this is on soundcloud: <br><br> <br>

<a href="https://soundcloud.com/robinly/podcast-peter-richtarik"><img alt="" src="imgs/Leaders_in_AI_podcast.png" width="600"> </a><br><br>

I am in excellent company:<br><br>
<a href="https://soundcloud.com/robinly/yoshua-bengio-from-deep-learning-to-consciousness-interview-by-song-han-neurips-2019">Yoshua Bengio</a><br>
<a href="https://soundcloud.com/robinly/kai-fu-lee-the-era-of-ai-the-rise-of-china-the-future-of-work">Kai-Fu Lee</a><br>
<a href="https://soundcloud.com/robinly/the-future-of-distributed-machine-learning-max-welling-u-of-amsterdam-qualcomm-neurips-2019">Max Welling</a><br>
<a href="https://soundcloud.com/robinly/my-first-cvpr-christopher-manning-professor-director-stanford-ai-lab">Christopher Manning</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>




<h3>February 8, 2020</h3>
<h1>AAAI in New York</h1>

<br> <a href="https://konstmish.github.io">Konstantin</a> is about to receive a Best Reviewer Award at AAAI 20 in New York. <a href="http://www.adelbibi.com">Adel</a> is presenting our paper
 <a href="https://arxiv.org/abs/1902.01272">"A stochastic derivative-free optimization method with importance sampling"</a>,
 joint work with El Houcine Bergou, Ozan Sener, Bernard Ghanem and myself, at the event.</br> <br>


 Update (May 3): Here is a <a href="https://cemse.kaust.edu.sa/vcc/news/kaust-cs-student-recognized-aaai-outstanding-program-committee-award">KAUST article about Konstantin and his achievements.</a> I am very proud.

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>February 6, 2020</h3>
<h1>ICML Deadline Today!</h1>

<br>I am a walking zombie, a being without a soul, a sleepless creature of the night. Do not approach me or
you will meet your destiny. Wait three days and I shall be alive again. </br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>February 3, 2020</h3>
<h1>Samuel in San Diego</h1>

<br><a href="https://samuelhorvath.github.io">Samuel Horváth</a>
is in San Diego, and will soon be presenting the paper <a href="https://arxiv.org/abs/1901.08689">"Don’t jump through hoops and remove those loops:
SVRG and Katyusha are better without the outer loop"</a>, joint work with Dmitry Kovalev and myself,
at <a href="http://alt2020.algorithmiclearningtheory.org">ALT 2020</a> in San Diego.<br><br> Here is the
list of <a href="http://alt2020.algorithmiclearningtheory.org/accepted-papers/">all accepted papers.</a> <br><br>
Samuel will be back at KAUST in about 10 days.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>February 2, 2020</h3>
<h1>Eduard Gorbunov is Visting Again</h1>

<br><a href="https://eduardgorbunov.github.io/index.html#header3-a">Eduard Gorbunov</a> came for a research visit - this is his third time at KAUST. This time, he wil stay for about 2 months.

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>January 31, 2020</h3>

<h1>Poster for AAAI-20</h1>
<br>
Our paper <a href="https://arxiv.org/abs/1902.01272">"A stochastic derivative-free optimization
  method with importance sampling"</a>, joint work with Adel Bibi, El Houcine Bergou, Ozan Sener, and Bernard Ghanem,
  will be presented at <a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a>, which will be held in New York
  during February 7-12. <br><br>

We have just prepared a poster, here it is:<br>
<br>
<a href="posters/Poster-STP_IS-AAAI20.pdf"><img src="posters/Poster-STP_IS-AAAI20_small.png" alt="STP" border="0" height="500"></a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 20, 2019</h3>
<h1>Paper Accepted to SIMAX</h1>

<br>The paper <a href="https://arxiv.org/abs/1706.01108">"Stochastic reformulations of linear systems: algorithms and convergence theory"</a>, joint work with Martin Takáč, was accepted to <a href="https://www.siam.org/publications/journals/siam-journal-on-matrix-analysis-and-applications-simax"> SIAM Journal on Matrix Analysis and Applications.</a><br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>



<h3>January 18, 2020</h3>
<h1>New Intern: Alexander Rogozin</h1>

<br>A new intern arrived today: Alexander Rogozin. Alexander is a final-year MSc student in the department of Control and Applied Mathematics at the <a href="https://mipt.ru/english/">Moscow Institute of Physics and Technology (MIPT).</a> <br>

<br>Some notable achievements of Alexander so far:<br>

<br>
- Co-authored <a href="https://scholar.google.com/citations?hl=en&amp;user=sEjyzkgAAAAJ">3 papers</a> in the area of decentralized optimization over time varying networks<br>
- His GPA ranks him among the top 5% students at MIPT<br>
- Tutor of Probability Theory at MIPT, 2018-now<br>
- Finished the <i>Yandex School for Data Analysis</i> (2017-2019)<br>
- Awardee of the Russian National Physics Olympiad, 2013<br>
- Certificate of Honor at Russian National Mathematics Olympiad, 2012<br>
- Winner of Russian National Physics Olympiad, 2012<br>

<br>
In 2018, Alexander participated in the Moscow Half-Marathon.
He is a holder of 4-kyu in Judo. Having studied the piano for
11 years, Alexander participated in city, regional, national
and international musical festivals and competitions. He
performed with a symphony orchestra as a piano soloist at
festivals in his hometown.
<br>
<br>
Welcome!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>January 16, 2020</h3>
<h1>Konstantin Mishchenko Among the Best Reviewers for AAAI-20</h1>
<br> Congratulations <a href="https://konstmish.github.io/">Kostya</a>!
(AAAI Conference on Artificial Intelligence is <a
href="https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_artificialintelligence">one of the top AI conferences</a>. The email below tells the
story.)<br> <br>
<i>
Dear Konstantin, <br>
<br>
On behalf of the Association for the Advancement of
Artificial Intelligence and the AAAI-20 Program Committee,
we are pleased to inform you that you have been selected as
one of 12 Outstanding Program Committee members for 2020 in
recognition of your outstanding service on this year's
committee. Your efforts were characterized by exceptional
care, thoroughness, and thoughtfulness in the reviews and
discussions of the papers assigned to you. <br>
<br>
In recognition of your achievement, you will be
presented with a certificate by the AAAI-20 Program
Cochairs, Vincent Conitzer and Fei Sha, during the AAAI-20
Award Ceremony on Tuesday, February 11 at 8:00am. There will
also be an announcement of this honor in the program. Please
let us know (aaai20@aaai.org) if you will be present at the
award ceremony to accept your award. <br>
<br>
Congratulations, and we look forward to seeing you in New York for AAAI-20, February 7-12. <br>
<br>
Warmest regards, <br>
<br>
Carol McKenna Hamilton<br>
Executive Director, AAAI <br>
<br>
for<br>
<br>
Vincent Conitzer and Fei Sha<br>
AAAI-20 Program Cochairs <br>
</i>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>


<h3>January 13, 2020</h3>
<h1>Konstantin Visiting Francis Bach's Group at INRIA</h1>

<br>
<a href="https://konstmish.github.io/">Konstantin Mishchenko</a>
is visiting the <a href="https://www.di.ens.fr/sierra/">SIERRA machine learning lab</a> at INRIA, Paris,
led by <a href="https://www.di.ens.fr/%7Efbach/">Francis Bach</a>. He will give a talk on January 14
entitled "Adaptive Gradient Descent Without Descent" and based on
<a href="https://arxiv.org/abs/1910.09529">this paper.</a> <br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

<h3>January 9, 2020</h3>
<h1>New Intern: Aleksandr Beznosikov</h1>
<br>
A new intern arrived today: Aleksandr Beznosikov. Aleksandr is a final-year BSc student in Applied Mathematics and Physics at <a href="https://mipt.ru/english/">Moscow Institute of Physics and Technology (MIPT).</a> <br>
<br>
Some notable achievements of Aleksandr so far:<br>
<br>
- paper <a href="https://arxiv.org/abs/1911.10645">"Derivative-Free Method For Decentralized Distributed Non-Smooth Optimization"</a>, joint work with Eduard Gorbunov and Alexander Gasnikov<br>
- Increased State Academic Scholarship for 4 year bachelor and master students at MIPT, 2018-2019 <br>
- Author of problems and organizer of the student olympiad in discrete mathematics, 2018-2019 <br>
- Abramov's Scholarship for students with the best grades at MIPT, 2017-2019<br>
- First Prize at MIPT's team mathematical tournament, 2017 <br>
- Silver Medal at International Experimental Physics Olympiad,
2015<br>
- Russian President’s Scholarship for High School Students, 2014-2015 <br>
- Prize-Winner, All-Russian School Physics Olympiad, Final Round, 2014 and 2015 <br>
- Winner, All-Russian School Programming Olympiad, Region Round, 2015-2016 <br>
- Winner, All-Russian School Physics Olympiad, Region Round, 2014-2016 <br>
- Winner, All-Russian School Maths Olympiad, Region Round, 2014-2016 <br>
<br>
Welcome!<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="36">
<br>
<br>

          <h3>January 7, 2020</h3>
          <h1>Four Papers Accepted to AISTATS 2020</h1>
          <br>
          Some of the first good news of 2020: We've had four papers
          accepted to <a href="https://www.aistats.org/">The 23rd
            International Conference on Artificial Intelligence and
            Statistics (AISTATS 2020)</a>, which will be held during
          June 3-5, 2020, in Palermo, Sicily, Italy. Here they are:<br>
          <br>
          1) <a href="https://arxiv.org/abs/1905.11261">"A unified
            theory of SGD: variance reduction, sampling, quantization
            and coordinate descent"</a> - joint work with <a
            href="https://eduardgorbunov.github.io/">Eduard Gorbunov</a>
          and <a href="https://fhanzely.github.io/index.html">Filip
            Hanzely.</a> <br>
          <br>
          Abstract: <i>In this paper we introduce a unified analysis of
            a large family of variants of proximal stochastic gradient
            descent (SGD) which so far have required different
            intuitions, convergence analyses, have different
            applications, and which have been developed separately in
            various communities. We show that our framework includes
            methods with and without the following tricks, and their
            combinations: variance reduction, importance sampling,
            mini-batch sampling, quantization, and coordinate
            sub-sampling. As a by-product, we obtain the first unified
            theory of SGD and randomized coordinate descent (RCD)
            methods, the first unified theory of variance reduced and
            non-variance-reduced SGD methods, and the first unified
            theory of quantized and non-quantized methods. A key to our
            approach is a parametric assumption on the iterates and
            stochastic gradients. In a single theorem we establish a
            linear convergence result under this assumption and
            strong-quasi convexity of the loss function. Whenever we
            recover an existing method as a special case, our theorem
            gives the best known complexity result. Our approach can be
            used to motivate the development of new useful methods, and
            offers pre-proved convergence guarantees. To illustrate the
            strength of our approach, we develop five new variants of
            SGD, and through numerical experiments demonstrate some of
            their properties. </i><br>
          <br>
          2) "Tighter theory for local SGD on identical and
          heterogeneous data" - joint work with <a
            href="https://rka97.github.io">Ahmed Khaled</a> and <a
            href="https://konstmish.github.io">Konstantin Mishchenko.</a>
          <br>
          <br>
          Abstract: <i>We provide a new analysis of local SGD, removing
            unnecessary assumptions and elaborating on the difference
            between two data regimes: identical and heterogeneous. In
            both cases, we improve the existing theory and provide
            values of the optimal stepsize and optimal number of local
            iterations. Our bounds are based on a new notion of variance
            that is specific to local SGD methods with different data.
            The tightness of our results is guaranteed by recovering
            known statements when we plug $H=1$, where $H$ is the number
            of local steps. The empirical evidence further validates the
            severe impact of data&nbsp; heterogeneity on the&nbsp;
            performance of local SGD. </i><br>
          <br>
          3) <a href="https://arxiv.org/abs/1905.11373">"Revisiting
            stochastic extragradient"</a> - joint work with <a
            href="https://konstmish.github.io">Konstantin Mishchenko</a>,
          <a href="https://www.dmitry-kovalev.com">Dmitry Kovalev</a>,
          Egor Shulgin and <a
            href="https://people.epfl.ch/yurii.malitskyi">Yura Malitsky</a>.
          <br>
          <br>
          Abstract: <i>We consider a new extension of the extragradient
            method that is motivated by approximating implicit updates.
            Since in a recent work of Chavdarova et al (2019) it was
            shown that the existing stochastic extragradient algorithm
            (called mirror-prox) of Juditsky et al (2011) diverges on a
            simple bilinear problem, we prove guarantees for solving
            variational inequality that are more general than in </i><i><i>Juditsky
et al (2011)</i>. Furthermore, we illustrate numerically
            that the proposed variant converges faster than many other
            methods on the example of</i><i><i> Chavdarova et al (2019)</i>.
            We also discuss how extragradient can be applied to training
            Generative Adversarial Networks (GANs). Our experiments on
            GANs demonstrate that the introduced approach may make the
            training faster in terms of data passes, while its higher
            iteration complexity makes the advantage smaller. To further
            accelerate method's convergence on problems such as bilinear
            minimax, we combine the extragradient step with negative
            momentum Gidel et al (2018) and discuss the optimal momentum
            value. </i><br>
          <br>
          4) <a href="https://arxiv.org/abs/1906.00506">"DAve-QN: A
            distributed averaged quasi-Newton method with local
            superlinear convergence rate"</a> - work of S. Soori, <a
            href="https://konstmish.github.io">K. Mishchenko</a>, A.
          Mokhtari, M. Dehnavi, and M. Gürbüzbalaban.<br>
          <br>
          Abstract: <i>In this paper, we consider distributed
            algorithms for solving the empirical risk minimization
            problem under the master/worker communication model. We
            develop a distributed asynchronous quasi-Newton algorithm
            that can achieve superlinear convergence. To our knowledge,
            this is the first distributed asynchronous algorithm with
            superlinear convergence guarantees. Our algorithm is
            communication-efficient in the sense that at every iteration
            the master node and workers communicate vectors of size
            O(p), where p is the dimension of the decision variable. The
            proposed method is based on a distributed asynchronous
            averaging scheme of decision vectors and gradients in a way
            to effectively capture the local Hessian information of the
            objective function. Our convergence theory supports
            asynchronous computations subject to both bounded delays and
            unbounded delays with a bounded time-average. Unlike in the
            majority of asynchronous optimization literature, we do not
            require choosing smaller stepsize when delays are huge. We
            provide numerical experiments that match our theoretical
            results and showcase significant improvement comparing to
            state-of-the-art distributed algorithms. </i> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h3>January 7, 2020</h3>
          <h1>Visiting ESET in Bratislava</h1>
          <br>
          I am on a visit to <a href="https://www.eset.com/int/">ESET</a>
          - a leading internet security company headquartered in
          Bratislava, Slovakia. I have given a couple of talks on
          stochastic gradient descent and have spoken to several very
          interesting people. <br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h3>January 5, 2020</h3>
          <h1>Filip Visiting Francis Bach's Group at INRIA</h1>
          <br>
          <a href="https://fhanzely.github.io/index.html">Filip Hanzely</a>
          is visiting the <a href="https://www.di.ens.fr/sierra/">SIERRA machine learning lab</a> at INRIA, Paris, led by Francis
          Bach. He will give a talk on January 7 entitled "One method to
          rule them all: variance reduction for data, parameters and
          many new methods", and <a
            href="https://arxiv.org/abs/1905.11266">based on a paper of
            the same title</a>. <a
href="https://fhanzely.github.io/wp-content/uploads/2020/01/jacsketch.pdf">Here are his slides.</a><br>

          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h3>January 5, 2020</h3>
          <h1>New Intern: Grigory Malinovsky</h1>
          <br>
          A new intern arrived today: Grigory Malinovsky from <a
            href="https://mipt.ru/english/">Moscow Institute of Physics
            and Technology (MIPT).</a> Grigory wrote his BS thesis
          "Averaged Heavy Ball Method" under the supervision of Boris
          Polyak. He is now pursuing an MS degree at MIPT in Machine
          Learning.<br>
          <br>
          Among Grigory's successes belong:<br>
          <br>
          - Abramov's cholarship for tudents with the best grades at
          MIPT, 2016<br>
          - Participant in the final round of All-Russian Physics
          Olympiad, 2014<br>
          - Bronze medal at International Zhautykov Olympiad in Physics,
          2014<br>
          - Prize winner in the final round of All-Russian Physics
          Olympiad, 2013<br>
          <br>
          Welcome!<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="36">
          <br>
          <br>

          <h1> Old News</h1>
          <br>
          <a href="i_oldnews.html">Read old news</a> (2017 and earlier)<br>
          <br>
        </div>


<div id="sidebar">




<h6>[2021]  I am on the Advisory Board of <a href="https://cemse.kaust.edu.sa/ai">KAUST AI Initiative.</a>
</h6>
<br>
<br>


<h6>[5/2020]  We have launched <a
href="https://sites.google.com/view/one-world-seminar-series-flow/home">FLOW: Federated Learning One World Seminar</a>.
</h6>
<br>
<br>


<h6>[11/2019] A KAUST Discovery article <a
href="https://discovery.kaust.edu.sa/en/article/892/machine-learning-models-gather-momentum">"Machine Learning Models Gather Momentum"</a>.
</h6>
<br>
<br>


<h6>[10/2019] A KAUST Discovery article <a
href="https://discovery.kaust.edu.sa/en/article/880/less-chat-leads-to-more-work-for-machine-learning">"Less Chat Leads to More Work for Machine Learning"</a>.
</h6>
<br>
<br>


<h6>[9/2019] A KAUST Discovery article <a
href="https://discovery.kaust.edu.sa/en/article/879/speeding-up-the-machine-learning-process">"Speeding up the Machine Learning Process"</a>.
</h6>
<br>
<br>


<h6>[10/2019] I have delivered a 5hr mini-course on Stochastic
Gradient Descent at Moscow Institute of Physics and
Technology (MIPT), entitled <a
href="https://youtu.be/a05S0kL5u30">"A Guided Walk Through
the ZOO of Stochastic Gradient Descent Methods".</a> The
lectures are now on YouTube.
</h6>
<br>
<br>


<h6>[8/2019] Sign up for my <a
href="https://piazza.com/kaust.edu.sa/fall2019/cs390ff/home">CS 390FF (Big Data Optimization) class</a> (Fall 2019). </h6>
<br>
<br>

<h6>[9/2018] A <a
href="https://www.youtube.com/watch?v=gjgEck0zU7w&amp;list=PLgKuh-lKre13E9dXSsif4KsGoFp4bjubd&amp;index=9">YouTube video</a> of my talk at the Simons Institute on the
JacSketch algorithm. </h6>
<br>
<br>

<h6> [8/2017] Video recording of my 5 hour mini-course on
"Randomized Optimization Methods" delivered at the Data
Science Summer School (DS3) at École Polytechnique: Parts <a
href="https://comm.medias.polytechnique.fr/videos/richtarik_1_randonopti/">1</a>,
<a
href="https://comm.medias.polytechnique.fr/videos/richtarik_2_randonopti/">2</a>,
<a
href="https://comm.medias.polytechnique.fr/videos/richtarik_3_randonopti_08033/">3</a>,
<a
href="https://comm.medias.polytechnique.fr/videos/richtarik_4_randonoptimization/">4</a>,
<a
href="https://comm.medias.polytechnique.fr/videos/richtarik_5_randonoptimization/">5</a>
</h6>
<br>
<br>


<h6> [3/2017] I have taken up an Associate Professor position
at <a href="https://www.kaust.edu.sa/en">KAUST</a>. I am on
leave from Edinburgh. </h6>
<br>
<br>


<h6> [12/2016] Video from my <a
href="https://events.yandex.ru/lib/talks/4294/">talk at
Yandex </a>entitled "Empirical Risk Minimization:
Complexity, Duality, Sampling, Sparsity and Big Data". </h6>
<br>
<br>


<h6> [10/2016] My Alan Turing Institute talk on Stochastic
Dual Ascent for Solving Linear Systems is now on <a
href="https://www.youtube.com/watch?v=RbkhWrTbrKs">YouTube</a>.
</h6>
<br>
<br>


<h6>[9/2015] <a
href="talks/2015-09-Toulouse-Summer-School-Optimization.pdf">Slides</a>
from a 6hr course on "Optimization in Machine Learning",
Toulouse, France. </h6>
<br>
<br>


<h6> [7/2015] ICML Tutorial (joint with Mark Schmidt) on <span
class="important">Modern Convex Optimization Methods for
Large-scale ERM:</span> <a
href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_I.pdf">Part I</a>, <a
href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_II.pdf">Part II</a> </h6>
<br>
<br>


<h6> [2/2014] A <a
href="https://www.youtube.com/watch?v=0sHOfqhCZw0">YouTube
video</a> of a talk on the APPROX algorithm delivered at
PreMoLab in Moscow. </h6>
<br>
<br>


<h6> [10/2013] A <a
href="http://www.youtube.com/watch?v=IQgnstB0n2E#t=538">YouTube video</a> of a talk I gave at the Simons Institute
workshop on <a
href="http://simons.berkeley.edu/workshops/bigdata2013-2">parallel
and distributed optimization and inference.</a> </h6>



              </div>

        <div style="clear: both;"> </div>
      </div>
    </div>
  </body>
</html>
