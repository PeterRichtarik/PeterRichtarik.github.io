<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <link rel="stylesheet" href="style.css">
    <title>Peter Richtarik</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-37355274-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-37355274-2');
    </script>



  </head>
  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <script src="table_header.js"></script> </div>
      </div>
      <ul class="menu">
        <li><a href="index.html">News</a></li>
        <li><a class="active" href="i_oldnews.html">Old News</a></li>
        <li><a href="i_papers.html">Papers</a></li>
        <li><a href="i_talks.html">Talks</a></li>
        <li><a href="i_events.html">Events</a></li>
        <li><a href="i_seminar.html">Seminar</a></li>
        <li><a href="i_software.html">Code</a></li>
        <li><a href="i_team.html">Team</a></li>
        <li><a href="i_join.html">Join</a></li>
        <li><a href="i_bio.html">Bio</a></li>
        <li><a href="i_teaching.html">Teaching</a></li>
        <li><a href="i_consulting.html">Consulting</a></li>
      </ul>
      <div id="wrapper" class="main">
        <div id="content">





          <h3>September 8, 2017</h3>
          <br>
          I am co-organizing the workshop "Sparse Approximation and
          Sampling" which is to be held in London sometime in May or
          June 2019. The precise dates will be fixed soon. This is a
          joint event of the <a href="https://www.newton.ac.uk/">Isaac
            Newton Institute</a> and <a
            href="https://www.turing.ac.uk/">The Alan Turing Institute</a>.
          This is one of three workshops which are part of a 6 month
          programme on "Approximation, sampling and compression in high
          dimensional problems" held at the Isaac Newton Institute
          during January-June 2019.
          <ul>
            <br>
            <li>Workshop organizers: <a
                href="http://ece.duke.edu/faculty/robert-calderbank">Robert










































                Calderbank</a> (Duke, USA), <a
                href="http://www.damtp.cam.ac.uk/research/afha/anders/">Anders










































                Hansen</a> (Cambridge, UK),<br>
              <a href="http://www.maths.ed.ac.uk/%7Eprichtar/">Peter
                Richtarik</a> (KAUST, KSA - Edinburgh, UK - The Alan
              Turing Institute, UK), <a
                href="https://www.ee.ucl.ac.uk/%7Euceemrd/">Miguel
                Rodrigues</a> (UCL, UK).<br>
            </li>
          </ul>
          <ul>
            <li>Programme Scientific Advisory Committee: <a
                href="http://ece.duke.edu/faculty/robert-calderbank">Robert










































                Calderbank</a> (Duke, USA), <a
                href="https://statweb.stanford.edu/%7Ecandes/">Emmanuel
                Candes</a> (Stanford, USA), <a
                href="http://www.math.tamu.edu/%7Erdevore/">Ronald
                DeVore</a> (Texas A&amp;M, USA), <a
                href="https://math.duke.edu/people/ingrid-daubechies">Ingrid












                Daubechies</a> (Duke, USA), <a
href="http://www.damtp.cam.ac.uk/user/ai/Arieh_Iserles/Arieh_Iserles.html">Arieh












                Iserles</a> (Cambridge, UK)</li>
          </ul>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>September 1, 2017</h3>
          <br>
          I am now back at <a href="https://www.kaust.edu.sa/en">KAUST</a>.
          The <a href="https://en.wikipedia.org/wiki/Eid_al-Adha">Eid
            al-Adha</a> holiday started yesterday. I am looking forward
          to a bit of rest (or "stayvacation", as spending vacation at
          KAUST as opposed to somewhere else is called).<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>August 29, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/papers/pdfixedpoint.pdf">"The


















            complexity of primal-dual fixed point methods for ridge
            regression"</a> - joint work with <a
            href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">Ademir















































            Ribeiro</a> (Federal University of Paraná).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">We study
            the ridge regression ($L_2$ regularized least squares)
            problem and its dual, which is also a ridge regression
            problem. We observe that the optimality conditions
            describing the primal and dual optimal solutions can be
            formulated in several different but equivalent ways. The
            optimality conditions we identify form a linear system
            involving a structured matrix depending on a single
            relaxation parameter which we introduce for regularization
            purposes. This leads to the idea of studying and comparing,
            in theory and practice, the performance of the fixed point
            method applied to these reformulations. We compute the
            optimal relaxation parameters and uncover interesting
            connections between the complexity bounds of the variants of
            the fixed point scheme we consider. These connections follow
            from a close link between the spectral properties of the
            associated matrices. For instance, some reformulations
            involve purely imaginary eigenvalues; some involve real
            eigenvalues and others have all eigenvalues on the complex
            circle. We show that the deterministic Quartz method--which
            is a special case of the randomized dual coordinate ascent
            method with arbitrary sampling recently developed by Qu,
            Richtarik and Zhang--can be cast in our framework, and
            achieves the best rate in theory and in numerical
            experiments among the fixed point methods we study. </span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>August 28, 2017</h3>
          <br>
          I have arrived to Paris. I am attending the <a
            href="http://www.ds3-datascience-polytechnique.fr">Data
            Science Summer School (DS3)</a> organized by <a
            href="http://www.ds3-datascience-polytechnique.fr/organizers/">Ecole















































            Polytechnique.</a> I am giving a 5 hour minicourse on
          Randomized Optimization Methods (<a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/talks/RandOpt-DS3.pdf">here


















            are the slides</a>).<br>
          <br>
          Some event stats (copy pasted from the event website):<br>
          <br>
          400 participants<br>
          220 students (MSc, PhD) &amp; postdocs, 100 professionals<br>
          16 experts (speakers, guests)<br>
          30 countries<br>
          6 continents<br>
          200 institutions<br>
          50 companies<br>
          6 sponsors<br>
          120 posters<br>
          female : male ratio = 3 : 10<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>August 20, 2017</h3>
          <br>
          The first day of the Fall 2017 semester at KAUST is today. I
          am teaching CS390FF: Selected Topics in Data Sciences (Big
          Data Optimization). <br>
          <br>
          <span style="font-style: italic;">Update (Sept 8):</span> 26
          students are enrolled in the course.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>August 13, 2017</h3>
          <br>
          I am back at <a href="https://www.kaust.edu.sa/en">KAUST</a>
          now. The Fall 2017 semester is starting in a week.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>August 10, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/papers/PET-SPDHGM.pdf">"Faster


















            PET reconstruction with a stochastic primal-dual hybrid
            gradient method"</a> - joint work with <a
            href="http://www.cmap.polytechnique.fr/%7Eantonin/">Antonin
            Chambolle</a> (Ecole Polytechnique), <a
            href="http://www.damtp.cam.ac.uk/user/me404/">Matthias J.
            Ehrhardt</a> (Cambridge), and <a
            href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">Carola-Bibiane















































            Schoenlieb</a> (Cambridge).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">Image
            reconstruction in positron emission tomography (PET) is
            computationally challenging due to Poisson noise,
            constraints and potentially non-smooth priors—let alone the
            sheer size of the problem. An algorithm that can cope well
            with the first three of the aforementioned challenges is the
            primal-dual hybrid gradient algorithm (PDHG) studied by
            Chambolle and Pock in 2011. However, PDHG updates all
            variables in parallel and is there- fore computationally
            demanding on the large problem sizes encountered with modern
            PET scanners where the number of dual variables easily
            exceeds 100 million. In this work, we numerically study the
            usage of SPDHG—a stochastic extension of PDHG—but is still
            guaranteed to converge to a solution of the deterministic
            optimization problem with similar rates as PDHG. Numerical
            results on a clinical data set show that by introducing
            randomization into PDHG, similar results as the
            deterministic algorithm can be achieved using only around
            10% of operator evaluations. Thus, making significant
            progress towards the feasibility of sophisticated
            mathematical models in a clinical setting.</span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>August 5, 2017</h3>
          <br>
          I have just arrived in Sydney, Australia - I am attending <a
            href="https://2017.icml.cc">ICML</a>. Looking forward to the
          excellent program!<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>July 10, 2017</h3>
          <br>
          I am reviewing <a href="https://nips.cc">NIPS</a> papers this
          week.<br>
          <br>
          <span style="font-style: italic;">Update (after rebuttal): </span>It's















































          never a good strategy for authors to deny obvious issues
          raised by the reviewers simply do not exist. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>July 3, 2017</h3>
          <br>
          <span class="important"></span><a
            href="http://jakubkonecny.com">Jakub's</a> PhD thesis is <a
            href="https://arxiv.org/abs/1707.01155">now on arXiv.</a><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>July 3, 2017</h3>
          <br>
          <span class="important"></span>I am on my way to the Fields
          Institute, Toronto, to attend a workshop entitled <a
            href="http://www.fields.utoronto.ca/activities/17-18/AN70">"Modern















































            Convex Optimization and Applications: AN70"</a>. This event
          is organized in honour of Arkadi Nemirovski's 70th birthday.
          Arkadi is one of the most influential individuals in
          optimization, directly resposible for the existence of several
          of its most important and most beautiful subfields. Here is a
          <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/keynote.html">very















































            brief profile</a> of this giant of our beloved field, taken
          from the website of a workshop I co-organized in Edinburgh in
          2015.<br>
          <br>
          Update (July 5, 2017): I have given my talk today, <a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/talks/TALK-2017-FieldsInstitute-AN70.pdf">here


















            are the slides.</a> <br>
          <br>
          Update (July 7, 2017): Filip delivered his pitch talk and
          presented his poster "Extending the Reach of Big Data
          Optimization: Randomized Algorithms for Minimizing Relatively
          Smooth Functions".<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>July 2, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1707.00281">"A batch-incremental
            video background estimation model using weighted low-rank
            approximation of matrices"</a> - joint work with Aritra
          Dutta (KAUST) and Xin Li (University of Central Florida).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">Principal
            component pursuit (PCP) is a state-of-the-art approach for
            background estimation problems. Due to their higher
            computational cost, PCP algorithms, such as robust principal
            component analysis (RPCA) and its variants, are not feasible
            in processing high definition videos. To avoid the curse of
            dimensionality in those algorithms, several methods have
            been proposed to solve the background estimation problem in
            an incremental manner. We propose a batch-incremental
            background estimation model using a special weighted
            low-rank approximation of matrices. Through experiments with
            real and synthetic video sequences, we demonstrate that our
            method is superior to the state-of-the-art background
            estimation algorithms such as GRASTA, ReProCS, incPCP, and
            GFL. </span><br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>June 27, 2017</h3>
          <br>
          <span class="important">IMA Fox Prize (2nd Prize) for Robert
            Mansel Gower <br>
            <br>
          </span><a href="http://www.di.ens.fr/%7Ergower/">Robert M.
            Gower<span style="font-style: italic;"></span></a> was
          awarded a Leslie Fox Prize (2nd Prize) by the <a
            href="https://ima.org.uk">Institute of Mathematics and its
            Applications (IMA)</a> for the paper <a
            href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">Randomized















































            Iterative Methods for Linear Systems (SIAM J. Matrix Anal.
            &amp; Appl., 36(4), 1660–1690, 2015)</a>, coauthored with
          me. The list of finalists can be found <a
            href="http://people.maths.ox.ac.uk/wathen/fox/shortlist.php">here.</a>
          <br>
          <br>
          The Leslie Fox Prize for Numerical Analysis of the Institute
          of Mathematics and its Applications (IMA) is a biennial prize
          established in 1985 by the IMA in honour of mathematician
          Leslie Fox (1918-1992). The prize honours "young numerical
          analysts worldwide" (any person who is less than 31 years
          old), and applicants submit papers for review. A committee
          reviews the papers, invites shortlisted candidates to give
          lectures at the Leslie Fox Prize meeting, and then awards
          First Prize and Second Prizes based on "mathematical and
          algorithmic brilliance in tandem with presentational skills".<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>June 26, 2017</h3>
          <br>
          <a href="http://jakubkonecny.com">Jakub Konečný</a> defended
          his PhD thesis <span style="font-style: italic;">"Stochastic,
            Distributed and Federated Optimization for Machine Learning"</span>
          today. Congratulations!<br>
          <br>
          Jakub joined my group as a PhD student in August 2013. His PhD
          was in his first year supported by the Principal's Career
          Development Scholarship, and in the subsequent years by a
          Google Europe Doctoral Fellowship in Optimization Algorithms.
          Jakub has co-authored 13 papers during his PhD (<a
href="https://scholar.google.com/citations?hl=en&amp;user=4vq7eXQAAAAJ&amp;view_op=list_works">links















































            to the papers can be found here</a>). He has worked on
          diverse topics such as distributed optimization, machine
          learning, derivative-free optimization, federated learning,
          gesture recognition, semi-stochastic methods and
          variance-reduced algorithms for empirical risk minimization.
          He is joining Google Seattle in August 2017 as a research
          scientist.<br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>June 21, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1706.07636">"Privacy Preserving
            Randomized Gossip Algorithms"</a> - joint work with <a
            href="http://filiphanzely.com">Filip Hanzely</a>
          (Edinburgh), <a href="http://jakubkonecny.com">Jakub Konečný</a>
          (Edinburgh), <a href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas















































            Loizou</a> (Edinburgh) and Dmitry Grishchenko (Higher School
          of Economics, Moscow).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">In this
            work we present three different randomized gossip algorithms
            for solving the average consensus problem while at the same
            time protecting the information about the initial private
            values stored at the nodes. We give iteration complexity
            bounds for all methods, and perform extensive numerical
            experiments. </span> <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>June 18, 2017</h3>
          <br>
          I am now in Slovakia, visiting <a
            href="http://www.iam.fmph.uniba.sk/ospm/Harman/index.htm">Radoslav
















            Harman</a> at <a href="http://uniba.sk/en/">Comenius
            University</a>. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>June 15, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1706.04957">"Stochastic
            primal-dual hybrid gradient algorithm with arbitrary
            sampling and imaging applications"</a> - joint work with <a
            href="http://www.cmap.polytechnique.fr/%7Eantonin/">Antonin
            Chambolle</a> (Ecole Polytechnique), <a
            href="http://www.damtp.cam.ac.uk/user/me404/">Matthias J.
            Ehrhardt</a> (Cambridge), and <a
            href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">Carola-Bibiane
















            Schoenlieb</a> (Cambridge).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">We propose
            a stochastic extension of the primal-dual hybrid gradient
            algorithm studied by Chambolle and Pock in 2011 to solve
            saddle point problems that are separable in the dual
            variable. The analysis is carried out for general
            convex-concave saddle point problems and problems that are
            either partially smooth / strongly convex or fully smooth /
            strongly convex. We perform the analysis for arbitrary
            samplings of dual variables, and obtain known deterministic
            results as a special case. Several variants of our
            stochastic method significantly outperform the deterministic
            variant on a variety of imaging tasks.</span> <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>June 4, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1706.01108">"Stochastic
            reformulations of linear systems: algorithms and convergence
            theory"</a> - joint work with <a href="http://mtakac.com">Martin















































            Takáč</a> (Lehigh).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">We develop
            a&nbsp; family of reformulations of an arbitrary consistent
            linear system into&nbsp; a stochastic problem. The
            reformulations are governed by two user-defined parameters:
            a positive definite matrix defining a norm, and an arbitrary
            discrete or continuous distribution over random matrices.
            Our reformulation has several equivalent interpretations,
            allowing for researchers from various communities to
            leverage their domain specific insights. In particular, our
            reformulation can be equivalently seen as a stochastic
            optimization problem, stochastic linear system, stochastic
            fixed point problem and a probabilistic intersection
            problem. We prove sufficient, and necessary and sufficient
            conditions for the reformulation to be exact. <br>
            <br>
            Further, we propose and analyze three stochastic algorithms
            for solving the reformulated problem---basic, parallel and
            accelerated methods---with global linear convergence rates.
            The rates can be interpreted as&nbsp; condition numbers of a
            matrix which depends on the system matrix and on the
            reformulation parameters. This gives rise to a new
            phenomenon&nbsp; which we call stochastic preconditioning,
            and which refers to the problem of finding parameters
            (matrix and distribution) leading to a sufficiently small
            condition number. Our basic method can be equivalently
            interpreted as&nbsp; stochastic gradient descent, stochastic
            Newton method, stochastic proximal point method, stochastic
            fixed point method, and stochastic projection method,&nbsp;
            with fixed stepsize (relaxation parameter), applied to the
            reformulations. </span><br>
          <br>
          Comment: I have taught a course at the University of Edinburgh
          in Spring 2017 which was largely based on the results in this
          paper. <br>
          <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>June 3, 2017</h3>
          <br>
          I am now back at KAUST to welcome <a
            href="https://scholar.google.com/citations?user=vquoiHsAAAAJ&amp;hl=en">Aritra















































            Dutta</a> who just joined my group at KAUST as a postdoc.
          Aritra: welcome! <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>May 26, 2017</h3>
          <br>
          I am in Edinburgh now - I'll be here until May 30. I am then
          giving a talk at Plymouth University on May 31 and at Cardiff
          University on June 1st. I'll be in London on June 2nd.<br>
          <br>
          Update (June 4): <a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/papers/stochastic_reformulations.pdf">This


















            is the paper</a> I talked about in Plymouth and Cardiff. <br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <br>
          <br>
          <h3>May 20, 2017</h3>
          <br>
          I am in Vancouver as of today, attending the <a
            href="http://www.siam.org/meetings/op17/">SIAM Conference on
            Optimization</a>. I am giving a talk on Monday, May 22
          (stochastic reformulations of linear systems), and so is <a
            href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas Loizou</a>
          (stochastic heavy ball method) and <a
            href="http://filiphanzely.com">Filip Hanzely</a> (randomized
          methods for minimizing relatively smooth functions).
          Strangely, none of these three papers are online yet! <a
            href="http://www.di.ens.fr/%7Ergower/">Robert Gower</a> is
          giving his talk on Tuesday (sketch and project: a tool for
          designing stochastic quasi-Newton methods and stochastic
          variance reduced gradient methods). The first part of the talk
          is based on <a href="https://arxiv.org/abs/1602.01768">this</a>
          and <a href="http://proceedings.mlr.press/v48/gower16.html">this</a>
          paper, the variance reduction part is also new and not online
          yet. <a href="http://jakubkonecny.com">Jakub Konečný</a> on
          Wednesday (<a href="https://arxiv.org/abs/1608.06879">AIDE:
            fast and communication efficient distributed optimization</a>).<br>
          <br>
          Update (June 4): <a
href="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/papers/stochastic_reformulations.pdf">This


















            is the paper</a> I talked about in Vancouver. Here are the
          talk slides.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>

          <h3>May 15, 2017</h3>
          <br>
          <a href="http://mtakac.com">Martin Takáč</a> is visiting me at
          <a href="https://www.kaust.edu.sa/en">KAUST</a> this week.<br>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="20">
</br>
</br>


          <h3>May 10, 2017</h3>
          <br>
          <span class="important">New Approach to AI: Federated Learning
          </span> <br>
            <br> Standard machine learning approaches require
          centralizing the training data on one machine or in a
          datacenter. For models trained from user interaction with
          mobile devices, a new approach was just released by Google, a
          result of collaboration between Google, <a
            href="http://jakubkonecny.com">Jakub Konečný</a> and myself.
          <br>
          <br>
          The new approach is called Federated Learning; it is described
          in the following four paper:<br>
          <br>
          [1] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio
          Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage,
          Aaron Segal and Karn Seth<br>
          <a href="http://eprint.iacr.org/2017/281">Practical Secure
            Aggregation for Privacy Preserving Machine Learning</a>
          (3/2017)<br>
          <br>
          [2] Jakub Konečný, H. Brendan McMahan, Felix X. Yu, Peter
          Richtárik, Ananda Theertha Suresh, Dave Bacon<br>
          <a href="https://arxiv.org/abs/1610.05492">Federated Learning:
            Strategies for Improving Communication Efficiency</a>
          (10/2016)<br>
          <br>
          [3] Jakub Konečný, H. Brendan McMahan, Daniel Ramage, Peter
          Richtárik<br>
          <a href="https://arxiv.org/abs/1610.02527">Federated
            Optimization: Distributed Machine Learning for On-Device
            Intelligence</a> (10/2016)<br>
          <br>
          [4] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth
          Hampson, Blaise Agüera y Arcas<br>
          <a href="https://arxiv.org/abs/1602.05629">Communication-Efficient Learning of Deep Networks from Decentralized Data</a>
          (2/2016)<br>
          <br>
          Federated Learning enables mobile phones to collaboratively
          learn a shared prediction model while keeping all the training
          data on device, decoupling the ability to do machine learning
          from the need to store the data in the cloud. This goes beyond
          the use of local models that make predictions on mobile
          devices by bringing model training to the device as well. The
          technology is now in use by around 1 billion Android devices.<br>
          <br>
          The CEO of Google, Sundar Pichai, <a
            href="http://avondaleam.com/alphabet-1q17-earnings-call-notes/">said:</a>
          <br>
          <br>
          <span style="font-style: italic;">“… we continue to set the
            pace in machine learning and AI research. We introduced a
            new technique for training deep neural networks on mobile
            devices called&nbsp;Federated Learning. This technique
            enables people to run a shared machine learning model, while
            keeping the underlying data stored locally on mobile
            phones." </span><br>
          <br>
          The&nbsp;new technology&nbsp;is described in a <a
href="https://research.googleblog.com/2017/04/federated-learning-collaborative.html">Google Research Blog, dated April 2017,</a> to a lay audience.
          Selected media coverage:<br>
          <br>
          <ul>
            <li><a
href="https://www.forbes.com/sites/kevinmurnane/2017/04/11/google-makes-your-smartphone-smarter-with-federated-learning/#7c1d9eb25a0f">Forbes</a></li>
            <li><a
href="https://www.theverge.com/2017/4/10/15241492/google-ai-user-data-federated-learning">The Verge</a></li>
            <li><a
href="https://qz.com/952986/google-goog-may-be-using-your-phones-keyboard-to-train-its-massive-ai-brain/?utm_source=dlvr.it&amp;utm_medium=twitter">Quartz</a></li>
            <li><a
href="http://www.techrepublic.com/article/how-google-wants-to-crowdsource-machine-learning-with-smartphones-and-federated-learning/">TechRepublic</a></li>
            <li><a
href="http://www.zdnet.com/article/smarter-android-ai-powered-google-services-will-get-better-as-you-use-them/">ZDNet</a></li>
            <li><a
href="http://www.cbronline.com/news/mobility/smartphones/google-wants-make-android-smarter-ai-algorithms-phone/">Computer Business Review</a></li>
            <li><a
href="https://motherboard.vice.com/en_us/article/google-thinks-it-can-solve-artificial-intelligences-privacy-problem">Motherboard Vice</a></li>
            <li><a
href="http://www.infoworld.com/article/3188430/artificial-intelligence/android-gboard-smartens-up-with-federated-machine-learning.html">Infoworld</a></li>
            <li><a href="http://www.silicon.co.uk">Silicon.co.uk</a> </li>
            <li><a
href="https://venturebeat.com/2017/04/06/following-apple-google-tests-differential-privacy-in-gboard-for-android/">Venturebeat</a></li>
            <li><a
href="https://www.engadget.com/2017/04/07/gboard-studies-your-behavior-without-sending-details-to-google/">Engadget</a></li>
            <li><a
href="http://www.technarratives.com/2017/04/06/google-develops-federated-machine-learning-method-which-keeps-personal-data-on-devices/">Tech Narratives</a><br>
            </li>
            <li><a
href="https://android.gadgethacks.com/news/google-introduces-ai-for-its-android-services-learns-from-you-without-compromising-privacy-0177003/">GadgetHacks</a><br>
            </li>
            <li><a
href="http://bgr.com/2017/04/10/googles-new-ai-doesnt-need-to-talk-to-the-cloud/">BGR</a></li>
            <li><a
href="http://www.androidauthority.com/google-machine-learning-privacy-federated-learning-762978/">AndroidAuthority</a></li>
            <li><a
href="https://www.androidheadlines.com/2017/04/google-intros-ai-solution-based-on-federated-learning.html">AndroidHeadlines</a></li>
            <li><a
href="http://www.tomsguide.com/us/google-federated-learning-autocorrect-improvements,news-24853.html">Tom's










































                Guide</a></li>
            <li><a
href="https://www.digitaltrends.com/mobile/google-federated-learning-android/">Digital










































                Trends</a>
            </li>
            <li><a
href="https://www.getrevue.co/profile/azeem/issues/publishing-in-the-platform-globalisation-google-s-deep-learning-chip-panpsychism-elephants-poo-graphene-108-52456">The










































                Exponential View</a></li>
            <li><a href="http://www.vvcat.com/view/187072.html">vvcat</a>
            </li>
            <li><a
href="https://9to5google.com/2017/04/06/google-testing-new-differential-privacy-strategy-with-gboard-for-android/">9to5google</a>
            </li>
          </ul>


          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <br>
          <h3>May 9, 2017</h3>
          <br>
          <span class="important">New paper out: </span><a
            href="https://arxiv.org/abs/1705.02005">"Parallel stochastic
            Newton method"</a> - joint work with Mojmír Mutný (ETH
          Zurich).<br>
          <span class="important"> <br>
          </span>Abstract: <span style="font-style: italic;">We propose
            a parallel stochastic Newton method (PSN) for minimizing
            smooth convex functions. We analyze the method in the
            strongly convex case, and give conditions under which
            acceleration can be expected when compared to it serial
            stochastic Newton. We show how PSN can be applied to the
            empirical risk minimization problem, and demonstrate the
            practical efficiency of the method through numerical
            experiments and models of simple matrix classes.</span><br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <br>
          <h3>May 6, 2017</h3>
          <br>
          I am hosting two interns from <a href="http://iitk.ac.in">Indian















































            Institute of Technology, Kanpur</a> this Summer at <a
            href="https://www.kaust.edu.sa/en">KAUST</a>; they just
          arrived: <a
            href="https://www.linkedin.com/in/aashutosh-tiwari/?ppe=1">Aashutosh















































            Tiwari</a> and <a
            href="https://www.linkedin.com/in/atal-narayan-a517ba104/">Atal















































            Narayan</a>. Welcome!<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <br>
          <h3>May 2, 2017</h3>
          <br>
          <span class="important">Most Downloaded SIMAX Paper</span><br>
          <br>
          The paper <a
            href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">"Randomized















































            Iterative Methods for Linear Systems"</a>, coauthored with <a
            href="http://www.di.ens.fr/%7Ergower/">Robert M. Gower</a>
          and published in 2015, is now the <a
href="http://epubs.siam.org/action/showMostReadArticles?journalCode=sjmael&amp;">Most















































            Downloaded Paper in the SIAM Journal on Matrix Analysis and
            Applications</a>. <br>
          <br>
          The paper was the Second Most Downloaded Paper since at least
          August 2016 when Robert noticed this and brought it to my
          attention. We have just noticed it climbed up to the #1
          position. Thanks for all the interest in our work!<br>
          <br>
          For those who want to pursue the line of work initiated in our
          paper, we recommend looking at the following follow-up papers
          where we address several extensions and obtain various
          additional insights and improvements:<br>
          <br>
          1) Stochastic dual ascent for solving linear systems, <a
            href="https://arxiv.org/abs/1512.06890">arXiv:1512.06890</a><br>
          <br>
          Here we lift the full rank assumption from our original SIMAX
          paper. In doing so, we discover a particularly beautiful
          duality theory behind the method. This also leads to the
          design of a novel stochastic method in optimization, which we
          call Stochastic Dual Ascent (SDA). With a bit of hindsight -
          we should have called it Stochastic Dual Subspace Ascent
          (SDSA).<br>
          <br>
          2) Randomized quasi-Newton updates are linearly convergent
          matrix inversion algorithms, <a
            href="https://arxiv.org/abs/1602.01768">arXiv:1602.01768</a><br>
          <br>
          The basic idea behind this paper is to apply a similar
          methodology we used to solve linear systems in the SIMAX paper
          to the problem of computing an inverse of a (large) matrix.
          I'll simply copy-paste the abstract here:<br>
          <br>
          <span style="font-style: italic;">We develop and analyze a
            broad family of stochastic/randomized algorithms for
            inverting a matrix. We also develop specialized variants
            maintaining symmetry or positive definiteness of the
            iterates. All methods in the family converge globally and
            linearly (i.e., the error decays exponentially), with
            explicit rates. In special cases, we obtain stochastic block
            variants of several quasi-Newton updates, including bad
            Broyden (BB), good Broyden (GB), Powell-symmetric-Broyden
            (PSB), Davidon-Fletcher-Powell (DFP) and
            Broyden-Fletcher-Goldfarb-Shanno (BFGS). Ours are the first
            stochastic versions of these updates shown to converge to an
            inverse of a fixed matrix. Through a dual viewpoint we
            uncover a fundamental link between quasi-Newton updates and
            approximate inverse preconditioning. Further, we develop an
            adaptive variant of randomized block BFGS, where we modify
            the distribution underlying the stochasticity of the method
            throughout the iterative process to achieve faster
            convergence. By inverting several matrices from varied
            applications, we demonstrate that AdaRBFGS is highly
            competitive when compared to the well established
            Newton-Schulz and minimal residual methods. In particular,
            on large-scale problems our method outperforms the standard
            methods by orders of magnitude. Development of efficient
            methods for estimating the inverse of very large matrices is
            a much needed tool for preconditioning and variable metric
            optimization methods in the advent of the big data era.</span><br>
          <br>
          3) Stochastic block BFGS: squeezing more curvature out of
          data, <a href="http://proceedings.mlr.press/v48/gower16.html">ICML















































            2016</a><br>
          <br>
          In this work we apply the stochastic block BFGS method
          developed in the above paper to empirical risk minimization.
          Of course, much more is needed than just a straightforward
          application - but this is the initial idea behind the work.<br>
          <br>
          4) Linearly convergent randomized iterative methods for
          computing the pseudoinverse, <a
            href="https://arxiv.org/abs/1612.06255">arXiv:1612.06255</a><br>
          <br>
          Here we show that after suitable insights and modifications,
          the iterative sketching framework for inverting matrices from
          the "quasi-Newton" paper above can be used to compute the
          Moore-Penrose pseudoinverse of arbitrary (rectangular) real
          matrices. Extension to the complex setting is possible, but we
          did not do it.<br>
          <br>
          5) Soon I will post a new paper on ArXiv which will go much
          deeper than the SIMAX paper - this work will represent what is
          to the best of my knowledge the deepest insight into the
          sketch-and-project we have at the moment. <br>
          <br>
          <span style="font-style: italic;">Update (18.6.2017):</span>
          The paper I mentioned in item 5 above is <a
            href="https://arxiv.org/abs/1706.01108">now on arXiv</a>. <br>
          <br>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 19, 2017</h3>
          <br>
          <a href="http://www.dominikcsiba.com">Dominik Csiba</a> is
          giving a talk entitled "The role of optimization in machine
          learning" at the <a href="http://www.mlmu.sk/program/">Machine















































            Learning MeetUp (MLMU) in Bratislava today (at 7pm)</a>. If
          you are around and interested in machine learning and/or
          optimization, I recommend you attend!<br>
          <br>
          <span style="font-style: italic;">Update (May 26, 2017):</span>
          A <a href="https://youtu.be/fCzxhe66kKc">video recording of
            Dominik's talk is now online</a>.<br>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 17, 2017</h3>
          <br>
          <a
            href="https://www.semanticscholar.org/author/Alibek-Sailanbayev/2476427">Alibek















































            Sailanbayev</a> is visiting me at <a
            href="https://www.kaust.edu.sa/en">KAUST</a> this week.<br>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 10, 2017</h3>
          <br>
          <div align="left">I am giving 2 talks this week. Today, I am
            giving a talk on stochastic Chambolle-Pock algorithm at the
            <a
              href="https://vcc.kaust.edu.sa/Conference-2017/Pages/default.aspx">Visual















































              Computing Conference</a> held at KAUST. My PhD students <a
              href="http://www.jakubkonecny.com">Jakub</a>, <a
              href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas</a>
            and <a href="http://filiphanzely.com">Filip </a>are
            presenting posters tomorrow. On Thursday (April 13), I am
            giving a talk on randomized algorithms in linear algebra at
            the <a
href="https://cemse.kaust.edu.sa/events/Pages/AMCS-Graduate-Peter-Richtarik.aspx">AMCS















































              Graduate Seminar</a> at KAUST.<br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 9, 2017</h3>
          <br>
          <div align="left">I have several visitors at KAUST at the
            moment. <a href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas















































              Loizou</a> arrived in late March and is staying until
            early may. <a href="http://filiphanzely.com">Filip Hanzely</a>
            and <a href="http://jakubkonecny.com">Jakub Konečný</a>
            arrived yesterday; Jakub will stay for a couple weeks and
            Filip will stay until late May, after which all of will go
            to Vancouver for <a
              href="http://www.siam.org/meetings/op17/">SIAM Conference
              on Optimization</a>. <a
              href="https://angel.co/konstantin-mishchenko">Konstantin
              Mishchenko</a> (Paris Dauphine / Ecole Normale Superieure)
            visited for a few days recently, and is now attending the <a
href="http://ljk.imag.fr/membres/Jerome.Malick/osl2017/people.html">OSL
              2017 workshop</a> in Les Houches, France. Dmitry I.
            Grishchenko (Higher School of Economics) arrived yesterday
            and is staying for 2 weeks.<br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 6, 2017</h3>
          <br>
          <div align="left"><span class="important">#1 Trending Paper in
              Mathematical Programming (Series A and B)</span><br>
            <br>
            About a week ago I have received an email (see a screenshot
            below) in which Springer notifies the optimization community
            about the top 5 trending articles in the Mathematical
            Programming (Series A and B) journals. It was a great
            surprise (and pleasure) to learn that our paper <a
              href="http://link.springer.com/article/10.1007/s10107-015-0901-6">Parallel















































              coordinate descent methods for big data optimization</a>
            (coauthored with <a href="http://www.mtakac.com">Martin
              Takáč</a>) is <span class="important">#1 on the list!</span><br>
            <br>
            <img style="width: 700px; height: 947px;" alt="Top 5
              trending articles in Mathematical Programming, Series A
              and B"
src="file:///Users/richtap/Documents/WORK/@WEBSITE/My%20Website/imgs/2017-MAPR-trending-papers.png"><br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>March 29, 2017</h3>
          <br>
          <div align="left"><a href="http://www.di.ens.fr/%7Ergower/">Robert















































              Gower's</a> paper <a
              href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">Randomized
Iterative















































              Methods for Linear Systems (SIAM Journal on Matrix
              Analysis and Applications 36(4):1660-1690, 2015)</a>,
            written in collaboration with me, was shortlisted for the <a
href="http://people.maths.ox.ac.uk/wathen/fox/shortlist.php">18th Leslie
              Fox Prize in Numerical Analysis</a>. <br>
            <br>
            This paper has been the <a
href="http://epubs.siam.org/action/showMostReadArticles?journalCode=sjmael&amp;">2nd
most















































              downloaded SIMAX paper</a> since (at least) August 2016.
            The first most downloaded paper was published in year 2000.<br>
            <br>
            The Leslie Fox Prize for Numerical Analysis of the <a
              href="https://ima.org.uk">Institute of Mathematics and its
              Applications (IMA)</a> is a biennial prize established in
            1985 by the IMA in honour of mathematician <a
              href="https://en.wikipedia.org/wiki/Leslie_Fox">Leslie Fox
              (1918-1992)</a>. The prize honours "young numerical
            analysts worldwide" (any person who is less than 31 years
            old), and applicants submit papers for review. A committee
            reviews the papers, invites shortlisted candidates to give
            lectures at the Leslie Fox Prize meeting, and then awards
            First Prize and Second Prizes based on "mathematical and
            algorithmic brilliance in tandem with presentational
            skills".<br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>March 17, 2017</h3>
          <br>
          <div align="left">As of today, <a
              href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
              Loizou</a> is visiting me at <a
              href="https://www.kaust.edu.sa/en">KAUST</a>. He will stay
            until early May.<br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>March 14, 2017</h3>
          <br>
          <div align="left"><a href="http://filiphanzely.com/">Filip
              Hanzely</a> is giving a talk in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big















































              data optimization seminar</a> today. He will speak on <a
              href="https://arxiv.org/abs/1611.01146">"Finding
              Approximate Local Minima for Nonconvex Optimization in
              Linear Time"</a>.<br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>March 13, 2017</h3>
          <br>
          <div align="left">I will be giving a tutorial on randomized
            optimization methods at the <a
              href="http://www.ds3-datascience-polytechnique.fr">Data
              Science Summer School</a>, held at CMAP, École
            Polytechnique, during August 28 - September 1, 2017. <br>
            <br>
            Full list of tutorials:<br>
            <br>
            - <a
              href="http://www.iro.umontreal.ca/%7Ebengioy/yoshua_en/index.html">Yoshua















































              Bengio</a> (Montreal): Deep Learning<br>
            - <a href="https://www.cs.cmu.edu/%7Epradeepr/">Pradeep
              Ravikumar</a> (Carnegie Mellon): Graphical Models<br>
            - Peter Richtarik (Edinburgh &amp; KAUST): Randomized
            Optimization Methods<br>
            - <a href="https://sites.ualberta.ca/%7Eszepesva/">Csaba
              Szepesvari</a> (Alberta): Bandits<br>
            <br>
            Plenary speakers:<br>
            <br>
            - <a href="https://www.linkedin.com/in/carchambeau/">Cedric
              Archambeau</a> (Amazon)<br>
            - <a
              href="https://research.google.com/pubs/OlivierBousquet.html">Olivier















































              Bousquet</a> (Google)<br>
            - <a href="http://blogs.ulg.ac.be/damien-ernst/">Damien
              Ernst</a> (Liege)<br>
            - <a href="https://who.rocq.inria.fr/Laura.Grigori/">Laura
              Grigori</a> (INRIA)<br>
            - <a href="http://www.meyn.ece.ufl.edu">Sean Meyn</a>
            (Florida)<br>
            - <a href="http://www.nowozin.net/sebastian/">Sebastian
              Nowozin</a> (Microsoft Research)<br>
            - <a href="https://people.eecs.berkeley.edu/%7Erussell/">Stuart















































              Russell</a> (Berkeley)<br>
            <br>
            The <a
              href="http://www.ds3-datascience-polytechnique.fr/program/">full
program















































              can be found here.</a><br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>March 7, 2017</h3>
          <br>
          <div align="left"><a href="http://www.macs.hw.ac.uk/%7Emp71/">Marcelo















































              Pereyra</a> is giving a talk in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big















































              data optimization seminar today.</a> He is talking about
            his recent paper "Efficient Bayesian computation by proximal
            Markov chain Monte Carlo: when Langevin meets Moreau".<br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>February 28, 2017</h3>
          <br>
          <div align="left"><a href="http://jakubkonecny.com">Jakub
              Konečný</a> is giving a talk in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big















































              data optimization seminar today.</a><br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>February 26, 2017</h3>
          <br>
          <div align="left">I am on a leave from Edinburgh as of today.
            I have taken up an Associate Professor position at <a
              href="https://www.kaust.edu.sa/en">King Abdullah
              University of Science and Technology (KAUST). </a>I am
            affiliated with the <a
              href="https://cs.kaust.edu.sa/Pages/Home.aspx">Computer
              Science (CS)</a> and <a
              href="https://amcs.kaust.edu.sa/Pages/Home.aspx">Applied
              Mathematics &amp; Computational Science (AMCS)</a>
            programs. I am also affiliated with the <a
              href="https://ecrc.kaust.edu.sa/Pages/Home.aspx">Extreme
              Computing Research Center (ECRC)</a> and the <a
              href="https://vcc.kaust.edu.sa/Pages/Home.aspx">Visual
              Computing Center (VCC)</a>. I have several positions open,
            contact me if interested!<br>
            <br>
            PS: I am pleasantly surprised to see that the weather at
            KAUST is great at this time of the year! <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>February 21, 2017</h3>
          <br>
          <div align="left"><a
              href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas
              Loizou</a> is giving a talk in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big















































              data optimization seminar today.</a><br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>February 14, 2017</h3>
          <br>
          <div align="left"><a
              href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=506">Kostas















































              Zygalakis</a> is giving a talk in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big















































              data optimization seminar today.</a><br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>February 7, 2017</h3>
          <br>
          <div align="left"><a href="http://personal.lse.ac.uk/veghl/">László















































              Végh</a> is visiting me for a couple days. He is also
            giving a talk in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big















































              data optimization seminar tomorrow.</a><br>
            <br>
          </div>
          <h3> </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3><br>
          </h3>
          <h3>January 29, 2017</h3>
          <br>
          <div align="left"> Between January 29 and February 3, I am in
            Villars-sur-Ollon, Switzerland, attending the <a
              href="http://www.baspfrontiers.org"> BASP Frontiers 2017
              workshop.</a><br>
            <br>
          </div>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3><br>
          </h3>
          <h3>January 25, 2017</h3>
          <br>
          <div align="left"> Due to certain administrative reasons,
            interviews for the 2 postdoc posts I have open will happen a
            bit later than anticipated. Originally I expected the
            interviews to happen this week - there will be some delay
            with this.<br>
          </div>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3><br>
          </h3>
          <h3>January 23, 2017</h3>
          <span class="important"><br>
          </span>
          <div align="left">I have two visitors this week: <a
              href="http://acse.pub.ro/person/ion-necoara/">Ion Necoara
              (Bucharest)</a> and <a
              href="http://maiage.jouy.inra.fr/?q=fr/node/516">Elhoucine
              Bergou (INRA)</a>.<br>
            <br>
            <br>
          </div>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3><br>
          </h3>
          <h3>January 19, 2017</h3>
          <span class="important"><br>
          </span>
          <div align="left">I am in London today and tomorrow. Today I
            am discussing research with people at the <a
              href="https://www.turing.ac.uk/">Alan Turing Institute</a>
            (we managed to start a new project today and prove a lemma
            to kick it off), and tomorrow I am giving a seminar talk in
            the <a href="http://www.imperial.ac.uk/computing">Department














































              of Computing at Imperial College</a>.<br>
            <br>
            <br>
          </div>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3><br>
          </h3>
          <h3>January 16, 2017</h3>
          <span class="important"><br>
          </span>
          <div align="left">As of this week, I am teaching an intensive
            course (6 hours per week) entitled "Modern Optimization
            Methods for Big Data Problems". This is a rigorous course
            covering some of the fundamentals of randomized algorithms
            for optimization problems described by very large quantities
            of data. It is open to anyone interested (the current
            composition of the class includes PhD students, Master's
            students, a few undergraduate students and even some
            faculty).<br>
            <br>
          </div>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>January 10, 2017</h3>
          <span class="important"><br>
          </span>
          <div align="left">We have several Lectureships (i.e.,
            positions equivalent to Assistant Professorships in the US)
            open in the School of Mathematics:<br>
            <br>
            <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/jobs/lectureship-in-industrial-mathematics">Lectureship
in















































              Industrial Mathematics</a>, application deadline: February
            1, 2017 (5pm UK time)<br>
            <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/jobs/lectureships-in-statistics-and-data-science">Two
Lectureships















































              in Statistics and Data Science</a>, application deadline:
            February 7, 2017 (5pm UK time)<br>
            <br>
          </div>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>January 7, 2017</h3>
          <span class="important"><br>
          </span>
          <div align="left">I am in Slovakia at the moment, and it's
            been crazy cold the last few days. Today we have -15 degrees
            Celsius, but it feels like -25. This is when your eyebrows
            freeze. Anyway, I am returning back to Edinburgh tomorrow.<br>
            <br>
            <i>Some news:</i> <a href="http://www.dominikcsiba.com/">Dominik















































              Csiba</a> is now based in Slovakia. He will be finishing
            off his PhD from there; and is expected to submit his thesis
            in the Summer of 2017. He will be picking up some
            optimization/machine learning related activities in Slovakia
            - do talk to him if you get a chance! For instance, on
            February 15, <a href="http://www.dominikcsiba.com/">Dominik</a>
            will give a talk at a <a href="http://www.mlmu.sk/">Slovak
              Machine Learning Meetup</a> (MLMU). Further, Dominik is a
            mentor in a Data Science <a
              href="http://www.basecamp.ai/student/#about">BaseCamp</a>.
            Here is a blurb from their website: "BaseCamp is an
            immersive full-time 8-week program for prospective data
            scientists. During 8 weeks you will deepen your theoretical
            knowledge, enhance your practical skills and become a
            qualified data scientist ready for your exciting data
            science career." <br>
            <br>
            <br>
          </div>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3><br>
          </h3>
          <h3>December 21, 2016</h3>
          <span class="important"><br>
          </span>
          <div align="left"><span class="important">!!! 2 postdoc posts:
            </span>I have two 1-year postdoctoral associate positions
            open, ideally starting on March 1, 2017 (Feb 1 or April 1
            are also possible starting dates). If interested, please get
            in touch with me! <br>
            <br>
            Areas: big data optimization, machine learning, randomized
            numerical linear algebra.<br>
            <br>
            <i>Update: </i>Application deadline: January 23, 2017 (5pm
            UK time)<br>
            <br>
            For more information on the position and the required
            application files, and to get to the online application
            form, <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=038477">click


















              here</a>.<br>
            <br>
          </div>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>December 20, 2016</h3>
          <span class="important"><br>
          </span><span class="important">New paper out:</span> <a
            href="https://arxiv.org/abs/1612.06255">Linearly convergent
            randomized iterative methods for computing the pseudoinverse</a>,
          joint with <a href="http://www.di.ens.fr/%7Ergower/">Robert
            M. Gower.</a><br>
          <br>
          Abstract: <i>We develop the first stochastic incremental
            method for calculating the Moore-Penrose pseudoinverse of a
            real rectangular matrix. By leveraging three alternative
            characterizations of pseudoinverse matrices, we design three
            methods for calculating the pseudoinverse: two general
            purpose methods and one specialized to symmetric matrices.
            The two general purpose methods are proven to converge
            linearly to the pseudoinverse of any given matrix. For
            calculating the pseudoinverse of full rank matrices we
            present additional two specialized methods which enjoy
            faster convergence rate than the general purpose methods. We
            also indicate how to develop randomized methods for
            calculating approximate range space projections, a much
            needed tool in inexact Newton type methods or quadratic
            solvers when linear constraints are present. Finally, we
            present numerical experiments of our general purpose methods
            for calculating pseudoinverses and show that our methods
            greatly outperform the Newton-Schulz method on large
            dimensional matrices.&nbsp;</i> <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>December 5, 2016</h3>
          <span class="important"><br>
          </span>
          <div align="left"><span class="important"> </span>This week,
            everyone is away. <a href="http://jakubkonecny.com/">Jakub</a>,
            <a href="http://www.dominikcsiba.com/">Dominik</a> and <a
              href="http://filiphanzely.com/">Filip</a> are at <a
              href="https://nips.cc/">NIPS in Barcelona</a>. <a
              href="http://www.maths.ed.ac.uk/%7Es1461357/">Nicolas</a>
            is at <a href="http://www.ieeeglobalsip.org/">GlobalSip</a>
            in Greater Washington, D.C. Because of this, we are not
            having the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">Big

















              Data Optimization Seminar</a> this week. Next week, we
            have an invited speaker from Imperial College giving a talk
            in the seminar: <a href="http://www.doc.ic.ac.uk/%7Epp500/">Panos

















              Parpas</a>. <br>
          </div>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 30, 2016</h3>
          <span class="important"><br>
          </span>
          <div align="left"><span class="important"> </span><span
              class="important">New poster: </span><a
              href="https://arxiv.org/abs/1611.07555">Federated
              learning: strategies for improving communication
              efficiency</a>, to be presented by <a
              href="http://jakubkonecny.com/">Jakub Konečný</a> at the <a
              href="https://pmpml.github.io/PMPML16/">NIPS Private
              Multi-Party Machine Learning Workshop</a> in Barcelona.
            The underlying paper is <a
              href="https://pmpml.github.io/PMPML16/papers/PMPML16_paper_20.pdf">here</a>.<br>
          </div>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 30, 2016</h3>
          <span class="important"><br>
          </span>For the rest of the week I am in Moscow, visiting <a
            href="http://www.skoltech.ru/en/">SkolTech</a>, <a
            href="https://mipt.ru/en/">MIPT</a> and <a
            href="https://www.yandex.ru/">Yandex</a> (Russian search
          engine company). I am giving a talk at SkolTech on Thursday
          and at Yandex/MIPT on Friday.<br>
          <br>
          <i>Update (December 15):</i> Here is a <a
            href="https://events.yandex.ru/lib/talks/4294/">video
            recording of my Yandex talk</a>. The talk was mostly based
          on the papers <a
            href="http://link.springer.com/article/10.1007%2Fs11590-015-0916-1">NSync</a>
          (Optimization Letters 2015) and <a
href="http://papers.nips.cc/paper/5926-quartz-randomized-dual-coordinate-ascent-with-arbitrary-sampling">Quartz</a>
          (NIPS 2015), with a few slides mentioning <a
            href="http://www.jmlr.org/papers/v17/15-001.html">Hydra</a>
          (JMLR 2016) and <a
href="http://ieeexplore.ieee.org/document/6958862/?tp=&amp;arnumber=6958862&amp;url=http:%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Ftp%3D%26arnumber%3D6958862">Hydra^2</a>
          (IEEE MLSP 2014).<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 25, 2016</h3>
          <span class="important"><br>
          </span>Today I am giving a talk at Telecom ParisTech, in a <a
href="http://machinelearningforbigdata.telecom-paristech.fr/fr/article/workshop-friday-novembre-25-distributed-machine-learning">Workshop
on


















            Distributed Machine Learning</a>. The other two speakers are
          <a href="http://researchers.lille.inria.fr/abellet/">Aurélien
            Bellet (INRIA, Lille)</a> and <a
            href="https://people.kth.se/%7Emikaelj/">Mikael Johansson
            (KTH, Stockholm)</a>.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 24, 2016</h3>
          <span class="important"><br>
          </span>I am in Paris for the next couple days. Today I was an
          external examiner for a PhD thesis of <a
            href="http://perso.telecom-paristech.fr/%7Ecolin/">Igor
            Colin</a> at Telecom ParisTech. Igor is supervised by <a
            href="http://www.josephsalmon.eu/">Joseph Salmon</a> and <a
href="http://perso.telecom-paristech.fr/%7Eclemenco/Home.html">Stephan
            Clemenson</a>. Igor's thesis, entitled "Adaptation des
          méthodes d’apprentissage aux U-statistiques", is an in-depth
          exploration of several important aspects of machine learning
          involving U-statistics. Igor first develops strong statistical
          learning guarantees for ERM (empirical risk minimization) with
          incomplete U-statistics, then moves to solving the problem of
          computing/estimating U-statistics in a distributed environment
          via a gossip-like method, and finally develops a decentralized
          dual averaging optimization method for solving an ERM problem
          with pairwise functions. The results in the thesis are very
          strong, and the work is beautifully written. Needless to say,
          Igor defended easily.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 22, 2016</h3>
          <span class="important"><br>
          </span><span class="important">New paper out:</span> <a
            href="https://arxiv.org/abs/1611.07555">Randomized
            distributed mean estimation: accuracy vs communication</a>,
          joint with <a href="http://jakubkonecny.com/">Jakub Konečný.</a><br>
          <br>
          Abstract: <i>We consider the problem of estimating the
            arithmetic average of a finite collection of real vectors
            stored in a distributed fashion across several compute nodes
            subject to a communication budget constraint. Our analysis
            does not rely&nbsp; on any statistical assumptions about the
            source of the vectors. This problem arises as a subproblem
            in many applications, including reduce-all operations within
            algorithms for distributed and federated optimization and
            learning. We propose a flexible family of randomized
            algorithms exploring the trade-off between expected
            communication cost and estimation error. Our family contains
            the full-communication and zero-error method on one extreme,
            and an $\epsilon$-bit communication and ${\cal
            O}\left(1/(\epsilon n)\right)$ error method on the opposite
            extreme. In the special case where we&nbsp; communicate, in
            expectation, a single bit per coordinate of each vector, we
            improve upon existing results by obtaining
            $\mathcal{O}(r/n)$ error, where $r$ is the number of bits
            used to represent a floating point value.</i> <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 22, 2016</h3>
          <span class="important"><br>
          </span>Today we had <a
            href="http://www.maths.ed.ac.uk/%7Elszpruch/">Lukasz Szpruch</a>
          giving a talk in our <a
            href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">Big


















            Data Optimization Seminar</a>. He spoke about optimization,
          stochastic differential equations and consensus-based global
          optimization. Double thanks as he was able to make time
          despite just becoming a father. Congratulations!&nbsp; <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 21, 2016</h3>
          <span class="important"><br>
          </span>I traveled a bit last week. I first visited the Alan
          Turing Institute on November 16 and had some nice discussions
          before and over lunch with <a
            href="https://www.turing.ac.uk/research-fellows/">Armin
            Eftekhari and Hemant Tyagi</a>. Later on the same day I gave
          a <a
href="http://www.lse.ac.uk/maths/Seminars/Seminar-on-Combinatorics-Games-and-Optimisation.aspx">talk
at


















            London School of Economics</a>, and subsequently had a nice
          discussion with <a href="http://personal.lse.ac.uk/veghl/">Laszlo


















            Vegh</a> who is working on some problems similar to those
          I've been working on recently. The next day I took a train
          down to Southampton, where I gave a <a
href="http://www.southampton.ac.uk/cormsis/news/events/2016/Nov-Dec/seminar-richtarik.page">talk
on


















            SDNA in the CORMSIS seminar</a>. Thanks to <a
            href="http://www.southampton.ac.uk/maths/about/staff/abz1e14.page">Alain</a>,
          <a
            href="http://www.southampton.ac.uk/maths/about/staff/hx.page?">Xuifu</a>,
          <a
            href="http://www.southampton.ac.uk/maths/about/staff/tn6g10.page?">Tri-Dung</a>,
          and <a
            href="http://www.southampton.ac.uk/maths/about/staff/sc2r15.page?">Stefano</a>
          for fun discussions about mathematics, life, travel and
          politics! &nbsp;&nbsp;&nbsp; <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 9, 2016</h3>
          <span class="important"><br>
          </span>I am at Warwick today, giving a talk in the <a
            href="http://warwicksiam.yolasite.com/stream-1.php">2016
            Warwick SIAM Annual Conference on Machine Learning and
            Statistics</a>. <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 8, 2016</h3>
          <span class="important"><br>
          </span>We have had three interesting talks in our Big Data
          Optimization seminar series (aka "All Hands Meetings on Big
          Data Optimization") over the past three weeks. This is an
          informal reading seminar, covering recent papers in the field.
          <br>
          <br>
          On October 25, <a href="http://www.dominikcsiba.com/">Dominik
            Csiba</a> talked about "Linear Coupling", a framework for
          unifying gradient and mirror descent proposed in 2015 by
          Allen-Zhu and Orecchia. The week after, <a
            href="http://filiphanzely.com/">Filip Hanzely</a> talked
          about variance reduction methods for nonconvex stochastic
          optimization. Yesterday, <a
            href="http://www.maths.ed.ac.uk/%7Eateckent/">Aretha
            Teckentrup</a> talked about scaling up Gaussian process
          regression via doubly stochastic gradient descent. <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 4, 2016</h3>
          <span class="important"><br>
          </span>My <a
            href="https://www.youtube.com/watch?v=BS0kF4YijGc">OR58
            plenary talk on "Introduction to Big Data Optimization" is
            now on YouTube</a>. This is a very introductory talk,
          delivered at a slow pace, touching on topics such as gradient
          descent, handling nonsmoothness, acceleration, and
          randomization. <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>October 25, 2016</h3>
          <span class="important"><br>
          </span>My Alan Turing Institute <a
            href="https://www.youtube.com/watch?v=RbkhWrTbrKs">talk on
            "Stochastic Dual Ascent for Solving Linear Systems" is now
            on YouTube.</a> <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>October 14, 2016</h3>
          <span class="important"><br>
          </span>At 12:00 today, I am giving a short talk at <a
            href="http://www.icms.org.uk/">ICMS</a> in a seminar
          organized by the PhD students in the <a
            href="http://www.maxwell.ac.uk/migsaa">MIGSAA programme (The
            Maxwell Institute Graduate School in Analysis &amp; its
            Applications).</a> The students invite MIGSAA affiliated
          faculty of their choosing to speak about some of their recent
          work, chosen by the students. <br>
          <br>
          The full schedule of the event today:<br>
          <br>
          Peter Richtarik (12:00 – 12:30) <br>
          Empirical Risk Minimization: Complexity, Duality, Sampling,
          Sparsity and Big Data<br>
          &nbsp;<br>
          Lyonell Boulton (12:30 – 13:00)<br>
          Analytical and computational spectral theory<br>
          &nbsp;<br>
          Martin Dindos (13:00 – 13:30)<br>
          Elliptic and Parabolic PDEs with coefficients of minimal
          smoothness<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>October 13, 2016</h3>
          &nbsp; <span class="important"><br>
          </span>Today, I am giving a short talk at the <a
            href="https://turing.ac.uk/">Alan Turing Institute</a> in
          London. The talks in this series are recorded and will be put
          on YouTube. I will speak about "Stochastic Dual Ascent for
          Solving Linear Systems"; the content is based on two papers
          written jointly with Robert M. Gower [<a
            href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">paper


















            1</a>, <a href="http://arxiv.org/abs/1512.06890">paper 2</a>].<br>
          <br>
          If you are interested in stochastic optimization or fast
          algorithms for solving empirical risk minimization (ERM)
          problems in machine learning, this talk can be seen as a very
          good introduction into these areas, in a somewhat simplified
          setting. <br>
          <br>
          The methods I will talk about fit the ERM framework, and are
          both primal and dual in nature, simultaneously. They are
          variance-reduced (if you have followed recent research on
          variance-reduced methods for minimizing finite sums, you know
          what I am talking about) by default, and have linear
          convergence rate despite lack of strong convexity. The duality
          here is simpler than standard ERM duality, and hence stronger
          claims can be made. The dual problem is an unconstrained
          concave (but not strongly concave) quadratic maximization
          problem. The dual method is a randomized subspace ascent
          algorithm: the update to the dual vector is selected greedily
          from a random subspace. That is, in each iteration, one picks
          the update from this subspace which maximizes the dual
          function value. If the subspace is one-dimensional, and
          aligned with coordinate axes, one recovers randomized
          coordinate ascent. However, the random direction does not have
          to be aligned with the coordinate axes: one can pick it, say,
          from a Gaussian distribution, or any other continuous or
          discrete distribution. If the subspace is more than
          1-dimensional, the dual algorithm can be seen as a randomized
          variant of Newton's method. This variant has close connections
          with a machine learning / optimization technique known as
          minibatching. <br>
          <br>
          The primal method arises as an affine image of the dual
          method. That is, the dual iterates are simply mapped via a
          fixed affine mapping to the primal iterates, defining the
          primal method. The primal method can be seen from several
          different yet equivalent perspectives. It can be seen as
          stochastic gradient descent (SGD) with fixed stepsize applied
          to a particular stochastic (and not necessarily finite-sum!)
          objective. Surprisingly, it can also be seen as a Stochastic
          Newton Method (SNM), applied to the same objective. However,
          it can also be seen as a stochastic fixed point method and as
          a stochastic projection method ("sketch-and-project"). The
          method can be made parallel, and can be accelerated in the
          sense of Nesterov. <br>
          <br>
          The point I am making here is that in this setup, many key
          concepts and algorithms from stochastic optimization/machine
          learning coalesce into a unified framework, making it an ideal
          introduction into modern stochastic methods in optimization /
          machine learning. While I will only be able to introduce some
          of these connections in the short talk, instead of scratching
          the surface, my aim in the talk is to provide a thorough and
          understandable introduction into the area.&nbsp; <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>October 12, 2016</h3>
          <span class="important"><br>
          </span><a href="http://www.dominikcsiba.com/">Dominik Csiba</a>
          won a Postgraduate Essay Prize for his essay on <a
            href="http://www.dominikcsiba.com/docs/essay.pdf">Sampling
            Strategies for Empirical Risk Minimization.</a> The prize is
          given to the best 2-page-long essay(s) written by a PhD
          student in the School of Mathematics, based on his/her recent
          research, for a general mathematical audience.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3> </h3>
          <h3>October 8, 2016</h3>
          <span class="important"><br>
          </span><span class="important">New paper out:</span> <a
            href="https://arxiv.org/abs/1610.02527">Federated
            optimization: distributed machine learning for on-device
            intelligence</a>, joint with <a
            href="http://jakubkonecny.com/">Jakub Konečný</a>, and two
          Google coauthors: <a
            href="http://research.google.com/pubs/author35837.html">H.
            Brendan McMahan</a> and <a
            href="http://nlp.stanford.edu/dramage/">Daniel Ramage</a>. <br>
          <br>
          Update (Oct 19): The paper is now on <a
            href="https://arxiv.org/abs/1610.05492">arXiv</a>.<br>
          <br>
          Abstract: <i>We introduce a new and increasingly relevant
            setting for distributed optimization in machine learning,
            where the data defining the optimization are unevenly
            distributed over an extremely large number of nodes. The
            goal is to train a high-quality centralized model. We refer
            to this setting as Federated Optimization. In this setting,
            communication efficiency is of the utmost importance and
            minimizing the number of rounds of communication is the
            principal goal.<br>
            <br>
            A motivating example arises when we keep the training data
            locally on users’ mobile devices instead of logging it to a
            data center for training. In federated optimization, the
            devices are used as compute nodes performing computation on
            their local data in order to update a global model. We
            suppose that we have extremely large number of devices in
            the network — as many as the number of users of a given
            service, each of which has only a tiny fraction of the total
            data available. In particular, we expect the number of data
            points available locally to be much smaller than the number
            of devices. Additionally, since different users generate
            data with different patterns, it is reasonable to assume
            that no device has a representative sample of the overall
            distribution.<br>
            <br>
            We show that existing algorithms are not suitable for this
            setting, and propose a new algorithm which shows encouraging
            experimental results for sparse convex problems. This work
            also sets a path for future research needed in the context
            of federated optimization.</i><br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>October 6, 2016</h3>
          <span class="important"><br>
          </span><span class="important">New paper out:</span> <a
            href="papers/federated_communication_NIPS16.pdf">Federated
            learning: strategies for improving communication efficiency</a>,
          joint with <a href="http://jakubkonecny.com/">Jakub Konečný</a>,
          and four Google coauthors: <a
            href="http://research.google.com/pubs/author35837.html">H.
            Brendan McMahan</a>, <a
            href="http://research.google.com/pubs/FelixYu.html">Felix Yu</a>,
          Ananda Theertha Suresh and <a
            href="https://scholar.google.com/citations?user=KebzzVQAAAAJ&amp;hl=en">Dave


















            Bacon</a>. <br>
          <br>
          The paper was accepted to the <a
            href="https://nips.cc/Conferences/2016/Schedule?showEvent=6250"><i>2016
NIPS


















              Private Multi-Party Machine Learning Workshop</i></a>.<br>
          <br>
          Abstract:<i> </i><i> </i> <i> Federated learning is a
            machine learning setting where the goal is to train a
            high-quality centralized model with training data
            distributed over a large number of clients each with
            unreliable and relatively slow network connections. We
            consider learning algorithms for this setting where on each
            round, each client independently computes an update to the
            current model based on its local data, and communicates this
            update to a central server, where the client-side updates
            are aggregated to compute a new global model. The typical
            clients in this setting are mobile phones, and communication
            efficiency is of utmost importance. In this paper, we
            propose two ways to reduce the uplink communication costs.
            The proposed methods are evaluated on the application of
            training a deep neural network to perform image
            classification. Our best approach reduces the upload
            communication required to train a reasonable model by two
            orders of magnitude.&nbsp; </i> <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>October 5, 2016</h3>
          <span class="important"><br>
          </span>This week, I am in the Netherlands, attending the <a
href="https://wsc.project.cwi.nl/woudschoten-conferences/2016-woudschoten-conference">41st
Woudschoten


















            Conference</a> (annual conference of Dutch-Flemish Numerical
          Analysis Communities). I am giving a series of two keynote
          lectures in the theme "Numerical Methods for Big Data
          Analytics". The other keynote speakers are <a
            href="http://glaros.dtc.umn.edu/">George Karypis</a>
          (Minnesota), <a href="http://web.maths.unsw.edu.au/%7Efkuo/">Frances

















            Kuo</a> (New South Wales), <a
            href="https://www.nottingham.ac.uk/mathematics/people/michael.tretyakov">Michael


















            Tretyakov</a> (Nottingham), <a
            href="https://www.ima.umn.edu/%7Earnold/">Douglas N. Arnold</a>
          (Minnesota), and <a href="http://www-dimat.unipv.it/boffi/">Daniele


















            Boffi</a> (Pavia).<br>
          <br>
          Update (Oct 8): Here are the slides from my talks: <a
            href="talks/TALK-Woudschoten2016-Lecture1.pdf">Lecture 1</a>
          and <a href="talks/TALK-Woudschoten2016-Lecture2.pdf">Lecture
            2</a>.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>October 4, 2016</h3>
          <span class="important"><br>
          </span>The <a
            href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big


















            data optimization seminar</a> (aka "all hands meetings on
          big data optimization") is restarting with new academic year.
          We'll be meeting on Tuesdays, at 12:15, in JCMB 6207 -
          everybody interested in the field is welcome! There is very
          little room for excuses not to attend as we are running this
          during lunchtime, with lunch being provided! <br>
          <br>
          <a href="http://dominikcsiba.com/">Dominik Csiba</a> kicked
          the seminar off last week with an introduction to online ad
          allocation via online optimization; work Dominik coauthored
          with colleagues from Amazon. <a
            href="http://jakubkonecny.com/"> </a><a
            href="http://jakubkonecny.com/">Jakub Konečný</a> is
          speaking today about differentially private empirical risk
          minimization. Next week, we have <a
            href="http://www.maths.ed.ac.uk/people/show?person=479">Nicolas


















            Loizou</a> covering a recent paper of Nutini et al entitled
          " Convergence rates for greedy Kaczmarz algorithms, and faster
          randomized Kaczmarz rules using the orthogonality graph".<br>
          <br>
          Thanks to generous support from the <a
            href="http://datascience.inf.ed.ac.uk/apply/">CDT in Data
            Science</a>, this year we have extra funding to invite a few
          external (to Edinburgh) speakers. <br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>September 23, 2016</h3>
          <span class="important"></span><br>
          <span class="important">CoCoA+</span> is now the default
          linear optimizer in <a href="https://www.tensorflow.org/">TensorFlow</a>!
          TensorFlow is an Open Source Software Library for Machine
          Intelligence. It was originally developed by Google, and is
          used extensively for deep learning. CoCoA+ was developed in
          the following two papers:<br>
          <br>
          [1] <a
            href="http://jmlr.org/proceedings/papers/v37/mab15.html">Ma,
            Smith, Jaggi, Jordan, Richtarik and Takac. Adding vs
            Averaging in Distributed Primal-Dual Optimization, ICML, pp.
            1973-1982, 2015</a><br>
          <br>
          [2] <a href="http://arxiv.org/abs/1512.04039">Ma, Konecny,
            Jaggi, Smith, Jordan, Richtarik and Takac. Distributed
            optimization with arbitrary local solvers, 2015</a><br>
          <br>
          The algorithm previously won the <a
href="http://mlconf.com/mlconf-industry-impact-student-research-award-winners/">2015
MLConf


















            Industry Impact Student Research Award</a>. The recipient of
          the award was Virginia Smith.<br>
          <br>
          Our adaptive SDCA+ method, called <span class="important">AdaSDCA+</span>,
          has also been implemented in TensorFlow (by Google)! This
          method was developed and analyzed in the following paper:<br>
          <br>
          [3] <a
            href="http://jmlr.org/proceedings/papers/v37/csiba15.html">Csiba,
Qu


















            and Richtarik. Stochastic dual coordinate ascent with
            adaptive probabilities. ICML, pp. 674-683, 2015</a><br>
          <br>
          This paper previously won a best contribution award at
          Optimization and Big Data 2015 (2nd place). Committee: A
          Nemirovski (GeorgiaTech) and R. Jenatton (Amazon). D Csiba was
          the recipient of the award.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>September 21, 2016</h3>
          <span class="important"><br>
          </span>Today I am attending (and giving a talk at) an event
          held at the Royal Society of Edinburgh: <br>
          <br>
          <a
href="http://www.ambafrance-uk.org/Franco-Scottish-Seminar-2016-Linear-Algebra-and-Parallel-Computing-at-the-Heart">Franco–Scottish
Science


















            Seminar: Linear Algebra and Parallel Computing at the Heart
            of Scientific Computing</a><br>
          <p><br>
            The event is organized by <a
              href="https://en.wikipedia.org/wiki/Iain_S._Duff">Iain
              Duff</a>.<br>
            <br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>September 12, 2016</h3>
          <span class="important"><br>
          </span>I am at Google, Seattle, on an invite by Google,
          attending the <a
            href="https://sites.google.com/site/learningprivacymobiledata/home">Learning,
Privacy


















            and Mobile Data workshop</a>. <a
            href="http://jakubkonecny.com/">Jakub Konecny</a> is
          attending, too. <br>
          <h3><br>
          </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>September 8, 2016</h3>
          <span class="important"><br>
          </span>Here are the slides from my closing plenary talk <a
            href="talks/TALK-OR58.pdf">"Introduction to Big Data
            Optimization"</a> at <a
            href="http://www.theorsociety.com/Pages/Conferences/OR58/OR58.aspx">OR58</a>.
          <br>
          <h3><br>
          </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>September 7, 2016</h3>
          <span class="important"><br>
          </span>This week, I am simultaneously attending and giving
          talks at two conferences (while this was bound to happen at
          some point, I am not planning to repeat this as I missed
          important talks at both events...). <br>
          <br>
          Today, I am speaking at the <a
href="http://www.ima.org.uk/conferences/conferences_calendar/5th_ima_conference_on_numerical_linear_algebra_and_optimisation.cfm.html">5th
IMA


















            Conference on Numerical Linear Algebra and Optimization</a>,
          where I am co-organizing 2 minisymposia with Nicolas Loizou
          and Jakub Konecny (randomized numerical linear algebra and big
          data optimization). I am speaking about <a
            href="http://arxiv.org/abs/1512.06890">"Stochastic dual
            ascent for solving linear systems"</a>; the talk is based on
          a joint paper with Robert M. Gower. Several other people from
          my group are attending and giving talks as well. <a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=417">Dominik


















            Csiba</a> (who is now back from an internship at Amazon)
          will speak about <a href="http://arxiv.org/abs/1602.02283">"Importance


















            sampling for minibatches"</a>. Jakub Konecny (who is now
          back from an internship at Google) will speak about <a
            href="https://arxiv.org/abs/1608.06879">"AIDE: Fast and
            communication-efficient distributed optimization"</a>. <a
            href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>
          (who moved to INRIA a month ago) will speak about <a
            href="http://arxiv.org/abs/1602.01768">"Randomized
            quasi-Newton methods are linearly convergent matrix
            inversion algorithms"</a>. <a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Nicolas


















            Loizou</a> will give a talk entitled "Randomized gossip
          algorithms: complexity, duality and new variants", based on <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/gossip.pdf">this
            paper</a>, which will appear in GlobalSip 2016. <a
            href="http://filiphanzely.com/">Filip Hanzely</a> is also
          attending. <br>
          <br>
          Tomorrow, I am giving the closing plenary talk at <a
            href="http://www.theorsociety.com/Pages/Conferences/OR58/OR58.aspx">OR58</a>
          - the annual conference of the OR Society - entitled
          "Introduction to Big Data Optimization". Update (Nov 4, 2016):
          The talk is now on <a
            href="https://www.youtube.com/watch?v=BS0kF4YijGc">YouTube</a>.<br>
          <h3><br>
          </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>September 6, 2016</h3>
          <br>
          <span class="important">PhD position Open in my group<br>
            <br>
          </span>I have a PhD position open in my group. Starting date:
          as soon as possible, but not later than January 1, 2017.
          Please fill out the <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/studying-here/pgr/phd-application">online


















            application</a> (apply for PhD in OR &amp; Optimization),
          and possibly also send me an email once you do. <br>
          <br>
          There is no application deadline. Applications will be
          reviewed as they arrive, and the position will be open and
          advertised until a suitable candidate is found and the post is
          filled. <br>
          <br>
          The position is funded by the School of Mathematics, and is
          associated with the <a
            href="http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/N005538/1">EPSRC
Grant


















            "Randomized Algorithms for Extreme Convex Optimization"</a>.<br>
          <h3><br>
          </h3>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>August 13, 2016</h3>
          <br>
          <span class="important">Highly Downloaded SIOPT Paper<br>
            <br>
          </span>The paper <a
            href="http://epubs.siam.org/doi/abs/10.1137/130949993">"Accelerated,
parallel


















            and proximal coordinate descent"</a> (SIOPT, 2015) (joint
          with <a href="http://perso.telecom-paristech.fr/%7Eofercoq/">Olivier


















            Fercoq</a>) is the <a
href="http://epubs.siam.org/action/showMostReadArticles?journalCode=sjope8">2nd
most


















            downloaded SIOPT paper</a>. The downloads are counted over
          the last 12 months, and include all SIOPT papers. The most
          downloaded paper is <a
            href="http://epubs.siam.org/doi/abs/10.1137/080738970">"A
            singular value thresholding algorithm for matrix completion"</a>
          by J. Cai, E. Candes and Z. Shen (SIOPT, 2010). The third most
          downloaded paper is <a
            href="http://epubs.siam.org/doi/abs/10.1137/100802001">"Efficiency
of


















            coordinate descent methods on huge-scale optimization
            problems"</a> (SIOPT, 2012) by Yu. Nesterov. The fourth in
          the list is <a
            href="http://epubs.siam.org/doi/abs/10.1137/S1052623400366802">"Global
optimization


















            with polynomials and the problem of moments"</a> (SIOPT,
          2001) by J.B. Lasserre.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>August 9, 2016</h3>
          <br>
          <span class="important">Highly Downloaded SIMAX Paper<br>
            <br>
          </span>The paper <a
            href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">"Randomized
Iterative


















            Methods for Linear Systems"</a> (joint with <a
            href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert M.
            Gower</a>) is the <a
href="http://epubs.siam.org/action/showMostReadArticles?journalCode=sjmael&amp;">2nd
most


















            downloaded SIMAX paper</a>. The downloads are counted over
          the last 12 months, and include all SIMAX papers. The first
          and third papers in the ranking are from 2000, the fourth was
          written in 1998 - nearly 20 years ago.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>August 8, 2016</h3>
          <br>
          <p>I am in Tokyo, at the <a
              href="http://www.iccopt2016.tokyo/">5th International
              Conference on Continuous Optimization</a>. It's warm and
            humid, but the food I just had was great! I am giving my
            talk on Tuesday, August&nbsp; 9.<br>
          </p>
          <p>Update (August 9): the conference dinner was fabulous!<br>
          </p>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>July 1, 2016</h3>
          <br>
          <p><span class="important">New paper out: </span><a
              href="papers/gossip.pdf">A new perspective on randomized
              gossip algorithms</a>, joint with <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/people/show?person=479">Nicolas














              Loizou</a>.<br>
          </p>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>July 28, 2016</h3>
          <br>
          <a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Nicolas


















            Loizou</a> is visiting Microsoft Research in Seattle this
          week.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>July 4, 2016</h3>
          <br>
          A belated message: Since about mid-May, and until
          mid/late-August, <a
            href="http://www.maths.ed.ac.uk/people/show?person=417">Dominik


















            Csiba</a> and <a href="http://jakubkonecny.com/">Jakub
            Konečný</a> are doing industrial internships. Dominik is
          with the Scalable Machine Learning Lab at Amazon, Berlin; and
          Jakub is with Google, Seattle. Nicolas Loizou is participating
          in the <a href="https://pcmi.ias.edu/program-index/2016">PCMI
            26th Annual PCMI Session on "The Mathematics of Data"</a>.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>June 26, 2016</h3>
          <br>
          I am on my way to Beijing to participate in the <a
            href="http://lsec.cc.ac.cn/%7Emoa2016/index.html">2016
            International Workshop on Modern Optimization and
            Applications (MOA 2016)</a>.<!--[if gte mso 9]><xml>
 <o:DocumentProperties>
  <o:Revision>0</o:Revision>
  <o:TotalTime>0</o:TotalTime>
  <o:Pages>1</o:Pages>
  <o:Words>10</o:Words>
  <o:Characters>59</o:Characters>
  <o:Company>University of Edinburgh</o:Company>
  <o:Lines>1</o:Lines>
  <o:Paragraphs>1</o:Paragraphs>
  <o:CharactersWithSpaces>68</o:CharactersWithSpaces>
  <o:Version>14.0</o:Version>
 </o:DocumentProperties>
 <o:OfficeDocumentSettings>
  <o:AllowPNG/>
 </o:OfficeDocumentSettings>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <w:WordDocument>
  <w:View>Normal</w:View>
  <w:Zoom>0</w:Zoom>
  <w:TrackMoves/>
  <w:TrackFormatting/>
  <w:PunctuationKerning/>
  <w:ValidateAgainstSchemas/>
  <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>
  <w:IgnoreMixedContent>false</w:IgnoreMixedContent>
  <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>
  <w:DoNotPromoteQF/>
  <w:LidThemeOther>EN-US</w:LidThemeOther>
  <w:LidThemeAsian>JA</w:LidThemeAsian>
  <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript>
  <w:Compatibility>
   <w:BreakWrappedTables/>
   <w:SnapToGridInCell/>
   <w:WrapTextWithPunct/>
   <w:UseAsianBreakRules/>
   <w:DontGrowAutofit/>
   <w:SplitPgBreakAndParaMark/>
   <w:EnableOpenTypeKerning/>
   <w:DontFlipMirrorIndents/>
   <w:OverrideTableStyleHps/>
   <w:UseFELayout/>
  </w:Compatibility>
  <m:mathPr>
   <m:mathFont m:val="Cambria Math"/>
   <m:brkBin m:val="before"/>
   <m:brkBinSub m:val="&#45;-"/>
   <m:smallFrac m:val="off"/>
   <m:dispDef/>
   <m:lMargin m:val="0"/>
   <m:rMargin m:val="0"/>
   <m:defJc m:val="centerGroup"/>
   <m:wrapIndent m:val="1440"/>
   <m:intLim m:val="subSup"/>
   <m:naryLim m:val="undOvr"/>
  </m:mathPr></w:WordDocument>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <w:LatentStyles DefLockedState="false" DefUnhideWhenUsed="true"
  DefSemiHidden="true" DefQFormat="false" DefPriority="99"
  LatentStyleCount="276">
  <w:LsdException Locked="false" Priority="0" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Normal"/>
  <w:LsdException Locked="false" Priority="9" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="heading 1"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 2"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 3"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 4"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 5"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 6"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 7"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 8"/>
  <w:LsdException Locked="false" Priority="9" QFormat="true" Name="heading 9"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 1"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 2"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 3"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 4"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 5"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 6"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 7"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 8"/>
  <w:LsdException Locked="false" Priority="39" Name="toc 9"/>
  <w:LsdException Locked="false" Priority="35" QFormat="true" Name="caption"/>
  <w:LsdException Locked="false" Priority="10" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Title"/>
  <w:LsdException Locked="false" Priority="1" Name="Default Paragraph Font"/>
  <w:LsdException Locked="false" Priority="11" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Subtitle"/>
  <w:LsdException Locked="false" Priority="22" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Strong"/>
  <w:LsdException Locked="false" Priority="20" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Emphasis"/>
  <w:LsdException Locked="false" Priority="59" SemiHidden="false"
   UnhideWhenUsed="false" Name="Table Grid"/>
  <w:LsdException Locked="false" UnhideWhenUsed="false" Name="Placeholder Text"/>
  <w:LsdException Locked="false" Priority="1" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="No Spacing"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading Accent 1"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List Accent 1"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid Accent 1"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1 Accent 1"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2 Accent 1"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1 Accent 1"/>
  <w:LsdException Locked="false" UnhideWhenUsed="false" Name="Revision"/>
  <w:LsdException Locked="false" Priority="34" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="List Paragraph"/>
  <w:LsdException Locked="false" Priority="29" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Quote"/>
  <w:LsdException Locked="false" Priority="30" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Intense Quote"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2 Accent 1"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1 Accent 1"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2 Accent 1"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3 Accent 1"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List Accent 1"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading Accent 1"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List Accent 1"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid Accent 1"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading Accent 2"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List Accent 2"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid Accent 2"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1 Accent 2"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2 Accent 2"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1 Accent 2"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2 Accent 2"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1 Accent 2"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2 Accent 2"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3 Accent 2"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List Accent 2"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading Accent 2"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List Accent 2"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid Accent 2"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading Accent 3"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List Accent 3"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid Accent 3"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1 Accent 3"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2 Accent 3"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1 Accent 3"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2 Accent 3"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1 Accent 3"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2 Accent 3"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3 Accent 3"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List Accent 3"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading Accent 3"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List Accent 3"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid Accent 3"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading Accent 4"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List Accent 4"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid Accent 4"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1 Accent 4"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2 Accent 4"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1 Accent 4"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2 Accent 4"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1 Accent 4"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2 Accent 4"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3 Accent 4"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List Accent 4"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading Accent 4"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List Accent 4"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid Accent 4"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading Accent 5"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List Accent 5"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid Accent 5"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1 Accent 5"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2 Accent 5"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1 Accent 5"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2 Accent 5"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1 Accent 5"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2 Accent 5"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3 Accent 5"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List Accent 5"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading Accent 5"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List Accent 5"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid Accent 5"/>
  <w:LsdException Locked="false" Priority="60" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Shading Accent 6"/>
  <w:LsdException Locked="false" Priority="61" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light List Accent 6"/>
  <w:LsdException Locked="false" Priority="62" SemiHidden="false"
   UnhideWhenUsed="false" Name="Light Grid Accent 6"/>
  <w:LsdException Locked="false" Priority="63" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 1 Accent 6"/>
  <w:LsdException Locked="false" Priority="64" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Shading 2 Accent 6"/>
  <w:LsdException Locked="false" Priority="65" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 1 Accent 6"/>
  <w:LsdException Locked="false" Priority="66" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium List 2 Accent 6"/>
  <w:LsdException Locked="false" Priority="67" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 1 Accent 6"/>
  <w:LsdException Locked="false" Priority="68" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 2 Accent 6"/>
  <w:LsdException Locked="false" Priority="69" SemiHidden="false"
   UnhideWhenUsed="false" Name="Medium Grid 3 Accent 6"/>
  <w:LsdException Locked="false" Priority="70" SemiHidden="false"
   UnhideWhenUsed="false" Name="Dark List Accent 6"/>
  <w:LsdException Locked="false" Priority="71" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Shading Accent 6"/>
  <w:LsdException Locked="false" Priority="72" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful List Accent 6"/>
  <w:LsdException Locked="false" Priority="73" SemiHidden="false"
   UnhideWhenUsed="false" Name="Colorful Grid Accent 6"/>
  <w:LsdException Locked="false" Priority="19" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Subtle Emphasis"/>
  <w:LsdException Locked="false" Priority="21" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Intense Emphasis"/>
  <w:LsdException Locked="false" Priority="31" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Subtle Reference"/>
  <w:LsdException Locked="false" Priority="32" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Intense Reference"/>
  <w:LsdException Locked="false" Priority="33" SemiHidden="false"
   UnhideWhenUsed="false" QFormat="true" Name="Book Title"/>
  <w:LsdException Locked="false" Priority="37" Name="Bibliography"/>
  <w:LsdException Locked="false" Priority="39" QFormat="true" Name="TOC Heading"/>
 </w:LatentStyles>
</xml><![endif]--> Update: <a
            href="talks/TALK-2016-06-SDNA-Beijing.pdf">my slides.</a><br>
          <br>
          <i>Speakers: </i><br>
          <br>
          <i>Day 1:</i><br>
          <br>
          <a href="http://web.stanford.edu/%7Eyyye/">Yinyu Ye</a>
          (Stanford)<br>
          <a href="http://coral.ie.lehigh.edu/%7Eted/">Ted Ralphs</a>
          (Lehigh)<br>
          <a href="http://www.caam.rice.edu/%7Ezhang/">Yin Zhang</a>
          (Rice)<br>
          <a href="http://www.zib.de/members/conrad">Tim Conrad</a>
          (ZIB)<br>
          <a href="http://people.ece.umn.edu/%7Eluozq/">Zhi-Quan (Tom)
            Luo</a> (Minnesota)<br>
          <br>
          <i>Day 2:</i><br>
          <br>
          Peter Richtarik (Edinburgh) [<a
            href="talks/TALK-2016-06-SDNA-Beijing.pdf">slides</a>]<br>
          <a
            href="https://www.microsoft.com/en-us/research/people/lixiao/">Lin


















            Xiao</a> (Microsoft Research)<br>
          <a href="http://www.zib.de/koch/">Thorsten Koch</a> (ZIB &amp;
          TU Berlin)<br>
          <a href="http://people.sutd.edu.sg/%7Enannicini/">Giacomo
            Nannicini</a> (IBM &amp; SUTD)<br>
          <a href="https://www.polyu.edu.hk/ama/staff/xjchen/ChenXJ.htm">Xiaojun


















            Chen</a> (Hong Kong Polytechnic)<br>
          <a href="http://www1.se.cuhk.edu.hk/%7Esqma/">Shiqian Ma</a>
          (Chinese University of Hong Kong)<br>
          <br>
          <i>Day 3:</i><br>
          <br>
          Zongben Xu<br>
          <a href="http://www1.se.cuhk.edu.hk/%7Emanchoso//">Anthony
            Man-Cho So</a> (Chinese University of Hong Kong)<br>
          <a href="http://ise.tamu.edu/people/faculty/butenko/">Sergiy
            Butenko</a> (Texas A&amp;M)<br>
          <a href="http://www.ie.uh.edu/faculty/peng">Jiming Peng</a>
          (Houston)<br>
          <a
            href="https://scholar.google.nl/citations?user=5jfgJR8AAAAJ&amp;hl=nl">Deren


















            Han</a> (Nanjing)<br>
          Naihua Xiu<br>
          <a href="http://lsec.cc.ac.cn/%7Eyyx/">Ya-xiang Yuan</a>
          (Chinese Academy of Sciences)<br>
          <a href="http://research.baidu.com/big-data-lab/">Tong Zhang</a>
          (Baidu)<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>June 12, 2016</h3>
          <br>
          I am attending the <a
            href="http://meetings2.informs.org/wordpress/2016international/">INFORMS
International


















            Conference</a>, held in Hawaii. I am giving an invited talk
          in the Large-Scale Optimization II session on Wednesday. Other
          speakers in the session: <a
            href="http://www.math.ucla.edu/%7Ewotaoyin/">Wotao Yin</a>
          and <a href="http://www.math.ucla.edu/%7Edamek/">Damek Davis</a>.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>May 29, 2016</h3>
          <p><span class="important">New paper out: </span><a
              href="https://arxiv.org/abs/1605.08982">Coordinate descent
              face-off: primal or dual?</a>, joint with <a
              href="http://www.dominikcsiba.com/">Dominik Csiba</a>.<br>
          </p>
          <p>Abstract:<br>
          </p>
          <p><i>Randomized coordinate descent (RCD) methods are
              state-of-the-art algorithms for training linear predictors
              via minimizing regularized empirical risk. When the number
              of examples ($n$) is much larger than the number of
              features ($d$), a common strategy is to apply RCD to the
              dual problem. On the other hand, when the number of
              features is much larger than the number of examples, it
              makes sense to apply RCD directly to the primal problem.
              In this paper we provide the first joint study of these
              two approaches when applied to L2-regularized ERM. First,
              we show through a rigorous analysis that for dense data,
              the above intuition is precisely correct. However, we find
              that for sparse and structured data, primal RCD can
              significantly outperform dual RCD even if $d \ll n$, and
              vice versa, dual RCD can be much faster than primal RCD
              even if $n \ll d$. Moreover, we show that, surprisingly, a
              single sampling strategy minimizes both the (bound on the)
              number of iterations and the overall expected complexity
              of RCD. Note that the latter complexity measure also takes
              into account&nbsp; the average cost of the iterations,
              which depends on the structure and sparsity of the data,
              and on the sampling strategy employed.&nbsp; We confirm
              our theoretical predictions using extensive experiments
              with both synthetic and real data sets. </i><br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>May 29, 2016</h3>
          <br>
          Today I am giving a seminar talk in the <a
href="https://cemse.kaust.edu.sa/events/Pages/CEMSE-Seminar-Peter-Richtarik.aspx">CEMSE


















            seminar</a> at <a href="https://www.kaust.edu.sa/en">KAUST</a>.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>May 27, 2016</h3>
          <br>
          <a href="docs/CV_Ziteng_Wang.pdf">Ziteng Wang</a> will join my
          group as a PhD student starting in October 2016. He will be
          affiliated with the <a
            href="https://turing.ac.uk/jobs/2016-doctoral-studentships/">Alan
Turing


















            Institute</a> and the School of Mathematics here in
          Edinburgh. Ziteng has an MS in Pattern Recognition and Machine
          Learning from <a href="http://english.pku.edu.cn/">Peking
            University</a> and a BS in Mathematics from <a
            href="http://www.scu.edu.cn/en/">Sichuan University</a>. He
          subsequently spent half a year as a research assistant at <a
            href="https://www.cse.ust.hk/">Hong Kong University of
            Science and Technology</a>. Ziteng has written 4 papers [<a
href="http://papers.nips.cc/paper/5670-fast-second-order-stochastic-backpropagation-for-variational-inference">1</a>,
          <a href="people.duke.edu/%7Ekf96/docs/jmlrDP.pdf">2</a>, <a
            href="http://arxiv.org/abs/1401.0987">3</a>, <a
href="http://papers.nips.cc/paper/5011-efficient-algorithm-for-privately-releasing-smooth-queries">4</a>],
two


















          of which appeared in NIPS, and one in JMLR. Ziteng: welcome to
          the group!<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>May 19, 2016</h3>
          <br>
          I am back in Edinburgh now. There is <a
            href="https://nips.cc/Conferences/2016/CallForPapers">NIPS
            deadline</a> tomorrow, still some stuff to do...<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>May 13, 2016</h3>
          <br>
          Tomorrow is an interesting day as almost everybody in my group
          is traveling away from Edinburgh (despite the fact that we are
          blessed with amazing weather these days!), including me. <a
            href="http://www.dominikcsiba.com/">Dominik Csiba</a> is
          starting his Amazon (Scalable Machine Learning group in
          Berlin) internship next week. <a
            href="http://jakubkonecny.com/">Jakub Konecny</a> is
          starting his Google (Seattle) internship also next week. I am
          visiting Stanford next week. All three of us are leaving
          Edinburgh tomorrow... ;-) To add to this, <a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Nicolas


















            Loizou</a> is also away, attending the <a
            href="https://learning.mpi-sws.org/mlss2016/">Machine
            Learning Summer School</a> in Cadiz, Spain (May 11-21). <a
            href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>
          is hanging around though, which is great, as he can take care
          of my visitor <a
            href="http://www.lix.polytechnique.fr/%7Evu/">Vu Khac Ky</a>.
          The three of us have started an interesting project earlier
          this week. If you are in Edinburgh then you might want to
          attend Ky's <a
            href="http://www.maths.ed.ac.uk/ERGO/seminars.html">ERGO
            seminar talk</a> on Wednesday next week (the website has not
          yet been updated -&nbsp; but it will soon include his talk).<br>
          <br>
          On another subject: we have just had <span class="important">two
papers


















            accepted</span> to <i>Optimization Methods and Software</i>:<br>
          <br>
          <a href="http://arxiv.org/abs/1412.8060">Coordinate descent
            with arbitrary sampling I: algorithms and complexity</a> (by
          <a href="http://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a> and
          myself)
          <p><a href="http://arxiv.org/abs/1412.8063">Coordinate descent
              with arbitrary sampling II: expected separable
              overapproximation</a> (by <a
              href="http://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a> and
            myself)<br>
          </p>
          <br>
          And now some relatively belated news: <a
            href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert</a> <span
            class="important">defended his PhD thesis </span>on Friday
          April 29. His PhD committee, composed of <a
            href="http://www.ma.man.ac.uk/%7Ehigham/">Nick Higham</a>
          (external examiner) and <a
            href="http://kac.maths.ed.ac.uk/%7Ebl/">Ben Leimkuhler</a>
          (internal examiner), suggested that his thesis should be
          nominated for the Householder Prize (for " best dissertation
          in numerical algebra"). I'd be delighted to do the nomination!
          Robert will join the <a href="http://www.di.ens.fr/sierra/">SIERRA

















            group</a> as a postdoc in August.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>May 10, 2016</h3>
          <br>
          <a href="http://www.lix.polytechnique.fr/%7Evu/">Vu Khac Ky</a>
          (LIX, Ecole Polytechnique) is visiting me until May 20.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>May 9, 2016</h3>
          <br>
          Today I am attending "Networking Workshop on Mathematical
          Optimization and Applications" taking place here in Edinburgh
          (room JCMB 4325a; if you are around and feel like
          attending...). In fact, I have just given my talk, and
          Sotirios Tsaftaris is speaking at the very moment I am writing
          this.<br>
          <br>
          <i>Speakers (listed in the order they deliver their talks):</i>
          Nickel, myself, Konecny, Tsaftaris, Giuffrida, Menolascina,
          Hall, Garcia Quiles, Kalcsics, van der Weijde, Gunda, Wallace,
          Joyce, Herrmann, Etessami, Buke, Francoise.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>May 5, 2016</h3>
          <br>
          I accepted an invite to give the closing plenary talk at <a
            href="http://www.theorsociety.com/Pages/Conferences/OR58/OR58.aspx">OR58</a>
          - the 58th Annual Conference of the Operational Research
          Society ("OR Society"). The conference will take place in
          Portsmouth, UK, during September 6-8, 2016. Established in
          1948, The OR Society is the oldest learned society in the
          field of OR.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>May 4, 2016</h3>
          <br>
          <span class="important">New poster out - </span><a
            href="posters/Poster-Gossip.pdf">Randomized Gossip
            Algorithms: New Insights</a>. Nicolas Loizou will present
          this poster on <a
            href="https://learning.mpi-sws.org/mlss2016/poster-sessions/">May

















            16</a> at the <a
            href="https://learning.mpi-sws.org/mlss2016/">Machine
            Learning Summer School</a> (MLSS) in Cádiz, Spain, which he
          is attending.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>May 4, 2016</h3>
          <br>
          We have had <span class="important">two minisymposia accepted</span>
          in the <a
href="http://www.ima.org.uk/conferences/conferences_calendar/5th_ima_conference_on_numerical_linear_algebra_and_optimisation.cfm.html">5th
IMA


















            Conference on Numerical Linear Algebra and Optimization</a>,
          to be held in Birmingham during September 7-9, 2016. The
          minisymposia are:<br>
          <br>
          1) <b>Randomized Numerical Linear Algebra</b><br>
          <br>
          <i>Organizers: </i><a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Loizou</a>,
          <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Gower</a>,
          myself<br>
          <br>
          <i>Speakers:</i><br>
          <br>
          <a href="https://www.lri.fr/%7Ebaboulin/">Marc Baboulin</a>
          (Paris-Sud), The Story of the Butterflies<br>
          <a href="https://ei.is.tuebingen.mpg.de/person/sbartels">Simon
            Bartels</a> (Max Planck Institute), TBA<br>
          <a href="https://www.cs.purdue.edu/homes/dgleich/">David
            Gleich</a> (Purdue), TBA<br>
          <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>
          (INRIA), Stochastic Quasi-Newton Methods and Matrix Inversion<br>
          <a href="https://www.maths.ox.ac.uk/people/raphael.hauser">Raphael


















            Hauser</a> (Oxford), TBA<br>
          <a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Nicolas


















            Loizou</a> (Edinburgh), Randomized Gossip Methods:
          Complexity, Duality and New Variants<br>
          Peter Richtárik (Edinburgh), Stochastic Dual Ascent for
          Solving Linear Systems<br>
          <a href="http://www.wisdom.weizmann.ac.il/%7Eshamiro/">Ohad
            Shamir</a> (Weizmann Institute), A Stochastic SVD and PCA
          Algorithm with Linear Convergence Rate<br>
          <br>
          2) <b>Optimization Methods in Machine Learning</b><br>
          <br>
          <i>Organizers:</i> <a
            href="http://www.maths.ed.ac.uk/school-of-mathematics/people?person=479">Loizou</a>,
          <a href="http://jakubkonecny.com/">Konečný</a>, myself<br>
          <br>
          <i>Speakers: </i><br>
          <br>
          <a href="http://www.dominikcsiba.com/">Dominik Csiba</a>
          (Edinburgh), Importance Sampling for Minibatches<br>
          <a href="https://people.epfl.ch/volkan.cevher?lang=en">Volkan
            Cevher</a> (EPFL), TBA<br>
          <a href="http://people.kth.se/%7Ehamidrez/">Hamid Reza
            Feyzmahdavian</a> (KTH), TBA<br>
          <a href="http://jakubkonecny.com/">Jakub Konečný</a>
          (Edinburgh), Federated Optimization: Distributed Optimization
          Beyond the Datacenter<br>
          <a href="http://lear.inrialpes.fr/people/mairal/">Julien
            Mairal</a> (INRIA Grenoble), A Universal Catalyst for First
          Order Optimization<br>
          <a href="http://www.doc.ic.ac.uk/%7Epp500/">Panos Parpas </a>(Imperial),


















          TBA<br>
          <a href="http://josephsalmon.eu/">Joseph Salmon</a> (Telecom
          ParisTech), GAP Safe Screening Rules for Sparsity Enforcing
          Penalties<br>
          <a href="http://www.wisdom.weizmann.ac.il/%7Eshamiro/">Ohad
            Shamir</a> (Weizmann Institute), Without Replacement for
          Stochastic Gradient Methods<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>May 3, 2016</h3>
          <br>
          We have <a
            href="http://www-syscom.univ-mlv.fr/%7Epesquet/index.htm">Jean
Christophe


















            Pesquet</a> (Universite Paris-Est) leading the <a
            href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big


















            data seminar</a> today. Prof Pesquet is a leading researcher
          in the area of inverse problems, and optimization for signal
          and image processing.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 25, 2016</h3>
          <br>
          I am visiting Stanford this week.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 24, 2016</h3>
          <p>We've just had <span class="important">3 papers accepted</span>
            to <a href="http://icml.cc/2016/">ICML 2016:</a><br>
          </p>
          <p>- <a href="http://arxiv.org/abs/1603.09649">Stochastic
              block BFGS: squeezing more curvature out of data</a> (<a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Gower</a>, <a
              href="http://www.columbia.edu/%7Egoldfarb/">Goldfarb</a>
            and R)<br>
            - <a href="http://arxiv.org/abs/1512.09103">Even faster
              accelerated coordinate descent using non-uniform sampling</a>
            (<a href="http://people.csail.mit.edu/zeyuan/">Allen-Zhu</a>,
            <a href="http://hkumath.hku.hk/%7Ezhengqu/">Qu</a>, R and <a
              href="http://www.callowbird.com/">Yuan</a>)<br>
            - <a href="http://arxiv.org/abs/1502.02268">SDNA:
              Stochastic dual Newton ascent for empirical risk
              minimization</a> (<a
              href="http://hkumath.hku.hk/%7Ezhengqu/">Qu</a>, R, <a
              href="http://mtakac.com/">Takáč</a> and <a
              href="http://perso.telecom-paristech.fr/%7Eofercoq/">Fercoq</a>)</p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 22, 2016</h3>
          <p>Today I am giving a talk at the <a
              href="http://www.ecmath.de/research/ECMathColloquium">ECMath














              Colloquium</a> in Berlin. I am speaking about "Empirical
            Risk Minimization: Complexity, Duality, Sampling, Sparsity
            and Big Data". </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 21, 2016</h3>
          <p><a
              href="http://math.mit.edu/directory/profile.php?pid=1640">Haihao
(Sean)














              Lu</a> (MIT) is visiting me this week. On Tuesday he led
            the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big














              data seminar</a>.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 14, 2016</h3>
          <p>Together with <a
              href="http://perso.telecom-paristech.fr/%7Eofercoq/">Olivier














              Fercoq</a>, a former postdoc and now an Assistant
            Professor at Telecom ParisTech, we are to receive the <span
              class="important">SIGEST Award</span> of the <a
              href="http://www.siam.org/">Society for Industrial and
              Applied Mathematics (SIAM)</a> for the paper “Accelerated,
            parallel and proximal coordinate descent”.<br>
            <br>
            The paper first appeared as a preprint on <i>arXiv</i> in
            December 2013 (<a href="http://arxiv.org/abs/1312.5799">arXiv:1312.5799</a>)
            before it was published in the <i><a
                href="http://epubs.siam.org/doi/abs/10.1137/130949993">SIAM














                Journal on Optimization (SIOPT)</a></i> in 2015. In
            addition to <i>SIOPT</i>, SIAM publishes further 16
            scholarly <a href="http://www.siam.org/journals/">journals</a>:<br>
            <br>
            &nbsp;&nbsp;&nbsp; Multiscale Modeling and Simulation<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Applied Algebra and
            Geometry<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Applied Dynamical Systems<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Applied Mathematics<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Computing<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Control and Optimization<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Discrete Mathematics<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Financial Mathematics <br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Imaging Sciences <br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Mathematical Analysis<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Matrix Analysis and
            Applications<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Numerical Analysis<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Optimization<br>
            &nbsp;&nbsp;&nbsp; SIAM Journal on Scientific Computing<br>
            &nbsp;&nbsp;&nbsp; SIAM/ASA Journal on Uncertainty
            Quantification<br>
            &nbsp;&nbsp;&nbsp; SIAM Review<br>
            &nbsp;&nbsp;&nbsp; Theory of Probability and Its
            Applications<br>
            <br>
            The paper will be reprinted in the SIGEST section of <a
              href="http://www.siam.org/journals/sirev.php"><i>SIAM
                Review</i></a> (Volume 58, Issue 4, 2016), the flagship
            journal of the society. A paper from SIOPT is chosen for the
            SIGEST award about once every two or three years. The award
            will be conferred at the <a
              href="http://www.siam.org/meetings/an17/">SIAM Annual
              Meeting in Pittsburgh in July 2017</a>.<br>
            <br>
            According to C. T. Kelley, editor-in-chief of SIAM Review,<i>
              “SIGEST highlights a recent paper from one of SIAM’s
              specialized research journals, chosen on the basis of
              exceptional interest to the entire SIAM community… The
              purpose of SIGEST is to make the 10,000+ readers of SIAM
              Review aware of exceptional papers published in SIAM's
              specialized journals. In each issue of SIAM Review, the
              SIGEST section contains an outstanding paper of general
              interest that has previously appeared in one of SIAM's
              specialized research journals; the issues rotate through
              the journals. We begin the selection by asking the
              editor-in-chief of the appropriate journal to send a short
              list of nominees, usually nominated by the associate
              editors. Then, the SIAM Review section editors make the
              final selection.”</i><i><br>
            </i><i><br>
            </i>Kelley further writes: <i>“In this case, your paper was
              recommended by members of the SIOPT editorial board and
              the editor in chief of the journal, and was selected by
              the SIREV editors for the importance of its contributions
              and topic, its clear writing style, and its accessibility
              for the SIAM community.”</i><br>
          </p>
          <p>The same paper also recently won the <a
              href="http://www.numerical.rl.ac.uk/people/nimg/fox/">17th
              Leslie Fox Prize in Numerical Analysis (2nd Prize, 2015)</a>,
            awarded biennially by the <a href="http://www.ima.org.uk/">Institute













              of Mathematics and its Applications (IMA)</a>.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 11, 2016</h3>
          <p><a href="http://www.uclouvain.be/sebastian.stich">Sebastian
              Stich</a> (Louvain) and <a
              href="http://www.damtp.cam.ac.uk/people/me404/">Matthias
              Ehrhardt</a> (Cambridge) are visiting me this week
            (Matthias is staying until Tuesday next week). Sebastian
            will lead the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">big














              data seminar</a> tomorrow and Matthias will give an <a
              href="http://www.maths.ed.ac.uk/ERGO/seminars.html">ERGO
              seminar</a> talk on Wednesday.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 9, 2016</h3>
          <p>We have a <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/jobs/lectureship-in-the-mathematics-of-data-science">Lectureship</a>
            (= Tenured Assistant Professorship) and a <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/jobs/chancellor-s-fellow-in-the-mathematics-of-data-sci">Chancellor's














              Fellowship</a> (= Tenure Track Assistant Professorship)
            position available in “Mathematics of Data Science” at the
            University of Edinburgh. Mathematics of Data Science
            includes but is not limited to areas such as Mathematical
            Optimization, Mathematics of Machine Learning, Operations
            Research, Statistics, Mathematics of Imaging and Compressed
            Sensing. <br>
            <br>
            Application deadline for the Lectureship post: May 9, 2016 <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/news?nid=669">[more














              info]</a><br>
          </p>
          <p>Application deadline for the Chancellor's Fellowship post:
            April 25, 2016 @ 5pm <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/jobs/chancellor-s-fellow-in-the-mathematics-of-data-sci">[more














              info]</a><br>
          </p>
          <p>Starting data: August 1, 2016 (or by mutual agreement)<br>
          </p>
          <p>We also have a <a
href="We%20also%20have%20a%20Chancellor%27s%20Fellowship%20post%20in%20Industrial%20Mathematics.">Chancellor's
Fellowship














              post in Industrial Mathematics.</a></p>
          <p>These positions are part of a larger activity in Edinburgh
            aimed at growing Data Science.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 8, 2016</h3>
          <p><a
href="http://www.maths.ed.ac.uk/school-of-mathematics/people/show?person=417">Dominik














              Csiba</a> is teaching a course entitled <a
              href="http://www.dominikcsiba.com/MML/index.html">Mathematics














              of Machine Learning</a> at Comenius University, Slovakia
            as of today; the course lasts until April 17th. Dominik
            developed the course himself and is offering it informally
            to anyone interested in the subject, free of charge! The
            first half of the material is based on Shai's book
            "Understanding Machine Learning: from Theory to Algorithms";
            and the second half is based on certain parts of my course
            "Modern Optimization Methods for Big Data Problems". His
            slides are in English, and the course is delivered in
            Slovak. A video recording of the course will be put online
            in due time.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 5, 2016</h3>
          <p>I've just learned that I am one of two people shortlisted
            for the <a
href="https://www.eusa.ed.ac.uk/representation/campaigns/teachingawards/nominees/">EUSA
Best














              Research or Dissertation Supervisor Award</a>. I had no
            clue I was nominated in the first place, so this came as a
            pleasant surprise! Thanks to those who nominated me! <br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>April 1, 2016</h3>
          <p>The list of the <a
              href="https://turing.ac.uk/faculty-fellows/">Faculty
              Fellows of the Alan Turing Institute</a> is live now. I am
            on the list. <br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>March 17, 2016</h3>
          <p><span class="important">New paper out: </span><a
              href="http://arxiv.org/abs/1602.02283"> </a><a
              href="http://arxiv.org/abs/1603.09649">Stochastic block
              BFGS: squeezing more curvature out of data</a>, joint with
            <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert M.
              Gower</a> and <a
              href="http://www.columbia.edu/%7Egoldfarb/">Donald
              Goldfarb</a>.<br>
          </p>
          <p>Abstract: <i>We propose a novel limited-memory stochastic
              block BFGS update for incorporating enriched curvature
              information in stochastic approximation methods. In our
              method, the estimate of the inverse Hessian matrix that is
              maintained by it, is updated at each iteration using a
              sketch of the Hessian, i.e., a randomly generated
              compressed form of the Hessian. We propose several
              sketching strategies, present a new quasi-Newton method
              that uses stochastic block BFGS updates combined with the
              variance reduction approach SVRG to compute batch
              stochastic gradients, and prove linear convergence of the
              resulting method. Numerical tests on large-scale logistic
              regression problems reveal that our method is more robust
              and substantially outperforms current state-of-the-art
              methods</i>.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>March 17, 2016</h3>
          <p>Results: 3 year postdoctoral position in big data
            optimization<br>
          </p>
          <p>I have received 61 applications for the 3 year postdoctoral
            position in big data optimization. Our of these, 13 were
            shortlisted and invited for an interview. One of the
            shortlisted applicants declined due to prior acceptance of
            another offer. Twelve excellent applicants were interviewed
            over a period of 2 days. An offer was recently made to the
            #1 ranked applicant (based on the application files,
            recommendation letters and performance in the interview),
            who accepted the post. <br>
          </p>
          <p>It is my pleasure to announce that <a
              href="http://people.kth.se/%7Ehamidrez/index.html">Dr
              Hamid Reza Feyzmahdavian</a> will join the group as of
            September 1, 2016, as a postdoc. Hamid has PhD from the
            Royal Institute of Technology (KTH), Sweden, supervised by <a
              href="http://people.kth.se/%7Emikaelj/">Prof Mikael
              Johansson</a>. His research spans several topics in
            control and optimization. In optimization, for instance, his
            past work spans topics such as analysis of the heavy ball
            method; development and analysis of randomized, asynchronous
            and mini-batch algorithms for regularized stochastic
            optimization; dual coordinate ascent for multi-agent
            optimization; asynchronous contractive iterations, and
            delayed proximal gradient method. <br>
          </p>
          <p>I wish to thank all unsuccessful applicants for expressing
            their interest in the position, and to shortlisted
            candidates for participating in the interview. Very hard
            decisions had to be made even at the shortlisting stage as
            many highly qualified applicants did not make it through due
            to necessary constraints on how many candidates it is
            feasible for us to interview. The situation was tougher yet
            at the interview stage. If I had more funding, I would be
            absolutely delighted to offer posts to several of the
            shortlisted candidates! Thank you all again, and I wish you
            best of luck in job search.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>March 10, 2016</h3>
          <p>According to <a
href="https://www.timeshighereducation.com/world-university-rankings/best-universities-in-europe-2016">2016
Times














              Higher Education World University Rankings</a>, The
            University of Edinburgh is the 7th best university in
            Europe.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>March 10, 2016</h3>
          <p>I am in Oberwolfach as of Sunday last week until tomorrow,
            attending the <a
              href="http://www.mfo.de/occasion/1610/www_view">Computationally
and














              Statistically Efficient Inference for Complex Large-Scale
              Data</a> workshop. Many of the talks so far were extremely
            interesting, and some downright entertaining! <a
              href="http://stat.ethz.ch/%7Ebuhlmann/">Peter Buhlmann</a>
            is a true master of ceremony ;-) <br>
          </p>
          <p>On Tuesday, I talked about stochastic methods for solving
            linear systems and inverting matrices; the talk is based on
            a trio of recent papers written in collaboration with <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert M
              Gower</a>:<br>
          </p>
          <p><a href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">Randomized
iterative














              methods for linear systems</a><br>
            <a href="http://arxiv.org/abs/1512.06890">Stochastic dual
              ascent for solving linear systems</a><br>
            <a href="http://arxiv.org/abs/1602.01768">Randomized
              quasi-Newton updates are linearly convergent matrix
              inversion algorithms</a><br>
          </p>
          <p>I mostly talked about the first two papers; but managed to
            spend a bit of time at the end on matrix inversion as well.
            Here are the <a href="talks/TALK-2016-03-Oberwolfach.pdf">slides</a>.<br>
          </p>
          <a href="http://epubs.siam.org/doi/abs/10.1137/15M1025487"> </a>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>March 1, 2016</h3>
          <p>As of today, we have a new group member. <a
              href="https://www.researchgate.net/profile/Tie_Ni">Tie Ni</a>
            is an Associate Professor at the Liaoning Technical
            University, China. He will stay in Edinburgh for a year as a
            postdoc. Tie obtained his PhD in mathematics from Tianjin
            University in 2010.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>February 29, 2016</h3>
          <p><a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert M.
              Gower</a> submitted his PhD thesis <i>"Sketch and
              Project: Randomized Iterative Methods for Linear Systems
              and Inverting Matrices"</i> today. As of tomorrow, he will
            become a postdoc in my group; I am looking forward to
            working with him for the next 4 months. After that, Robert
            will join the <a
              href="http://www.di.ens.fr/sierra/index.php">SIERRA team</a>.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>February 24, 2016</h3>
          <p>Shortlisting for the 3-year postdoc post will be finalized
            this week. We shall get in touch with the shortlisted
            candidates by the end of the week (Sunday). <i><br>
            </i></p>
          <p><i>Update (February 28):</i> All shortlisted candidates
            have now been notified via email.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>February 16, 2016</h3>
          <p>We are continuing with the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














              Hands Meetings in Big Data Optimization</a> this semester,
            thanks to funding from the <a
              href="http://www.maths.ed.ac.uk/%7Eigordon/">Head of
              School</a>. We already had two meetings, the third one is
            today at 12:15 in JCMB 5323. <a
              href="http://jakubkonecny.com/">Jakub Konečný</a> will
            speak about the NIPS 2015 paper <a
href="http://papers.nips.cc/paper/5717-taming-the-wild-a-unified-analysis-of-hogwild-style-algorithms.pdf">Taming
the














              wild: a unified analysis of Hogwild!-style algorithms</a>
            by De Sa, Zhang, Olukotun and Re. </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>February 15, 2016</h3>
          <p>I am visiting Cambridge this week.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>January 11, 2016</h3>
          <br>
          <a href="http://www.di.ens.fr/%7Easpremon/Houches/index.html">Workshop</a>
          group photo:<br>
          <br>
          <p> </p>
          <p> <a href="imgs/LesHouches2016.jpg"><img
                src="imgs/LesHouches2016_small.png" alt="Les Houches
                2016" width="600" border="0" height="400"></a> </p>
          <p> </p>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <h3>February 9, 2016</h3>
          <p><span class="important">New paper out: </span><a
              href="http://arxiv.org/abs/1602.02283">Importance sampling
              for minibatches</a>, joint with <a
              href="http://www.dominikcsiba.com/">Dominik Csiba</a>.<br>
          </p>
          <p>Abstract:<br>
          </p>
          <p> <i>Minibatching is a very well studied and highly popular
              technique in supervised learning, used by practitioners
              due to its ability to accelerate training through better
              utilization of parallel processing power and reduction of
              stochastic variance. Another popular technique is
              importance sampling -- a strategy for preferential
              sampling of more important examples also capable of
              accelerating the training process. However, despite
              considerable effort by the community in these areas, and
              due to the inherent technical difficulty of the problem,
              there is no existing work combining the power of
              importance sampling with the strength of minibatching. In
              this paper we propose the first importance sampling for
              minibatches and give simple and rigorous complexity
              analysis of its performance. We illustrate on synthetic
              problems that for training data of certain properties, our
              sampling can lead to several orders of magnitude
              improvement in training time. We then test the new
              sampling on several popular datasets, and show that the
              improvement can reach an order of magnitude. </i> </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>February 7, 2016</h3>
          <p>As of today, I am in Les Houches, France, attending the <a
              href="http://www.di.ens.fr/%7Easpremon/Houches/index.html">"Optimization
without














              Borders"</a> workshop. <a
              href="http://www.dominikcsiba.com/">Dominik</a>, <a
              href="http://www.jakubkonecny.com/">Jakub</a> and <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert</a>
            are here, too (Robert beat me in table tennis 1:2 tonight;
            he is so getting beaten tomorrow). The workshop is dedicated
            to the 60th birthday of Yurii Nesterov - my former postdoc
            advisor.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>February 4, 2016</h3>
          <p><span class="important">New paper out: </span><a
              href="http://arxiv.org/abs/1602.01768">Randomized
              quasi-Newton updates are linearly convergent matrix
              inversion algorithms</a>, joint with <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>.<br>
          </p>
          <p>Abstract:<br>
          </p>
          <p><i>We develop and analyze a broad family of
              stochastic/randomized algorithms for inverting a matrix.
              We also develop a specialized variant which maintains
              symmetry or positive definiteness of the iterates. All
              methods in the family converge globally and linearly
              (i.e., the error decays exponentially), with explicit
              rates. <br>
            </i></p>
          <p><i>In special cases, we obtain stochastic block variants of
              several quasi-Newton updates, including bad Broyden (BB),
              good Broyden (GB), Powell-symmetric-Broyden (PSB),
              Davidon-Fletcher-Powell (DFP) and
              Broyden-Fletcher-Goldfarb-Shanno (BFGS). Ours are the
              first stochastic versions of these updates shown to
              converge to an inverse of a fixed matrix. <br>
            </i></p>
          <p><i>Through a dual viewpoint we uncover a fundamental link
              between quasi-Newton updates and approximate inverse
              preconditioning. Further, we develop an adaptive variant
              of randomized block BFGS, where we modify the distribution
              underlying the stochasticity of the method throughout the
              iterative process to achieve faster convergence. <br>
            </i></p>
          <p><i>By inverting several matrices from varied applications,
              we demonstrate that AdaRBFGS is highly competitive when
              compared to the well established Newton-Schulz and minimal
              residual methods. In particular, on large-scale problems
              our method outperforms the standard methods by orders of
              magnitude. <br>
            </i></p>
          <p><i>Development of efficient methods for estimating the
              inverse of very large matrices is a much needed tool for
              preconditioning and variable metric methods in the advent
              of the big data era.</i><br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>January 22, 2016</h3>
          <p>Today I am examining a Machine Learning PhD thesis at the <a
              href="http://www.ed.ac.uk/informatics">School of
              Informatics</a>.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>January 19, 2016</h3>
          <p><span class="important"></span>Today, I am giving a talk on
            randomized iterative methods for solving linear systems (see
            papers [<a
              href="http://epubs.siam.org/doi/abs/10.1137/15M1025487">1</a>,
            <a href="http://arxiv.org/abs/1512.06890">2</a>]) in our <a
href="http://www.maths.ed.ac.uk/events/nbp/working-probability">Working
              Probability Seminar</a>. Next Tuesday, <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>
            will speak about randomized iterative methods for inverting
            very large matrices (a preprint of this work should be
            available on arXiv by the end of January).&nbsp;&nbsp; <br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>January 17, 2016</h3>
          <p><span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1512.09103">Even faster
              accelerated coordinate descent using non-uniform sampling</a>,
            joint with <a href="http://people.csail.mit.edu/zeyuan/">Zeyuan














              Allen-Zhu</a>, <a
              href="http://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a> and
            <a href="http://www.callowbird.com/">Yang Yuan</a>. </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>January 14, 2016</h3>
          <p><a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert
              Gower</a> is visiting Cambridge and giving a talk today or
            tomorrow...<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>January 14, 2016</h3>
          <p>Today and tomorrow I am in Stockholm, Sweden, as an
            external examiner (opponent) for a PhD thesis at <a
              href="https://www.kth.se/en">KTH Royal Institute of
              Technology</a>.<br>
          </p>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>January 12, 2016</h3>
          <span class="important">Open Position: Postdoctoral Research
            Associate in Big Data Optimization </span>
          <p>A postdoctoral position in big data optimization is
            available at the School of Mathematics, University of
            Edinburgh, under the supervision of Dr Peter Richtarik. The
            post is funded through the <a
              href="http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/N005538/1">EPSRC
Fellowship














              "Randomized Algorithms for Extreme Convex Optimization”.</a>
          </p>
          <p>PhD in optimization, computer science, applied mathematics,
            engineering, operations research, machine learning or a
            related discipline is required. Strong research track record
            is essential.</p>
          <p> Duration: 3 years<br>
            Starting date: August or September 2016<br>
            Research travel budget <br>
            Application closing date: January 29, 2016<br>
          </p>
          More information and online application form:<br>
          <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=034907"
            target="_blank">https://www.vacancies.ed.ac.<wbr>uk/pls/corehrrecruit/erq_<wbr>jobspec_version_4.jobspec?p_<wbr>id=034907</a><br>
          &nbsp;<br>
          The University of Edinburgh is a founding partner of the Alan
          Turing Institute -- the national data science institute.
          Edinburgh is the home of Archer, the national supercomputing
          facility.<br>
          &nbsp;<br>
          For informal inquiries, send me an email.<br>
          <br>
          <h3><img alt="" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>January 10, 2016</h3>
          <p> <span class="important">Prize: </span> <a
              href="http://mtakac.com/">Martin Takáč</a> is the winner
            of the <span class="important">2014 OR Society Doctoral
              Prize.</span> This is an annual award of the <a
              href="https://en.wikipedia.org/wiki/Operational_Research_Society">OR














              Society</a> for "<i>the most distinguished body of
              research leading to the award of a doctorate in the field
              of Operational Research</i>”. A cash prize of £1500 is
            attached to the award. Martin's thesis, "Randomized
            coordinate descent methods for big data optimization",
            defended in 2014, <a
href="http://mtakac.com/data/_uploaded/file/papers/2014/takac_phd_thesis_final.pdf">can
be














              found here</a>.&nbsp; </p>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <h3>January 8, 2016</h3>
          <br>
          <a href="http://www.maths.ed.ac.uk/%7Es1065527/index.html">Robert


















            Gower</a> gave a seminar talk in Paris (<a
            href="http://www.di.ens.fr/sierra/people.php">SIERRA team</a>).<br>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20"><br>
          <h3>January 8, 2016</h3>
          <br>
          <a href="http://mtakac.com/">Martin Takáč</a>'s "traditional"
          Christmas cookies:<br>
          <br>
          <p> <a href="imgs/cookies.jpg"><img alt="cookies"
                src="imgs/cookies.jpg" width="500" border="0"
                height="401"></a> </p>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <h3>December 21, 2015 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1512.06890">Stochastic dual
              ascent for solving linear systems</a>, joint with <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>
          </p>
          Abstract:
          <p> <i>We develop a new randomized iterative
              algorithm---stochastic dual ascent (SDA)---for finding the
              projection of a given vector onto the solution space of a
              linear system. The method is dual in nature: with the dual
              being a non-strongly concave quadratic maximization
              problem without constraints. In each iteration of SDA, a
              dual variable is updated by a carefully chosen point in a
              subspace spanned by the columns of a random matrix drawn
              independently from a fixed distribution. The distribution
              plays the role of a parameter of the method. </i></p>
          <i> </i>
          <p><i> Our complexity results hold for a wide family of
              distributions of random matrices, which opens the
              possibility to fine-tune the stochasticity of the method
              to particular applications. We prove that primal iterates
              associated with the dual process converge to the
              projection exponentially fast in expectation, and give a
              formula and an insightful lower bound for the convergence
              rate. We also prove that the same rate applies to dual
              function values, primal function values and the duality
              gap. Unlike traditional iterative methods, SDA converges
              under no additional assumptions on the system (e.g., rank,
              diagonal dominance) beyond consistency. In fact, our lower
              bound improves as the rank of the system matrix drops. </i></p>
          <p><i> Many existing randomized methods for linear systems
              arise as special cases of SDA, including randomized
              Kaczmarz, randomized Newton, randomized coordinate
              descent, Gaussian descent, and their variants. In special
              cases where our method specializes to a known algorithm,
              we either recover the best known rates, or improve upon
              them. Finally, we show that the framework can be applied
              to the distributed average consensus problem to obtain an
              array of new algorithms. The randomized gossip algorithm
              arises as a special case.</i> </p>
          <br>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <h3>December 15, 2015</h3>
          <p>I have accepted an invite to give a keynote talk (actually,
            a combo of two 1hr talks: one to a general audience, and one
            more specialized) at the 41st Woudschoten Conference, to be
            held during 5-7 October, 2016 in Zeist, Netherlands. Here is
            a <a
              href="https://wsc.project.cwi.nl/woudschoten/2015/conferentieE.php">link
to














              the website of the 2015 edition</a>.<br>
          </p>
          <p>For the 2016 conference, the following three themes have
            been selected:</p>
          <p>1.&nbsp; Numerical methods for big data analytics <br>
            2.&nbsp; Monte Carlo methods for partial and stochastic
            differential equations <br>
            3.&nbsp; Mixed finite element methods <br>
            <br>
            I have been invited to be a keynote lecturer within the
            theme "Numerical methods for big data analytics".<br>
            <br>
          </p>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <h3>December 13, 2015 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1512.04039">Distributed
              optimization with arbitrary local solvers,</a> joint with
            <a href="http://mtakac.com/"> </a><a
              href="https://ise.lehigh.edu/content/chenxin-ma">Chenxin
              Ma</a> (Lehigh) <a href="http://jakubkonecny.com/">Jakub
              Konečný</a> (Edinburgh), <a
              href="http://people.inf.ethz.ch/jaggim/">Martin Jaggi</a>
            (ETH), <a href="http://www.cs.berkeley.edu/%7Evsmith/">Virginia














              Smith</a> (Berkeley), <a
              href="http://www.cs.berkeley.edu/%7Ejordan/">Michael I.
              Jordan</a> (Berkeley) and <a href="http://mtakac.com/">Martin














              Takáč</a> (Lehigh).<br>
          </p>
          <p>Abstract: <i>With the growth of data and necessity for
              distributed optimization methods, solvers that work well
              on a single machine must be re-designed to leverage
              distributed computation. Recent work in this area has been
              limited by focusing heavily on developing highly specific
              methods for the distributed environment. These
              special-purpose methods are often unable to fully leverage
              the competitive performance of their well-tuned and
              customized single machine counterparts. Further, they are
              unable to easily integrate improvements that continue to
              be made to single machine methods. To this end, we present
              a framework for distributed optimization that both allows
              the flexibility of arbitrary solvers to be used on each
              (single) machine locally, and yet maintains competitive
              performance against other state-of-the-art special-purpose
              distributed methods. We give strong primal-dual
              convergence rate guarantees for our framework that hold
              for arbitrary local solvers. We demonstrate the impact of
              local solver selection both theoretically and in an
              extensive experimental comparison. Finally, we provide
              thorough implementation details for our framework,
              highlighting areas for practical performance gains.</i><br>
            <br>
          </p>
          <img alt="" src="imgs/fancy-line.png" width="196" height="20">
          <h3>December 10, 2015</h3>
          <br>
          The workshop <a
            href="http://icms.org.uk/workshop.php?id=391#programme">"Mathematical
aspects


















            of big data"</a>, which I am co-organizing, has just
          started. This is a joint meeting of the <a
            href="https://www.lms.ac.uk/">London Mathematical Society</a>
          (LMS) and the <a href="http://www.ems.ac.uk/">Edinburgh
            Mathematical Society</a> (EMS). This event marks the end of
          the 150th anniversary celebrations of the LMS.<br>
          <br>
          <i>The mathematical aspects of the analysis of big data cut
            across pure mathematics, applied mathematics, and
            statistics. The invited speakers at this workshop will
            include a broad range of international experts in
            mathematics, statistics, and computer science, whose
            research covers fields that are inspired by, or have
            applications to, big data analysis.</i><i> The workshop is
            aimed at an audience of general mathematicians but is open
            to all and attendance is free of charge. It will cover
            current trends and developments, and will hopefully enable
            participants to discover or imagine new connections between
            their own research and this rapidly growing subject.</i><br>
          <br>
          Speakers:<br>
          <br>
          Jacek Brodzki, University of Southampton<br>
          Coralia Cartis, University of Oxford<br>
          Ronald Coifman, Yale University <br>
          Ilias Diakonikolas, University of Edinburgh<br>
          Colin McDiarmid, University of Oxford<br>
          Sofia Olhede, University College London<br>
          Igor Rivin, University of St. Andrews<br>
          Marian Scott, University of Glasgow<br>
          Eva Tardos, Cornell University <br>
          <br>
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>December 2, 2015</h3>
          <p>The <a href="http://arxiv.org/abs/1409.1458">CoCoA</a>
            [NIPS 2014] / <a
              href="http://www.jmlr.org/proceedings/papers/v37/mab15.html">CoCoA+</a>
            [ICML 2015] distributed optimization algorithm developed in
            a duo of papers with two co-authors from Edinburgh (<a
              href="http://mtakac.com/">Martin Takáč</a>, myself) has
            won the <a
href="http://mlconf.com/mlconf-industry-impact-student-research-award-winners/">MLconf
Industry














              Impact Student Research Award</a>. The award goes to our
            coauthor <a href="http://www.cs.berkeley.edu/%7Evsmith/">Virginia
Smith














              (UC Berkeley)</a>. Other co-authors: <a
              href="http://people.inf.ethz.ch/jaggim/">M. Jaggi</a> (ETH
            Zurich), <a href="http://www.cs.berkeley.edu/%7Ejordan/">M.I.














              Jordan</a> (Berkeley), <a
              href="https://ise.lehigh.edu/content/chenxin-ma">C. Ma</a>
            (Lehigh), <a
              href="http://statistics.berkeley.edu/people/jonathan-terhorst">J.














              Terhorst</a> (UC Berkeley), <a
              href="https://www.ocf.berkeley.edu/%7Esanjayk/">S.
              Krishnan</a> (UC Berkeley), <a
              href="http://www.da.inf.ethz.ch/people/ThomasHofmann/">T.
              Hofmann</a> (ETH Zurich).<br>
          </p>
          <p><i>About the award:</i> <span style="font-weight: 400;">"This














              year, we started a new award program called the MLconf
              Industry Impact Student Research Award, which is sponsored
              by Google. This fall, our committee of distinguished ML
              professionals reviewed several nominations sent in from
              members of the MLconf community. There were several great
              researchers that were nominated and the committee arrived
              at awarding 2 students whose work, they believe, has the
              potential to disrupt the industry in the future. The two
              winners that were announced at MLconf SF 2015 are UC
              Irvine Student, Furong Huang and UC Berkeley Student,
              Virginia Smith. Below are summaries of their research.
              We’ve invited both researchers to present their work at
              upcoming MLconf events."<br>
            </span></p>
          <p class="entry-title"><span style="font-weight: 400;"><i>The
                citation:</i> "</span><span style="font-weight: 400;"> <span
                style="font-weight: 400;">Virginia Smith’s research
                focuses on distributed optimization for large-scale
                machine learning. The main challenge in many large-scale
                machine learning tasks is to solve an optimization
                objective involving data that is distributed across
                multiple machines. In this setting, optimization methods
                that work well on single machines must be re-designed to
                leverage parallel computation while reducing
                communication costs. This requires developing new
                distributed optimization methods with both competitive
                practical performance and strong theoretical convergence
                guarantees. Virginia’s work aims to determine policies
                for distributed computation that meet these
                requirements, in particular through the development of a
                novel primal-dual framework, CoCoA, which is written on
                Spark. The theoretical and practical development of
                CoCoA is an important step for future data scientists
                hoping to deploy efficient large-scale machine learning
                algorithms.</span>"<br>
            </span></p>
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>December 2, 2015</h3>
          <p><span class="important">Alan Turing Workshop:</span><span
              class="important"> Theoretical and computational
              approaches to large scale inverse problems</span> </p>
          The <a
            href="http://icms.org.uk/workshops/largescaleinverseproblems">workshop</a>
          starts today. We have a line-up of excellent speakers and
          exciting topics. Most importantly, this workshop will inform a
          part of the future research activity of the newly established
          <a href="https://turing.ac.uk/">Alan Turing Institute:</a>
          UK's national research centre for <a
            href="https://en.wikipedia.org/wiki/Data_science">Data
            Science</a>.
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 28, 2015</h3>
          <p><span class="important">2 POSITIONS starting in September
              2016: 3-year postdoc post + PhD post. </span> I am
            looking for 2 highly talented and motivated people to join
            my <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_team.html">big














              data optimization team</a>. </p>
          <p> The closing date for applications for the postdoctoral
            post is on January 29, 2016. <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=034907">Apply
online














              here.</a> To work with me, you may also wish to apply for
            funding through the <a
href="https://turing.ac.uk/content/uploads/2015/06/ATI_fellows_advertfinal131115-NTv02.docx">Alan
Turing














              Fellowship Programme</a>. </p>
          <p> To apply for the PhD post, <a
              href="http://www.maths.ed.ac.uk/studying-here/pgr/phd-application/apply">click














              here</a> and choose the "Operational Research and
            Optimization" PhD programme. Apply as soon as possible. You
            may also wish to apply to our <a
              href="http://datascience.inf.ed.ac.uk/apply/">PhD
              programme in Data Science</a> - this is another way how
            you can get a funded post to work with me. The closing date
            for applications is also January 29, 2016 (for applicants
            from outside the UK/EU, the deadline is December 11, 2015).
          </p>
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 25, 2015</h3>
          <p><span class="important">Alan Turing Workshop: Distributed
              Machine Learning and Optimization<br>
            </span></p>
          The <a href="http://www.icms.org.uk/workshop.php?id=367">workshop</a>
          starts today, I am looking forward to seeing you all there! We
          have a line-up of excellent speakers and exciting topics. Most
          importantly, this workshop will inform a part of the future
          research activity of the newly established <a
            href="https://turing.ac.uk/">Alan Turing Institute:</a> UK's
          national research centre for <a
            href="https://en.wikipedia.org/wiki/Data_science">Data
            Science</a>. <br>
          <br>
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 17, 2015</h3>
          <p><span class="important">Alan Turing Fellowships</span></p>
          <p><a href="https://turing.ac.uk/">The Alan Turing Institute</a>
            is the UK's new national data science institute, established
            to bring together world-leading expertise to provide
            leadership in the emerging field of <a
              href="https://en.wikipedia.org/wiki/Data_science">data
              science</a>. The Institute has been founded by the
            universities of Cambridge, Edinburgh, Oxford, UCL and
            Warwick and EPSRC.<br>
            <br>
            Fellowships for Early Career Researchers are available for 3
            years with the potential for an additional 2 years of
            support following interim review. Fellows will pursue
            research based at the Institute hub in the British Library,
            London. Fellowships will be awarded to individual candidates
            and fellows will be employed by a joint<br>
            venture partner university (Cambridge, Edinburgh, Oxford,
            UCL or Warwick).</p>
          <p>The closing date for applications is 20 December 2015.<br>
            Key requirements: Successful candidates are expected to have
            <br>
            &nbsp; i) a PhD in a data science (or adjacent) subject (or
            to have submitted their doctorate before taking up the
            post), <br>
            &nbsp; ii) an excellent publication record and/or
            demonstrated excellent research potential such as via
            preprints, <br>
            &nbsp; iii) a novel and challenging research agenda that
            will advance the strategic objectives of the Institute, and
            <br>
            &nbsp; iv) leadership potential. Fellowships are open to all
            qualified applicants regardless of background.<br>
            <br>
            Alan Turing Fellowship applications can be made in all data
            science research areas. The Institute’s research roadmap is
            available at <a href="https://turing.ac.uk/#the-vision">https://turing.ac.uk/#the-vision</a>.
            <br>
          </p>
          <p>In addition to this open call, there are two specific
            fellowship programmes:<br>
            <br>
            <span class="important">Fellowships addressing data-centric
              engineering</span></p>
          <p><a href="http://www.lrfoundation.org.uk/">The Lloyd’s
              Register Foundation (LRF)</a> / Alan Turing Institute
            programme to support data-centric engineering is a 5-year,
            £10M global programme, delivered through a partnership
            between LRF and the Alan Turing Institute. This programme
            will secure high technical standards (for example the
            next-generation algorithms and analytics) to enhance the
            safety of life and property around the major infrastructure
            upon which modern society relies. For further information on
            data-centric engineering, see LRF’s Foresight Review of Big
            Data. Applications for Fellowships under this call, which
            address the aims of the LRF/Turing programme, may also be
            considered for funding under the data-centric engineering
            programme. Fellowships awarded under this programme may vary
            from the conditions given above; for more details contact
            fellowship@turing.ac.uk.<br>
            <br>
            Fellowships addressing data analytics and high-performance
            computing Intel and the Alan Turing Institute will be
            supporting additional Fellowships in data analytics and
            high-performance computing. Applications for Fellowships
            under this call may also be considered for funding under the
            joint Intel-Alan Turing Institute programme. Fellowships
            awarded under this joint programme may vary from the
            conditions given above; for more details contact
            fellowship@turing.ac.uk.<br>
            <br>
            <a
href="https://turing.ac.uk/content/uploads/2015/06/ATI_fellows_advertfinal131115-NTv02.docx">Download
full














              information on the Turing fellowships</a>.<br>
            <br>
            Diversity and equality are promoted in all aspects of the
            recruitment and career management of our researchers. In
            keeping with the principles of the Institute, we especially
            encourage applications from female researchers.<br>
          </p>
          <p>I would be happy to closely work with successful applicants
            interested in working in the areas of big data optimization
            / machine learning / numerical linear algebra. If you have a
            strong background, are considering to apply and want to chat
            about this, send me an email.<br>
          </p>
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 17, 2015</h3>
          <p><span class="important">Tenured Position in my School:</span>
            <a
href="http://www.maths.ed.ac.uk/news/2015/lectureship-in-mathematical-sciences">Assistant
Professor














              or Associate Professor post in Mathematical Sciences.</a>
            Preference may be given to candidates who strengthen
            existing research interests in the School or connections
            between them. I would welcome strong applicants in
            Optimization, Operational Research, Statistical Learning and
            Data Science. Starting date: Aug 1, 2016 or by agreement.
            Apply by December 9, 2015.<br>
          </p>
          <h3><img alt=";-)" src="imgs/fancy-line.png" width="196"
              height="20"></h3>
          <h3>November 16, 2015</h3>
          <p>This week, I am again in Louvain-la-Neuve, Belgium,
            teaching the course <a
              href="http://sites.uclouvain.be/socn/Courses/Courses2015-2">Randomized
algorithms














              for big data optimization </a>within the <a
              href="http://sites.uclouvain.be/socn/">SOCN Graduate
              School in Systems, Optimization, Control and Networks</a>.
            Course material for this week: <a href="docs/SOCN-Lec4.pdf">Lecture













              4</a>, <a href="docs/SOCN-Lab4.ipynb">Lab 4</a>, <a
              href="docs/SOCN-Lec5.pdf">Lecture 5</a>, <a
              href="docs/SOCN-Lab5.ipynb">Lab 5</a>. </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="20"></h3>
          <h3>October 27, 2015</h3>
          <p>Today I gave a seminar talk at Universite catholique de
            Louvain, Belgium.<br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="20"></h3>
          <h3>October 26, 2015</h3>
          <p>This week, I am in Louvain-la-Neuve, Belgium, teaching the
            course <a
              href="http://sites.uclouvain.be/socn/Courses/Courses2015-2">Randomized
algorithms














              for big data optimization </a>within the <a
              href="http://sites.uclouvain.be/socn/">SOCN Graduate
              School in Systems, Optimization, Control and Networks</a>.
            Course material: <a href="docs/SOCN-Lec1.pdf">Lecture 1</a>,
            <a href="docs/SOCN-Lab1.ipynb">Lab 1</a>, <a
              href="docs/SOCN-Lec2.pdf">Lecture 2 (and more)</a>, <a
              href="docs/SOCN-Lab2.ipynb">Lab 2</a>, <a
              href="docs/SOCN-Lec3.pdf">Lecture 3</a>, <a
              href="docs/SOCN-Lab3.zip">Lab 3</a>.<br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="20"></h3>
          <h3>October 23, 2015</h3>
          <p>Jakub Kisiala, an MSc student I had the pleasure to teach
            in my Optimization Methods in Finance class and whose <a
href="http://www.maths.ed.ac.uk/%7Erichtarik/docs/Kisiala_Dissertation.pdf">MSc














              Dissertation</a> I supervised has won the <a
              href="http://msc.maths.ed.ac.uk/or/students/prize">Prize
              for Best Performance on the Operational Research MSc</a>
            in the 2014-2015 academic year.<br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="20"></h3>
          <h3>October 22, 2015</h3>
          <p><a href="http://jakubkonecny.com/">Jakub Konečný </a>is
            visiting Comenius University this week; he gave a <a
href="http://www.fmph.uniba.sk/index.php?id=186&amp;no_cache=1&amp;tx_ttnews%5Btt_news%5D=2203&amp;tx_ttnews%5BbackPid%5D=17&amp;cHash=9f366814dd">seminar














              talk</a> there yesterday. <a
              href="http://www.dominikcsiba.com/">Dominik Csiba</a> is
            on a research visit in <a
              href="http://lear.inrialpes.fr/people/mairal/">Julien
              Mairal</a>'s group in Grenoble. He gave a talk on AdaSDCA
            on Monday in the <a
              href="https://sites.google.com/site/smileingrenoble/seminar-ogre">OGre














              seminar</a>. I am giving a guest lecture today, on
            Randomized Methods for Linear Systems (based on <a
              href="http://arxiv.org/abs/1506.03296">this paper</a>), in
            the <a
              href="http://www.inf.ed.ac.uk/teaching/courses/irds/">"Introduction














              to Research in Data Science"</a> doctoral course to the
            students in our <a href="http://datascience.inf.ed.ac.uk/">Data














              Science PhD programme.</a><br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="20"></h3>
          <h3>October 21, 2015</h3>
          <p>Having been away from internet for a week (I am behind my
            email; so if you are expecting a response from me, I hope to
            be able to take care of all of the backlog in the next few
            weeks), I am now in Paris at the <a
              href="http://dsaa2015.lip6.fr/">2015 IEEE International
              Conference on Data Science and Advanced Analytics</a>.
            Today I am giving a tutorial entitled "Randomized Methods
            for Big Data: from Linear Systems to Optimization". Update:
            <a href="talks/TALK-2015-DSAA-tutorial.pdf">the slides are
              here</a>. </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="20"></h3>
          <h3>October 6, 2015</h3>
          <p>I am visiting <a href="http://www.ox.ac.uk/">Oxford </a>for














            a few days. (Just arrived at the train station and heading
            to my place in a taxi. Passed by a huge crowd of happy
            freshmen hoping to get into a club or bar or something.
            Judging by the numbers, it looked quite hopeless, although
            this did not diminish their enthusiasm so maybe something
            else was going on over there...). Tomorrow I am serving as
            an external examiner for a PhD thesis at the <a
              href="https://www.maths.ox.ac.uk/">Mathematical Institute</a>
            and the day after I am giving a <a
              href="https://www.maths.ox.ac.uk/node/14919">seminar talk</a>.
            If anyone of you locals wants to meet with me, I am staying
            at the <a
href="http://www.chem.ox.ac.uk/oxfordtour/exetercollege/Map-Exeter-College---Landscape-Colour.jpg">Exeter














              College</a>.<br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="20"></h3>
          <h3>September 30, 2015</h3>
          <p>Our paper <a href="http://arxiv.org/abs/1506.03296">Randomized
iterative














              methods for linear systems</a> (joint with <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert M
              Gower</a>) was accepted to SIAM Journal on Matrix Analysis
            and Applications. Here are the <a
              href="talks/TALK-2015-09-Linear_Systems.pdf">slides</a>
            from a recent talk I gave about this work.<br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="20"></h3>
          <h3>September 28, 2015 </h3>
          <p>I am in <a href="http://www.cms.cam.ac.uk/">Cambridge</a>
            as of today, attending <a href="https://turing.ac.uk/">The
              Alan Turing Institute</a> (TATI) scoping workshop on <a
              href="https://atiworkshopstatscs.wordpress.com/">"Statistical














              and Computational Challenges in Large-Scale Data Analysis"</a>.
            This is the 2nd of <a
              href="https://turing.ac.uk/#data-summits-workshops">several
scoping














              workshops</a> taking place between September and December
            2015, aimed at shaping the research agenda of TATI for the
            years to come. I am co-organizing two TATI scoping workshops
            in Edinburgh later this year: one focusing on <a
              href="http://www.icms.org.uk/workshop.php?id=367">distributed














              achine learning &amp; optimization</a> and the other one
            on <a
              href="http://icms.org.uk/workshops/largescaleinverseproblems">large-scale
inverse














              problems</a>. <br>
          </p>
          <h3><img src="imgs/fancy-line.png" align="absmiddle"
              height="20"></h3>
          <h3>September 21, 2015 </h3>
          <p> Today I am giving a talk at the <a
href="http://www.damtp.cam.ac.uk/user/cbs31/LMS_Inverse_Day_Edinburgh/Home.html">LMS
Inverse














              Day on "Large-scale and nonlinear inverse problems"</a>. I
            do not have to travel far for this as the event is taking
            place on my campus. I will be speaking about a recent joint
            work with <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert













              M Gower</a> on <a href="http://arxiv.org/abs/1506.03296">randomized














              iterative methods for linear systems</a>. My talk
            certainly does not belong to the "nonlinear" category, but
            fortunately it does belong to the "large-scale" category
            which allowed me to sneak it in ;-) <br>
          </p>
          <p>If you want to see how methods such as randomized Kaczmarz,
            randomized coordinate descent, randomized Newton and
            randomized Gaussian descent (and many others) all arise as
            special cases of a single unifying method that admits
            complexity analysis in its general form, you may wish to
            have a brief look at the <a
              href="http://arxiv.org/abs/1506.03296">paper</a> or skim
            through the <a href="TALK-2015-09-Linear_Systems">slides</a>
            (I will only cover a subset of these slides in the
            workshop).<br>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>September 14, 2015 </h3>
          <p> I am in Toulouse this week, lecturing in a <a
              href="http://www.irit.fr/cimi-machine-learning/node/1">Machine
Learning














              Summer School.</a> This is part of a larger event,<a
              href="http://www.irit.fr/cimi-machine-learning/"> Machine
              Learning Thematic Trimester</a>, which also includes
            several workshops which will be run throughout the year. My
            course is an introduction to optimization for machine
            learning. Here are the <a
              href="talks/2015-09-Toulouse-Summer-School-Optimization.pdf">slides</a>.
            Julia code for the practical session (based on <a
              href="https://www.juliabox.org/">JuliaBox</a>) is <a
              href="code/dfSDCA.zip">here.</a> <br>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>September 8, 2015 </h3>
          <p> I am returning back to Edinburgh today after a week-long
            visit in Austria. I first attended a conference in Vienna,
            and then visited <a href="https://ist.ac.at/">IST Austria</a>,
            where I gave a <a
href="https://ist.ac.at/events/lectures-talks/seminar-talks/2015/09/sdna-stochastic-dual-newton-ascent-for-empirical-risk-minimization/date/462/">talk</a>
            yesterday. I had nice discussions throughout my stay with <a
href="https://ist.ac.at/research/research-groups/barton-group/">Nick
              Barton</a>, <a
href="https://ist.ac.at/research-groups-pages/barton-group/team/katka-bodova/">Katka














              Bodova</a>, <a
              href="https://ist.ac.at/research/research-groups/henzinger-group/">Thomas














              Henzinger</a>, <a
              href="http://pub.ist.ac.at/%7Eaklimova/">Anna Klimova</a>,
            <a
              href="https://ist.ac.at/research/research-groups/kolmogorov-group/">Vladimir














              Kolmogorov</a>, <a
              href="https://ist.ac.at/research/research-groups/lampert-group/">Christoph














              Lampert</a> and <a
              href="https://ist.ac.at/research/research-groups/uhler-group/">Caroline














              Uhler</a>. Thanks!</p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>September 5, 2015 </h3>
          <p> Our paper ``Quartz: Randomized Dual Coordinate Ascent with
            Arbitrary Sampling'', joint with <a
              href="http://hkumath.hku.hk/%7Ezhengqu/">Zheng Qu</a> and
            <a href="http://www.stat.rutgers.edu/home/tzhang/">Tong
              Zhang,</a> has been <a
              href="https://nips.cc/Conferences/2015/AcceptedPapers">accepted
to














              NIPS.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>September 2, 2015 </h3>
          <p> I am in Vienna this week, attending the <a
              href="https://or2015.univie.ac.at/home/">"OR2015: Optimal
              Decisions and Big Data"</a> conference. I have given a
            talk on <a href="http://arxiv.org/abs/1502.02268">SDNA</a>
            today. <a
              href="http://www.maths.ed.ac.uk/people/show?person=417">Dominik</a>
            is here, too - he talked about the <a
              href="http://www.jmlr.org/proceedings/papers/v37/csiba15.pdf">AdaSDCA</a>
            algorithm. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>August 26, 2015 </h3>
          <p> I have been awarded a <a
              href="http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/N005538/1">5-year
EPSRC














              Fellowship,</a> starting in January 2016. The project
            title is: <span class="important">Randomized Algorithms for
              Extreme Convex Optimization.</span> </p>
          <p> A total of <a
href="http://gow.epsrc.ac.uk/NGBOViewPanelROL.aspx?PanelId=1-2VB991&amp;RankingListId=1-2VB998">5
              Fellowships</a> were awarded this year, out of a total of
            <a
href="http://gow.epsrc.ac.uk/NGBOViewPanelROL.aspx?PanelId=1-2Q1DOC&amp;RankingListId=1-2Q1DPX">43














              proposals</a> across all areas of mathematics and all
            levels of seniority (established career, early career and
            postdoctoral fellowships). It is clear that many excellent
            proposals had to be turned down, which is quite unfortunate
            for the mathematical community. I wish there were more funds
            to fund these! </p>
          <p> <span class="important">!!! Postdoc Position:</span> I
            will be hiring a postdoc to work on the project; the
            position will start in <span class="important">September
              2016</span> (however, there is some flexibility with the
            staring date). The position is initially for <span
              class="important">2 years</span>; with a possible
            extension for a third year (to be decided by the end of the
            1st year). The position has not yet been formally advertised
            - but I encourage strong potential applicants to contact me
            by email!<br>
          </p>
          <p><span class="important">!!! PhD Position:</span> I will
            also be hiring a PhD student to work on the project.
            Starting date: by September 2016. If you are interested,
            apply via our <a
              href="http://www.maths.ed.ac.uk/studying-here/pgr/phd-application/apply">online














              system</a> (to the OR &amp; Optimization programme) and
            then drop me an email.<br>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>July 31, 2015 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1507.08322">Distributed
              mini-batch SDCA,</a> joint with <a
              href="http://mtakac.com/">Martin Takáč</a> and <a
              href="http://ttic.uchicago.edu/%7Enati/">Nati Srebro.</a>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>July 16, 2015 </h3>
          <p>I am in Pittsburgh this week, attending <a
              href="http://www.ismp2015.org/">ISMP 2015.</a> <a
              href="http://jakubkonecny.com/">Jakub</a>, <a
href="http://researcher.watson.ibm.com/researcher/view.php?person=ie-jakub.marecek">Jakub</a>,
            <a href="http://www.mtakac.com/CV">Martin</a>, <a
              href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier</a>,
            <a href="http://www.maths.ed.ac.uk/%7Ertappend/index.html">Rachael</a>,
            <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert</a>
            and <a href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng</a>
            are here, too. I am giving a talk tomorrow. Update: Here are
            <a href="TALK-SDNA-ISMP2015.pdf">the slides.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>July 8, 2015 </h3>
          <p> I am at <a href="http://icml.cc/2015/">ICML in Lille</a>
            this week. The ICML brochure we were all given visualizes
            the hot topics at this year's conference. Notice just how
            central optimization is to machine learning: </p>
          <p> <a href="imgs/ICML2015-cover.jpg"><img
                src="imgs/ICML2015-cover.jpg" align="absmiddle"
                width="600"></a> </p>
          <p> I gave a tutorial on "Modern Convex Optimization Methods
            for Large-scale Empirical Risk Minimization", jointly with <a
              href="http://www.cs.ubc.ca/%7Eschmidtm/">Mark Schmidt</a>,
            on Monday. The slides are <a
              href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_I.pdf">here
(part














              I)</a> and <a
              href="http://icml.cc/2015/tutorials/2015_ICML_ConvexOptimization_II.pdf">here
(part














              II)</a>. [Unfortunately, there were some serious technical
            issues with the setup during my talk.] </p>
          <p> We have had two papers accepted to ICML this year, both
            were presented on Tuesday: <a
              href="http://jmlr.org/proceedings/papers/v37/mab15.pdf">Adding














              vs. Averaging in Distributed Primal-Dual Optimization
              (Chenxin Ma, Virginia Smith, Martin Jaggi, Michael Jordan,
              Peter Richtarik, Martin Takac) </a> and <a
              href="http://jmlr.org/proceedings/papers/v37/csiba15.pdf">
              Stochastic Dual Coordinate Ascent with Adaptive
              Probabilities (Dominik Csiba, Zheng Qu, Peter Richtarik)</a>.
          </p>
          <p> Here is a photo of Dominik presenting his poster: </p>
          <p> <a href="imgs/ICML2015_Dominik-fullsize.jpg"><img
                src="imgs/ICML2015_Dominik.png" align="absmiddle"></a> </p>
          Dominik is in the upper right corner of the room...
          <p> <a href="imgs/ICML2015poster_session-fullsize.jpg"><img
                src="imgs/ICML2015poster_session.png" align="absmiddle"></a>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>June 29, 2015 </h3>
          <p> <a href="http://jakubkonecny.com/">Jakub Konečný</a> is
            spending the summer at Google as an intern. He has been
            there for a month already, and will be there until the end
            of August. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>June 22, 2015 </h3>
          <p> I have attended the <a
              href="http://www.numerical.rl.ac.uk/people/nimg/fox/programme.php">IMA
Fox














              Prize meeting</a> in Glasgow today. All the talks were
            great, and the research inspiring. </p>
          <p> <a href="http://perso.telecom-paristech.fr/%7Eofercoq/">Olivier














              Fercoq</a> --- a former postdoc in my group and now an
            Assistant Professor at Telecom ParisTech --- received the
            17th IMA Leslie Fox Prize (2nd Prize) with his paper: <a
              href="http://arxiv.org/abs/1312.5799">Accelerated,
              parallel and proximal coordinate descent</a>, coathored
            with me. </p>
          <p>The Leslie Fox Prize for Numerical Analysis of the
            Institute of Mathematics and its Applications (IMA) is a
            biennial prize established in 1985 by the IMA in honour of
            mathematician Leslie Fox (1918-1992). The prize honours
            "young numerical analysts worldwide" (any person who is less
            than 31 years old), and applicants submit papers for review.
            A committee reviews the papers, invites shortlisted
            candidates to give lectures at the Leslie Fox Prize meeting,
            and then awards First Prize and Second Prizes based on
            "mathematical and algorithmic brilliance in tandem with
            presentational skills" </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>June 16, 2015 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1506.03296">Randomized
              iterative methods for linear systems,</a> joint work with
            <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert
              Gower</a> </p>
          <p> <em> We develop a novel, fundamental and surprisingly
              simple randomized iterative method for solving consistent
              linear systems. Our method has five different but
              equivalent interpretations: sketch-and-project,
              constrain-and-approximate, random intersect, random linear
              solve and ran- dom fixed point. By varying its two
              parameters—a positive definite matrix (defining geometry),
              and a random matrix (sampled in an i.i.d. fashion in each
              iteration)—we recover a comprehensive array of well known
              algorithms as special cases, including the randomized
              Kaczmarz method, randomized Newton method, randomized
              coordinate descent method and random Gaussian pursuit. We
              naturally also obtain variants of all these methods using
              blocks and importance sampling. However, our method allows
              for a much wider selection of these two parameters, which
              leads to a number of new specific methods. We prove
              exponential convergence of the expected norm of the error
              in a single theorem, from which existing complexity
              results for known vari- ants can be obtained. However, we
              also give an exact formula for the evolution of the
              expected iterates, which allows us to give lower bounds on
              the convergence rate. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>June 7, 2015 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1506.02227">Primal method for
              ERM with flexible mini-batching schemes and non-convex
              losses,</a> joint work with Dominik Csiba. </p>
          <p> <em>Abstract: In this work we develop a new algorithm for
              regularized empirical risk minimization. Our method
              extends recent techniques of Shalev-Shwartz [02/2015],
              which enable a dual-free analysis of SDCA, to arbitrary
              mini-batching schemes. Moreover, our method is able to
              better utilize the information in the data defining the
              ERM problem. For convex loss functions, our complexity
              results match those of QUARTZ, which is a primal-dual
              method also allowing for arbitrary mini-batching schemes.
              The advantage of a dual-free analysis comes from the fact
              that it guarantees convergence even for non-convex loss
              functions, as long as the average loss is convex. We
              illustrate through experiments the utility of being able
              to design arbitrary mini-batching schemes. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>June 1, 2015 </h3>
          <p> Today I gave a talk at <a
              href="https://www.math.ucdavis.edu/">UC Davis</a>, on an
            invitation by <a
              href="https://www.math.ucdavis.edu/%7Empf/">Michael
              Friedlander</a>. I've talked about <a
              href="http://arxiv.org/abs/1502.02268">SDNA: Stochastic
              Dual Newton Ascent for empirical risk minimization.</a></p>
          <p> Trivia: First time I used Amtrak in my life (liked it!),
            first time I lost a T-shirt, first time I thought I was
            supposed to give talk X when in fact I agreed to give talk
            Y, discussed a new and interesting joint research idea
            during the visit (a pleasant surprise), walked 1hr to the
            train station and 1hr back. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>May 27, 2015 </h3>
          <p> Today I am giving a seminar talk at AMPLab, UC Berkeley.
            Coordinates: 465H Soda Hall, Time: noon. I'll be talking
            about <a href="http://arxiv.org/abs/1502.02268">SDNA:
              Stochastic Dual Newton Ascent for empirical risk
              minimization.</a></p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>May 26, 2015 </h3>
          <p> Totday, <a href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng
              Qu</a> is giving a talk on our Quartz algorithms (<a
              href="http://arxiv.org/abs/1411.5873">here is the paper</a>)
            at the <a href="http://kac.maths.ed.ac.uk/%7Ebl/mmmds/">Mathematical














              Methods for Massive Data Sets</a> workshop. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>May 24, 2015 </h3>
          <p> I am visiting UC Berkeley during for the next couple
            weeks.</p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>May 8, 2015 </h3>
          <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/">Optimization
and


















            Big Data 2015:</a> The award committee consisting of Arkadi
          Nemirovski (Georgia Institute of Technology) and Rodolphe
          Jenatton (Amazon Berlin) announced the Best Contribution Award
          Winners:
          <p> <span class="important">Winner:</span> Rodrigo
            Mendoza-Smith (University of Oxford) <br>
            for "Expander l0 decoding" <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/slides/08-Mendoza-Smith.pdf">[slides]</a>
            <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/posters/Mendoza-Smith.pdf">[poster]</a>
            [paper] <br>
            The first prize carries a 500 EUR cash award, sponsored by
            Amazon Berlin </p>
          <p> <img
src="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/files/Award-Mendoza-Smith.png"
              alt="UoE" width="400"> </p>
          <p> <span class="important">Runner-up:</span> Dominik Csiba
            (University of Edinburgh) <br>
            for "Stochastic dual coordinate ascent with adaptive
            probabilites" <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/slides/02-Csiba.pdf">[slides]</a>
            <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/posters/Csiba.pdf">[poster]</a>
            <a href="http://arxiv.org/abs/1502.08053">[paper]</a> </p>
          <p> <img
src="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/files/Award-Csiba.png"
              alt="UoE" width="400"> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>May 6, 2015 </h3>
          <p> <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/">Optimization
and














              Big Data 2015</a> is starting today! </p>
          <p> <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/files/OBD2015.pdf"><img
src="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/files/booklet-thmb.png"
                alt="Optimization and Big Data 2015 booklet"></a> </p>
          <p> We have an amazing lineup of <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/schedule.html">speakers</a>;
            I am looking forward to all the talks and to the discussions
            during the rest of the week. </p>
          <p> A message to all participants: Welcome to Edinburgh and
            enjoy the event! </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>April 25, 2015 </h3>
          <p><span class="important">Two papers accepted</span> to <a
              href="http://icml.cc/2015/">ICML 2015:</a> </p>
          <p><a href="http://arxiv.org/abs/1502.08053">Stochastic dual
              coordinate ascent with adaptive probabilities</a> (code:
            AdaSDCA)<br>
            joint with: Dominik Csiba and Zheng Qu <br>
          </p>
          <p><a href="http://arxiv.org/abs/1502.03508">Adding vs.
              averaging in distributed primal-dual optimization</a>
            (code: CoCoA+)<br>
            joint with: Chenxin Ma, Virginia Smith, Martin Jaggi,
            Michael I. Jordan and Martin Takáč<br>
          </p>
          <p>The ICML decisions were announced today. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>April 20, 2015</h3>
          <p><span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1504.04407">Mini-Batch
              Semi-Stochastic Gradient Descent in the Proximal Setting,</a>
            joint work with <a href="http://jakubkonecny.com/">Jakub
              Konečný</a>, <a href="http://jieliucn.weebly.com/">Jie
              Liu</a> and <a href="http://www.mtakac.com/CV">Martin
              Takáč</a>. This is the full-size version of the following
            <a href="http://arxiv.org/abs/1410.4744">short paper</a>
            which was presented at the NIPS Optimization workshop. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>April 20, 2015</h3>
          <p> Today I am giving a talk at the <a
              href="http://www.macs.hw.ac.uk/%7Efd78/miapr15">Maxwell
              Institute Probability Day.</a> I will be talking about
            randomized optimization methods with ``arbitrary sampling''.
          </p>
          <p>This is a line of work which we started with my former PhD
            student <a href="http://www.mtakac.com/CV">Martin Takac</a>
            in <a href="http://arxiv.org/abs/1310.3438">this work on
              the NSync algorithm</a>, and continued in various falvours
            and settings in a sequence of papers with Zheng Qu, Martin
            Takac and Tong Zhang and Olivier Fercoq: <a
              href="http://arxiv.org/abs/1411.5873">QUARTZ</a>
            (primal-dual setup for empirical risk minimization), <a
              href="http://arxiv.org/abs/1412.8060">ALPHA</a>
            (non-accelerated and accelerated coordinate descent), <a
              href="http://arxiv.org/abs/1412.8063">ESO</a> (theory of
            expected separable overapproximation enabling the
            computation of closed form formulae for certain stepsize
            parameters), <a href="http://arxiv.org/abs/1502.02268">SDNA</a>
            (arbitrary sampling + second order information). In the
            workshop today I will focus on NSync and QUARTZ. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>April 17, 2015</h3>
          <p> The early bird deadline and abstract submission deadline
            for <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/">Optimization
and














              Big Data 2015</a> is tomorrow (April 18, 2015).</p>
          <p> <a
href="http://www.eventbrite.com/e/optimization-and-big-data-2015-tickets-15692485647?ref=ebtnebregn"
              target="_blank"><img
                src="https://www.eventbrite.co.uk/custombutton?eid=15692485647"
                alt="Eventbrite - Optimization and Big Data 2015"></a> </p>
          <p> For a list of participants already registered, <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/registration.html">click














              here.</a> For a list of contributions already accepted (we
            were doing this on a rolling basis), <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/schedule.html">look














              here.</a> The list of invited speakers and their talk
            titles <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/index.html">is














              here.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>April 12, 2015</h3>
          <p> The paper <a
              href="http://link.springer.com/article/10.1007/s10107-015-0901-6">Parallel
coordinate














              descent methods for big data optimization</a>, joint with
            <a href="http://www.mtakac.com/CV">Martin Takac</a>, has now
            appeared (online) in Mathematical Programming, Series A. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3>March 28, 2015</h3>
          <p> <a href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert
              Gower</a> has joined my <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_team.html">team</a>
            as a PhD student effective yesterday. He started his PhD in
            2012, and has until now been working under the supervision
            of <a href="http://www.maths.ed.ac.uk/%7Egondzio/">Jacek
              Gondzio.</a> Robert's past work is on automatic
            differentiation <a
href="http://www.tandfonline.com/doi/abs/10.1080/10556788.2011.580098?journalCode=goms20#.VRbZOkaPKpp">[1]</a>,
            <a
              href="http://dl.acm.org/citation.cfm?doid=2594412.2490254">[2]</a>,
            <a
              href="http://link.springer.com/article/10.1007%2Fs10107-014-0827-4">[3]</a>
            and quasi-Newton methods <a
              href="http://arxiv.org/abs/1412.8045"> [4]</a>. Robert:
            welcome to the group! </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> March 17, 2015 </h3>
          <p> <a href="http://homepages.inf.ed.ac.uk/gsanguin/">Guido
              Sanguinetti</a> and <a
              href="https://sites.google.com/site/tommayoresearch/cv-publications">Tom














              Mayo</a> talked today in our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">Big














              Data Seminar</a> about their work in the field of
            neuroinformatics and how it relates to big data and
            optimization.</p>
          <p> <a href="http://www.mtakac.com/CV">Martin Takac</a> has
            put together (and presented in New York) a poster about our
            <a href="http://arxiv.org/abs/1502.02268">SDNA paper.</a> <a
              href="posters/Poster-SDNA.pdf">Here it is.</a></p>
          <a href="posters/Poster-SDNA.pdf"> <img
              src="imgs/fancy-line.png" align="absmiddle" height="20">
            <h3> March 16, 2015 </h3>
          </a>
          <p><a href="posters/Poster-SDNA.pdf"> We have </a><a
              href="http://www.maths.ed.ac.uk/news/2015/lecturers_ds_or">2
              Lectureships</a> (Lecturer in the UK= tenured Assistant
            Professor in the USA) open in the School of Mathematics: <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=032764">Lectureship
in














              the Mathematics of Data Science</a> and <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=032763">Lectureship
in














              Operational Research.</a> </p>
          <span class="important">Application deadline: April 14, 2015 </span>
          <br>
          <span class="important">Starting date: August 1, 2015 </span>
          <p> The University of Edinburgh, alongside Oxford, Cambridge,
            Warwick and UCL, is a partner in the <a
              href="http://en.wikipedia.org/wiki/Alan_Turing_Institute">Alan














              Turing Institute</a>, which is being formed at the moment.
            This constitutes a major investment by the UK government
            (£42 million) into <span class="important">Big Data</span>
            research and <span class="important">Algorithms.</span> The
            successful candidates will benefit from the vibrant
            community of the Alan Turing Institute. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> March 15, 2015 </h3>
          <p><a href="http://www.maths.ed.ac.uk/people/show?person=417">Dominik














              Csiba</a> was selected to participate at the <a
              href="http://www.cs.rpi.edu/%7Edrinep/G2S3_RandNLA_2015/index.htm">Gene
Golub














              SIAM Summer School on Randomization in Numerical Linear
              Algebra (RandLNA),</a> to be held in June 2015 in Delphi,
            Greece. </p>
          <p> He was also selected to take part in the <a
              href="http://mlss.tuebingen.mpg.de/2015/index.html">2015
              Machine Learnig Summer School,</a> which will be held in
            July 2015 at the <a href="http://www.is.tuebingen.mpg.de/">Max













              Planck Institute for Intelligent Systems, Germany.</a> The
            selection procedure was highly competitive, only 20% of the
            applicants were offered a place. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> March 12, 2015 </h3>
          <p><span class="important">New paper:</span> <a
              href="http://arxiv.org/abs/1503.03033">On the Complexity
              of Parallel Coordinate Descent,</a> joint work with <a
              href="http://www.maths.ed.ac.uk/%7Ertappend/index.html">
              Rachael Tappenden </a> and <a href="http://mtakac.com/">Martin














              Takáč.</a> </p>
          <p> <em>Abstract: In this work we study the parallel
              coordinate descent method (PCDM) proposed by Richtarik and
              Takac [26] for minimizing a regularized convex function.
              We adopt elements from the work of Xiao and Lu [39], and
              combine them with several new insights, to obtain sharper
              iteration complexity results for PCDM than those presented
              in [26]. Moreover, we show that PCDM is monotonic in
              expectation, which was not confirmed in [26], and we also
              derive the first high probability iteration complexity
              result where the initial levelset is unbounded. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> March 10, 2015 </h3>
          <p> In today's <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">Big














              Data Optimization meeting</a> we have <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu</a>
            covering a <a
href="http://www.uclouvain.be/cps/ucl/doc/core/documents/coredp2015_3web.pdf">recent
paper














              of Yurii Nesterov</a> on primal dual Frank-Wolfe type
            methods. These methods have attracted a considerable
            attention in the recent years, and were for instance
            featured in a <a
              href="https://sites.google.com/site/frankwolfegreedytutorial/home">2014
ICML














              tutorial by Jaggi and Harchaoui.</a></p>
          <p> An unrelated announcement: Dominik Csiba is away this week
            and next; attending the <a
              href="http://www.siam.org/meetings/cse15/index.php">SIAM
              Conference on Computational Science and Engineering</a> in
            Salt Lake City.</p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> March 9, 2015 </h3>
          <p> <a href="http://perso.telecom-paristech.fr/%7Eofercoq/">Olivier














              Fercoq</a> --- a former postdoc in my group and now a
            postdoc at Telecom Paris Tech --- is a <a
              href="http://www.numerical.rl.ac.uk/people/nimg/fox/shortlist.php">finalist
for














              the 17th IMA Leslie Fox Prize</a> with his paper: <a
              href="http://arxiv.org/abs/1312.5799">Accelerated,
              parallel and proximal coordinate descent</a>, coathored
            with me. The paper will appear in the SIAM Journal on
            Optimization. Also shortlisted is <a
              href="http://www.maths.ed.ac.uk/people/show?person=372">John














              Pearson</a>, who too was a postdoc in the School recently,
            with his paper <a
              href="http://epubs.siam.org/doi/abs/10.1137/120892003">Fast














              iterative solution of reaction-diffusion control problems
              arising from chemical processes,</a> which he wrote prior
            to joining Edinburgh.</p>
          <p>A First and a number of Second Prizes will be awarded on
            June 22, 2015 in Glasgow, at the Leslie Fox Prize meeting
            collocated with the <a
              href="http://numericalanalysisconference.org.uk/">26th
              Biennial Conference on Numerical Analysis.</a></p>
          <p>The Leslie Fox Prize for Numerical Analysis of the
            Institute of Mathematics and its Applications (IMA) is a
            biennial prize established in 1985 by the IMA in honour of
            mathematician Leslie Fox (1918-1992). The prize honours
            "young numerical analysts worldwide" (any person who is less
            than 31 years old), and applicants submit papers for review.
            A committee reviews the papers, invites shortlisted
            candidates to give lectures at the Leslie Fox Prize meeting,
            and then awards First Prize and Second Prizes based on
            "mathematical and algorithmic brilliance in tandem with
            presentational skills" </p>
          <p> Two years ago, a Second Prize was awarded to <a
              href="http://www.mtakac.com/CV">Martin Takáč.</a> A
            complete <a
              href="http://www.numerical.rl.ac.uk/people/nimg/fox/winners.php%20">list
of














              past winners is here.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> March 5, 2015 </h3>
          <p> <a href="http://people.ufpr.br/%7Eademir.ribeiro/">Ademir
              Ribeiro</a> (Federal University of Parana and University
            of Edinburgh) gave a talk today in our ERGO seminar. The
            talk is based on paper we are writing. Title: The Complexity
            of Primal-Dual Fixed Point Methods for Ridge Regression. The
            <a
              href="http://www.maths.ed.ac.uk/ERGO/abstracts/2015-03-ribeiro.html">abstract
can














              be found here.</a></p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> March 4, 2015 </h3>
          <p> Today, <a href="http://jakubkonecny.com/">Jakub Konečný</a>
            is giving (actually it seems it already happened, due to the
            time difference) a <a
href="http://esd.sutd.edu.sg/ai1ec_event/jakub-konecny-university-edinburgh-semi-stochastic-gradient-descent-methods/">talk
at














              the Singapore University of Technology and Design.</a></p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> March 3, 2015 </h3>
          <p> I have several news items packed into a single entry
            today:</p>
          <p> <a href="https://gol.dinfo.unifi.it/lab/en/node/5521">Luca














              Bravi</a> is visiting my group starting this week, he will
            stay for four months until the end of June. Luca is a PhD
            student at the University of Florence, supervised by <a
              href="https://gol.dsi.unifi.it/users/sciandrone/">Marco
              Sciandrone.</a> If you see a new face again and again at
            the All Hands and ERGO seminars, that's probably him. Take
            him to lunch. </p>
          <p> After being lost in a jungle in Australia last week, and
            then finding his way back again, apparently still with
            enough blood left (leeches take their toll), <a
              href="http://jakubkonecny.com/">Jakub Konecny</a> is now
            visiting <a
              href="http://istd.sutd.edu.sg/faculty/ngai-man-man-cheung/">Ngai-Man














              Cheung</a> and <a
              href="http://esd.sutd.edu.sg/faculty/selin-damla-ahipasaoglu/">Selin
Damla














              Ahipasaoglu</a> at the <a href="http://www.sutd.edu.sg/">Singapore
University














              of Technology and Design (SUTD).</a> I am wondering what
            will happen to him there ;-) </p>
          <p> We had two very interesting talks in the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














              Hands Meetings on Big Data Optimization</a> in the past
            two weeks. Last Tuesday, <a
              href="http://www.maths.ed.ac.uk/%7Es1065527/">Robert Gower</a>
            spoke about <a href="http://arxiv.org/abs/1412.8045">"Action
constrained














              quasi-Newton methods"</a>. Today, <a
              href="http://www.maths.ed.ac.uk/%7Ekfount/">Kimon
              Fountoulakis</a> talked about a recent paper from
            Stanford/Berkeley about equipping <a
              href="http://arxiv.org/abs/1502.03571"> stochastic
              gradient descent with randomized preconditioning.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> February 20, 2015 </h3>
          <p><span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1502.08053">Stochastic Dual
              Coordinate Ascent with Adaptive Probabilities,</a> joint
            work with <a
              href="http://www.maths.ed.ac.uk/people/show?person=417">Dominik














              Csiba</a><a> and </a><a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu.</a> </p>
          <p> <em>Abstract: This paper introduces AdaSDCA: an adaptive
              variant of stochastic dual coordinate ascent (SDCA) for
              solving the regularized empirical risk minimization
              problems. Our modification consists in allowing the method
              adaptively change the probability distribution over the
              dual variables throughout the iterative process. AdaSDCA
              achieves provably better complexity bound than SDCA with
              the best fixed probability distribution, known as
              importance sampling. However, it is of a theoretical
              character as it is expensive to implement. We also propose
              AdaSDCA+: a practical variant which in our experiments
              outperforms existing non-adaptive methods. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> February 16, 2015 </h3>
          <p>As of today, <a href="http://jakubkonecny.com/">Jakub
              Konečný</a> is attending <a
              href="http://nicta.com.au/research/machine_learning/mlss2015">Machine
Learning














              Summer School</a> in Australia, Sydney. The school runs
            between Feb 16 and Feb 25. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> February 13, 2015 </h3>
          <p><span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1502.03508">Adding vs.
              Averaging in Distributed Primal-Dual Optimization,</a>
            joint work with <a
              href="http://ise.lehigh.edu/content/chenxin-ma">Chenxin Ma</a>,
            <a href="http://www.cs.berkeley.edu/%7Evsmith/">Virginia
              Smith</a>, <a href="http://people.inf.ethz.ch/jaggim/">Martin














              Jaggi</a>, <a
              href="http://www.cs.berkeley.edu/%7Ejordan/">Michael I.
              Jordan</a>, and <a href="http://mtakac.com/">Martin
              Takáč.</a> </p>
          <p> <em>Abstract: Distributed optimization algorithms for
              large-scale machine learning suffer from a communication
              bottleneck. Reducing communication makes t he efficient
              aggregation of partial work from different machines more
              challenging. In this paper we present a novel
              generalization of the recent communication efficient
              primal-dual coordinate ascent framework (CoCoA). Our
              framework, CoCoA+, allows for additive combination of
              local updates to the global parameters at each iteration,
              whereas previous schemes only allowed conservative
              averaging. We give stronger (primal-dual) convergence rate
              guarantees for both CoCoA as well as our new variants, and
              generalize the theory for both methods to also cover
              non-smooth convex loss functions. We provide an extensive
              experimental comparison on several real-world distributed
              datasets, showing markedly improved performance,
              especially when scaling up the number of machines. </em>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> February 10, 2015 </h3>
          <p>Today we have had <a
              href="http://homepages.inf.ed.ac.uk/ckiw/">Chris Williams</a>
            give a talk in the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">Big














              Data Optimization Seminar.</a> The topic was ``linear
            dynamical systems applied to condition monitoring''. </p>
          <p> <em>Abstract: We develop a Hierarchical Switching Linear
              Dynamical System (HSLDS) for the detection of sepsis in
              neonates in an intensive care unit. The Factorial
              Switching LDS (FSLDS) of Quinn et al. (2009) is able to
              describe the observed vital signs data in terms of a
              number of discrete factors, which have either
              physiological or artifactual origin. We demonstrate that
              by adding a higher-level discrete variable with semantics
              sepsis/non-sepsis we can detect changes in the
              physiological factors that signal the presence of sepsis.
              We demonstrate that the performance of our model for the
              detection of sepsis is not statistically different from
              the auto-regressive HMM of Stanculescu et al. (2013),
              despite the fact that their model is given "ground truth"
              annotations of the physiological factors, while our HSLDS
              must infer them from the raw vital signs data. Joint work
              with Ioan Stanculescu and Yvonne Freer. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> February 9, 2015 </h3>
          <p><span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1502.02268">SDNA: Stochastic
              dual Newton ascent for empirical risk minimization,</a>
            joint work with <a href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng













              Qu</a>, <a href="http://mtakac.com/">Martin Takáč</a> and
            <a href="http://perso.telecom-paristech.fr/%7Eofercoq/">
              Olivier Fercoq.</a> </p>
          <p> <em>Abstract: We propose a new algorithm for minimizing
              regularized empirical loss: Stochastic Dual Newton Ascent
              (SDNA). Our method is dual in nature: in each iteration we
              update a random subset of the dual variables. However,
              unlike existing methods such as stochastic dual coordinate
              ascent, SDNA is capable of utilizing all curvature
              information contained in the examples, which leads to
              striking improvements in both theory and practice –
              sometimes by orders of magnitude. In the special case when
              an L2-regularizer is used in the primal, the dual problem
              is a concave quadratic maximization problem plus a
              separable term. In this regime, SDNA in each step solves a
              proximal subproblem involving a random principal submatrix
              of the Hessian of the quadratic function; whence the name
              of the method. If, in addition, the loss functions are
              quadratic, our method can be interpreted as a novel
              variant of the recently introduced Iterative Hessian
              Sketch. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> February 8, 2015</h3>
          <p> Congratulations to <a href="http://jakubkonecny.com/">Jakub














              Konečný</a> who is the recipient of the <a
              href="http://www.baspfrontiers.org/award.php">Best
              Contribution Prize</a> in the field of Signal Processing
            at the <a href="http://www.baspfrontiers.org/index.php">
              2015 International BASP Frontiers Workshop.</a> The prize
            carries a cash award and is given to a young scientist (a
            PhD student or a postdoc) based on the quality of their talk
            and the presented research. Jakub gave <a
              href="http://jakubkonecny.com/files/01-15%20Semi-Stochastic%20BASP.ppsx">this














              talk</a>, which is based on these papers: <a
              href="http://arxiv.org/abs/1312.1666">S2GD</a>, <a
              href="http://arxiv.org/abs/1410.4744">mS2GD</a>, <a
              href="http://arxiv.org/abs/1412.6293">S2CD</a>. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> January 29, 2015</h3>
          <p> Mojmir Mutny (Edinburgh, Physics) has worked with me last
            Summer on a research project funded by an undergraduate
            research bursary. He has generated many interesting ideas,
            and written a report on his various findings (e.g., on a
            novel optimization formulation of an imaging problem).
            However, the reason why I am writing this post is to provide
            a <a href="https://bitbucket.org/Mojusko/biglearning/src">link














              to the code</a> he wrote, implementing gradient descent,
            coordinate descent and parallel coordinate descent. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> January 28, 2015</h3>
          <p> Alongside with Cambridge, Oxford, Warwick and UCL,
            Edinburgh will lead the new <span class="important">Alan
              Turing "Big Data" Institute</span>. This great piece of
            news was <a
href="http://www.techworld.com/news/big-data/five-universities-named-by-government-lead-alan-turing-institute-3595950/">announced














              today</a> by business secretary Vincent Cable. The
            Edinburgh bid was co-led by Mathematics and Informatics, and
            I am doubly happy about the annoucement as I was one of the
            people involved in the process. I am truly excited about the
            opportunities this will bring. </p>
          <p> Update (Feb 3, 2015): <a
              href="http://www.maths.ed.ac.uk/news/2015/alan-turing-institute">The
School














              of Mathematics news article about this.</a> </p>
          <p> This seems like an excellent excuse to announce <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data_2015/">Optimization
and














              Big Data 2015</a>, a workshop which will be held in
            Edinburgh during May 6-8, 2015. This is the third event in a
            series of very successful workshops run since 2012. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> January 27, 2015 </h3>
          <p> <a href="http://www.baspfrontiers.org/program.php"> The
              International BASP Frontiers workshop</a> is running this
            week in Villars-sur-Ollon, Switzerland. There are three
            streams (Signal Processing, Astro-Imaging and Bio-Imaging),
            all composed of three sessions. I have put together the <a
href="http://www.baspfrontiers.org/program.php?session=2#day4">"Modern
              Scalable Algorithms for Convex Optimization"</a> session
            which runs tomorrow. Speakers: <a
              href="http://www-syscom.univ-mlv.fr/%7Epesquet/index.htm">JC














              Pesquet (Universite Paris-Est)</a>, <a
              href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">CB
              Schonlieb (Cambridge)</a>, <a
              href="http://iew3.technion.ac.il/Home/Users/becka.html">A
              Beck (Technion)</a>, <a href="http://jakubkonecny.com/">J
              Konecny (Edinburgh)</a>, <a
              href="http://lear.inrialpes.fr/people/mairal/index_eng.php">J
              Mairal (INRIA)</a>. Deluxe posters: <a
              href="http://lions.epfl.ch/postdocs/quoc.tran-dinh">Q
              Tran-Dinh (EPFL)</a>, <a
              href="http://www-syscom.univ-mlv.fr/%7Epirayre/">A Pirayre
              (University Paris-Est)</a>, <a
              href="http://people.epfl.ch/vassilis.kalofolias">V
              Kalofolias (EPFL)</a>, <a
              href="http://www.see.ed.ac.uk/%7Es0574225/">M Yaghoobi
              (Edinburgh)</a>. </p>
          <p> We have started the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














              Hands Meeting in Big Data Optimization</a> again this
            year. Last week we had Ilias Diakonikolas (Edinburgh
            Informatics) giving a wonderful talk about Algorithms in
            Statistics, losely based on <a
href="http://www.iliasdiakonikolas.org/papers/piecewise-poly-learning.pdf">this














              paper</a>. Today we have <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu</a>
            covering a recent paper of Alekh Agarwal and Leon Bottou on
            <a href="http://arxiv.org/abs/1410.0723">lower bounds for
              the problem of minimizing the sum of a large number of
              convex functions</a> (Alekh: I can't wait to play some
            more TT with you ;-). Next week, after his return from
            Switzerland, <a href="http://jakubkonecny.com/">Jakub
              Konecny</a> will speak about <a
              href="http://arxiv.org/abs/1312.7853">DANE (Communication
              Efficient Distributed Optimization using an Approximate
              Newton-type Method) - a recent paper of Ohad Shamir, Nati
              Srebro and Tong Zhang</a>. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> January 15, 2015 </h3>
          <p> If you wish to work with me on exciting new optimization
            algorithms and machine learning techniques applicable to big
            data problems, apply to our PhD programme in Data Science.
            The deadline for September 2016 entry is on January 30,
            2015. </p>
          <p> <a href="http://datascience.inf.ed.ac.uk/apply/"> <img
                src="imgs/Data_Science_PhD_small.png" align="absmiddle"
                height="407"> </a> </p>
          <p> You may also want to apply for PhD in <a
              href="http://www.maths.ed.ac.uk/studying-here/pgr/phd-application">Optimization
and














              Operations Research</a> and/or to the <a
              href="http://www.maxwell.ac.uk/MIGSAA.php"> Maxwell
              Institite Graduate School in Analysis and its
              Applications.</a> To apply for a PhD in MIGSAA, send your
            CV, transcript and a cover note to explain you interests to
            apply2MIGSAA@maxwell.ac.uk. I am affiliated with all three
            PhD programmes. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> January 14, 2015 </h3>
          <p> Today I gave my <a
              href="talks/TALK-2015-01-ArbitrarySampling.ppsx">talk on
              Coordinate Descent Methods with Arbitrary Sampling</a> --
            at the <a href="http://lear.inrialpes.fr/workshop/osl2015/">Optimization














              and Statistical Learning workshop (Les Houches, France).</a>
          </p>
          <p> Randomized coordinate descent methods with arbitrary
            sampling are optimization algorithms which at every
            iteration update a random subset of coordinates (i.i.d.
            throughout the iterations), with the distribution of this
            random set-valued mapping allowed to be arbitrary. It turns
            out that methods of this type work as long as every
            coordinate has a positive probability of being chosen by the
            sampling, and hence being updated. This is clearly a
            necessary condition if we want the method to converge from
            any starting point. However, it turns out it is also
            sufficient. Naturaly, certain characteristics of the
            distribution of the random set-valued mapping (or
            "sampling", for simplicity) manifest itself in the
            complexity bound. For some distributions, the bounds is
            good, for some it is bad -- which opens the possibility to
            design samplings optimizing the complexity bound. If we
            restrict our attention to the case of samplings picking a
            single coordinate at a time, the optimal distribution is
            known as <em>importance sampling.</em> Usually, the
            difference between the uniform and importance sampling in
            terms of complexity is in the replacement of the maximum of
            certain problem-dependent quantities in the bound by their
            average. If these quantities have a very nonuniform
            distribution, this is a major imporvement - and this can be
            clearly seen in practice as well. The above general setup
            opens the possibility to efficiently solve optimization
            problems arising in applications where it is more natural to
            update structured subsets of variables (e.g., overlapping
            blocks) and in situations where the sampling is implicitly
            defined by the computing environment (e.g., faulty
            processors). </p>
          <p> To the best of my knowledge, at the moment there are only
            four papers dealing with this topic. </p>
          <p> The <a href="http://arxiv.org/abs/1310.3438">first paper
              (NSync) </a> was coauthored by <a
              href="http://www.mtakac.com/CV">Martin Takac</a> and
            myself. In it we focuse on the simple case of unconstrained
            smooth minimization of a strongly convex function. The paper
            is very brief (you could end reading at the end of page 2!)
            and the complexity result compact. We show that in order to
            find an eps-solution with probability at least 1-rho, it is
            sufficient to take </p>
          <p> max_i (v_i/p_i*lambda) * log((f(x^0)-f(x^*))/(eps*rho)) </p>
          <p> iterations, where the max is taken over the coordinates, f
            is the objective function, x^0 and x^* are the starting and
            optimal points, respectively, lambda is the strong
            covnvexity parameter of f, p_i is the probability that
            coordinate i is chosen by the sampling and v_i are certain
            parameters that depend on both f and the sampling. Warning:
            we use different notation in the paper. </p>
          <p> The <a href="http://arxiv.org/abs/1411.5873">second paper
              on the topic </a> deals with a primal-dual optimization
            formulation which has received much attention due to its
            relevance to machine learning. The method we design (Quartz;
            this is joint work with <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu</a> and
            <a href="http://stat.rutgers.edu/home/tzhang/">Tong Zhang</a>)
            in each iteration updates a random subset of dual variables
            (again, arbitrary sampling is allowed). The analysis is
            directly primal dual, and the resulting complexity bounds is
            again very simple. In order to find a pair (w^t,alpha^t) of
            primal and dual vectors for which the expected duality gap
            is below eps (E [P(w^0)-D(alpha^0)]&lt;= eps), it is
            sufficient to take </p>
          <p> max_i (1/p_i + v_i/(p_i*lambda*gamma*n)) * log
            ((P(w^0)-D(alpha^0))/epsilon) </p>
          <p>iterations. The maximum is again taken over the n
            "coordinates" (dual variables), p_i, v_i and lambda have the
            same meaning as above, with gamma being a certain smoothness
            parameter associated with the loss functions in the problem
            formulation. For insatnce, if we focus on the uniform
            sampling over individual coordinates, we recover the rate of
            SDCA (with an improvement in the log factor). However, we
            now have more flexibility and can deduce importance
            sampling, introduce minibatching of various kinds, and even
            derandomize the method by choosing the sampling which always
            updates all variables. </p>
          <p> In the <a href="http://arxiv.org/abs/1412.8060">third
              paper</a> (joint with Zheng Qu) we focus on the problem of
            minimizing the sum of a smooth convex function (which is not
            strongly convex) and a separable convex regularizer (such as
            the L1 norm). We design a new method (ALPHA) which is
            remarkably general. For the deterministic sampling (i.e.,
            always picking all coordinates), for instance, ALPHA
            specializes to gradient descent and accelerated gradient
            descent, depending on how we select a certain sequence
            appearing in the method. If we focus on samplings updatinga
            single coordinate at a time, the method specializes to
            non-accelerated or accelerated coordinate descent. The
            bounds we obtain improve on the best known bounds for
            coordinate descent for this problem. For instance, in its
            accelerated variant, the complexity of ALPHA is </p>
          <p> (2/(t+1)^2) * sum_i (x^0_i - x^*_i)^2 * (v_i/p_i^2), </p>
          <p>where t is the iteration counter. </p>
          <p> In the <a href="http://arxiv.org/abs/1412.8063">fourth
              paper</a> we develop a simple calculus for computing
            constants v_i appearing in the above bounds (they are also
            needed as parameters of all the methods: NSync, Quartz and
            ALPHA). Recall that these constants depend on both the
            objective function and the sampling. In this paper we give
            closed-form expressions for these constants for a large
            class of functions and samplings. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> January 13, 2015 </h3>
          <p> Participants of the <a
              href="http://lear.inrialpes.fr/workshop/osl2015/">Optimization














              and Statistical Learning</a> workshop: </p>
          <p> <img src="imgs/LesHouches2015_small.jpg" alt="Les Houches
              2015" width="600"> </p>
          The <a href="imgs/LesHouches2015.jpg"> full size image is
            here (16 MB)</a>. The photo was taken with my super-cool <a
            href="http://www.dpreview.com/reviews/fujifilm-x100s">Fujifilm


















            x100s</a> camera. People know me as always running around
          with a DSLR - this thingy takes better shots than my old Canon
          ESO 50d and is very compact. <img src="imgs/fancy-line.png"
            align="absmiddle" height="20">
          <h3> January 12, 2015 </h3>
          <p> This week I am in Les Houches, France, attending the <a
              href="http://lear.inrialpes.fr/workshop/osl2015/program.html">Optimization
and














              Statistical Learning</a> workshop. This is a fantastic
            event in a beautiful Alpine environment. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> January 8, 2015 </h3>
          <p> I am in London, attending the <a
              href="http://www.ucl.ac.uk/bigdata-theory/">1st UCL
              workshop on the Theory of Big Data. </a> My talk is on
            Friday, I'll talk about Randomized Dual Coordinate Ascent
            with Arbitrary Sampling, based on <a
              href="http://arxiv.org/abs/1411.5873">this paper.</a>
            Other closely related work (all related to stochastic
            methods using an <em>arbitrary</em> sampling): <a
              href="http://arxiv.org/abs/1310.3438">NSync</a>, <a
              href="http://arxiv.org/abs/1412.8060">ALPHA</a> and <a
              href="http://arxiv.org/abs/1412.8063">ESO.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> December 27, 2014 </h3>
          <p><span class="important">Two new papers out:</span> </p>
          <p> <a href="papers/alpha1.pdf">Coordinate descent with
              arbitrary sampling I: algorithms and complexity,</a> joint
            with <a href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu</a>.
          </p>
          <p><em>Abstract: We study the problem of minimizing the sum of
              a smooth convex function and a convex block-separable
              regularizer and propose a new randomized coordinate
              descent method, which we call ALPHA. Our method at every
              iteration updates a random subset of coordinates,
              following an arbitrary distribution. No coordinate descent
              methods capable to handle an arbitrary sampling have been
              studied in the literature before for this problem. ALPHA
              is a remarkably flexible algorithm: in special cases, it
              reduces to deterministic and randomized methods such as
              gradient descent, coordinate descent, parallel coordinate
              descent and distributed coordinate descent -- both in
              nonaccelerated and accelerated variants. The variants with
              arbitrary (or importance) sampling are new. We provide a
              complexity analysis of ALPHA, from which we deduce as a
              direct corollary complexity bounds for its many variants,
              all matching or improving best known bounds. </em> </p>
          <p><a href="papers/alpha2.pdf">Coordinate descent with
              arbitrary sampling II: expected separable
              overapproximation,</a> joint with <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu</a>. </p>
          <p><em>Abstract: The design and complexity analysis of
              randomized coordinate descent methods, and in particular
              of variants which update a random subset (sampling) of
              coordinates in each iteration, depends on the notion of
              expected separable overapproximation (ESO). This refers to
              an inequality involving the objective function and the
              sampling, capturing in a compact way certain smoothness
              properties of the function in a random subspace spanned by
              the sampled coordinates. ESO inequalities were previously
              established for special classes of samplings only, almost
              invariably for uniform samplings. In this paper we develop
              a systematic technique for deriving these inequalities for
              a large class of functions and for arbitrary samplings. We
              demonstrate that one can recover existing ESO results
              using our general approach, which is based on the study of
              eigenvalues associated with samplings and the data
              describing the function. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> December 20, 2014 </h3>
          <p><span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1412.6293">Semi-stochastic
              coordinate descent,</a> joint with <a
              href="http://jakubkonecny.com/">Jakub Konečný</a> and <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu</a>.
            This is the full-length version of <a
              href="http://opt-ml.org/papers/opt2014_submission_22.pdf">this














              brief paper</a>, which was accepted to and presented at
            the <a href="http://opt-ml.org/papers.html">2014 NIPS
              Workshop on Optimization in Machine Learning.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> December 17, 2014 </h3>
          <p> Here are the <a href="talks/TALK-Quartz.ppsx">slides from
              my yesterday's talk</a>; I talked about the Quartz
            algorithm. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> December 15, 2014 </h3>
          <p> The <a
              href="http://www.andrew.cmu.edu/user/jfp/FoCM2014.html">continuous
optimization














              workshop at FoCM 2014</a> has been kicked off today
            through a very nice plenary lecture by <a
              href="http://pages.cs.wisc.edu/%7Eswright/">Steve Wright</a>
            on asynchronous stochastic optimization. The quality lineup
            of speakers and topics promises a very fine event; the fun
            begins. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> December 12, 2014 </h3>
          <p> I have accepted an invite to become an Associate Editor of
            Optimization in a new <a
              href="http://en.wikipedia.org/wiki/Frontiers_Media">Frontiers</a>
            journal (Frontiers in Applied Mathematics and Statistics; to
            be launched in 2014). I am now building a team of Review
            Editors. Frontiers is a 21st century open access publisher
            with an interactive online platform which goes a long way
            beyond simple publishing. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> December 10, 2014 </h3>
          <p> Tomorrow I am travelling to Montevideo, Uruguay, to
            participate at <a
              href="https://www.fing.edu.uy/eventos/focm2014/workshops.html">
              FoCM 2014.</a> In particular, I am giving a talk in the <a
              href="http://www.andrew.cmu.edu/user/jfp/FoCM2014.html">Continuous
Optimization














              workshop</a> on the <a
              href="http://arxiv.org/abs/1411.5873">Quartz algorithm
              (randomized dual coordinate ascent with arbitrary
              sampling). </a> This is joint work with <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zheng Qu
              (Edinburgh)</a> and <a
              href="http://stat.rutgers.edu/home/tzhang/">Tong Zhang
              (Rutgers/Baidu).</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> December 9, 2014 </h3>
          <p> This week, <a href="http://jakubkonecny.com/">Jakub
              Konečný </a> and <a href="http://mtakac.com/">Martin
              Takáč</a> are presenting our joint work (also with Jie Liu
            and Zheng Qu) on the <a
              href="http://arxiv.org/abs/1410.4744">mS2GD algorithm
              (minibatch semistochastic gradient descent)</a> [<a
              href="posters/Poster-mS2GD.pdf">poster</a>] and the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/S2CD.pdf">S2CD
algorithm














              (semi-stochastic coordinate descent)</a> [<a
              href="posters/Poster-S2CD.pdf">poster</a>] at the <a
              href="http://www.opt-ml.org/papers.html">NIPS Optimization
              Workshop.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> December 5, 2014 </h3>
          <p> Today I gave a <a
              href="talks/Talk-Hydra_and_Hydra_2-NAIS2014.ppsx">talk</a>
            on <a href="http://arxiv.org/abs/1310.2059">Hydra</a> and <a
              href="http://arxiv.org/abs/1405.5300v2">Hydra^2</a>
            (simple and accelerated distributed coordinate descent) in a
            workshop (which I coorganized with James Madisson) on <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/NAIS2014/">Numerical
Algorithms














              and Intelligent Software.</a> The workshop was funded by <a
              href="http://www.nais.org.uk/">NAIS</a>, which helped to
            fund my research in the past 4 years, for which I am very
            thankful. The workshop was a celebration of the achievements
            of the <a href="http://www.nais.org.uk/">NAIS centre </a>
            as the grant supporting the centre expires at the end of the
            year. However, the activities of the centre continue in a
            number of follow-up projects. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> December 2, 2014 </h3>
          <p> Tomorrow I am giving a talk in <a
              href="http://www.maths.ed.ac.uk/research/psa">a local
              probability seminar</a>, on randomized coordinate descent.
            This is the second in a series of talks on stochastic
            methods in optimization; last week I talked about
            semi-stochastic gradient descent. Incidentaly, <a
              href="http://jakubkonecny.com/">Jakub Konecny</a> will be
            speaking about semi-stochastic gradient descent at <a
              href="docs/konecny-lehigh.pdf">Lehigh tomorrow.</a> He is
            there on a research visit (visiting <a
              href="http://mtakac.com/">Martin Takac</a> and his team),
            after which he will go to present some stuff at the NIPS
            Optimization workshop, after which both Jakub and Martin
            will join me at <a
              href="http://www.andrew.cmu.edu/user/jfp/FoCM2014.html">FoCM














              in Montevideo, Uruguay.</a> </p>
          <p> <a href="http://homepages.inf.ed.ac.uk/csutton/">Charles
              Sutton</a> was <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">speaking
today














              in our big data optimization meeting </a> about some of
            his work in machine learning that intersects with
            optimization. It was double pleasure for us as we had sushi
            for lunch today. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> November 27, 2014 </h3>
          <p> <a
              href="http://www.damtp.cam.ac.uk/people/t.j.m.valkonen/">Tuomo
Valkonen














              (Cambridge)</a> is visiting me today and tomorrow. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> November 26, 2014 </h3>
          <p> Today I gave a talk on semi-stochastic gradient descent in
            the <a href="http://www.maths.ed.ac.uk/research/psa">probability
group














              seminar</a> in our school. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> November 25, 2014 </h3>
          <p> In the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














              Hands Meeting on Big Data Optimization</a> today we have
            Dominik Csiba talking about Iterative Hessian Sketching. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> November 21, 2014</h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/pdf/1411.5873v1.pdf"> Randomized
              Dual Coordinate Ascent with Arbitrary Sampling,</a> joint
            with Zheng Qu (Edinburgh) and Tong Zhang (Rutgers and Baidu
            Inc). </p>
          <p> <em> Abstract: We study the problem of minimizing the
              average of a large number of smooth convex functions
              penalized with a strongly convex regularizer. We propose
              and analyze a novel primal-dual method (Quartz) which at
              every iteration samples and updates a random subset of the
              dual variables, chosen according to an {\em arbitrary
              distribution}. In contrast to typical analysis, we
              directly bound the decrease of the primal-dual error (in
              expectation), without the need to first analyze the dual
              error. Depending on the choice of the sampling, we obtain
              efficient serial, parallel and distributed variants of the
              method. In the serial case, our bounds match the best
              known bounds for SDCA (both with uniform and importance
              sampling). With standard mini-batching, our bounds predict
              initial data-independent speedup as well {as \em
              additional data-driven speedup} which depends on spectral
              and sparsity properties of the data. We calculate
              theoretical speedup factors and find that they are
              excellent predictors of actual speedup in practice.
              Moreover, we illustrate that it is possible to design an
              efficient {\em mini-batch importance} sampling. The
              distributed variant of Quartz is the first distributed
              SDCA-like method with an analysis for non-separable data.
            </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> November 20, 2014</h3>
          <p> As of today, and until the end of the week, I am in Jasna,
            Slovakia, at the <a
              href="http://www.konferenciajasna.sk/category/1/"> 46th
              Conference of Slovak Mathematicians.</a> I am giving a
            plenary talk on Saturday. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> November 13, 2014</h3>
          <p> A revised version of our "simplified direct search" paper
            with <a href="http://jakubkonecny.com/">Jakub Konecny</a>
            is available <a href="papers/sds.pdf"> locally here</a> and
            on <a href="http://arxiv.org/abs/1410.0390v2">arXiv.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> November 12, 2014</h3>
          <p> Together with James Madisson, I am organizing a one day
            workshop on <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/NAIS2014/">Numerical
Algorithms














              and Intelligent Software.</a> It will take place in
            Edinburgh on December 5, 2014. The event is funded by <a
              href="http://www.nais.org.uk/">NAIS</a> (whose website
            seems to be hacked - so you migt not be able to get through
            the last link). </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> November 11, 2014</h3>
          <p> Today we have the <a
              ref="http://www.maths.ed.ac.uk/~prichtar/i_seminar.html">All














              Hands Meeting on Big Data Optimization.</a> <a
              href="http://www.maths.ed.ac.uk/%7Ezqu/">Zeng Qu</a>
            talked about 3 papers describing randomized coordinate
            descent methods for convex optimization problems subject to
            one or more linear constraints. The paper "A random
            coordinate descent method on large optimization problems
            with linear constraints" of Necoara, Nesterov and Glineur
            can handle a single linear constraint - by updating two
            coordinates at a time. Current best results <a
              href="http://www.optimization-online.org/DB_FILE/2012/11/3669.pdf">(of
Necoara














              and Patrascu)</a> for more constraints lead to an
            exponential dependence on the number of constraints, and
            hence are very pessimistic. The focus of the meeting was the
            paper <a href="http://arxiv.org/pdf/1409.2617v3.pdf">"Large-scale














              randomized-coordinate descent methods with non-separable
              linear constraints"</a> which claimed to have obtained an
            efficient method of handling many constraints. Based on the
            disussion we had (throug observations of Zheng Qu wo read
            the paper in soem detail), we were not convinced the
            analysis is correct. It seems some steps in the analysis are
            problematic. So, it seems, the problem of designing a
            coordinate descent method which can efficiently handle
            multiple linear constraints remains open. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> November 7, 2014</h3>
          <p> <span class="important"> NEW: 3 Postdoctoral Fellowships
              Available: Apply by December 8, 2014! </span> All three
            fellowships are for 2 years, starting date: by Sept 1, 2015.
            Optimization qualifies in all three cases; especially in the
            case of the two Whittaker Fellowships. </p>
          <ul>
            <li>(1) <a
                href="http://www.maths.ed.ac.uk/jobs/seggiebrown2014">William
Gordon














                Seggie Brown Research Fellowship.</a> The successful
              fellow will pursue world-class research as well as
              contributing to the teaching activity of the School. </li>
            <li>(2) <a href="http://www.maths.ed.ac.uk/jobs/wrfdata">Whittaker
Research














                Fellow in Mathematics for Data Science.</a> Candidates
              should have a track record of research developing new
              mathematical or statistical methods for the analysis and
              processing of large-scale datasets. </li>
            <li>(3) <a
                href="http://www.maths.ed.ac.uk/jobs/wrfindustry">Whittaker
Research














                Fellow in Mathematics for Industry or Business.</a>
              Candidates should have a track record of reseach in
              mathematics with demonstrable impact on industry or
              business (understood in their broadest sense). </li>
          </ul>
          Feel free to contact me if you are interested to apply. <img
            src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> November 4, 2014</h3>
          <p> This week, <a href="http://jakubkonecny.com/">Jakub
              Konecny</a> is on a research visit to the <a
              href="http://www.da.inf.ethz.ch/">Data Analytics Lab </a>
            led by Thomas Hofmann at ETH Zurich. Yesterday, Jakub gave a
            talk on S2GD there (and I am told, almost got lost in the
            Swiss Alps, or hurt his back, or neither of these, or some
            other such thing). I also gave a <a
              href="talks/TALK-2014-11-S2GD.ppsx">talk on S2GD </a>
            (and mentioned <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/mS2GD.pdf">mS2GD</a>
            and <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/S2CD.pdf">S2CD</a>
            as well), today, in the <a
href="http://www.anc.ed.ac.uk/events/anc-dtc-seminar-peter-richtarik-school-of-maths">machine
learning














              seminar</a> at the School of Informatics. We then had the
            <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














              Hands Meeting on Big Data Optimization </a>where Ademir
            Ribeiro described a <a
              href="http://arxiv.org/abs/1410.0390">recent paper of mine
              with Jakub on direct search</a> and outlined possible
            avenues for an extension to an adaptive setting. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> October 27, 2014</h3>
          <p> I am about to give a guest lecture in the new <a
              href="http://www.inf.ed.ac.uk/teaching/courses/irds/">Introduction














              to Research in Data Science </a> course - aimed at the
            PhD students in the first cohort of our new <a
              href="http://datascience.inf.ed.ac.uk/">Centre for
              Doctoral Training (CDT) in Data Science.</a> I will speak
            about Semi-Stochastic Gradient Descent (joint work with <a
              href="http://jakubkonecny.com/">Jakub Konecny</a>: <a
              href="http://arxiv.org/abs/1312.1666">paper</a>, <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/posters/Poster-S2GD.pdf">poster</a>).














          </p>
          <p>Recent extensions: <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/S2CD.pdf">S2CD</a>
            (one can get away with computing (random) partial
            derivatives instead of gradients) and <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/S2CD.pdf">mS2GD</a>
            (the method accelerates if mini-batches are used; i.e., if
            we compute gradients of multiple random loss functions
            instead of just a single one). </p>
          <p>The lecture will be recorded I believe and the <a
              href="http://www.inf.ed.ac.uk/teaching/courses/irds/lectures.html">slides
and














              the video will appear here</a> at some point.</p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> October 24, 2014</h3>
          <p> A very interesting read: <a
href="http://www.math.utk.edu/%7Evasili/refs/Papers/NAS13.math_in_2025.pdf">The
Mathematical














              Sciences in 2025</a>. I think this is a must-read for all
            scientists.</p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> October 22, 2014</h3>
          <p> Today we had <a href="http://www.columbia.edu/%7Ejw2966/">John
Wright














              (Columbia)</a> give a talk in the ``Mathematics and Big
            Data'' distinghished lecture series which I organize. John
            has talked about a very intriguinging phenomenon occuring in
            the modelling of high-dimensional data as the product of an
            unknown (square) dictionary matrix and a (random) sparse
            matrix: the inverse problem of finding such factors leads to
            a nonconvex problem with many local optima which can
            efficiently be solved to global optimality. The trick is in
            the observation that, surprisingly, all local minima of the
            function turn out to be global minima. Moreover, the
            function has a strcuture which allows a trust region method
            on the sphere to find a local minimum. </p>
          <p> Title: Provably Effective Representations for
            High-Dimensional Data </p>
          <p> Abstract: Finding concise, accurate representations for
            sample data is a central problem in modern data analysis. In
            this talk, we discuss several intriguing “high-dimensional”
            phenomena which arise when we try to build effective
            representations for application data. The first qualitative
            surprise involves nonconvexity. We prove that a certain
            family of nonconvex optimization problems arising in data
            analysis can actually be solved globally via efficient
            numerical algorithms, provided the data are sufficiently
            large and random. Using based on this observation, we
            describe algorithms which provably learn “dictionaries” for
            concisely representing n-dimensional signals, even when the
            representation requires O(n) non zeros for each input
            signal; the previous best results ([Spielman et. al. ’12]
            via LP relaxation) only allowed \tilde{O}(\sqrt{n}) nonzeros
            per input. The second qualitative surprise involves
            robustness. Application data are often dirty: corrupted,
            incomplete, noisy. Recovering low-dimensional models from
            corrupted data is hopelessly intractable in the worst case.
            In contrast to this worst-case picture, we show that natural
            convex programming relaxations recover low-dimensional
            objects such as sparse vectors and low-rank matrices from
            substantial fractions of “typical” errors. We illustrate the
            talk with application examples drawn from computer vision,
            audio processing, and scientific imaging. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> October 21, 2014 </h3>
          <p> I am back in Edinburgh. Today we have another <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














              Hands Meeting on Big Data Optimization</a>, led by Dominik
            Csiba. He will be speaking about a recent <a
              href="http://arxiv.org/abs/1409.2848">paper of Ohad Shamir
              on Stochastic PCA method.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> October 17, 2014 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="papers/mS2GD.pdf">mS2GD: Mini-batch semi-stochastic
              gradient descent in the proximal setting,</a> joint with <a
              href="http://jakubkonecny.com/">Jakub Konecny</a>
            (Edinburgh), <a href="http://jieliucn.weebly.com/">Jie Liu</a>
            (Lehigh) and <a href="http://www.mtakac.com/CV">Martin
              Takac</a> (Lehigh). </p>
          <p> <em>Abstract: We propose a mini-batching scheme for
              improving the theoretical complexity and practical
              performance of semi-stochastic gradient descent applied to
              the problem of minimizing a strongly convex composite
              function represented as the sum of an average of a large
              number of smooth convex functions, and simple nonsmooth
              convex function. Our method first performs a deterministic
              step (computation of the gradient of the objective
              function at the starting point), followed by a large
              number of stochastic steps. The process is repeated a few
              times with the last iterate becoming the new starting
              point. The novelty of our method is in introduction of
              mini-batching into the computation of stochastic steps. In
              each step, instead of choosing a single function, we
              sample b functions, compute their gradients, and compute
              the direction based on this. We analyze the complexity of
              the method and show that it benefits from two speedup
              effects. First, we prove that as long as b is below a
              certain threshold, we can reach predefined accuracy with
              less overall work than without mini-batching. Second, our
              mini-batching scheme admits a simple parallel
              implementation, and hence is suitable for further
              acceleration by parallelization. In the b=1 case we
              recover the complexity achieved by the Prox-SVRG method of
              Liao and Zhang. In the smooth case, our method is
              identical to the S2GD method of Konecny and Richtarik. </em>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> October 16, 2014 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="papers/S2CD.pdf">S2CD: Semi-stochastic coordinate
              descent,</a> joint with <a
              href="http://jakubkonecny.com/">Jakub Konecny</a> and <a
              href="http://www.cmapx.polytechnique.fr/%7Equ/">Zheng Qu</a>.
          </p>
          <p> <em>Abstract: We propose a novel reduced variance
              method---semi-stochastic coordinate descent (S2CD)---for
              the problem of minimizing a strongly convex function
              represented as the average of a large number of smooth
              convex functions: f(x) = (1/n)*sum_{i=1}^n f_i(x). Our
              method first performs a deterministic step (computation of
              the gradient of f at the starting point), followed by a
              large number of stochastic steps. The process is repeated
              a few times, with the last stochastic iterate becoming the
              new starting point where the deterministic step is taken.
              The novelty of our method is in how the stochastic steps
              are performed. In each such step, we pick a random
              function f_i and a random coordinate j---both using
              nonuniform distributions---and update a single coordinate
              of the decision vector only, based on the computation of
              the jth partial derivative of f_i at two different points.
              Each random step of the method constitutes an unbiased
              estimate of the gradient of f and moreover, the squared
              norm of the steps goes to zero in expectation, meaning
              that the method enjoys a reduced variance property. The
              complexity of the method is the sum of two terms: O(n
              log(1/ε)) evaluations of gradients ∇f_i and O(κ log(1/ε))
              evaluations of partial derivatives ∇j f_i, where κ is a
              novel condition number. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> October 9, 2014 </h3>
          <p> I have arrived to Hong Kong yesterday (and I think I just
            managed to get de-jetlagged). I am visiting the group of <a
              href="http://www1.se.cuhk.edu.hk/%7Esqma/">Shiqian Ma</a>
            at the Chinese University of Hong Kong and will be around
            for a couple weeks (you can find me in office #708 in the
            William M.W. Mong Building). The weather here is great, the
            campus is built on a mountain and looks and feels really
            nice. The view from the top of the university hill is
            allegedly the second best in Hong Kong. I have been there,
            the view is indeed great, although I can't confirm the local
            rank as I have not seen anything else. I am giving a <a
href="http://seminar.se.cuhk.edu.hk/content/randomized-coordinate-descent-methods-unified-theory">talk














              tomorrow</a>. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> October 3, 2014 </h3>
          <p> <a href="http://people.ufpr.br/%7Eademir.ribeiro/">Ademir
              Ribeiro</a> has joined the <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_team.html">team</a>
            as a postdoc - he will stay for 6 months. </p>
          <p><em> Short bio: </em> Ademir is an Associate Professor at
            the <a href="http://www.ufpr.br/portalufpr/">Federal
              University of Parana</a>, Brazil. Among other things, he
            has worked on global and local convergence of filter and
            trust region methods for nonlinear programming and convex
            optimization. He has recently published a book entitled
            "Continuous Optimization: Theoretical and Computational
            Aspects" (in Portuguese). </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> October 3, 2014 </h3>
          <p> Today I am participating (by giving a brief talk) in an
            industrial sandpit put together by the newly established <a
              href="http://www.maxwell.ac.uk/MIGSAA.php">Maxwell
              Institute Graduate School in Analysis and its
              Applications,</a> of which I am a faculty member. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> October 1, 2014 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1410.0390">Simple complexity
              analysis of direct search,</a> joint with Jakub Konecny. </p>
          <p> <em>Abstract: We consider the problem of unconstrained
              minimization of a smooth function in the derivative-free
              setting. In particular, we study the direct search method
              (of directional type). Despite relevant research activity
              spanning several decades, until recently no complexity
              guarantees — bounds on the number of function evaluations
              needed to find a satisfying point — for methods of this
              type were established. Moreover, existing complexity
              results require long proofs and the resulting bounds have
              a complicated form. In this paper we give a very brief and
              insightful analysis of direct search for nonconvex, convex
              and strongly convex objective function, based on the
              observation that what is in the literature called an
              “unsuccessful step”, is in fact a step that can drive the
              analysis. We match the existing results in their
              dependence on the problem dimension (n) and error
              tolerance (ε), but the overall complexity bounds are much
              simpler, easier to interpret, and have better dependence
              on other problem parameters. In particular, we show that
              the number of function evaluations needed to find an
              ε-solution is O(n^2/ε) (resp. O(n^2 log(1/ε))) for the
              problem of minimizing a convex (resp. strongly convex)
              smooth function. In the nonconvex smooth case, the bound
              is O(n^2/ε^2), with the goal being the reduction of the
              norm of the gradient below ε. </em> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> September 30, 2014</h3>
          <p> We have our <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">third














              All Hands Meeting on Big Data Optimization</a> today. <a
              href="http://jakubkonecny.com/">Jakub Konecny</a> will
            tell us about what machine learning is about (i.e., quick
            and dirty intrduction to ML for optimizers) - i.e., that
            besides optimization error, there are other things a ML
            person needs to worry about, such as approximation error,
            estimation error, sample complexity and so on. Everybody is
            invited; lunch will be provided. </p>
          <p> In the afternoon, I am giving a <a
href="http://wcms.inf.ed.ac.uk/lfcs/events/lfcs-seminars-1/lfcs-seminar-by-dr.-peter-richtarik">talk
in














              the LFCS (Lab for Foundations of Computer Science)
              Seminar.</a> If you happen to be around the Informatics
            Forum in the afternoon, <a
href="http://www.ph.ed.ac.uk/news/events/2014/how-humans-and-machines-integrate-language-and-vision-inaugural-lecture-3649">this














              talk</a> looks interesting.</p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> September 16, 2014</h3>
          <p> Here are the <a href="docs/Povh_Day1_slides.pdf">slides
              from Day 1</a> of Janez Povh's course. Plenty of covered
            material is not on the slides - Janez used the whiteboard a
            lot. Tomorrow we are starting at 9:15am instead of 9:00am.
            Update (17.9.2014): <a href="docs/Povh_Day2_slides.pdf">slides













              from Day 2.</a> </p>
          <p> Today, we have had our first seminar in the "All Hands
            Meetings on Big Data Optimization" series this semester. <a
              href="http://www.maths.ed.ac.uk/%7Ekfount/">Kimon
              Fountoulakis</a> talked about <a
              href="http://arxiv.org/abs/1407.7573">Robust Block
              Coordinate Descent</a> (joint work with <a
              href="http://www.maths.ed.ac.uk/%7Ertappend/">Rachael
              Tappenden</a>) - work that arose from the discussions
            initiated at the seminar last semester. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> September 15, 2014</h3>
          <p> A few unrelated bits of news from today: It's the first
            day of the semester and I met with my 10 MSc tutees. My PhD
            student <a href="http://jakubkonecny.com/">Jakub Konecny</a>
            had his qualifying exam; he gave an impressive talk and good
            answers in the Q&amp;A session. The committee passed him and
            even uttered a few words of praise. My postdoc <a
              href="http://www.cmapx.polytechnique.fr/%7Equ/">Zheng Qu</a>
            started teaching a class and I am her TA (= tutor). Janez
            Povh (Slovenia) is visiting this week and his short course
            (6 hours) on <a href="docs/JANEZ%20POVH-sdp-co-rag.pdf">Semidefine














              Programming, Combinatorial Optimization and Real Algebraic
              Geometry </a>starts tomorrow at 9am, as earlier announced
            on this site. Also, it was unusually misty today in
            Edinburgh! I had to decline an invite for a funded visit to
            Berkeley due to a conflict with <a
              href="https://www.fing.edu.uy/eventos/focm2014/">FoCM in
              Uruguay</a>. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> September 12, 2014</h3>
          <p> Today I attended the Edinburgh Data Science Research Day:
            the official launch of our new Centre for Doctoral Training
            in Data Science. Many of our <a
              href="http://datascience.inf.ed.ac.uk/partners/">industrial














              partners</a> were present. I have thoroughly enjoyed my
            conversations with Sean Murphy (Amazon), Gary Kazantsev
            (Bloomberg), Heiga Zen (Google), Julien Cornebise (Google
            Deepmind), Leighton Pritchard (James Hutton Institute),
            Andrew Lehane (Keysight Technologies / Agilent), Igor Muttik
            (McAfee), Mike Lincoln (Quorate) and Phil Scordis (UCB
            Celltech). </p>
          <p><a href="http://www.cmapx.polytechnique.fr/%7Equ/">Zheng Qu</a>
            and I have presented a total of 4 posters at the event
            (which attracted quite a bit of attention): <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/posters/Poster-Hydra2.pdf">Hydra2</a>,
            <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/posters/Poster-S2GD.pdf">S2GD</a>,
            <a
              href="http://www.maths.ed.ac.uk/%7Eprichtar/posters/Poster-TOPSPIN.pdf">TopSpin</a>
            and <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/posters/Tappenden.pdf">ICD</a>.
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> September 7, 2014</h3>
          <p> This week I am at the <a
              href="http://www.iam.fmph.uniba.sk/mmei2014/"> 18th
              International Conference in Mathematical Methods and
              Economy and Industry </a>, in the beautiful <a
              href="http://en.wikipedia.org/wiki/Smolenice_Castle">Smolenice
Castle














            </a> (now a congress centre of the Slovak Academy of
            Sciences) in Slovakia. The conference history dates back to
            1973. I am giving a plenary talk on Wednesday. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> September 3, 2014</h3>
          <p> As of today and until the end of the week, I am at the <a
href="http://www.ima.org.uk/conferences/conferences_calendar/4th_ima_conference_on_numerical_linear_algebra_and_optimisation.cfm.html">IMA
Conference














              on Numerical Linear Algebra and Optimisation </a> in
            Birmingham. I am co-organizing two minisymposia: </p>
          <ul>
            <li>Thursday, Sept 4, 14:50-17:55, <span class="important">Optimization
and














                decomposition for image processig and related topics</span>
              (organized with C.B. Shoenlieb and T. Valkonen) </li>
            <br>
            <li> Friday, Sept 5, 9:50-14:50, <span class="important">First














                order methods and big data optimization</span>
              (organized with Z. Qu and J. Konecny) </li>
          </ul>
          I am giving a talk in the Friday minisymposium. <img
            src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> September 1, 2014 </h3>
          <p> Janez Povh (Slovenia) will deliver a short course on <span
              class="important">"Semidefinite Programming, Combinatorial
              Optimization and Real Algebraic Geometry" </span> in
            Edinburgh during September 16-17, 2014. Attendance is FREE
            -- but <a href="https://www.surveymonkey.com/s/F8GXYXG">please














              register here.</a> The course is aimed at PhD students,
            postdocs and senior researchers interested in the topic.</p>
          <p> Venue: 4325B James Clerk Maxwell Building, Kings
            Buildings, Edinburgh. The course will be delivered in two
            parts: Part I (Sept 16; 9:00-12:00) and Part II (Sept 17;
            9:00-12:00). </p>
          <em>
            <p>Abstract: In the last decade, semidefinite programming
              (loosely speaking: optimization problems with variables
              being symmetric positive semidefinite matrices) has proved
              to be a very successful and powerful tool for
              approximately solving hard problems arising in
              combinatorial optimization (e.g., MAX-CUT, Quadratic
              assignment problem, Graph colouring problem) and for
              approximately computing the optimum of a real polynomial
              over a semialgebraic set. In both cases, the objective
              function and the feasible set is simplified so that the
              new problem is an instance of the semidefinite programming
              problem. The solution of the relaxation provides lower or
              upper bound for the original problem and often also a
              starting point for ! obtaining good feasible solutions.
              This short course will cover basic definitions and
              fundamental results in the theory of semidefinite
              programming, and will demonstrate how these can be used to
              approach several well-known problems arising in
              combinatorial optimization and real algebraic geometry. </p>
          </em>
          <p> The event poster can be <a
              href="docs/JANEZ%20POVH-sdp-co-rag.pdf">downloaded here.</a>
          </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> August 22, 2014 </h3>
          <p> This year, we are launching a new PhD programme in <a
              href="http://en.wikipedia.org/wiki/Data_science">Data
              Science. </a> Data Science is an emerging new
            interdisciplinary field, and we are quite excited to be able
            to offer this. However, the novelty also means that it makes
            sense to read about this all a bit. As a start, I recommend
            looking <a
              href="http://cilvr.cs.nyu.edu/doku.php?id=publications:cds-manifesto">here</a>
            and <a href="http://en.wikipedia.org/wiki/Data_science">here.</a>
          </p>
          <p> I felt, however, that perhaps I had a good enough excuse
            to actually pick up some book on the topic; this one caught
            my attention: <a
href="http://www.amazon.com/Doing-Data-Science-Straight-Frontline/dp/1449358659">"Doing
Data














              Science: Straight Talk from the Frontline" </a> by Rachel
            Shutt and Cathy O'Neill. At the end of the chapter on
            algorithms I've seen a spotlight column titled "Modeling and
            Algorithms at Scale". I was naturally interested. Much to my
            surprise, the text included a quote from Peter Richtarik
            from Edinburgh university... I was naturally intrigued about
            this: first, because I did not immediately recognize the
            text and, most importantly, because I did not fully agree
            with it. Funny, I know.</p>
          <p> Here is the relevant excerpt from the book: </p>
          <em>Optimization with Big Data calls for new approaches and
            theory -- this is the frontier! From a 2013 talk by Peter
            Richtarik from the University of Edinburgh: "In the big data
            domain classical approaches that rely on optimization
            methods with multiple iterations are not applicable as the
            computational cost of even a single iteration is often too
            excessive; these methods were developed in the past when
            problems of huge sizes were rare to find. We thus needs new
            methods which would be simple, gentle with data handling and
            memory requirements, and scalable. Our ability to solve
            truly huge scale problems goes hand in hand with our ability
            to utilize modern parallel computing architectures such as
            multicore processors, graphical processing units, and
            computer clusters." </em>
          <p> I was thinking: this guy apparently has some bold vision
            of some futuristic optimization algorithms which can do with
            a single iteration only! Awesome! In reality, I was
            convinced I could not have said that, as I do not know of
            any new approaches that would transcend <em>iterative</em>
            algorithmic thinking. It did not take me long to figure out
            what I actually said (turns out, at the <a
href="http://wwwf.imperial.ac.uk/%7Egmontana/bigdatamining/abstracts.html">Big
Data














              Mining workshop at Imperial College, London</a>): </p>
          <em> "Optimization with big data calls for new approaches and
            theory helping us understand what we can and cannot expect.
            In the big data domain classical approaches are not
            applicable as the computational cost of even a single
            iteration is often too excessive; these methods were
            developed in the past when problems of huge sizes were rare
            to find. We thus need new methods which would be simple,
            gentle with data handling and memory requirements, and
            scalable. Our ability to solve truly huge scale problems
            goes hand in hand with our ability to utilize modern
            parallel computing architectures such as multicore
            processors, graphical processing units, and computer
            clusters. In this talk I will describe a new approach to big
            data (convex) optimization which uses what may seem to be an
            'excessive' amount of randomization and utilizes what may
            look as a 'crazy' parallelization scheme. I will explain why
            this approach is in fact efficient and effective and well
            suited for big data optimization tasks arising in many
            fields, including machine and statistical learning, social
            media and engineering.Time permitting, I may comment on
            other optimization methods suited for big data application
            which also utilize randomization and parallelization." </em>
          <p> </p>
          <p> This is just an amusing story -- I am not really unhappy
            about the confusion caused as the statements are pretty
            vague anyway (as is often the case with abstracts for
            longish talks). I think the book is a valuable read for any
            student interested in data science. </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> August 19, 2014 </h3>
          <p> <a href="http://ttic.uchicago.edu/%7Enati/">Nati Srebro
              (TTI Chicago)</a> is visiting -- he will stay until the
            end of August. Tomorrow he is giving a <a
              href="http://www.maths.ed.ac.uk/ERGO/abstracts/2014-08-srebro.html">talk
on














              Distributed Stochastic Optimization.</a> </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> August 15, 2014 </h3>
          <p> We have a new Head of School as of August 1st - a <a
              href="http://en.wikipedia.org/wiki/Representation_theory">
              representation theorist. </a> It is a funny fact that <a
href="http://genealogy.math.ndsu.nodak.edu/id.php?id=161672"> the
              advisor of my advisor's advisor was a representation
              theorist,</a> too. I wonder whether one of my descendants
            will become a representation theorist to complete the
            circle... </p>
          <img src="imgs/fancy-line.png" align="absmiddle" height="20">
          <h3> August 14, 2014 </h3>
          <p> <span class="important">New paper out:</span> <a
              href="http://arxiv.org/abs/1408.2467">Inequality-Constrained














              Matrix Completion: Adding the Obvious Helps!</a>, joint
            with Martin Takac (Edinburgh) and Jakub Marecek (IBM
            Research). It was written (and announced on my website) in
            January, but we only got around posting it to arXiv now.</p>
          <p>Abstract: <em>We propose imposing box constraints on the
              individual elements of the unknown matrix in the matrix
              completion problem and present a number of natural
              applications, ranging from collaborative filtering under
              interval uncertainty to computer vision. oreover, we
              design an alternating direction parallel coordinate
              descent method (MACO) for a smooth unconstrained
              optimization reformulation of the problem. In large scale
              numerical experiments in collaborative filtering under
              uncertainty, our method obtains solution with considerably
              smaller errors compared to classical matrix completion
              with equalities. We show that, surprisingly, seemingly
              obvious and trivial inequality constraints, when added to
              the formulation, can have a large impact. This is
              demonstrated on a number of machine learning problems.</em></p>
          <span class="rss-newpost"></span> <img
            src="imgs/fancy-line.png" align="absmiddle" height="20"> <span
            class="rss-content">
            <h3> July 29, 2014 </h3>
            <p> A revised version of the paper "Fast distributed
              coordinate descent for minimizing non-strongly convex
              losses" is now on <a
                href="http://arxiv.org/pdf/1405.5300v2.pdf">ArXiv.</a>
              The paper was accepted to <a
                href="http://mlsp2014.conwiz.dk/"> IEEE Machine Learning
                for Signal Processing (MLSP 2014).</a> </p>
            <span class="rss-newpost"></span> <img
              src="imgs/fancy-line.png" align="absmiddle" height="20"> <span
              class="rss-content">
              <h3> July 2, 2014 </h3>
              <p>As of this week, my postdoc <a
                  href="http://www.cmap.polytechnique.fr/%7Equ/">Zheng
                  Qu</a> is on a research visit at the Big Data Lab at <a
                  href="http://www.baidu.com/">Baidu</a> in Beijing
                (Baidu is China's Google). This visit is part of a joint
                research project with <a
                  href="http://stat.rutgers.edu/home/tzhang/">Tong Zhang</a>,
                who directs the Big Data Lab. The Lab conducts research
                in problems related to big data analysis, including
                large scale big data optimization. Tong Zhang
                concurrently holds a <a
                  href="http://stat.rutgers.edu/home/tzhang/">chair in
                  Statistics at Rutgers University</a>. Zheng will be
                back in the UK by the time the <a
href="http://www.ima.org.uk/conferences/conferences_calendar/4th_ima_conference_on_numerical_linear_algebra_and_optimisation.cfm.html">IMA
NLA














                  &amp; Optimisation Conference in Birmingham</a>
                starts, where she is co-organizing a minisymosium on
                gradient methods for big data problems with<a
                  href="http://jakubkonecny.com/"> Jakub Konečný</a> and
                myself.</p>
              <span class="rss-newpost"></span> <img
                src="imgs/fancy-line.png" align="absmiddle" height="20">
              <span class="rss-content">
                <h3> July 1, 2014 </h3>
                <p> I am in Lancaster, giving a keynote talk at the
                  workshop <a
                    href="http://www.lancaster.ac.uk/uclid2014/index.html">Understanding
Complex














                    and Large Industrial Data.</a> </p>
                <span class="rss-newpost"></span> <img
                  src="imgs/fancy-line.png" align="absmiddle"
                  height="20"> <span class="rss-content">
                  <h3> June 26, 2014 </h3>
                  <p> This week (June 23-27) we are running a <a
                      href="http://www.maths.ed.ac.uk/hall/NATCOR/">
                      Convex Optimization PhD course in Edinburgh.</a>
                    It is attended by students from all around the UK
                    and a few from continental Europe as well. The
                    instructors are: Stephen Boyd (Stanford), Paresh
                    Date (Brunel), Olivier Fercoq (Edinburgh), Jacek
                    Gondzio (Edinburgh), Julian Hall (Edinburgh),
                    Michael Perregaard (FICO), Sergio Garcia Quiles
                    (Edinburgh), myself, Rachael Tappenden (Edinburgh).
                    I am teaching two hours on first order methods
                    tomorrow. Here are the slides (I will only cover a
                    subset of this): <a href="docs/cdm-NATCOR.ppsx">overview</a>,
                    <a href="docs/cdm-NATCOR.pdf">theory</a>.</p>
                  <span class="rss-newpost"></span> <img
                    src="imgs/fancy-line.png" align="absmiddle"
                    height="20"> <span class="rss-content">
                    <h3> June 24, 2014 </h3>
                    <p> <a
                        href="http://acse.pub.ro/person/ion-necoara/">Ion












                        Necoara (Bucharest)</a> is visiting this week.
                      He will give a <a
                        href="http://www.maths.ed.ac.uk/ERGO/abstracts/2014-06-necoara.html">talk












                        tomorrow at 1:30pm on coordinate descent
                        methods.</a></p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> June 18, 2014 </h3>
                    <p> Congratulations to <a
                        href="http://jakubkonecny.com/">Jakub Konečný</a>,
                      1st year PhD student in the <a
                        href="http://www.maths.ed.ac.uk/">School of
                        Mathematics</a>, for being awarded the <span
                        class="important">2014 Google Europe Doctoral
                        Fellowship in Optimization Algorithms!</span>
                      The news was announced today in the <a
href="http://googleresearch.blogspot.ie/2014/06/2014-google-phd-fellowships-supporting.html">Google
Research














                        Blog.</a></p>
                    <p> This is what Google says about these
                      Fellowships: <em>Nurturing and maintaining strong
                        relations with the academic community is a top
                        priority at Google. Today, we're announcing the
                        2014 Google PhD Fellowship recipients. These
                        students, recognized for their incredible
                        creativity, knowledge and skills, represent some
                        of the most outstanding graduate researchers in
                        computer science across the globe. We're excited
                        to support them, and we extend our warmest
                        congratulations. </em></p>
                    <p> This year, Google has announced 38 Fellowships
                      to PhD students across the globe: 15 in Europe, 14
                      in North America, 4 in China, 3 in India and 2 in
                      Australia. These fellowships provide generous
                      funding for the students for up to three years to
                      help them better achieve their research
                      objectives, and open the doors to a closer
                      collaboration with Google through the
                      establishment of Google mentors and other
                      activities. Out of the 15 Europe Fellowships, 4
                      were awarded to universities in the UK: 2 in
                      Cambridge and 2 in Edinburgh. The rest went to
                      students in Switzerland (4), Germany (3), Israel
                      (2), Austria (1) and Poland (1).</p>
                    <p> Jakub has started his PhD in August 2013 at the
                      <a href="http://www.ed.ac.uk/home">University of
                        Edinburgh</a>, working under my supervision. He
                      spent his first semester of PhD studies at <a
                        href="http://www.berkeley.edu/index.html">University














                        of California Berkeley</a> as a <a
                        href="http://simons.berkeley.edu/people/jakub-konecny">visiting
graduate














                        student</a> (thanks to <a
                        href="http://www.nais.org.uk/">NAIS</a> for
                      generous support of this visit), where he
                      participated in the semester-long programme on <a
href="http://simons.berkeley.edu/programs/bigdata2013">Theoretical
                        Foundations of Big Data Analysis</a> at the
                      newly established <a
                        href="http://simons.berkeley.edu/">Simons
                        Institute for the Theory of Computing</a>. Jakub
                      also managed to take a few PhD courses, put some
                      final touches on a <a
                        href="http://arxiv.org/abs/1312.4190">JMLR paper
                        on Gesture Recognition</a> (for this work he and
                      his coauthor were awarded 2nd Prize at the
                      ChaLearn gesture challenge competition and
                      presented the work at ICPR in Tsukuba, Japan),
                      write a new paper on <a
                        href="http://arxiv.org/abs/1312.1666">Semi-Stochastic














                        Gradient Descent</a> and make progress towards
                      one more paper in collaboration with a couple of
                      Simons long term visitors. Since returning to
                      Edinburgh, besides doing research and volunteering
                      for various projects around Scotland, Jakub has
                      been co-organizing weekly <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All












                        Hands Meetings on Big Data Optimization</a>.
                      Prior to coming to Edinburgh, he studied
                      mathematics at the Comenius University in
                      Slovakia. In 2010, Jakub Konecny represented his
                      country in the International Mathematical Olympiad
                      in Kazachstan, earning a Honorable Mention.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> June 17, 2014 </h3>
                    <p> We have had our last <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">"All














                        Hands'</a>' meeting on big data optimization
                      this academic year. The speaker was Mojmír Mutný -
                      and the topic was the <a
                        href="http://jmlr.org/proceedings/papers/v28/jaggi13-supp.pdf">Frank-Wolfe














                        algorithm</a>.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> June 11, 2014 </h3>
                    <p> I've arrived to Grenoble. Having been first
                      welcome by a storm (to make me feel at home, I am
                      sure) yesterday when I arrived, today it is warm
                      and sunny. The campus is located in a beautiful
                      setting surrounded by mountains. </p>
                    <p> I will be <a
                        href="http://lear.inrialpes.fr/people/harchaoui/projects/khronos/">teaching</a>
                      (Randomized Coordinate Descent for Big Data
                      Problems) 3 hours today and 3 hours tomorrow. I
                      have two sets of slides: powerpoint for nice
                      flashy arrows, pictures, animations and, most
                      importantly, aimed at delivering insight from
                      bird's eye perspective. I also have <a
                        href="docs/cdm-talk.pdf">technical slides</a>
                      with proofs (here is a <a
                        href="docs/cdm-print.pdf">version for printing</a>).












                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> June 10, 2014 </h3>
                    <p>Today I am travelling to Grenoble, where I will
                      give a 6 hour mini-course on Randomized Coordinate
                      Descent Methods for Big Data Optimization. The
                      course is part of the <a
                        href="http://lear.inrialpes.fr/people/harchaoui/projects/khronos/">Khronos-Persyval
Days














                        on "High-Dimensional Learning and Optimization".</a>
                      Meanwhile, <a href="http://jakubkonecny.com/">Jakub













                        Konecny</a> and <a
                        href="http://www.cmap.polytechnique.fr/%7Equ/">Zheng














                        Qu</a> are attending the <a
href="http://www.kcl.ac.uk/nms/depts/mathematics/events/eventsrecords/londonoptimizationworkshop.aspx">London
Optimization














                        Workshop.</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> June 9, 2014 </h3>
                    <p><a href="http://de.linkedin.com/in/carchambeau">Cedric














                        Archambeau</a>, a manager and senior research
                      scientist at Amazon Berlin is visiting and giving
                      a <a
                        href="http://www.maths.ed.ac.uk/ERGO/abstracts/2014-06-archambeau.html">talk














                        today</a> in our seminar. It turns out Amazon is
                      currently imlementing &amp; testing the <a
                        href="http://arxiv.org/abs/1310.2059">Hydra
                        algorithm</a> developed by <a
                        href="http://www.mtakac.com/CV">Martin Takac</a>
                      and myself (here is a different variant, <a
                        href="http://arxiv.org/abs/1405.5300">Hydra^2</a>).</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> June 3, 2014 </h3>
                    <p>Today at 12:15 we have <a
                        href="http://people.maths.ox.ac.uk/%7Eszpruch/">Lukas














                        Szpruch</a> speaking in the <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














                        Hands Meetings on Big Data Optimization</a>
                      (room: JCMB 4312) about his recent work on <a
                        href="http://arxiv.org/pdf/1212.1377v1.pdf">Multilevel
Monte














                        Carlo Methods for Applications in Finance</a>.
                      Connections to optimization will be outlined. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> May 31, 2014 </h3>
                    <p><span class="important">New paper out:</span> <a
href="papers/distributed-partially-separable.pdf">Distributed Block
                        Coordinate Descent for Minimizing Partially
                        Separable Functions</a>. Joint with <a
href="http://researcher.watson.ibm.com/researcher/view.php?person=ie-jakub.marecek">Jakub
Mareček














                        (IBM)</a> and <a
                        href="http://www.mtakac.com/CV">Martin Takáč
                        (Edinburgh/Lehigh)</a>. Update (June 3): now
                      also on <a href="http://arxiv.org/abs/1406.0238">arXiv</a>.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> May 27, 2014 </h3>
                    <p> <a href="http://jakubkonecny.com/">Jakub
                        Konečný</a> was speaking today in the <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All














                        Hands Meeting on Big Data Optimization</a> about
                      a recent paper of Julian Mairal on <a
                        href="http://arxiv.org/abs/1402.4419">deterministic














                        and stochastic optimization methods with
                        surrogate functions.</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> May 21, 2014 </h3>
                    <p> Two days behind us, two more to go: the SIAM
                      Conference on optimization in San Diego is its
                      middle. Likewise, we have had the first day of the
                      minisymposium on coordinate descent methods on
                      Tuesday; one more to go with further three
                      sessions on Thursday. </p>
                    <p>The first session on Tuesday was started off by
                      Yurii Nesterov (Louvain) talking about <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62749">a
                        new primal-dual subgradient algorithm</a> which
                      in the dual can be interpreted as coordinate
                      descent in the space of Lagrange multipliers. The
                      ideas are intreaguing and deserve attention. I
                      have then given a <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62750">talk</a>
                      on the <a href="http://arxiv.org/abs/1312.5799">APPROX














                        algorithm</a>, which is a coordinate descent
                      method that is at the same accelerated, parallel
                      and proximal and avoids full dimensional
                      oprations. I gave a <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/talks/Tutorial-London-2014.ppsx">3h














                        tutorial</a> on this recently at Imperial
                      College - feel free to dive into the slides if
                      interested. The session was concluded by Taiji
                      Suzuki (Tokyo) with an interesting <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62993">talk</a>
                      on combining <a
                        href="http://arxiv.org/abs/1311.0622">stochastic
                        dual coordinate ascent and the ADMM method</a>.
                      Tong Zhang will give his talk on Thursday instead
                      as he is arriving to San Diego a bit later.</p>
                    <p> In the second session, Lin Xiao (Microsoft) <a
href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62754">talked</a>
                      about a way to improve some constants in the
                      complexity analysis of coordinate descent methods
                      as analyzed by <a
href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CC4QFjAA&amp;url=http%3A%2F%2Fwww.optimization-online.org%2FDB_FILE%2F2010%2F01%2F2527.pdf&amp;ei=F6R8U9_zG8vsoASzsoDIAw&amp;usg=AFQjCNFd-NHgYy5DDdGGINuvn0B4UKGp4w&amp;bvm=bv.67229260,d.cGU">Nesterov</a>
                      and <a
                        href="http://link.springer.com/article/10.1007/s10107-012-0614-z#">Takac
and














                        myself</a>. Here is the <a
                        href="http://arxiv.org/abs/1305.4723">paper</a>.
                      I was then subbing for Olivier Fercoq (Edinburgh)
                      and delivered his talk on <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62755">univeral
coordinate













                        descent</a> - in parallel, proximal and
                      accelerated variants. Yurii Nesterov gave a
                      plenary talk on universal gradient descent the day
                      before - our work was motivated by his. Cong Dong
                      (Florida) then talked about <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62756">stochastic














                        block mirror descent</a>, joint work with
                      Guanghui Lan. As usual with papers coathored by
                      Guanghui - this was an impressive tour de force
                      march through theorems covering every conceivable
                      case and setting (convex, nonconvex, stochastic -
                      whatever you want). Tom Luo (Minnesota) was not
                      able to deliver his talk, but his coauthor Mingyi
                      Hong gave the <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62757">talk</a>
                      instead. They looked at a <a
                        href="http://arxiv.org/abs/1310.6957">wide class
                        of coordinate descent methods</a> (cyclic,
                      randomized, greedy ...) and gave O(1/k)
                      guarantees. Due to the generality of the setting,
                      however, the leading constants of these types of
                      analysis are necessarily quite pessimistic and do
                      not reflect the actual behavior of the methods
                      very well - unlike the anaysis of randomized
                      cooridnate descent, they hide big
                      dimension-dependent constants. It is an important
                      open problem to see whether it is possible to
                      prove O(n/epsilon) complexity for cyclic
                      coordinate descent.</p>
                    <p> In the <a
href="http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=18493">final
coordinate














                        descent session</a> on Tuesday we had three
                      speakers: Martin Takac (Edinburgh/Lehigh), Ambuj
                      Tewari (Michigan) and Ji Liu (Wisconsin). Martin
                      talked about the analysis and implemetation of two
                      variants of distributed coordinate descent (<a
                        href="http://arxiv.org/abs/1310.2059">Hydra</a>
                      &amp; <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/papers/Hydra2.pdf">Hydra^2</a>)
                      and showed that the methods are indeed able to
                      solve big data problems (400GB, 3TB). Ambuj then
                      gave a very entertaining talk on his work on a
                      unifyng framework for analysing a class of
                      parallel coordinate descent methods and greedy
                      coordinate descent methods which he calls <a
                        href="http://meetings.siam.org/sess/dsp_talk.cfm?p=62760">block-greedy.</a>
                      Finally, Ji Liu talked about his joint work with
                      Steve Wright, Chris Re and Victor Bittorf on the
                      analysis of <a
                        href="http://arxiv.org/abs/1311.1873">asynchronous














                        parallel coordinate descent</a>. These methods
                      seem to work well in the non-sparse setting while
                      the Hogwild! method (asynchronous SGD) requires
                      sparsity to avoid collisions. This reminded me
                      that Martin Takac and I need to post our paper -
                      all results of which were ready in Summer 2012!
                      (ok, we have done some improvements by the end of
                      the year, but that's it) - an improved analysis of
                      <a
href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=4&amp;cad=rja&amp;uact=8&amp;ved=0CEIQFjAD&amp;url=http%3A%2F%2Fwww.eecs.berkeley.edu%2F%7Ebrecht%2Fpapers%2FhogwildTR.pdf&amp;ei=ILB8U4ubMobfoASumoGQAw&amp;usg=AFQjCNHqA532YKC_P0JgCL-C_Iu8d3Wcyg&amp;bvm=bv.67229260,d.cGU">Hogwild!</a>
                      on arXiv. Many people were asking about it - as <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/slides/Wright.pdf">Steve
is












                        advertising the analysis in his talks</a> -
                      apologies to all. This is a perfect example of the
                      situation when a minor polishing exercise that
                      should take a few days tops takes 2 years.
                      Sometimes, coming up with the results is easier
                      than writing the paper ;-)</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> May 17, 2014 </h3>
                    <p> <span class="important">New paper announcement:</span>
                      <a href="papers/Hydra2.pdf">Fast distributed
                        coordinate descent for minimizing non-strongly
                        convex losses.</a> Joint work with <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier














                        Fercoq</a>, <a
                        href="http://www.cmap.polytechnique.fr/%7Equ/">Zheng














                        Qu</a> and <a href="http://www.mtakac.com/CV">Martin














                        Takáč.</a></p>
                    <p> The method has the optimal O(1/k^2) rate. We
                      develop new stepsizes for distribited coordinate
                      descent; they apply to the <a
                        href="http://arxiv.org/abs/1310.2059">Hydra</a>
                      algorithm as well. We show that the partitioning
                      of the data among the nodes of the cluster has
                      negligible effect on the number of iterations of
                      the method, with the effect vanishing with
                      increasing levels of parallelization inside each
                      node.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> May 17, 2014 </h3>
                    <p> I am now off to San Diego, California, for the <a
                        href="http://www.siam.org/meetings/op14/">SIAM
                        Conference on Optimization</a>, where I am
                      co-organizing (and giving a talk in) a <a
                        href="Coordinate_Descent_Methods_SIOPT_San_Diego_2014.pdf">'mini'-symposium
on














                        coordinate descent methods</a> with <a
                        href="http://research.microsoft.com/en-us/people/lixiao/">
                        Lin Xiao (Microsoft Research)</a> and <a
                        href="http://people.math.sfu.ca/%7Ezhaosong/">Zhaosong














                        Lu (Simon Fraser).</a> People from the team
                      giving talks: <a href="http://jakubkonecny.com/">Jakub














                        Konečný</a>, <a href="http://www.mtakac.com/CV">Martin














                        Takáč</a>, <a
                        href="http://www.maths.ed.ac.uk/%7Ertappend/">Rachael














                        Tappenden</a> and <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier














                        Fercoq</a>.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> May 15, 2014 </h3>
                    <p> <a
                        href="http://www.maths.ed.ac.uk/%7Ertappend/">Rachael














                        Tappenden</a> will be leaving Edinburgh this
                      Summer as her contract will be over then. She has
                      accepted a postdoc position at Johns Hopkins
                      University starting in Fall 2014 where she will
                      join the group of <a
                        href="https://sites.google.com/site/danielprobinson/">Prof














                        Daniel Robinson.</a> No goodbyes yet as Rachael
                      will be around for couple more months. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> May 13, 2014 </h3>
                    <p> This week I was supposed to be in <a
                        href="http://www.math.hkbu.edu.hk/SIAM-IS14/index.html">Hong














                        Kong</a> (and give a talk 'minisymposium 50':
                      Parallel and Distributed Computation in Imaging)
                      but unfortunately could not go. </p>
                    <p>Today at 12:15 we have <a
                        href="http://www.cmap.polytechnique.fr/%7Equ/">Zheng














                        Qu</a> speaking in the <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All
Hands














                        Meetings on Big Data Optimization</a> (room
                      change: JCMB 4312) about a recent paper of <a
                        href="http://perso.uclouvain.be/olivier.devolder/">Devolder</a>,
                      <a
                        href="http://perso.uclouvain.be/francois.glineur/">Glineur</a>
                      and <a href="http://www.uclouvain.be/32349.html">Nesterov</a>
                      on <a
href="http://www.premolab.ru/sites/default/files/devolder_glineur_nesterov_first_order_methods_of_smooth_convex_optimization_with_inexact_oracle.pdf">First
order














                        methods with inexact oracle</a>. As usual,
                      refreshments are provided!</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> May 6, 2014 </h3>
                    <p> I have just arrived in Rabat, Morocco. Tomorrow
                      I am giving a keynote talk at the <a
                        href="http://3w.inpt.ac.ma/sita14/">9th
                        International Conference on Intelligent Systems.</a>
                      Needless the say, the weather is fantastic.
                      Correction (after having looked from the window:
                      seems it's going to rain...). </p>
                    <p><em>Update:</em> The conference was very nice; an
                      impressive university campus. I even got an
                      impromptu interview with the press just before my
                      talk. Also, I somehow managed to get a rooftop
                      view of the city from the medina. Climate in Rabat
                      seems to be similar to that in California.
                      Morocco: I shall be back some day!</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> May 5, 2014 </h3>
                    <p> Today I am giving a talk in the <a
                        href="http://www.ann.jussieu.fr/%7Eplc/spo.html">Seminaire














                        Parisien d'Optimisation</a> at the <a
                        href="http://www.ihp.fr/">Institut Henri
                        Poincare</a> in Paris.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> May 4, 2014 </h3>
                    <p>The <a href="http://arxiv.org/abs/1212.0873">parallel














                        coordinate descent method</a> developed by
                      Martin Takáč and myself has recently been used by
                      a team from <a href="http://www.hrl.com/">HRL
                        Labs</a> to <a
                        href="http://arxiv.org/abs/1404.7152">geotag 100
                        million public twitter accounts</a>. They have
                      used an Apache Spark implementation of the method
                      - the network they analyzed had 1 billion edges. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> April 30, 2014 </h3>
                    <p>Today, <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier














                        Fercoq</a> was leading the <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All












                        Hands Meeting on Big Data Optimization</a>. The
                      meeting was then followed by a very nice talk on <a
href="http://www.maths.ed.ac.uk/ERGO/abstracts/2014-04-drineas.html">Randomized
Algorithms














                        in Numerical Linear Algebra</a> by <a
                        href="http://www.cs.rpi.edu/%7Edrinep/">Petros
                        Drineas (RPI)</a>, who is visiting me and <a
                        href="http://www.iliasdiakonikolas.org/">Ilias
                        Diakonikolas</a> in Edinburgh this week.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> April 29, 2014 </h3>
                    <p> Martin Takáč's PhD thesis "Randomized Coordinate
                      Descent Methods for Big Data Optimization" is now
                      <a
href="http://mtakac.com/data/_uploaded/file/papers/2014/takac_phd_thesis_final.pdf">available














                        here.</a></p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> April 24, 2014 </h3>
                    <p> <a href="http://www.mtakac.com/CV">Martin Takáč</a>
                      has recently accepted a tenure-track Assistant
                      Professor position in the <a
                        href="http://www.lehigh.edu/ise/">Department of
                        Industrial and Systems Engineering</a> at <a
                        href="http://www4.lehigh.edu/">Lehigh University</a>.
                      The department is the home of a top research
                      center in computational optimization: <a
                        href="http://coral.ie.lehigh.edu/">COR@L</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> April 16, 2014 </h3>
                    <p> I've managed to submit a grant proposal today
                      but failed to locate a leak on the tube on the
                      front wheel of my bike. Maybe the tire was never
                      really flat in the first place. Or maybe I should
                      focus on applying for grants and leave mending
                      punctures to someone else...</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> April 15, 2014 </h3>
                    <p> <a
                        href="http://people.kyb.tuebingen.mpg.de/suvrit/">Suvrit














                        Sra</a> has included some of my results (joint
                      with <a href="http://mtakac.com/">Martin Takáč</a>,
                      based on <a href="http://arxiv.org/abs/1107.2848">this</a>
                      and <a href="http://arxiv.org/abs/1212.0873">this</a>
                      paper) on randomized coordinate descent methods in
                      <a
                        href="http://people.kyb.tuebingen.mpg.de/suvrit/teach/ee227a/lect21.pdf">this














                        lecture</a> of a <a
href="http://people.kyb.tuebingen.mpg.de/suvrit/teach/ee227a/lectures.html">Convex
Optimization














                        course</a> he taught at UC Berkeley last year. </p>
                    <p> Besides the obvious fact that this kind of stuff
                      makes the authors happy (thanks, Suvrit!), I am
                      also of the opinion that it is time to refresh
                      syllabuses of convex optimization courses with
                      some modern results, methods and theory. A lot of
                      exciting work has been done by the community in
                      the last 10 years or so and there is plenty of
                      material to choose from to build a modern course.
                      I am launching such a course (Modern optimization
                      methods for big data problems) in Spring 2015 (it
                      takes a year or more from start to finish to get a
                      new course approved and run over here...) here in
                      Edinburgh. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> April 11, 2014 </h3>
                    <p> Here is the <a
                        href="docs/Coordinate_Descent_Methods_SIOPT_San_Diego_2014.pdf">detailed
program














                      </a> of the <em>Coordinate Descent
                        "Mini"-Symposium</em> at the <a
                        href="http://www.siam.org/meetings/op14/">SIAM
                        Conference on Optimization</a> to be held in San
                      Diego in May 2014. The symposium consists of 6
                      sessions: 3 on May 20th and 3 on May 22nd.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> April 2, 2014 </h3>
                    <p> Today I am attending a <a
                        href="http://www.icms.org.uk/workshop.php?id=306">workshop</a>
                      in the honour of John Napier's discovery of the
                      logarithm. Napier was born and spent his life in
                      Edinburgh. Many of the talks were excellent. </p>
                    <p>Olivier Fercoq presented a <a
                        href="posters/Poster-APPROX">poster related to
                        the APPROX algorithm</a> (Accelerated, Parallel
                      and PROXimal coordinate descent), Rachael
                      Tappenden presented a poster on<a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/posters/Tappenden.pdf">Inexact














                        Coordinate Descent</a>, Martin Takáč had one on
                      the Hydra algorithm (Distributed Coordinate
                      Descent) and Jakub Konečný presented his work on <a
                        href="posters/Poster-S2GD.pdf">S2GD</a>
                      (semi-stochastic gradient descent).</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> April 1, 2014 </h3>
                    <p>Today, <a href="http://www.mtakac.com/CV">Martin
                        Takáč</a> is leading a discussion at the <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All












                        Hands Meeting on Big Data Optimization</a> about
                      a very recent (2 weeks old!) <a
                        href="http://arxiv.org/abs/1403.4699">paper</a>
                      by Xiao and Zhang.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> March 31, 2014 </h3>
                    <p> Our School was successful in obtaining funding
                      for <a
                        href="http://www.epsrc.ac.uk/newsevents/news/2014/Pages/newcdts.aspx">EPSRC
Centre














                        for Doctoral Training in Mathematical Analysis
                        and its Applications: "Maxwell Institute
                        Graduate School in Analysis and Applications"</a>.
                      I am one of the potential PhD supervisors.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> March 28, 2014 </h3>
                    <p> <a href="http://www.mtakac.com/CV">Martin Takáč</a>
                      is defending his PhD thesis today. Update: The
                      defense was successful; congratulations, Dr Takáč!</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> March 25, 2014 </h3>
                    <p> I just submitted an important grant proposal
                      (length = 26p; an effort comparable to writing a
                      paper...). Wish me luck! </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> March 12, 2014 </h3>
                    <p> Debadri Mukherjee and Mojmir Mutny have each
                      been awarded a Vacation Scholarship to work with
                      me this Summer on an undergraduate research
                      project. Debadri will work on "Applications of
                      Semi-Stochastic Gradient Descent" and Mojmir will
                      work on "Denoising and filtering of sparsely
                      sampled images and other possible applications of
                      gradient descent minimizing tools".</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> March 11, 2014 </h3>
                    <p> The video from my February talk in Moscow on the
                      APPROX algorithm is now on <a
                        href="https://www.youtube.com/watch?v=0sHOfqhCZw0">Youtube</a>.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> March 11, 2014 </h3>
                    <p> <a href="http://www.maths.ed.ac.uk/%7Ekfount/">Kimonas














                        Fountoulakis</a>, as an expert on second order
                      methods, lead the discussion today in the <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All












                        Hands Meeting on Big Data Optimization</a> about
                      Coordinate Descent Newton. In the preceding two
                      weeks we had <a
                        href="http://www.see.ed.ac.uk/%7Es0574225/">Mehrdad












                        Yaghoobi</a> and <a
                        href="http://www.cmapx.polytechnique.fr/%7Equ/">Zheng












                        Qu</a> speaking about projection onto the L1
                      ball in high dimensions, and on iterative methods
                      for finding stationary states of Markov chains,
                      respectively.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> February 25, 2014 </h3>
                    <p> Today I gave a talk at the <a
                        href="http://www.ipam.ucla.edu/programs/sgm2014/">Stochastic














                        Gradient Methods</a> workshop, <a
                        href="talks/Talk-IPAM-2014.ppsx">here are the
                        slides.</a> I primarily talked about the APPROX
                      algorithm (an efficient accelerated version of
                      parallel coordinate descent; joint work with <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier












                        Fercoq</a>), with a hint at the end at a version
                      using importance sampling (joint work with <a
                        href="http://www.cmap.polytechnique.fr/%7Equ/">Zheng














                        Qu</a>).</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> February 25, 2014 </h3>
                    At the <a
                      href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All
Hands


















                      Meeting on Big Data Optimization</a> today, <a
                      href="http://www.cmapx.polytechnique.fr/%7Equ/">Zheng


















                      Qu</a> will be leading a discussion about a recent
                    paper of Nesterov and Nemirovski: <a
href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCcQFjAA&amp;url=http%3A%2F%2Fwww.ecore.be%2FDPs%2Fdp_1359373295.pdf&amp;ei=yx_xUp-EM9Ou7AaA0IH4Cg&amp;usg=AFQjCNE9bLYSlYgyTDyyzQkFu2IkzqpWdg&amp;sig2=NzQRd7P_SUVyemFCilu8VQ&amp;bvm=bv.60444564,d.ZGU&amp;cad=rja">Finding
the
















                      stationary states of Markov chains by iterative
                      methods.</a> <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> February 23, 2014 </h3>
                    <p> Today I am heading off for California again,
                      this time to Los Angeles, to give a talk at a
                      workshop on <a
                        href="http://www.ipam.ucla.edu/programs/sgm2014/">Stochastic














                        Gradient Methods</a> at the <a
                        href="http://www.ipam.ucla.edu/default.aspx">Institute
for














                        Pure and Applied Mathematics (IPAM)</a>. I can
                      already sense <a
                        href="https://www.ipam.ucla.edu/schedule.aspx?pc=sgm2014">many














                        talks</a> will be very exciting.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> February 17, 2014 </h3>
                    <p> This week I am in London, attending the <a
                        href="http://www2.imperial.ac.uk/%7Ebm508/bigdata14.html">Big














                        Data: Challenges and Applications</a> workshop
                      at Imperial College. I have just listened to an
                      interesting general talk By David Hand, and the
                      announcement by Yike Guo of the new <a
href="http://www3.imperial.ac.uk/newsandeventspggrp/imperialcollege/newssummary/news_6-12-2013-10-46-20">Data
Science














                        Institute</a> at Imperial. </p>
                    <p>In the afternoon I am giving a 3 hour <a
                        href="talks/Tutorial-London-2014.ppsx">tutorial
                        on Big Data Convex Optimization</a> (36MB file,
                      sorry!). In the tutorial I describe 8 first-order
                      algorithms: gradient descent, projected gradient
                      descent, proximal gradient descent, fast proximal
                      gradient descent (essentially a new version of <a
href="http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CC0QFjAA&amp;url=http%3A%2F%2Fmechroom.technion.ac.il%2F%7Ebecka%2Fpapers%2F71654.pdf&amp;ei=ggECU-3tI4aphAfsi4GoBA&amp;usg=AFQjCNGuAEhwN1B11vVKlUU7SyQZIdbK8A&amp;sig2=7aFZNniH2oeJMad0Hcqi4g&amp;bvm=bv.61535280,d.ZG4">FISTA</a>),














                      <a
                        href="http://link.springer.com/article/10.1007/s10107-012-0614-z#">randomized
coordinate














                        descent</a>, <a
                        href="http://arxiv.org/abs/1212.0873">parallel
                        coordinate descent (PCDM)</a>, <a
                        href="http://arxiv.org/abs/1310.2059">distributed
coordinate














                        descent (Hydra)</a> and, finally, <a
                        href="http://arxiv.org/abs/1312.5799">fast
                        parallel coordinate descent (APPROX)</a>. </p>
                    <img src="imgs/8-in-1-chart.png"
                      longdesc="imgs/8-in-1-chart.png" width="654"
                      height="471">
                    <p> As the above chart shows, all algorithms arise
                      as special cases of the last one, which we call
                      APPROX (joint work with <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier













                        Fercoq</a>). This is the first time such a
                      synthesis is possible.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> February 11, 2014 </h3>
                    <p> I am in Moscow for the next couple days, giving
                      a <a
                        href="http://premolab.ru/content/main-premolab-seminar">talk














                        tomorrow at the Main Seminar</a> of the <a
                        href="http://premolab.ru/">Laboratory for
                        Structural Methods of Data Analysis in
                        Predictive Modelling (PreMoLab)</a> at the <a
                        href="http://phystech.edu/">Moscow Institute of
                        Physics and Technology.</a></p>
                    <p> In fact, our plane was not allowed to land, and
                      after three failed attempts he pilot decided to
                      head off to Vilnius, Lithuania. Fortunately, they
                      did not leave us there: we refueled and flew back
                      to Moscow. Happy ending. </p>
                    <p>Update (February 12): Here are the <a
                        href="talks/TALK-2014-02-Moscow.ppsx">slides</a>
                      - I talked about <a
                        href="http://arxiv.org/abs/1312.5799">Accelerated,














                        Parallel and PROXimal coordinate descent</a>
                      (joint work with <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier














                        Fercoq</a>).</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> February 4, 2014 </h3>
                    <p> <a href="http://www.mtakac.com/CV">Martin Takáč</a>
                      submitted his PhD thesis yesterday and is
                      interviewing in the US during the next couple
                      weeks. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> February 4, 2014 </h3>
                    <p> <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">All
Hands














                        Meetings on Big Data Optimization:</a> <a
                        href="http://www.maths.ed.ac.uk/%7Ertappend/">Rachael














                        Tappenden</a> is speaking today about feature
                      clustering for parallel coordinate descent.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> February 3, 2014 </h3>
                    <p> <a
                        href="http://www.damtp.cam.ac.uk/people/t.j.m.valkonen/">Tuomo
Valkonen














                        (Cambridge)</a> is visiting this week. He will
                      give a talk on Wednesday Feb 5 in our ERGO
                      seminar: Extension of the Chambolle-Pock method to
                      non-linear operators. Applications to MRI </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> January 28, 2014 </h3>
                    <p> <a href="http://jakubkonecny.com/">Jakub
                        Konečný</a> has been nominated by the University
                      of Edinburgh for the <a
href="http://research.google.com/university/relations/doctoral_fellowships_europe.html">2014
Google












                        Doctoral Fellowship. </a>Good luck! </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> February 11, 2014 </h3>
                    <p> </p>
                    <span class="rss-newpost"></span><span
                      class="rss-content">
                      <h3> January 21, 2014 </h3>
                      <p> I am launching a new seminar series
                        (co-organized with <a
                          href="http://jakubkonecny.com/">Jakub Konečný</a>):













                        <span class="important">All Hands Meetings on
                          Big Data Optimization.</span></p>
                      <p>The idea is to meet for up to an hour, eat a
                        pizza (or some other food, provided) and listen
                        to someone giving an informal (perhaps
                        blackboard) talk and leading a discussion about
                        a recent paper on the topic of big data
                        optimization. Discussions are encouraged
                        throughout - and hence it would be nice (but
                        certainly not required!) if participants could
                        have (at least a brief) look at the paper
                        beforehand. </p>
                      <p> <a
                          href="http://www.maths.ed.ac.uk/%7Eprichtar/i_seminar.html">More
info














                          here</a>.</p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> January 19, 2014 </h3>
                      <p> I came back to Edinburgh last week after
                        having spent a semester at Berkeley. </p>
                      <p> A quick `man-count': <a
                          href="http://www.cmapx.polytechnique.fr/%7Equ/">Zheng












                          Qu</a> has just started as a postdoc in the
                        group. For <a href="http://jakubkonecny.com/">Jakub












                          Konečný</a> this is the first semester in
                        Edinburgh, too (since he has spent last semester
                        at Berkeley with me). <a
                          href="http://mtakac.com/">Martin Takáč</a>
                        will soon be defending his thesis and is on the
                        job market. <a
                          href="http://www.maths.ed.ac.uk/%7Ertappend/">Rachael














                          Tappenden</a> and <a
                          href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier














                          Fercoq</a> are on the job market now as well.
                      </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> December 19, 2013 </h3>
                      <p><span class="important">New paper is out:</span>
                        <a href="http://arxiv.org/abs/1312.5799">Accelerated,














                          Parallel and Proximal Coordinate Descent</a>
                        (joint with <a
                          href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier












                          Fercoq</a>).</p>
                      <p> <strong>Abstract:</strong> We propose a new
                        stochastic coordinate descent method for
                        minimizing the sum of convex functions each of
                        which depends on a small number of coordinates
                        only. <strong>Our method (APPROX) is
                          simultaneously Accelerated, Parallel and
                          PROXimal</strong>; this is the first time such
                        a method is proposed. In the special case when
                        the number of processors is equal to the number
                        of coordinates, the method converges at the rate
                        <strong>$2\bar{\omega}\bar{L} R^2/(k+2)^2$</strong>,
                        where $k$ is the iteration counter, \bar{\omega}
                        is an <em>average</em> degree of separability
                        of the loss function, $\bar{L}$ is the <em>average</em>
                        of Lipschitz constants associated with the
                        coordinates and individual functions in the sum,
                        and $R$ is the distance of the initial point
                        from the minimizer. We show that the method can
                        be implemented <strong>without the need to
                          perform full-dimensional vector operations</strong>,
                        which is considered to be the major bottleneck
                        of accelerated coordinate descent. The fact that
                        the method depends on the average degree of
                        separability, and not on the maximum degree of
                        separability, can be attributed to the use of <strong>new
safe














                          large stepsizes</strong>, leading to improved
                        expected separable overapproximation (ESO).
                        These are of independent interest and can be
                        utilized in all existing parallel stochastic
                        coordinate descent algorithms based on the
                        concept of ESO. </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> December 18, 2013 </h3>
                      <p> I am offering a PhD project on <a
href="http://www.maths.ed.ac.uk/news/2013/phd-studentships-in-mathematics-for-the-earth-and-environment">modelling
and














                          optimization in the oil and gas</a> industry.
                        <a
                          href="http://e3partnership.wordpress.com/how-to-apply/">Application













                          deadline: January 24, 2014</a>. Feel free to
                        get in touch if interested.</p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> December 18, 2013 </h3>
                      <p> <a href="http://jakubkonecny.com/">Jakub
                          Konecny</a> has a <a
                          href="http://arxiv.org/abs/1312.4190">new
                          paper</a> out, accepted to JMLR. It's on
                        one-shot learning of gestures with Microsoft
                        Kinect sensor.</p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> December 9, 2013 </h3>
                      <p> Today I am attending and giving a talk at the
                        Optimization in Machine Learning workshop at
                        NIPS. For some reason there are two versions of
                        the schedule (<a
                          href="https://sites.google.com/site/mloptstat/opt-2013/schedule">1</a>,
                        <a
                          href="https://sites.google.com/site/mloptstat/opt-2013/schedule">2</a>).
Here














                        are my <a
                          href="talks/talk-OPT2013_workshop_at_NIPS.ppsx">slides</a>.
                      </p>
                      <p> Update (Dec 11): I am back in Berkeley. I had
                        a packed room during my talk - many more than <em>n</em>
                        people showed up... </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> December 6, 2013 </h3>
                      <p> Turns out on Monday at <a
                          href="https://nips.cc/">NIPS</a> I am giving
                        my talk at the same time when Mark Zuckerberg is
                        on a discussion panel. I am buying a beer to
                        everyone who shows up during my talk (and I am
                        confident I will be able to afford it*)</p>
                      <p>*Small (illegible) script: Should more than <em>n</em>
                        people show up for my talk, I claim the right
                        not to pay anyone. Moreover, I will only reveal
                        the value of <em>n</em> after the talk. </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> December 4, 2013 </h3>
                      <p> <span class="important">A new paper is out:</span>
                        <a href="http://arxiv.org/abs/1312.1666">Semi-Stochastic












                          Gradient Descent Methods</a>, joint with <a
                          href="http://jakubkonecny.com/">Jakub Konečný</a>.
                      </p>
                      <p> We propose S2GD: a method belonging to
                        second-generation stochastic gradient descent
                        (SGD) methods, combining the stability of
                        gradient descent and computational efficiency of
                        SGD. The method runs in several epochs, in each
                        of which a full gradient is first computed, and
                        then a random number of stochastic gradients are
                        evaluated, following a geometric law. The <a
href="http://media.nips.cc/nipsbooks/nipspapers/paper_files/nips26/238.pdf">SVRG
method














                          of Johnson and Zhang</a> arises as a special
                        case. </p>
                      <p> We also propose S2GD+, which in our
                        experiments substantially outperforms all
                        methods we tested, incuding S2GD, SGD and <a
                          href="http://arxiv.org/abs/1202.6258">SAG
                          (Stochastic Average Gradient) of Le Roux,
                          Schmidt and Bach</a>. </p>
                      <img src="papers/s2gd.png" alt="cool image"
                        width="576">
                      <p> <em>Figure:</em> Comparison of SGD, SAG, S2GD
                        and S2GD+ on a badly conditioned problem with
                        million training examples. On the x-axis: #
                        evalutaions of the <em>stochastic</em>
                        gradient. </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> December 4, 2013 </h3>
                      <p> Here is a <a href="posters/Poster-NSync.pdf">new














                          poster on the `NSync algorithm</a> to be
                        presented by Martin Takáč at <a
                          href="https://nips.cc/">NIPS</a>. I am off to
                        Lake Tahoe tomorrow. Seems like the weather
                        there is <a
                          href="https://www.google.com/search?q=weather+Lake+Tahoe">a
                          bit different</a> from what I got used to in
                        Berkeley ;-)</p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> November 26, 2013 </h3>
                      <p> A revised version of the paper <em>Parallel
                          coordinate descent methods for big data
                          optimization</em> (submitted to Mathematical
                        Programming) is now <a
                          href="http://arxiv.org/abs/1212.0873v2">available














                          here</a>. Extended contributions section, new
                        experiments with real data, you can even enjoy
                        an uber-cool table summarizing the key notation
                        (thanks to the reviewer suggesting this!) in the
                        appendix. Page count: 35 -&gt; 43. I bet you are
                        wondering about the meaning of the two dates on
                        the paper...</p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> November 22, 2013 </h3>
                      <p> University of Edinburgh received cca £5
                        million in funding from <a
href="http://www.epsrc.ac.uk/newsevents/news/2013/Pages/phdnewcentres.aspx">EPSRC</a>
                        for a <a
                          href="http://datascience.inf.ed.ac.uk/">Centre
                          for Doctoral Training in Data Science</a>. I
                        am <a
                          href="http://datascience.inf.ed.ac.uk/people/">one














                          of the involved faculty</a> who will be
                        supervising PhD students in the centre. These
                        are good times for <a
                          href="http://datascience.inf.ed.ac.uk/research/">data













                          science research</a> in Edinburgh! </p>
                      <p> We have about <a
                          href="http://datascience.inf.ed.ac.uk/apply/">10
PhD














                          positions open</a> for the brightest,
                        analytically gifted students (future stars of
                        data science!), starting in September 2014. </p>
                      <p><span class="important">For full consideration,
                          apply by January 27, 2014. </span></p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> November 18, 2013 </h3>
                      <p> I have just learned (having received a request
                        for a reference letter) that <a
                          href="http://www.mtakac.com/index.php/">Martin
                          Takáč</a> was nominated (not by me) for the <a
href="http://www.claymath.org/fas/research_fellows/">Clay Research
                          Fellowship.</a> </p>
                      <p> "Clay Research Fellows are selected for their
                        research achievements and their potential to
                        become leaders in research mathematics." </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> November 18, 2013 </h3>
                      <p> This week I am attending the Simons Institute
                        workshop on <a
                          href="http://simons.berkeley.edu/workshops/bigdata2013-3">Unifying













                          Theory and Experiment for Large-Scale Networks</a>.
                        You can watch <a
                          href="http://simons.berkeley.edu/workshops/schedule/77">live














                          video of the talks.</a></p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> November 8, 2013 </h3>
                      <p> The <a href="http://itis2013.fis.unm.si/">ITIS














                          2013</a> conference is over; I met many new
                        people (virtually everybody was new to me) and
                        had a very good time. </p>
                      <p> <a href="http://www.matjazperc.com/">Matjaž
                          Perc</a> showed us all how one can have fun
                        during one's own talk; <a
                          href="http://users.ictp.it/%7Emarsili/">Matteo
                          Marsili</a> talked about an interesting
                        connection between stochastic programming,
                        sampling, entropy and nature; <a
                          href="https://sites.google.com/site/santofortunato/">Santo














                          Fortunato</a> gave a very accessible and
                        enjoyable talk about community detection in
                        social networks and <a
                          href="http://www.cse.nd.edu/%7Etmilenko/">Tijana












                          Milenkovic</a> gave an exciting talk on the
                        applications of the network alignment problem.
                        Many of the local talks were interesting.</p>
                      <p>The fact that the hotel location was a spa did
                        not hurt either. </p>
                      <br>
                      <br>
                      <img alt="" src="imgs/fancy-line.png" width="196"
                        height="20"> <br>
                      <br>
                      <h3> November 7, 2013 </h3>
                    </span>
                    <p> <span class="important">New paper announcement:</span><span
                        class="rss-content"> TOP-SPIN: <a
                          href="http://arxiv.org/abs/1311.1406">TOPic
                          discovery via Sparse Principal component
                          INterference.</a> This is Joint work with <a
                          href="http://mtakac.com/">Martin Takáč</a>, <a
                          href="http://www.sutd.edu.sg/ahipasaoglu.aspx">Selin














                          D. Ahipasaoglu</a> and <a
                          href="http://classx.sutd.edu.sg/%7Engaiman_cheung/">Ngai-Man














                          Cheung</a> (this paper alrwady was announced
                        in April, but was not posted onto arXiv until
                        now...)</span></p>
                    <table width="450" border="0">
                      <tbody>
                        <tr>
                          <td><img src="imgs/bmw_0_1.jpg" width="200"></td>
                          <td><img src="imgs/bmw_0_1_subset.jpg"
                              width="200"></td>
                        </tr>
                      </tbody>
                    </table>
                    <p> <em>Abstract:</em> We propose a novel topic
                      discovery algorithm for unlabeled images based on
                      the bag-of-words (BoW) framework. We first extract
                      a dictionary of visual words and subsequently for
                      each image compute a visual word occurrence
                      histogram. We view these histograms as rows of a
                      large matrix from which we extract sparse
                      principal components (PCs). Each PC identifies a
                      sparse combination of visual words which co-occur
                      frequently in some images but seldom appear in
                      others. Each sparse PC corresponds to a topic, and
                      images whose interference with the PC is high
                      belong to that topic, revealing the common parts
                      possessed by the images. We propose to solve the
                      associated sparse PCA problems using an
                      Alternating Maximization (AM) method, which we
                      modify for purpose of efficiently extracting
                      multiple PCs in a deflation scheme. Our approach
                      attacks the maximization problem in sparse PCA
                      directly and is scalable to high-dimensional data.
                      Experiments on automatic topic discovery and
                      category prediction demonstrate encouraging
                      performance of our approach. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> November 6, 2013 </h3>
                    <p> I am now in Paris, on my way to Zagreb and from
                      there to Dolenjske Toplice, Slovenia, to give a
                      plenary talk at the <a
                        href="http://itis2013.fis.unm.si/">ITIS
                        conference</a>. My talk is tomorrow: I'll be
                      talking about why parallelizing like crazy and
                      being lazy can be good.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> October 31, 2013 </h3>
                    <p> <a href="http://mtakac.com/">Martin Takáč </a>
                      lead a 1hr long technical discussion at <a
                        href="https://amplab.cs.berkeley.edu/">AmpLab</a>
                      on various issues related to parallelizing
                      coordinate descent (on multicore machines, GPUs
                      and supercomputers). </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> October 28, 2013 </h3>
                    <p> Tomorrow at 11:30am (actually, after everyone,
                      including me, is finished with the provided lunch
                      - kudos to the organizers!) I am giving a talk at
                      the <a href="https://amplab.cs.berkeley.edu">AmpLab</a>
                      All Hands meeting, Berkeley (Wozniak Lounge, SODA
                      Hall). I'll be speaking about <a
                        href="http://arxiv.org/abs/1310.2059">Hydra:
                        scaling coordinate descent to a cluster
                        environment.</a> Here are the slides.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> October 23, 2013 </h3>
                    <p> The slides from my today's talk at the workshop
                      <a
                        href="http://simons.berkeley.edu/workshops/schedule/76">Parallel














                        and Distributed Algorithms for Inference and
                        Optimization</a>, are <a
                        href="talks/Talk-BigData-Simons.ppsx">here</a>.
                      You can <a
                        href="http://www.youtube.com/watch?v=IQgnstB0n2E#t=538">watch














                        the talk on Youtube.</a></p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> October 21, 2013 </h3>
                    <p> This week I am attending the Simons workshop <a
href="http://simons.berkeley.edu/workshops/schedule/76">Parallel and
                        Distributed Algorithms for Inference and
                        Optimization</a>, my talk is on Wednesday. Today
                      I particularly enjoyed the talks by <a
                        href="http://simons.berkeley.edu/talks/sergei-vassilvitskii-2013-10-21">Sergei
Vassilvitskii














                        (Google)</a>, <a
                        href="http://simons.berkeley.edu/talks/sergei-vassilvitskii-2013-10-21">Joseph
Gonzalez














                        (Berkeley)</a>, <a
                        href="http://simons.berkeley.edu/talks/alekh-agarwal-2013-10-21">Alekh
Agarwal














                        (Microsoft Research)</a> and <a
                        href="http://simons.berkeley.edu/talks/tim-kraska-2013-10-21">Tim
Kraska














                        (Brown)</a>.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> October 16, 2013 </h3>
                    <p> This website got a facelift; the main change is
                      the addition of a menu leading to dedicated pages.
                      The old site with everything on a single page
                      started to look like it could one day seriously
                      rival <a
                        href="http://worlds-highest-website.com/">this
                        beauty.</a> Should you find any broken link and
                      feel like letting me know, please do. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>October 11, 2013</h3>
                    <p><span class="important">New short paper is out:</span>
                      <a href="http://arxiv.org/abs/1310.3438">On
                        optimal probabilities in stochastic coordinate
                        descent methods.</a> Joint work with <a
                        href="http://mtakac.com/index.php/">Martin Takáč</a>.
                    </p>
                    <p>We propose and analyze a new parallel coordinate
                      descent method---<span class="important">`NSync</span>---in












                      which at each iteration a random subset of
                      coordinates is updated, in parallel, allowing for
                      the subsets to be chosen <em>non-uniformly</em>.
                      Surprisingly, the strategy of updating a single
                      randomly selected coordinate per iteration---with
                      <em>optimal probabilities</em>---may require less
                      iterations, both in theory and practice, than the
                      strategy of updating all coordinates at every
                      iteration. </p>
                    <p> We believe this text is ideal as a quick point
                      of entry to the subject of parallel coordinate
                      descent. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>October 8, 2013</h3>
                    <p> <a href="http://www.ph.ed.ac.uk/higgs/">Peter
                        Higgs</a> won the Nobel Prize in Physics. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>October 8, 2013</h3>
                    <p><span class="important">New paper announcement:</span>
                      <a href="http://arxiv.org/abs/1310.2059">Distributed














                        coordinate descent method for learning with big
                        data</a>. Joint work with <a
                        href="http://mtakac.com/index.php/">Martin Takáč</a>.
                    </p>
                    <img src="img/hydra.png" height="100">
                    <p> We propose and analyze <span class="important">Hydra</span>:
                      the first distributed-memory coordinate descent
                      method. This extends methods such as PCDM, Shotgun
                      and mini-batch SDCA to big data computing. It is
                      capable of solving terabyte optimization/learning
                      problems on a cluster in minutes. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>October 7, 2013</h3>
                    <p>My university nominated me for the <a
href="http://research.microsoft.com/en-us/collaboration/awards/msrff.aspx">2014
Microsoft














                        Research Faculty Fellowship</a>. Each university
                      is only allowed to nominate a single person, and
                      every year about 7 awards are made, worldwide.
                      Wish me luck... </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>September 22, 2013</h3>
                    <p><span class="important">New paper announcement:</span>
                      <a href="http://arxiv.org/abs/1309.5885">Smooth
                        minimization of nonsmooth functions with
                        parallel coordinate descent methods</a>. This is
                      joint work with <a
                        href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier












                        Fercoq</a>. </p>
                    <p> In this paper we show that parallel coordinate
                      descent methods can be applied to a fairly general
                      class of nonsmooth convex optimization problems
                      and prove that the number of iterations decreases
                      as more processors are used. The class of
                      functions includes, as special cases, L1
                      regularized L1 regression, L-infinity regression
                      and the "AdaBoost" problem (minimization of the
                      exponential loss). </p>
                    <p> The first 5 pages give a brief tutorial on
                      coordinate descent and on the issues related to
                      making the method parallel. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>September 16, 2013</h3>
                    <p>This week I am attending the first workshop of
                      the Big Data program at the <a
                        href="http://simons.berkeley.edu/">Simons
                        Institute</a>: <a
                        href="http://simons.berkeley.edu/workshops/schedule/75">Succinct














                        Data Representations and Applications.</a> All
                      talks are streamed online and also recorded. All
                      talks today were great; I particularly enjoyed
                      those by <a
                        href="http://simons.berkeley.edu/talks/michael-mahoney-2013-09-16">Michael














                        Mahoney</a>, <a
                        href="http://simons.berkeley.edu/talks/michael-mahoney-2013-09-16">Petros














                        Drineas</a> and <a
                        href="http://simons.berkeley.edu/talks/ronitt-rubinfeld-2013-09-16">Ronitt














                        Rubinfeld</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>September 10, 2013</h3>
                    <p>I am now at Google (Mountain View) to give a talk
                      on various flavors of parallel coordinate descent.
                      I have just met with Yoram Singer; the talk will
                      start at 1pm after lunch (in case any local
                      googlers are reading this). </p>
                    <p><em>Update:</em> My visit went well, there will
                      be follow-up visits. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>September 10, 2013</h3>
                    <p>University of Edinburgh has ranked <a
href="http://www.topuniversities.com/university-rankings/world-university-rankings/2013#sorting=rank+region=+country=+faculty=+stars=false+search=">17th
in














                        the 2013 QS World University Rankings</a>. I
                      doubt we could have ranked higher even if the sole
                      ranking factor was the number of UK-born
                      faculty...</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>September 7, 2013</h3>
                    <p>This week (Sept 3-6) I participated in the <a
                        href="http://simons.berkeley.edu/workshops/bigdata2013-boot-camp">Big
Data














                        Boot Camp</a>, the launching event of the <a
                        href="http://simons.berkeley.edu/programs/bigdata2013">Theoretical













                        Foundations of Big Data Analysis</a> program at
                      the <a href="http://simons.berkeley.edu/">Simons
                        Institute for the Theory of Computing</a>,
                      Berkeley. </p>
                    <p>Several colleagues blogged about it, including <a
href="https://blogs.princeton.edu/imabandit/2013/09/02/first-week-of-activity-at-the-simons-institute/">Sebastian














                        Bubeck</a>, <a
                        href="http://mrtz.org/blog/what-should-a-theory-of-big-data-do/">Moritz













                        Hardt</a>, <a
href="http://mysliceofpizza.blogspot.com/2013/09/simons-big-data-boot-camp.html">Muthu














                        Muthukrishnan</a> and Suresh Venkat (<a
                        href="http://geomblog.blogspot.com/2013/08/on-theory-of-big-data.html">1</a>,
                      <a
href="http://geomblog.blogspot.com/2013/09/statistics-geometry-and-computer-science.html">2</a>,
                      <a
href="http://geomblog.blogspot.com/2013/09/more-camping-with-high-dimensional-boots.html">3</a>),
so














                      I can stop here. Next week is more relaxed for the
                      big data folks (that is, time for research),
                      although the <a
                        href="http://simons.berkeley.edu/programs/realanalysis2013">Real













                        Analysis in Computer Science</a> people here at
                      Simons have their own boot camp then, with some
                      very nice program. I plan to attend some of the
                      lectures, for instance, <a
                        href="http://simons.berkeley.edu/workshops/schedule/317">Analytical
Methods














                        for Supervised Learning</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>August 30, 2013</h3>
                    <p><span class="important">New paper is out:</span>
                      <a href="http://arxiv.org/abs/1308.6774">Separable
                        approximations and decomposition methods for the
                        augmented Lagrangian</a>, coathored with Rachael
                      Tappenden and Burak Buke. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> August 20, 2013</h3>
                    <p>As of today (and until the end of the year) I am
                      on a sabbatical at UC Berkeley, affiliated with
                      the Simons Institute for the Theory of Computing
                      and participating in the <a
                        href="http://simons.berkeley.edu/programs/bigdata2013">Theoretical
Foundations














                        of Big Data Analysis</a> program. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>August 14, 2013</h3>
                    <p>On September 10 I will give a talk at <a
                        href="http://research.google.com/">Google</a> on
                      an invitation by <a
                        href="http://research.google.com/pubs/author28.html">Yoram














                        Singer</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>August 13, 2013</h3>
                    <p>I have accepted an invitation to become a member
                      of the <a
                        href="http://www.epsrc.ac.uk/funding/peerrev/college/Pages/intro.aspx">EPSRC
Peer














                        Review College</a>. While I've been reviewing
                      grant proposals for EPSRC for some time now, this
                      apparently makes me eligible to be asked to sit on
                      a prioritization panel. Earlier today I declined
                      to review a <a
href="http://www.epsrc.ac.uk/newsevents/news/2013/Pages/cdtoutlinecallresults.aspx">CDT
full














                        proposal</a> due to a conflict of interests (I
                      am involved in two bids) - perhaps EPSRC wanted to
                      test my honesty first and I passed the test... </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>August 9, 2013</h3>
                    <p>I have accepted an invitation to give a plenary
                      talk at the 6th <a
                        href="http://nips.cc/Conferences/2013/">NIPS</a>
                      workshop on <em>Optimization for Machine Learning</em>.
                      A link to the 2012 edition (which contains links
                      to previous editions) is <a
                        href="http://opt.kyb.tuebingen.mpg.de/index.html">here</a>.
                      The workshop will be held during December 9-10,
                      2013, at Lake Tahoe, Nevada. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>August 7, 2013</h3>
                    <p><a
                        href="http://www.maths.ed.ac.uk/people/show?person=317">Rachael














                        Tappenden</a> will stay in Edinburgh longer
                      after her current EPSRC funded appointment expires
                      at the end of January 2014. She will continue as a
                      member of the<a href="#a_team"> big data
                        optimization lab</a> as a postdoc, her work will
                      now be funded by <a
                        href="http://www.nais.org.uk/Index.php">NAIS</a>.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>August 6, 2013</h3>
                    <p><a
                        href="http://www.maths.ed.ac.uk/%7Ejfowkes/publications.html">Jaroslav
(Jari)














                        Fowkes</a> will be joining the <a
                        href="#a_team">big data optimization lab</a> as
                      a <a href="http://www.nais.org.uk/Index.php">NAIS</a>
                      postdoc, starting in October 2013. Jari has
                      recently worked on Global Optimization of <a
                        href="http://www.maths.ed.ac.uk/ERGO/pubs/ERGO-13-010.html">Lipschitz</a>
                      and <a
                        href="http://link.springer.com/article/10.1007%2Fs10898-012-9937-9">Hessian














                        Lipschitz</a> functions. He has obtained his PhD
                      in 2012 from Oxford, working under the supervision
                      of <a
                        href="http://www.maths.ox.ac.uk/people/profiles/nick.gould">Nick














                        Gould</a>; and is now working with <a
                        href="http://www.maths.ed.ac.uk/%7Eccartis/">Coralia














                        Cartis</a>, who will soon leave Edinburgh for
                      Oxford. [There seems to be a lot of movement
                      between Edinburgh and Oxford...] </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> August 6, 2013</h3>
                    <p>Zheng Qu will be joining the <a href="#a_team">big














                        data optimization lab</a> as a postdoc, starting
                      in January 2014 (her appointment is for 2 years,
                      funded by <a
                        href="http://www.epsrc.ac.uk/Pages/default.aspx">EPSRC</a>
                      and <a href="http://www.nais.org.uk/">NAIS</a>).
                    </p>
                    <p> Zheng is currently studying at École
                      Polytechnique, France, under the supervision of
                      Stéphane Gaubert. Zheng has written several
                      papers, including <a
                        href="http://arxiv.org/abs/1109.5241">Curse of
                        dimensionality reduction in max-plus based
                        approximation methods: theoretical estimates and
                        improved pruning algorithms</a>, <a
                        href="http://arxiv.org/pdf/1307.4649.pdf">Markov
                        operators on cones and non-commutative consensus</a>,
                      <a href="http://arxiv.org/abs/1206.0448">The
                        contraction rate in Thompson metric of
                        order-preserving flows on a cone</a> and <a
                        href="http://arxiv.org/abs/1302.5226">Dobrushin
                        ergodicity coefficient for Markov operators on
                        cones, and beyond</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>August 5, 2013</h3>
                    <p>I am told that <a
                        href="http://www.cs.wisc.edu/users/swright">Steve














                        Wright</a> has covered a few of my papers on
                      coordinate descent and stochastic gradient descent
                      in his <a
href="http://eventos.fct.unl.pt/iccopt2013/pages/sparse-optimization-and-applications-information-processing">Summer














                        Course</a> on <em>Sparse Optimization and
                        Applications to Information Processing</em>,
                      delivered at <a
                        href="http://eventos.fct.unl.pt/iccopt2013/">ICCOPT












                        2013 in Lisbon</a>. One of the papers is not
                      online yet (and has been 'on my desk' for quite
                      some while now) - it will be put online in August
                      or early September - apologies if you are looking
                      for it and can't find it! </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>August 2, 2013</h3>
                    <p>I am now back from the <a
                        href="http://eventos.fct.unl.pt/iccopt2013/">ICCOPT














                        conference</a>; some very inspiring talks and
                      some very blue skies. Nearly 500 participants, 412
                      session talks and 14 people from Edinburgh: <a
                        href="http://www.maths.ed.ac.uk/%7Eccartis/">Cartis</a>,
                      <a href="http://www.maths.ed.ac.uk/%7Eofercoq/">Fercoq</a>,
                      <a href="http://www.maths.ed.ac.uk/%7Ekfount/">Fountoulakis</a>,
                      <a
                        href="http://www.maths.ed.ac.uk/%7Ejfowkes/publications.html">Fowkes</a>,
                      <a href="http://www.maths.ed.ac.uk/%7Egondzio/">Gondzio</a>,
                      <a
                        href="http://www.maths.ed.ac.uk/people/show?person=195">Gonzalez-Brevis</a>,
                      <a
                        href="http://www.maths.ed.ac.uk/people/show?person=330">Gower</a>,
                      <a href="http://www.maths.ed.ac.uk/%7Eagr/">Grothey</a>,
                      <a href="http://www.maths.ed.ac.uk/hall/">Hall</a>,
                      <a
                        href="http://www.maths.ed.ac.uk/people/show?person=223">Qiang</a>,
                      Richtárik, <a href="http://mtakac.com/index.php/">Takáč</a>,
                      <a
                        href="http://www.maths.ed.ac.uk/people/show?person=317">Tappenden</a>,
                      <a href="http://www.maths.ed.ac.uk/%7Eyan/">Yan</a>
                      (that's 3.4%)! ICCOPT 2016 will be held in Tokyo.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>July 28, 2013</h3>
                    <p> Traveling to Caparica, Portugal, for the <a
                        href="http://eventos.fct.unl.pt/iccopt2013">ICCOPT














                        conference (July 27-August 1, 2013).</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>July 18, 2013</h3>
                    <p> <a
                        href="http://www.nap.edu/catalog.php?record_id=18374">Frontiers
in














                        Massive Data Analysis</a> : a 119p report
                      written by a National Academy of Sciences
                      committee chaired by <a
                        href="http://www.cs.berkeley.edu/%7Ejordan/">Michael












                        Jordan</a>. Mike asked me (and others attending
                      <a
                        href="http://simons.berkeley.edu/programs/bigdata2013">this</a>)
                      to distribute this document around - it is a good
                      read for a general reader - recommended. Free to
                      download! </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>July 3, 2013</h3>
                    <p> My baggage arrived &amp; my talk is tomorrow - I
                      am no longer forced to sport my new (lovely)
                      Chinggis Khaan T-shirt during the talk. We had a
                      nice conference dinner today, perhaps with one
                      (read: 5+) too many toast speeches. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>June 29, 2013</h3>
                    <p>Arrived. My baggage did not. I am told I may be
                      lucky tomorrow. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>June 28, 2013</h3>
                    <p> Off to Ulaanbaatar, Mongolia, to attend and give
                      a talk at <a
                        href="http://iom.num.edu.mn/iccso2013/index.html">this














                        conference.</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>June 26, 2013</h3>
                    <p> Here is a bit of news from 2012 relevant for
                      2013; apparently I forgot to post this here. I
                      will spend the Fall 2013 semester as a <span
                        class="important">visiting</span> Professor at <span
                        class="important">Berkeley</span>, participating
                      in the <a
                        href="http://simons.berkeley.edu/program_bigdata2013.html">Theoretical
Foundations














                        of Big Data Analysis</a> program run at the
                      newly established <a
                        href="http://simons.berkeley.edu/index.html">Simons












                        Institute for the Theory of Computing.</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>June 25, 2013</h3>
                    <p> Giving a talk at the <a
                        href="http://numericalanalysisconference.org.uk/">25th












                        Biennial Numerical Analysis Conference</a> in
                      Strathclyde, Glasgow. Our group has organized a
                      minisymposium on <em>Recent Advanced in Big Data
                        Problems</em>; it will be held on the first day
                      of the conference, June 25th. </p>
                    <p> <em>Speakers:</em> Rachael Tappenden
                      (Edinburgh), myself (Edinburgh), Olivier Fercoq
                      (Edinburgh), James Turner (Birmingham), Ke Wei
                      (Oxford), Martin Takáč (Edinburgh). </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>June 24, 2013</h3>
                    <p> My PhD student <span class="important"><a
                          href="http://www.mtakac.com/CV">Martin Takáč</a></span>
                      was honoured with a <span class="important">Second












                        Prize in the 16th Leslie Fox Prize Competition
                        in Numerical Analysis</span>. Here is his <a
                        href="http://arxiv.org/abs/1212.0873">winning
                        paper</a> and his <a
                        href="http://prezi.com/lnpd5w-mdj43/big-data-optimization/">talk</a>
                      (as one can expect, the slides make exponentially
                      more sense with Martin's voice-over!). </p>
                    <p> <em>The Leslie Fox Prize for Numerical Analysis
                        of the Institute of Mathematics and its
                        Applications (IMA) is a biennial prize
                        established in 1985 by the IMA in honour of
                        mathematician Leslie Fox (1918-1992). The prize
                        honours "young numerical analysts worldwide"
                        (any person who is less than 31 years old), and
                        applicants submit papers for review. A committee
                        reviews the papers, invites shortlisted
                        candidates to give lectures at the Leslie Fox
                        Prize meeting, and then awards First Prize and
                        Second Prizes based on "mathematical and
                        algorithmic brilliance in tandem with
                        presentational skills".</em> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>June 24, 2013</h3>
                    <p> Attending the <a
                        href="http://icms.org.uk/workshops/fox2013#programme">Fox














                        Prize meeting</a>.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>June 20, 2013</h3>
                    <p> Interviewing postdoc candidates. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>June 14, 2013</h3>
                    <p> A <a
                        href="posters/Poster-Minibatch-ICML2013.pdf">new
                        poster</a> to go with the <a
                        href="http://jmlr.org/proceedings/papers/v28/takac13-supp.pdf">Mini-batch
primal














                        and dual methods for SVMs</a> (ICML 2013) paper.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>June 6, 2013</h3>
                    <p>I am visiting (on invitation) the <a
                        href="https://www.dstl.gov.uk/">Defence Science
                        and Technology Lab</a> of the Ministry of
                      Defence of the United Kingdom. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>June 4, 2013</h3>
                    <p> <a href="http://www.maths.ed.ac.uk/%7Eofercoq/">Olivier














                        Fercoq</a> won the<span class="important"> Best
                        PhD Thesis Prize </span>[<a
                        href="http://www.fondation-hadamard.fr/fr/prixthese">1</a>,
                      <a
href="http://www.fondation-hadamard.fr/pgmo/students/prixthese/reglement">2</a>]
                      awarded by the <a
                        href="http://www.fondation-hadamard.fr/en/PGMO">Gaspard
Monge














                        Program for Optimization and Operations Research</a><a
                        href="http://www.fondation-hadamard.fr/fr">,</a>
                      sponsored by <a
                        href="http://www.roadef.org/content/index.htm">ROADEF</a>
                      (French Operations Research Society) and <a
                        href="http://www.fondation-hadamard.fr/fr">SMAI</a>
                      (French Society for Industrial and Applied
                      Mathematics). </p>
                    <p> The prize recognizes two doctoral theses
                      defended in France in 2012, in mathematics or
                      computer science, with significant contributions
                      to optimization and operations research, both from
                      a theoretical and applied point of view. The Prize
                      attracts a 1,000 EUR check. </p>
                    <p> Olivier Fercoq wrote his thesis <em><a
                          href="http://pastel.archives-ouvertes.fr/pastel-00743187">Optimization













                          of Perron eigenvectors and applications: from
                          web ranking to chronotherapeutics</a></em>
                      under the supervision of Stéphane Gaubert and
                      Marianne Akian (CMAP + INRIA). </p>
                    <p> Prize citation (I do not dare to translate this
                      from French): <em>Cette thèse constitue une
                        "contribution majeure dans le domaine de
                        l'optimisation de fonctions d'utilité sur
                        l'ensemble des matrices positives" (selon l'un
                        des rapporteurs). Elle présente à la fois un
                        ensemble de résultats théoriques (propriétés,
                        analyse de complexité,...) et des applications
                        intéressantes.</em> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>June 2, 2013</h3>
                    <p> Travelling to Brussels for a 3-day visit to the
                      European Commission (June 3-5). </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>May 27, 2013</h3>
                    <p>Shortlisting of candidates for the <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=013502">2y
postdoc














                        position</a> is under way; interviews will take
                      place in the third week of June. I received more
                      than 50 applications. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>May 14, 2013</h3>
                    <p><a
href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDAQFjAA&amp;url=http%3A%2F%2Fwww.core.ucl.ac.be%2F%7Enesterov%2F&amp;ei=gY6SUfGGMMmN7Qbr7IHQDQ&amp;usg=AFQjCNHyLNCHVnC2cb8e0vXjkvfHox_g1w&amp;sig2=cwej73iuQ4pdu0dXARlaZA&amp;bvm=bv.46471029,d.ZGU">Yurii














                        Nesterov</a> (CORE, Louvain) is visiting me and
                      my <a href="#a_team">group</a> for a week.
                      Tomorrow at 3:30pm in 6206 JCMB he will deliver a
                      NAIS/ERGO talk titled <a
                        href="http://www.maths.ed.ac.uk/ERGO/abstracts/2013-05-nesterov.html">Dual
methods














                        for minimizing functions with bounded variation</a>.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>May 13, 2013</h3>
                    <p>I am in London, giving a <a
                        href="talks/Big_Data_Mining_London_2013.ppsx">talk</a>
                      at <a
href="http://www2.imperial.ac.uk/%7Egmontana/bigdatamining/schedule.html">Big
Data














                        Mining</a> (Imperial College) tomorrow. This
                      promises to be a very nice event, with a few
                      academia speakers (British Columbia, Edinburgh,
                      Bristol, Cambridge, UCL, Columbia) and plenty of
                      industry speakers (IBM, Financial Times, Barclays,
                      Bloomberg News, SAP, Cloudera, Last.fm, Johnson
                      Research Lab, QuBit and SAS). </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>May 8, 2013</h3>
                    <p>'Fresh' news from last week. Two new posters
                      (presented at the Optimization &amp; Big Data
                      workshop): <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/posters/Tappenden.pdf">Inexact
coordinate














                        descent</a> (joint with Rachael Tappenden and
                      Jacek Gondzio) + <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/posters/Takac.pdf">Distributed
coordinate














                        descent for big data optimization</a> (joint
                      with Martin Takáč and Jakub Mareček). </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>May 3, 2013</h3>
                    <p>The Best Poster Prize (<a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/">Optimization
&amp;














                        Big Data workshop</a>) goes to <a
                        href="http://tuomov.iki.fi/">Tuomo Valkonen
                        (Cambridge)</a>, for the poster <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/posters/Valkonen.pdf">Computational
problems














                        in magnetic resonance imaging</a>. Jury: Prof
                      Stephen Wright (Wisconsin-Madison) and Dr Imre
                      Pólik (SAS Institute). Steve's plenary/colloquium
                      talk was amazing. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>May 1, 2013 </h3>
                    <p>The <a
                        href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/">Optimization
&amp;














                        Big Data</a> workshop has started! Today there
                      were 3 talks about coordinate descent methods, a
                      conditional gradient talk, an industry talk, an
                      optimization in statistics talk and a mirror
                      descent talk. I gave a talk today, too. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>April 25, 2013</h3>
                    <p>Dr Michael Grant, the co-creator of the CVX
                      matlab package for Disciplined Convex Programming,
                      has accepted my invitation to give a talk about
                      CVX and the fun behind it. He will speak on Monday
                      April 29 at 4:45pm in 6206 JCMB: <em><a
                          href="http://www.maths.ed.ac.uk/ERGO/abstracts/2013-04-grant.html">Disciplined
convex














                          programming and CVX (and thoughts on
                          academically valuable software)</a></em>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>April 22, 2013</h3>
                    <p>Our School (The School of Mathematics) has today
                      opened the Michael and Lily Atiyah Portrait
                      Gallery (3rd floor of the James Clerk Maxwell
                      Building). Here is a <a
                        href="http://www.maths.ed.ac.uk/%7Eaar/atiyahpg.pdf">pdf












                        file</a> with the portraits and some very
                      interesting comments! </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> April 19, 2013</h3>
                    <p>Fresh from the bakery, <a
                        href="http://arxiv.org/abs/1304.5530">Inexact
                        coordinate descent: complexity and
                        preconditioning</a> is a new paper, coauthored
                      with Jacek Gondzio and Rachael Tappenden. </p>
                    <p> <em>Brief blurb:</em> We prove complexity
                      bounds for a randomized block coordinate descet
                      method in which the proximal step defining an
                      iteration is performed inexactly. This is often
                      useful in the case when blocks contain more than a
                      single variable - we illustrate this on the
                      example of minimizing a quadratic function with
                      explicit block structure. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>April 19, 2013</h3>
                    <p>I am attending the <a
                        href="http://bigdata.holyrood.com/">Big Data in
                        the Public Sector</a> workshop held at Dynamic
                      Earth, Edinburgh. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> April 16, 2013</h3>
                    <p>The paper <a
                        href="http://arxiv.org/abs/1303.2314">Mini-batch
                        primal and dual methods for SVMs</a> was
                      accepted to the Proceedings of the <a
                        href="http://icml.cc/2013/">30th International
                        Conference on Machine Learning (ICML 2013)</a>.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> April 14, 2013</h3>
                    <p>New paper out: <a href="topspin2013.pdf">TOP-SPIN:














                        TOPic discovery via Sparse Principal component
                        INterference</a>, coauthored by Martin Takáč,
                      Selin Damla Ahipasaoglu and Ngai-Man Cheung. </p>
                    <p> <em>Blurb:</em> We propose an unsupervised
                      computer vision method, based on sparse PCA, for
                      discovering topics in a database of images. Our
                      approach is scalable and three or more orders of
                      magnitude faster than a competing method for
                      object recognition. It gives nearly 90% prediction
                      accuracy on a benchmark Berkeley image database. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> April 5, 2013</h3>
                    <p>I've accepted an offer to become a Field Chief
                      Editor of a new <em>open-access</em> journal: <a
href="http://www.iapress.org/index.php/soic/index">Statistics,
                        Optimization and Information Computing</a>. The
                      journal aims to publish interdisciplinary work at
                      the interface of statistics, optimization and
                      information sciences and will appear in four
                      issues annualy. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>April 3, 2013</h3>
                    <p> Several <span class="important">Chancellor's
                        Fellowships (5-year tenure-track positions)</span>
                      are available in the School of Mathematics. <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.display_form">Application</a>
                      deadline: April 18th, 2013. </p>
                    <p> <em>We welcome candidates in any area of
                        Operational Research but in particular those
                        specializing for example in nonlinear
                        programming, mixed integer programming,
                        stochastic optimization and candidates
                        interested in applying optimization to modelling
                        and solving real-life problems. </em> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>March 20, 2013</h3>
                    <p> During March 19-21 I am in Paris, giving <a
                        href="TALK-2013-03-20-Paris.pdf">a talk</a>
                      today at <a
href="http://www.ihes.fr/jsp/site/Portal.jsp?document_id=3270&amp;portlet_id=14">Fête
Parisienne














                        in Computation, Inference and Optimization</a>.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> March 18, 2013</h3>
                    <p>My EPSRC "First Grant" proposal <em>Accelerated
                        Coordinate Descent Methods for Big Data Problems</em>
                      was approved. I will be advertising a 2 year
                      postdoc position soon (most probably starting
                      sometime between June 1st 2013 and September 1st
                      2013). </p>
                    <p> It is likely the postdoc will be able to spend a
                      few weeks at UC Berkeley in the period
                      September-December 2013, participating in the <a
href="http://simons.berkeley.edu/program_bigdata2013.html">Foundations
                        of Big Data Analysis</a> programme at the <a
                        href="http://simons.berkeley.edu/">Simons
                        Institute for the Theory of Computing.</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> March 18, 2013</h3>
                    <p> <a href="http://jakubkonecny.com/">Jakub
                        Konečný</a> has been awarded the <a
href="http://www.ed.ac.uk/schools-departments/student-funding/postgraduate/uk-eu/university-scholarships/development">Principal's
Career














                        Development Scholarship</a> and will be joining
                      the group as a PhD student starting in August
                      2013. </p>
                    <p> He will spend his first semester at University
                      of California Berkeley as a visiting student
                      affiliated with the newly established <a
                        href="http://simons.berkeley.edu/">Simons
                        Institute for the Theory of Computing</a> and
                      will participate in the <a
                        href="http://simons.berkeley.edu/program_bigdata2013.html">Theoretical
Foundations














                        of Big Data Analysis</a> programme. </p>
                    <p> <em>Short bio:</em> Jakub studied mathematics
                      at Comenius University, Slovakia. In the past he
                      represented his country in the International
                      Mathematical Olympiad. Most recently, together
                      with another student teammate, Jakub won 2nd place
                      at the <a
                        href="http://gesture.chalearn.org/dissemination/icpr2012">ChaLearn













                        Gesture Challenge Competition</a> - an
                      international contest in designing a one-shot
                      video gesture recognition system. Here is a brief
                      <a
href="http://techcrunch.com/2011/12/07/chalearn-challenges-you-to-teach-a-kinect-instant-gesture-recognition/">news














                        story</a>. Jakub was invited to present the
                      results at the <a href="http://www.icpr2012.org/">21st














                        International Conference on Pattern Recognition</a>
                      in Tsukuba, Japan, and was invited to submit a
                      paper describing the system to a special issue of
                      the Journal of Machine Learning Research. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> March 14, 2013</h3>
                    <p>Poster announcement: <a
href="http://on-demand.gputechconf.com/gtc/2013/poster/pdf/P0183_ChristosDelivorias.pdf">GPU
acceleration














                        of financial models</a>. <a
                        href="http://www.gputechconf.com/page/home.html">GPU












                        Technology Conference</a>, San Jose, California.
                      Joint with Christos Delivorias, Erick Vynckier and
                      Martin Takáč. </p>
                    <p> Based on 2012 MSc thesis <a
href="http://www.hpcfinance.eu/sites/www.hpcfinance.eu/files/Christos_Delivorias_0.pdf">Case
studies














                        in acceleration of Heston's stochastic
                        volatility financial engineering model: GPU,
                        cloud and FPGA implementations</a> of Christos
                      Delivorias at the School of Mathematics,
                      University of Edinburgh. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> March 12, 2013</h3>
                    <p>New paper announcement: <a
                        href="http://arxiv.org/abs/1303.2314">Mini-batch
                        primal and dual methods for SVMs</a>, coauthored
                      with <span class="style1" style="width: 300px;">Martin













                        Takáč, Avleen Bijral and Nathan Srebro</span>. </p>
                    <p> <em>Brief blurb:</em> We parallelize Pegasos
                      (stochastic subgradient descent) and SDCA
                      (stochastic dual coordinate ascent) for support
                      vector machines and prove that the theoretical
                      parallelization speedup factor of both methods is
                      the same, and depends on the spectral norm of the
                      data. The SDCA approach is primal-dual in nature,
                      our guarantees are given in terms of the original
                      hinge loss formulation of SVMs.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> March 6, 2013</h3>
                    <p> Today I gave a <a
                        href="TALK-2013-02-SIAM-Conference.ppsx">talk</a>
                      at the Annual meeting of the <a
                        href="http://www.edsiamchapter.co.uk/esscc2013_program">Edinburgh














                        SIAM Student Chapter</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> March 5, 2013</h3>
                    <p>Martin Takáč has become a finalist in the <a
                        href="http://www.numerical.rl.ac.uk/fox/">16th
                        IMA Leslie Fox Prize competition</a>. </p>
                    <p> <em>The Leslie Fox Prize for Numerical Analysis
                        of the Institute of Mathematics and its
                        Applications (IMA) is a biennial prize
                        established in 1985 by the IMA in honour of
                        mathematician Leslie Fox (1918-1992). The prize
                        honours "young numerical analysts worldwide"
                        (any person who is less than 31 years old), and
                        applicants submit papers for review. A committee
                        reviews the papers, invites shortlisted
                        candidates to give lectures at the Leslie Fox
                        Prize meeting, and then awards First Prize and
                        Second Prizes based on "mathematical and
                        algorithmic brilliance in tandem with
                        presentational skills".</em> </p>
                    <p> The prize meeting will be held on June 24, 2013
                      at <a href="http://www.icms.org.uk/">ICMS</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> March 5, 2013</h3>
                    <p>I am attending <a
                        href="http://icms.org.uk/workshops/ergoenergy">Optimization












                        in Energy Day</a>, International Centre for
                      Mathematical Sciences, Edinburgh. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> February 26, 2013</h3>
                    <p> Today I am attending (and giving a <a
                        href="TALK-2013-02-Big-Data-and-Social-Media.ppsx">talk</a>
                      at) <a
                        href="http://www.icms.org.uk/workshops/bigdata#programme">Big














                        Data and Social Media</a>, a workshop organized
                      by Des Higham at Strathclyde university. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>February 21, 2013</h3>
                    <p>I am giving a "Research Recap" <a
                        href="ILW.ppsx">talk</a> during the Innovative
                      Learning Week at the University of Edinburgh.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> February 4-6, 2013</h3>
                    <p>Visiting <a href="http://www.uclouvain.be/">Université












                        Catholique de Louvain</a>, Belgium, and giving a
                      talk at the <a
                        href="http://www.uclouvain.be/en-44416.html">CORE














                        mathematical programming seminar</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> January 30, 2013</h3>
                    <p>Giving a <a href="TALK-2013-01-ERGO.pdf">talk</a>
                      at the <a
                        href="http://www.maths.ed.ac.uk/ERGO/abstracts/2013-01-richtarik.html">ERGO
research














                        seminar.</a> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> January 6-11, 2013</h3>
                    <p>I am speaking at <a
                        href="http://lear.inrialpes.fr/workshop/osl2013/index.html">Optimization













                        and Statistical Learning</a>; a workshop in Les
                      Houches, France, on the slopes of Mont Blanc. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> December 17, 2012</h3>
                    <p> <span class="important">New paper is out:</span>
                      <a href="24AM.pdf">Alternating maximization:
                        unifying framework for 8 sparse PCA formulations
                        and efficient parallel codes</a>, joint work
                      with Martin Takáč and Selin Damla Ahipasaoglu. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>December 16, 2012</h3>
                    <p> I am organizing <span class="important"><a
                          href="http://www.maths.ed.ac.uk/%7Eprichtar/Optimization_and_Big_Data/">Optimization
and














                          Big Data</a></span> (workshop, trek and
                      colloquium; <a
href="http://www.maths.ed.ac.uk/%7Eprichtar/Advances_in_Large_Scale_Optimization/index.html">a
                        sequel to this 2012 event</a>). </p>
                    <p> This event will be held in Edinburgh during May
                      1-3, 2013. Headline speaker: Steve Wright
                      (Wisconsin-Madison). More info and website later.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>December 11, 2012</h3>
                    <p> I am giving a talk at the <a
                        href="http://www.strath.ac.uk/mathstat/seminars/">Numerical














                        Analysis seminar</a>, University of Strathclyde.
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> December 10, 2012</h3>
                    <p><span class="important">New paper is out:</span>
                      <a href="http://arxiv.org/abs/1212.2617">Optimal
                        diagnostic tests for sporadic Creutzfeldt-Jakob
                        disease based on SVM classification of RT-QuIC
                        data</a>, joint work with William Hulme, Lynne
                      McGuire and Alison Green. In brief, we come up
                      with optimal tests for detecting the sporadic
                      Creutzfeldt-Jakob disease. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> December 7, 2012</h3>
                    <p> <a
href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=007223">Five
3-year














                        Whittaker Postdoctoral Fellowships in the School
                        of Mathematics.</a> If you are an exceptional
                      candidate and are interested in working with me,
                      send me an email. </p>
                    <p> Closing date for applications: January 22, 2013.
                      Starting date: no later than Sept 1, 2013. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> December 4, 2012</h3>
                    <p> Our group has an opening: <a
                        href="http://www.maths.ed.ac.uk/news/2012/lectureship-id-007224">Visiting
Assistant














                        Professor position (=2.5 year Lectureship).</a>
                      Closing date of applications: January 22, 2013. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>November 23, 2012</h3>
                    <p> <span class="important">New paper is out:</span>
                      <a href="pcdm.pdf">Parallel coordinate descent
                        methods for big data optimization</a>, joint
                      work with Martin Takáč. </p>
                    <p> <em> Brief info: </em> We propose and analyze
                      a rich family of randomized parallel block
                      coordinate descent methods and show that
                      parallelization leads to acceleration on partially
                      separable problems, which naturally occur in many
                      big data application. We give simple expressions
                      for the speedup factors. We have tested one of our
                      methods on a huge-scale LASSO instance with 1
                      billion variables; while a serial coordinate
                      descent method needs 41 hours to converge, when 24
                      processors are used, the parallel method needs
                      just 2 hours. </p>
                    <p> Download the code <a
                        href="http://code.google.com/p/ac-dc/downloads/list">here.</a>
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> November 15-December 23, 2012</h3>
                    <p> Martin Takáč is on a research visit to <a
                        href="http://www.sutd.edu.sg/">SUTD (Singapore
                        University of Technology and Design)</a>. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> October 26, 2012</h3>
                    <p> I am giving a short talk, representing the
                      School of Mathematics, at a miniworkshop organized
                      around the visit of <a
                        href="http://research.microsoft.com/en-us/people/semmott/">Stephen














                        Emmott</a> (Head of Computational Science @
                      Microsoft Research) to Edinburgh. The slides do
                      not make much sense without the voice-over, but <a
                        href="Extreme_Mountain_Climbing.pps">here they
                        are</a> anyway. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> October 21-November 11, 2012</h3>
                    <p> Martin Takáč is on a research visit to TTI
                      Chicago. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> October 11-17, 2012</h3>
                    <p> I am at the <a
                        href="http://meetings2.informs.org/phoenix2012/">INFORMS
Annual














                        Meeting</a> in Phoenix, Arizona. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> October 3, 2012</h3>
                    <p> <a href="http://www.mtakac.com/CV">Martin Takáč</a>
                      was successful in the <a
                        href="http://www.informs.org/Community/ICS/Prizes/Student-Paper-Award">INFORMS
Computing














                        Society Student Paper Award Competition</a>. As
                      the sole runner-up, he won the 2nd prize with the
                      paper <a
                        href="http://www.optimization-online.org/DB_HTML/2011/07/3089.html"><em>Iteration
Complexity&nbsp;of














                          Randomized Block-Coordinate Descent Methods
                          for Minimizing a Composite Function</em></a>,
                      coauthored with myself. </p>
                    <p> <a href="http://www.informs.org/About-INFORMS">INFORMS</a>
                      (Institute for Operations Research and the
                      Management Sciences) is the largest professional
                      society in the world for professionals in the
                      field of operations research, management science,
                      and business analytics. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> October 1, 2012</h3>
                    <p> <a
                        href="http://www.cmapx.polytechnique.fr/%7Efercoq/index.en.html">Olivier














                        Fercoq</a> joined the group as a Postdoctoral
                      Researcher. He will be working on the <em>Mathematics














                        for Vast Digital Resources</em> project funded
                      by EPSRC. </p>
                    <p> Dr Fercoq obtained his PhD in September 2012
                      from CMAP, École Polytechnique, France, under the
                      supervision of Stéphane Gaubert. His PhD
                      dissertation: <em>Optimization of Perron
                        eigenvectors and applications: from web ranking
                        to chronotherapeutics.</em> </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3> September 2012</h3>
                    <p> I am now a <a
                        href="http://www.nais.org.uk/People-Lecturers.php">NAIS





                        Lecturer</a>; i.e., I am more closely affiliated
                      with the Centre for Numerical Algorithms and
                      Intelligent Software (I was a NAIS member before).
                    </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>September 2012</h3>
                    <p> Minnan Luo (Tsinghua University) joined the
                      group as a visiting PhD student - she will stay
                      for 6 months. Minnan is the recipient of the 2012
                      Google China Anita Borg Scholarship. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>September 9-12, 2012</h3>
                    <p> I am in Birmingham at the <a
href="http://www.ima.org.uk/conferences/conferences_calendar/numerical_linear_algebra_and_optimisation.cfm">3rd
IMA





                        Conference on Numerical Linear Algebra and
                        Optimization</a>. Edinburgh has 10 people in
                      attendance + a few alumni.</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>August 2012</h3>
                    <p> 16 members of <a
                        href="http://www.maths.ed.ac.uk/ERGO/">ERGO</a>
                      are attending <a href="http://ismp2012.org/">ISMP
                        Berlin</a>!</p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>
                    <h3>July 2012</h3>
                    <p>I am organizing (with F. Glineur) the <a
                        href="http://eventos.fct.unl.pt/iccopt2013">ICCOPT





                        (July 29 - Aug 1, 2013, Lisbon, Portugal)</a>
                      cluster <em>"Convex and Nonsmooth Optimization"</em>.
                      If you want to give a talk in a session in the
                      cluster and/or organize a session yourself, please
                      let me know. </p>
                    <br>
                    <br>
                    <img alt="" src="imgs/fancy-line.png" width="196"
                      height="20"> <br>
                    <br>


<h3>July 2012</h3>

<p>Some of my work was covered by Steve Wright in a <a href="http://www.ipam.ucla.edu/programs/gss2012/">Graduate Summer School on 'Deep Learning' at IPAM/UCLA</a>;
see <a href="https://www.ipam.ucla.edu/publications/gss2012/gss2012_10763.pdf">slides 65-67 here (analysis of Hogwild!)</a> and <a
href="https://www.ipam.ucla.edu/publications/gss2012/gss2012_10766.pdf">slides 95-102 here (coordinate descent)</a>.</p>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="20">
<br>
<br>

<h3> June 16-23, 2012</h3>
<p>I am visiting the Wisconsin Institutes for Discovery, University of Wisconsin-Madison. </p>

<br>
<br>
<img alt="" src="imgs/fancy-line.png" width="196" height="20">
<br>
<br>



<h3>May 2012</h3>
<p><a href="http://www.mtakac.com/CV">Martin Takáč</a> won the <a href="http://www.siam.org/pdf/news/1998.pdf">Best Talk Prize</a> at the <a href="http://www.maths.manchester.ac.uk/%7Esiam/snscc12">SIAM National Student Chapter conference</a> in Manchester. </p>



                  </span></span></span></span></span></div>
        <div style="clear: both;"> </div>
      </div>
      <div id="footer">
        <script src="x_footer.js"></script> </div>
    </div>
  </body>
</html>
